{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Re-program the implicit differentiation optimization to check whether the program is corrected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch\n",
    "#\n",
    "#\n",
    "# class NeuralTest(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralTest, self).__init__()\n",
    "#         self.layer = nn.Linear(3, 1)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.layer(x)\n",
    "#\n",
    "#\n",
    "# x = torch.rand(3)\n",
    "# net = NeuralTest()\n",
    "# opt = torch.optim.Adam(net.parameters())\n",
    "# print(list(net.parameters()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad, Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gather_flat_grad(loss_grad):\n",
    "    #Helper function to flatten the grad\n",
    "    return torch.cat([p.view(-1) for p in loss_grad])  #g_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7ff1160efad0>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- Even I set the the weight to be very close to w, the loss still fuctuates.\n",
    "    - Thought: Maybe the update is too stochastic? Learning rate too high? or rand in x? Too small dataset?, no bias term?\n",
    "    - Answer: rand in x, Fix by randn in x, increases number of epochs\n",
    "- Predicted loss < True loss means do we find a better line than the one we are generated?\n",
    "    - Happens when x is generated with rand (uniform generation) and when regularization\n",
    "\n",
    "Conclusions:\n",
    "- The SGD solution is approximately equal to the closed-form solution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat:  tensor([[3.6566],\n",
      "        [1.2004]])\n",
      "y_predicted:  tensor([[-6.4791],\n",
      "        [-4.3234],\n",
      "        [ 3.4589],\n",
      "        [-1.4549],\n",
      "        [ 3.0117],\n",
      "        [ 6.3483],\n",
      "        [-0.5179],\n",
      "        [-3.0762]])\n",
      "loss:  tensor(2.3744)\n",
      "true_loss:  tensor(2.9038)\n"
     ]
    }
   ],
   "source": [
    "# THIS BLOCK SERVES AS AN EXACT SOLUTION\n",
    "w_hat = torch.matmul(torch.matmul(torch.inverse(torch.matmul(x_train.T, x_train)), x_train.T), y_train)\n",
    "print('w_hat: ', w_hat)\n",
    "y_train_predicted = torch.matmul(x_train, w_hat)\n",
    "print('y_predicted: ', y_train_predicted)\n",
    "loss = torch.nn.functional.mse_loss(y_train_predicted, y_train)\n",
    "print('loss: ', loss)\n",
    "true_loss = torch.nn.functional.mse_loss(torch.matmul(x_train, true_w), y_train)\n",
    "print('true_loss: ', true_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 0: tensor(3.6336, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5057],\n",
      "        [1.3033]], requires_grad=True)\n",
      "Train loss at 1: tensor(3.6201, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5105],\n",
      "        [1.3053]], requires_grad=True)\n",
      "Train loss at 2: tensor(3.6084, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5153],\n",
      "        [1.3071]], requires_grad=True)\n",
      "Train loss at 3: tensor(3.5970, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5201],\n",
      "        [1.3089]], requires_grad=True)\n",
      "Train loss at 4: tensor(3.5856, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5249],\n",
      "        [1.3107]], requires_grad=True)\n",
      "Train loss at 5: tensor(3.5744, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5297],\n",
      "        [1.3124]], requires_grad=True)\n",
      "Train loss at 6: tensor(3.5632, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5344],\n",
      "        [1.3141]], requires_grad=True)\n",
      "Train loss at 7: tensor(3.5521, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5392],\n",
      "        [1.3159]], requires_grad=True)\n",
      "Train loss at 8: tensor(3.5411, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5439],\n",
      "        [1.3176]], requires_grad=True)\n",
      "Train loss at 9: tensor(3.5302, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5487],\n",
      "        [1.3192]], requires_grad=True)\n",
      "Train loss at 10: tensor(3.5194, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5534],\n",
      "        [1.3209]], requires_grad=True)\n",
      "Train loss at 11: tensor(3.5086, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5581],\n",
      "        [1.3226]], requires_grad=True)\n",
      "Train loss at 12: tensor(3.4980, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5628],\n",
      "        [1.3242]], requires_grad=True)\n",
      "Train loss at 13: tensor(3.4874, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5675],\n",
      "        [1.3258]], requires_grad=True)\n",
      "Train loss at 14: tensor(3.4770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5722],\n",
      "        [1.3274]], requires_grad=True)\n",
      "Train loss at 15: tensor(3.4666, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5768],\n",
      "        [1.3290]], requires_grad=True)\n",
      "Train loss at 16: tensor(3.4563, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5814],\n",
      "        [1.3306]], requires_grad=True)\n",
      "Train loss at 17: tensor(3.4461, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5861],\n",
      "        [1.3321]], requires_grad=True)\n",
      "Train loss at 18: tensor(3.4360, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5907],\n",
      "        [1.3337]], requires_grad=True)\n",
      "Train loss at 19: tensor(3.4261, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5953],\n",
      "        [1.3352]], requires_grad=True)\n",
      "Train loss at 20: tensor(3.4162, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5998],\n",
      "        [1.3367]], requires_grad=True)\n",
      "Train loss at 21: tensor(3.4063, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6044],\n",
      "        [1.3382]], requires_grad=True)\n",
      "Train loss at 22: tensor(3.3966, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6089],\n",
      "        [1.3397]], requires_grad=True)\n",
      "Train loss at 23: tensor(3.3870, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6134],\n",
      "        [1.3411]], requires_grad=True)\n",
      "Train loss at 24: tensor(3.3775, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6180],\n",
      "        [1.3426]], requires_grad=True)\n",
      "Train loss at 25: tensor(3.3680, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6224],\n",
      "        [1.3440]], requires_grad=True)\n",
      "Train loss at 26: tensor(3.3587, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6269],\n",
      "        [1.3454]], requires_grad=True)\n",
      "Train loss at 27: tensor(3.3494, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6314],\n",
      "        [1.3468]], requires_grad=True)\n",
      "Train loss at 28: tensor(3.3402, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6358],\n",
      "        [1.3482]], requires_grad=True)\n",
      "Train loss at 29: tensor(3.3311, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6402],\n",
      "        [1.3496]], requires_grad=True)\n",
      "Train loss at 30: tensor(3.3221, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6446],\n",
      "        [1.3510]], requires_grad=True)\n",
      "Train loss at 31: tensor(3.3132, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6490],\n",
      "        [1.3523]], requires_grad=True)\n",
      "Train loss at 32: tensor(3.3044, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6534],\n",
      "        [1.3536]], requires_grad=True)\n",
      "Train loss at 33: tensor(3.2956, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6577],\n",
      "        [1.3549]], requires_grad=True)\n",
      "Train loss at 34: tensor(3.2870, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6621],\n",
      "        [1.3562]], requires_grad=True)\n",
      "Train loss at 35: tensor(3.2784, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6664],\n",
      "        [1.3575]], requires_grad=True)\n",
      "Train loss at 36: tensor(3.2699, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6707],\n",
      "        [1.3588]], requires_grad=True)\n",
      "Train loss at 37: tensor(3.2615, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6750],\n",
      "        [1.3601]], requires_grad=True)\n",
      "Train loss at 38: tensor(3.2532, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6792],\n",
      "        [1.3613]], requires_grad=True)\n",
      "Train loss at 39: tensor(3.2449, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6835],\n",
      "        [1.3625]], requires_grad=True)\n",
      "Train loss at 40: tensor(3.2367, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6877],\n",
      "        [1.3637]], requires_grad=True)\n",
      "Train loss at 41: tensor(3.2286, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6919],\n",
      "        [1.3649]], requires_grad=True)\n",
      "Train loss at 42: tensor(3.2206, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6961],\n",
      "        [1.3661]], requires_grad=True)\n",
      "Train loss at 43: tensor(3.2127, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7003],\n",
      "        [1.3673]], requires_grad=True)\n",
      "Train loss at 44: tensor(3.2048, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7044],\n",
      "        [1.3685]], requires_grad=True)\n",
      "Train loss at 45: tensor(3.1970, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7086],\n",
      "        [1.3696]], requires_grad=True)\n",
      "Train loss at 46: tensor(3.1893, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7127],\n",
      "        [1.3707]], requires_grad=True)\n",
      "Train loss at 47: tensor(3.1817, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7168],\n",
      "        [1.3719]], requires_grad=True)\n",
      "Train loss at 48: tensor(3.1741, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7209],\n",
      "        [1.3730]], requires_grad=True)\n",
      "Train loss at 49: tensor(3.1667, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7250],\n",
      "        [1.3740]], requires_grad=True)\n",
      "Train loss at 50: tensor(3.1592, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7291],\n",
      "        [1.3751]], requires_grad=True)\n",
      "Train loss at 51: tensor(3.1519, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7331],\n",
      "        [1.3762]], requires_grad=True)\n",
      "Train loss at 52: tensor(3.1446, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7371],\n",
      "        [1.3772]], requires_grad=True)\n",
      "Train loss at 53: tensor(3.1374, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7411],\n",
      "        [1.3783]], requires_grad=True)\n",
      "Train loss at 54: tensor(3.1303, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7451],\n",
      "        [1.3793]], requires_grad=True)\n",
      "Train loss at 55: tensor(3.1233, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7491],\n",
      "        [1.3803]], requires_grad=True)\n",
      "Train loss at 56: tensor(3.1163, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7531],\n",
      "        [1.3813]], requires_grad=True)\n",
      "Train loss at 57: tensor(3.1093, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7570],\n",
      "        [1.3823]], requires_grad=True)\n",
      "Train loss at 58: tensor(3.1025, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7610],\n",
      "        [1.3833]], requires_grad=True)\n",
      "Train loss at 59: tensor(3.0957, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7649],\n",
      "        [1.3842]], requires_grad=True)\n",
      "Train loss at 60: tensor(3.0890, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7688],\n",
      "        [1.3852]], requires_grad=True)\n",
      "Train loss at 61: tensor(3.0823, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7726],\n",
      "        [1.3861]], requires_grad=True)\n",
      "Train loss at 62: tensor(3.0757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7765],\n",
      "        [1.3870]], requires_grad=True)\n",
      "Train loss at 63: tensor(3.0692, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7804],\n",
      "        [1.3879]], requires_grad=True)\n",
      "Train loss at 64: tensor(3.0628, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7842],\n",
      "        [1.3888]], requires_grad=True)\n",
      "Train loss at 65: tensor(3.0564, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7880],\n",
      "        [1.3897]], requires_grad=True)\n",
      "Train loss at 66: tensor(3.0500, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7918],\n",
      "        [1.3906]], requires_grad=True)\n",
      "Train loss at 67: tensor(3.0438, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7956],\n",
      "        [1.3914]], requires_grad=True)\n",
      "Train loss at 68: tensor(3.0375, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7993],\n",
      "        [1.3923]], requires_grad=True)\n",
      "Train loss at 69: tensor(3.0314, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8031],\n",
      "        [1.3931]], requires_grad=True)\n",
      "Train loss at 70: tensor(3.0253, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8068],\n",
      "        [1.3939]], requires_grad=True)\n",
      "Train loss at 71: tensor(3.0193, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8105],\n",
      "        [1.3948]], requires_grad=True)\n",
      "Train loss at 72: tensor(3.0133, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8142],\n",
      "        [1.3956]], requires_grad=True)\n",
      "Train loss at 73: tensor(3.0074, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8179],\n",
      "        [1.3963]], requires_grad=True)\n",
      "Train loss at 74: tensor(3.0015, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8216],\n",
      "        [1.3971]], requires_grad=True)\n",
      "Train loss at 75: tensor(2.9957, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8253],\n",
      "        [1.3979]], requires_grad=True)\n",
      "Train loss at 76: tensor(2.9900, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8289],\n",
      "        [1.3986]], requires_grad=True)\n",
      "Train loss at 77: tensor(2.9843, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8325],\n",
      "        [1.3994]], requires_grad=True)\n",
      "Train loss at 78: tensor(2.9787, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8361],\n",
      "        [1.4001]], requires_grad=True)\n",
      "Train loss at 79: tensor(2.9731, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8397],\n",
      "        [1.4008]], requires_grad=True)\n",
      "Train loss at 80: tensor(2.9676, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8433],\n",
      "        [1.4016]], requires_grad=True)\n",
      "Train loss at 81: tensor(2.9621, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8469],\n",
      "        [1.4022]], requires_grad=True)\n",
      "Train loss at 82: tensor(2.9567, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8504],\n",
      "        [1.4029]], requires_grad=True)\n",
      "Train loss at 83: tensor(2.9514, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8539],\n",
      "        [1.4036]], requires_grad=True)\n",
      "Train loss at 84: tensor(2.9461, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8575],\n",
      "        [1.4043]], requires_grad=True)\n",
      "Train loss at 85: tensor(2.9408, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8610],\n",
      "        [1.4049]], requires_grad=True)\n",
      "Train loss at 86: tensor(2.9356, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8644],\n",
      "        [1.4056]], requires_grad=True)\n",
      "Train loss at 87: tensor(2.9305, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8679],\n",
      "        [1.4062]], requires_grad=True)\n",
      "Train loss at 88: tensor(2.9254, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8714],\n",
      "        [1.4068]], requires_grad=True)\n",
      "Train loss at 89: tensor(2.9203, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8748],\n",
      "        [1.4074]], requires_grad=True)\n",
      "Train loss at 90: tensor(2.9153, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8782],\n",
      "        [1.4080]], requires_grad=True)\n",
      "Train loss at 91: tensor(2.9104, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8816],\n",
      "        [1.4086]], requires_grad=True)\n",
      "Train loss at 92: tensor(2.9055, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8850],\n",
      "        [1.4092]], requires_grad=True)\n",
      "Train loss at 93: tensor(2.9006, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8884],\n",
      "        [1.4098]], requires_grad=True)\n",
      "Train loss at 94: tensor(2.8958, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8918],\n",
      "        [1.4103]], requires_grad=True)\n",
      "Train loss at 95: tensor(2.8911, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8951],\n",
      "        [1.4109]], requires_grad=True)\n",
      "Train loss at 96: tensor(2.8864, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8985],\n",
      "        [1.4114]], requires_grad=True)\n",
      "Train loss at 97: tensor(2.8817, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9018],\n",
      "        [1.4120]], requires_grad=True)\n",
      "Train loss at 98: tensor(2.8771, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9051],\n",
      "        [1.4125]], requires_grad=True)\n",
      "Train loss at 99: tensor(2.8725, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9084],\n",
      "        [1.4130]], requires_grad=True)\n",
      "Train loss at 100: tensor(2.8680, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9116],\n",
      "        [1.4135]], requires_grad=True)\n",
      "Train loss at 101: tensor(2.8635, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9149],\n",
      "        [1.4140]], requires_grad=True)\n",
      "Train loss at 102: tensor(2.8590, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9182],\n",
      "        [1.4145]], requires_grad=True)\n",
      "Train loss at 103: tensor(2.8546, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9214],\n",
      "        [1.4149]], requires_grad=True)\n",
      "Train loss at 104: tensor(2.8503, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9246],\n",
      "        [1.4154]], requires_grad=True)\n",
      "Train loss at 105: tensor(2.8460, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9278],\n",
      "        [1.4159]], requires_grad=True)\n",
      "Train loss at 106: tensor(2.8417, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9310],\n",
      "        [1.4163]], requires_grad=True)\n",
      "Train loss at 107: tensor(2.8375, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9342],\n",
      "        [1.4167]], requires_grad=True)\n",
      "Train loss at 108: tensor(2.8333, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9373],\n",
      "        [1.4172]], requires_grad=True)\n",
      "Train loss at 109: tensor(2.8292, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9405],\n",
      "        [1.4176]], requires_grad=True)\n",
      "Train loss at 110: tensor(2.8251, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9436],\n",
      "        [1.4180]], requires_grad=True)\n",
      "Train loss at 111: tensor(2.8210, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9467],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 112: tensor(2.8170, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9498],\n",
      "        [1.4188]], requires_grad=True)\n",
      "Train loss at 113: tensor(2.8130, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9529],\n",
      "        [1.4192]], requires_grad=True)\n",
      "Train loss at 114: tensor(2.8090, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9560],\n",
      "        [1.4195]], requires_grad=True)\n",
      "Train loss at 115: tensor(2.8051, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9591],\n",
      "        [1.4199]], requires_grad=True)\n",
      "Train loss at 116: tensor(2.8013, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9621],\n",
      "        [1.4203]], requires_grad=True)\n",
      "Train loss at 117: tensor(2.7974, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9652],\n",
      "        [1.4206]], requires_grad=True)\n",
      "Train loss at 118: tensor(2.7936, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9682],\n",
      "        [1.4209]], requires_grad=True)\n",
      "Train loss at 119: tensor(2.7899, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9712],\n",
      "        [1.4213]], requires_grad=True)\n",
      "Train loss at 120: tensor(2.7862, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9742],\n",
      "        [1.4216]], requires_grad=True)\n",
      "Train loss at 121: tensor(2.7825, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9772],\n",
      "        [1.4219]], requires_grad=True)\n",
      "Train loss at 122: tensor(2.7788, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9801],\n",
      "        [1.4222]], requires_grad=True)\n",
      "Train loss at 123: tensor(2.7752, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9831],\n",
      "        [1.4225]], requires_grad=True)\n",
      "Train loss at 124: tensor(2.7716, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9860],\n",
      "        [1.4228]], requires_grad=True)\n",
      "Train loss at 125: tensor(2.7681, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9890],\n",
      "        [1.4231]], requires_grad=True)\n",
      "Train loss at 126: tensor(2.7646, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9919],\n",
      "        [1.4233]], requires_grad=True)\n",
      "Train loss at 127: tensor(2.7611, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9948],\n",
      "        [1.4236]], requires_grad=True)\n",
      "Train loss at 128: tensor(2.7577, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9977],\n",
      "        [1.4239]], requires_grad=True)\n",
      "Train loss at 129: tensor(2.7543, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0005],\n",
      "        [1.4241]], requires_grad=True)\n",
      "Train loss at 130: tensor(2.7509, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0034],\n",
      "        [1.4243]], requires_grad=True)\n",
      "Train loss at 131: tensor(2.7476, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0062],\n",
      "        [1.4246]], requires_grad=True)\n",
      "Train loss at 132: tensor(2.7443, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0091],\n",
      "        [1.4248]], requires_grad=True)\n",
      "Train loss at 133: tensor(2.7410, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0119],\n",
      "        [1.4250]], requires_grad=True)\n",
      "Train loss at 134: tensor(2.7377, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0147],\n",
      "        [1.4252]], requires_grad=True)\n",
      "Train loss at 135: tensor(2.7345, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0175],\n",
      "        [1.4254]], requires_grad=True)\n",
      "Train loss at 136: tensor(2.7314, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0203],\n",
      "        [1.4256]], requires_grad=True)\n",
      "Train loss at 137: tensor(2.7282, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0231],\n",
      "        [1.4258]], requires_grad=True)\n",
      "Train loss at 138: tensor(2.7251, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0258],\n",
      "        [1.4260]], requires_grad=True)\n",
      "Train loss at 139: tensor(2.7220, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0286],\n",
      "        [1.4262]], requires_grad=True)\n",
      "Train loss at 140: tensor(2.7189, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0313],\n",
      "        [1.4263]], requires_grad=True)\n",
      "Train loss at 141: tensor(2.7159, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0340],\n",
      "        [1.4265]], requires_grad=True)\n",
      "Train loss at 142: tensor(2.7129, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0367],\n",
      "        [1.4266]], requires_grad=True)\n",
      "Train loss at 143: tensor(2.7099, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0394],\n",
      "        [1.4268]], requires_grad=True)\n",
      "Train loss at 144: tensor(2.7070, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0421],\n",
      "        [1.4269]], requires_grad=True)\n",
      "Train loss at 145: tensor(2.7041, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0448],\n",
      "        [1.4271]], requires_grad=True)\n",
      "Train loss at 146: tensor(2.7012, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0474],\n",
      "        [1.4272]], requires_grad=True)\n",
      "Train loss at 147: tensor(2.6983, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0501],\n",
      "        [1.4273]], requires_grad=True)\n",
      "Train loss at 148: tensor(2.6955, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0527],\n",
      "        [1.4274]], requires_grad=True)\n",
      "Train loss at 149: tensor(2.6927, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0553],\n",
      "        [1.4275]], requires_grad=True)\n",
      "Train loss at 150: tensor(2.6899, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0579],\n",
      "        [1.4276]], requires_grad=True)\n",
      "Train loss at 151: tensor(2.6872, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0605],\n",
      "        [1.4277]], requires_grad=True)\n",
      "Train loss at 152: tensor(2.6845, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0631],\n",
      "        [1.4278]], requires_grad=True)\n",
      "Train loss at 153: tensor(2.6818, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0657],\n",
      "        [1.4279]], requires_grad=True)\n",
      "Train loss at 154: tensor(2.6791, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0683],\n",
      "        [1.4279]], requires_grad=True)\n",
      "Train loss at 155: tensor(2.6764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0708],\n",
      "        [1.4280]], requires_grad=True)\n",
      "Train loss at 156: tensor(2.6738, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0733],\n",
      "        [1.4281]], requires_grad=True)\n",
      "Train loss at 157: tensor(2.6712, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0759],\n",
      "        [1.4281]], requires_grad=True)\n",
      "Train loss at 158: tensor(2.6687, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0784],\n",
      "        [1.4282]], requires_grad=True)\n",
      "Train loss at 159: tensor(2.6661, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0809],\n",
      "        [1.4282]], requires_grad=True)\n",
      "Train loss at 160: tensor(2.6636, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0834],\n",
      "        [1.4282]], requires_grad=True)\n",
      "Train loss at 161: tensor(2.6611, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0859],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 162: tensor(2.6586, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0883],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 163: tensor(2.6562, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0908],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 164: tensor(2.6537, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0932],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 165: tensor(2.6513, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0957],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 166: tensor(2.6490, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0981],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 167: tensor(2.6466, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1005],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 168: tensor(2.6443, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1029],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 169: tensor(2.6420, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1053],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 170: tensor(2.6397, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1077],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 171: tensor(2.6374, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1100],\n",
      "        [1.4283]], requires_grad=True)\n",
      "Train loss at 172: tensor(2.6352, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1124],\n",
      "        [1.4282]], requires_grad=True)\n",
      "Train loss at 173: tensor(2.6329, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1147],\n",
      "        [1.4282]], requires_grad=True)\n",
      "Train loss at 174: tensor(2.6307, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1171],\n",
      "        [1.4281]], requires_grad=True)\n",
      "Train loss at 175: tensor(2.6285, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1194],\n",
      "        [1.4281]], requires_grad=True)\n",
      "Train loss at 176: tensor(2.6264, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1217],\n",
      "        [1.4280]], requires_grad=True)\n",
      "Train loss at 177: tensor(2.6242, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1240],\n",
      "        [1.4280]], requires_grad=True)\n",
      "Train loss at 178: tensor(2.6221, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1263],\n",
      "        [1.4279]], requires_grad=True)\n",
      "Train loss at 179: tensor(2.6200, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1286],\n",
      "        [1.4278]], requires_grad=True)\n",
      "Train loss at 180: tensor(2.6179, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1308],\n",
      "        [1.4278]], requires_grad=True)\n",
      "Train loss at 181: tensor(2.6159, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1331],\n",
      "        [1.4277]], requires_grad=True)\n",
      "Train loss at 182: tensor(2.6138, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1353],\n",
      "        [1.4276]], requires_grad=True)\n",
      "Train loss at 183: tensor(2.6118, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1376],\n",
      "        [1.4275]], requires_grad=True)\n",
      "Train loss at 184: tensor(2.6098, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1398],\n",
      "        [1.4274]], requires_grad=True)\n",
      "Train loss at 185: tensor(2.6078, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1420],\n",
      "        [1.4273]], requires_grad=True)\n",
      "Train loss at 186: tensor(2.6058, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1442],\n",
      "        [1.4272]], requires_grad=True)\n",
      "Train loss at 187: tensor(2.6039, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1464],\n",
      "        [1.4271]], requires_grad=True)\n",
      "Train loss at 188: tensor(2.6020, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1486],\n",
      "        [1.4270]], requires_grad=True)\n",
      "Train loss at 189: tensor(2.6001, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1508],\n",
      "        [1.4269]], requires_grad=True)\n",
      "Train loss at 190: tensor(2.5982, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1529],\n",
      "        [1.4268]], requires_grad=True)\n",
      "Train loss at 191: tensor(2.5963, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1551],\n",
      "        [1.4266]], requires_grad=True)\n",
      "Train loss at 192: tensor(2.5944, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1572],\n",
      "        [1.4265]], requires_grad=True)\n",
      "Train loss at 193: tensor(2.5926, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1594],\n",
      "        [1.4264]], requires_grad=True)\n",
      "Train loss at 194: tensor(2.5908, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1615],\n",
      "        [1.4262]], requires_grad=True)\n",
      "Train loss at 195: tensor(2.5889, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1636],\n",
      "        [1.4261]], requires_grad=True)\n",
      "Train loss at 196: tensor(2.5872, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1657],\n",
      "        [1.4259]], requires_grad=True)\n",
      "Train loss at 197: tensor(2.5854, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1678],\n",
      "        [1.4258]], requires_grad=True)\n",
      "Train loss at 198: tensor(2.5836, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1699],\n",
      "        [1.4256]], requires_grad=True)\n",
      "Train loss at 199: tensor(2.5819, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1719],\n",
      "        [1.4255]], requires_grad=True)\n",
      "Train loss at 200: tensor(2.5802, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1740],\n",
      "        [1.4253]], requires_grad=True)\n",
      "Train loss at 201: tensor(2.5784, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1761],\n",
      "        [1.4251]], requires_grad=True)\n",
      "Train loss at 202: tensor(2.5768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1781],\n",
      "        [1.4250]], requires_grad=True)\n",
      "Train loss at 203: tensor(2.5751, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1801],\n",
      "        [1.4248]], requires_grad=True)\n",
      "Train loss at 204: tensor(2.5734, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1822],\n",
      "        [1.4246]], requires_grad=True)\n",
      "Train loss at 205: tensor(2.5718, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1842],\n",
      "        [1.4244]], requires_grad=True)\n",
      "Train loss at 206: tensor(2.5701, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1862],\n",
      "        [1.4242]], requires_grad=True)\n",
      "Train loss at 207: tensor(2.5685, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1882],\n",
      "        [1.4241]], requires_grad=True)\n",
      "Train loss at 208: tensor(2.5669, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1902],\n",
      "        [1.4239]], requires_grad=True)\n",
      "Train loss at 209: tensor(2.5653, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1922],\n",
      "        [1.4237]], requires_grad=True)\n",
      "Train loss at 210: tensor(2.5638, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1941],\n",
      "        [1.4235]], requires_grad=True)\n",
      "Train loss at 211: tensor(2.5622, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1961],\n",
      "        [1.4232]], requires_grad=True)\n",
      "Train loss at 212: tensor(2.5607, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1980],\n",
      "        [1.4230]], requires_grad=True)\n",
      "Train loss at 213: tensor(2.5591, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2000],\n",
      "        [1.4228]], requires_grad=True)\n",
      "Train loss at 214: tensor(2.5576, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2019],\n",
      "        [1.4226]], requires_grad=True)\n",
      "Train loss at 215: tensor(2.5561, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2038],\n",
      "        [1.4224]], requires_grad=True)\n",
      "Train loss at 216: tensor(2.5546, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2057],\n",
      "        [1.4222]], requires_grad=True)\n",
      "Train loss at 217: tensor(2.5531, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2076],\n",
      "        [1.4219]], requires_grad=True)\n",
      "Train loss at 218: tensor(2.5517, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2095],\n",
      "        [1.4217]], requires_grad=True)\n",
      "Train loss at 219: tensor(2.5502, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2114],\n",
      "        [1.4215]], requires_grad=True)\n",
      "Train loss at 220: tensor(2.5488, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2133],\n",
      "        [1.4212]], requires_grad=True)\n",
      "Train loss at 221: tensor(2.5474, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2152],\n",
      "        [1.4210]], requires_grad=True)\n",
      "Train loss at 222: tensor(2.5460, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2170],\n",
      "        [1.4208]], requires_grad=True)\n",
      "Train loss at 223: tensor(2.5446, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2189],\n",
      "        [1.4205]], requires_grad=True)\n",
      "Train loss at 224: tensor(2.5432, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2207],\n",
      "        [1.4203]], requires_grad=True)\n",
      "Train loss at 225: tensor(2.5418, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2226],\n",
      "        [1.4200]], requires_grad=True)\n",
      "Train loss at 226: tensor(2.5405, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2244],\n",
      "        [1.4198]], requires_grad=True)\n",
      "Train loss at 227: tensor(2.5391, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2262],\n",
      "        [1.4195]], requires_grad=True)\n",
      "Train loss at 228: tensor(2.5378, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2280],\n",
      "        [1.4192]], requires_grad=True)\n",
      "Train loss at 229: tensor(2.5365, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2298],\n",
      "        [1.4190]], requires_grad=True)\n",
      "Train loss at 230: tensor(2.5351, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2316],\n",
      "        [1.4187]], requires_grad=True)\n",
      "Train loss at 231: tensor(2.5338, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2334],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 232: tensor(2.5326, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2352],\n",
      "        [1.4182]], requires_grad=True)\n",
      "Train loss at 233: tensor(2.5313, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2369],\n",
      "        [1.4179]], requires_grad=True)\n",
      "Train loss at 234: tensor(2.5300, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2387],\n",
      "        [1.4176]], requires_grad=True)\n",
      "Train loss at 235: tensor(2.5288, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2404],\n",
      "        [1.4173]], requires_grad=True)\n",
      "Train loss at 236: tensor(2.5275, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2422],\n",
      "        [1.4170]], requires_grad=True)\n",
      "Train loss at 237: tensor(2.5263, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2439],\n",
      "        [1.4168]], requires_grad=True)\n",
      "Train loss at 238: tensor(2.5251, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2456],\n",
      "        [1.4165]], requires_grad=True)\n",
      "Train loss at 239: tensor(2.5239, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2473],\n",
      "        [1.4162]], requires_grad=True)\n",
      "Train loss at 240: tensor(2.5227, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2491],\n",
      "        [1.4159]], requires_grad=True)\n",
      "Train loss at 241: tensor(2.5215, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2508],\n",
      "        [1.4156]], requires_grad=True)\n",
      "Train loss at 242: tensor(2.5203, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2524],\n",
      "        [1.4153]], requires_grad=True)\n",
      "Train loss at 243: tensor(2.5191, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2541],\n",
      "        [1.4150]], requires_grad=True)\n",
      "Train loss at 244: tensor(2.5180, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2558],\n",
      "        [1.4147]], requires_grad=True)\n",
      "Train loss at 245: tensor(2.5168, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2575],\n",
      "        [1.4144]], requires_grad=True)\n",
      "Train loss at 246: tensor(2.5157, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2591],\n",
      "        [1.4141]], requires_grad=True)\n",
      "Train loss at 247: tensor(2.5146, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2608],\n",
      "        [1.4137]], requires_grad=True)\n",
      "Train loss at 248: tensor(2.5135, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2624],\n",
      "        [1.4134]], requires_grad=True)\n",
      "Train loss at 249: tensor(2.5124, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2641],\n",
      "        [1.4131]], requires_grad=True)\n",
      "Train loss at 250: tensor(2.5113, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2657],\n",
      "        [1.4128]], requires_grad=True)\n",
      "Train loss at 251: tensor(2.5102, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2673],\n",
      "        [1.4125]], requires_grad=True)\n",
      "Train loss at 252: tensor(2.5091, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2689],\n",
      "        [1.4122]], requires_grad=True)\n",
      "Train loss at 253: tensor(2.5080, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2705],\n",
      "        [1.4118]], requires_grad=True)\n",
      "Train loss at 254: tensor(2.5070, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2721],\n",
      "        [1.4115]], requires_grad=True)\n",
      "Train loss at 255: tensor(2.5059, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2737],\n",
      "        [1.4112]], requires_grad=True)\n",
      "Train loss at 256: tensor(2.5049, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2753],\n",
      "        [1.4108]], requires_grad=True)\n",
      "Train loss at 257: tensor(2.5039, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2769],\n",
      "        [1.4105]], requires_grad=True)\n",
      "Train loss at 258: tensor(2.5029, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2785],\n",
      "        [1.4102]], requires_grad=True)\n",
      "Train loss at 259: tensor(2.5018, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2800],\n",
      "        [1.4098]], requires_grad=True)\n",
      "Train loss at 260: tensor(2.5008, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2816],\n",
      "        [1.4095]], requires_grad=True)\n",
      "Train loss at 261: tensor(2.4998, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2831],\n",
      "        [1.4092]], requires_grad=True)\n",
      "Train loss at 262: tensor(2.4989, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2847],\n",
      "        [1.4088]], requires_grad=True)\n",
      "Train loss at 263: tensor(2.4979, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2862],\n",
      "        [1.4085]], requires_grad=True)\n",
      "Train loss at 264: tensor(2.4969, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2877],\n",
      "        [1.4081]], requires_grad=True)\n",
      "Train loss at 265: tensor(2.4960, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2893],\n",
      "        [1.4078]], requires_grad=True)\n",
      "Train loss at 266: tensor(2.4950, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2908],\n",
      "        [1.4074]], requires_grad=True)\n",
      "Train loss at 267: tensor(2.4941, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2923],\n",
      "        [1.4071]], requires_grad=True)\n",
      "Train loss at 268: tensor(2.4931, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2938],\n",
      "        [1.4067]], requires_grad=True)\n",
      "Train loss at 269: tensor(2.4922, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2953],\n",
      "        [1.4064]], requires_grad=True)\n",
      "Train loss at 270: tensor(2.4913, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2967],\n",
      "        [1.4060]], requires_grad=True)\n",
      "Train loss at 271: tensor(2.4904, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2982],\n",
      "        [1.4057]], requires_grad=True)\n",
      "Train loss at 272: tensor(2.4895, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2997],\n",
      "        [1.4053]], requires_grad=True)\n",
      "Train loss at 273: tensor(2.4886, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3012],\n",
      "        [1.4049]], requires_grad=True)\n",
      "Train loss at 274: tensor(2.4877, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3026],\n",
      "        [1.4046]], requires_grad=True)\n",
      "Train loss at 275: tensor(2.4868, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3041],\n",
      "        [1.4042]], requires_grad=True)\n",
      "Train loss at 276: tensor(2.4859, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3055],\n",
      "        [1.4038]], requires_grad=True)\n",
      "Train loss at 277: tensor(2.4851, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3069],\n",
      "        [1.4035]], requires_grad=True)\n",
      "Train loss at 278: tensor(2.4842, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3084],\n",
      "        [1.4031]], requires_grad=True)\n",
      "Train loss at 279: tensor(2.4834, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3098],\n",
      "        [1.4027]], requires_grad=True)\n",
      "Train loss at 280: tensor(2.4825, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3112],\n",
      "        [1.4024]], requires_grad=True)\n",
      "Train loss at 281: tensor(2.4817, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3126],\n",
      "        [1.4020]], requires_grad=True)\n",
      "Train loss at 282: tensor(2.4808, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3140],\n",
      "        [1.4016]], requires_grad=True)\n",
      "Train loss at 283: tensor(2.4800, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3154],\n",
      "        [1.4012]], requires_grad=True)\n",
      "Train loss at 284: tensor(2.4792, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3168],\n",
      "        [1.4009]], requires_grad=True)\n",
      "Train loss at 285: tensor(2.4784, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3182],\n",
      "        [1.4005]], requires_grad=True)\n",
      "Train loss at 286: tensor(2.4776, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3196],\n",
      "        [1.4001]], requires_grad=True)\n",
      "Train loss at 287: tensor(2.4768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3209],\n",
      "        [1.3997]], requires_grad=True)\n",
      "Train loss at 288: tensor(2.4760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3223],\n",
      "        [1.3993]], requires_grad=True)\n",
      "Train loss at 289: tensor(2.4752, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3237],\n",
      "        [1.3989]], requires_grad=True)\n",
      "Train loss at 290: tensor(2.4745, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3250],\n",
      "        [1.3986]], requires_grad=True)\n",
      "Train loss at 291: tensor(2.4737, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3263],\n",
      "        [1.3982]], requires_grad=True)\n",
      "Train loss at 292: tensor(2.4729, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3277],\n",
      "        [1.3978]], requires_grad=True)\n",
      "Train loss at 293: tensor(2.4722, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3290],\n",
      "        [1.3974]], requires_grad=True)\n",
      "Train loss at 294: tensor(2.4714, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3303],\n",
      "        [1.3970]], requires_grad=True)\n",
      "Train loss at 295: tensor(2.4707, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3317],\n",
      "        [1.3966]], requires_grad=True)\n",
      "Train loss at 296: tensor(2.4700, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3330],\n",
      "        [1.3962]], requires_grad=True)\n",
      "Train loss at 297: tensor(2.4692, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3343],\n",
      "        [1.3958]], requires_grad=True)\n",
      "Train loss at 298: tensor(2.4685, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3356],\n",
      "        [1.3954]], requires_grad=True)\n",
      "Train loss at 299: tensor(2.4678, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3369],\n",
      "        [1.3950]], requires_grad=True)\n",
      "Train loss at 300: tensor(2.4671, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3382],\n",
      "        [1.3946]], requires_grad=True)\n",
      "Train loss at 301: tensor(2.4664, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3395],\n",
      "        [1.3943]], requires_grad=True)\n",
      "Train loss at 302: tensor(2.4657, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3407],\n",
      "        [1.3939]], requires_grad=True)\n",
      "Train loss at 303: tensor(2.4650, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3420],\n",
      "        [1.3935]], requires_grad=True)\n",
      "Train loss at 304: tensor(2.4643, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3433],\n",
      "        [1.3931]], requires_grad=True)\n",
      "Train loss at 305: tensor(2.4636, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3445],\n",
      "        [1.3927]], requires_grad=True)\n",
      "Train loss at 306: tensor(2.4629, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3458],\n",
      "        [1.3923]], requires_grad=True)\n",
      "Train loss at 307: tensor(2.4623, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3470],\n",
      "        [1.3919]], requires_grad=True)\n",
      "Train loss at 308: tensor(2.4616, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3483],\n",
      "        [1.3915]], requires_grad=True)\n",
      "Train loss at 309: tensor(2.4609, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3495],\n",
      "        [1.3910]], requires_grad=True)\n",
      "Train loss at 310: tensor(2.4603, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3508],\n",
      "        [1.3906]], requires_grad=True)\n",
      "Train loss at 311: tensor(2.4596, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3520],\n",
      "        [1.3902]], requires_grad=True)\n",
      "Train loss at 312: tensor(2.4590, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3532],\n",
      "        [1.3898]], requires_grad=True)\n",
      "Train loss at 313: tensor(2.4583, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3544],\n",
      "        [1.3894]], requires_grad=True)\n",
      "Train loss at 314: tensor(2.4577, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3556],\n",
      "        [1.3890]], requires_grad=True)\n",
      "Train loss at 315: tensor(2.4571, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3568],\n",
      "        [1.3886]], requires_grad=True)\n",
      "Train loss at 316: tensor(2.4565, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3580],\n",
      "        [1.3882]], requires_grad=True)\n",
      "Train loss at 317: tensor(2.4558, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3592],\n",
      "        [1.3878]], requires_grad=True)\n",
      "Train loss at 318: tensor(2.4552, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3604],\n",
      "        [1.3874]], requires_grad=True)\n",
      "Train loss at 319: tensor(2.4546, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3616],\n",
      "        [1.3870]], requires_grad=True)\n",
      "Train loss at 320: tensor(2.4540, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3628],\n",
      "        [1.3866]], requires_grad=True)\n",
      "Train loss at 321: tensor(2.4534, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3639],\n",
      "        [1.3862]], requires_grad=True)\n",
      "Train loss at 322: tensor(2.4528, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3651],\n",
      "        [1.3857]], requires_grad=True)\n",
      "Train loss at 323: tensor(2.4522, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3663],\n",
      "        [1.3853]], requires_grad=True)\n",
      "Train loss at 324: tensor(2.4517, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3674],\n",
      "        [1.3849]], requires_grad=True)\n",
      "Train loss at 325: tensor(2.4511, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3686],\n",
      "        [1.3845]], requires_grad=True)\n",
      "Train loss at 326: tensor(2.4505, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3697],\n",
      "        [1.3841]], requires_grad=True)\n",
      "Train loss at 327: tensor(2.4499, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3709],\n",
      "        [1.3837]], requires_grad=True)\n",
      "Train loss at 328: tensor(2.4494, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3720],\n",
      "        [1.3833]], requires_grad=True)\n",
      "Train loss at 329: tensor(2.4488, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3731],\n",
      "        [1.3828]], requires_grad=True)\n",
      "Train loss at 330: tensor(2.4483, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3743],\n",
      "        [1.3824]], requires_grad=True)\n",
      "Train loss at 331: tensor(2.4477, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3754],\n",
      "        [1.3820]], requires_grad=True)\n",
      "Train loss at 332: tensor(2.4472, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3765],\n",
      "        [1.3816]], requires_grad=True)\n",
      "Train loss at 333: tensor(2.4466, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3776],\n",
      "        [1.3812]], requires_grad=True)\n",
      "Train loss at 334: tensor(2.4461, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3787],\n",
      "        [1.3808]], requires_grad=True)\n",
      "Train loss at 335: tensor(2.4456, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3798],\n",
      "        [1.3803]], requires_grad=True)\n",
      "Train loss at 336: tensor(2.4450, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3809],\n",
      "        [1.3799]], requires_grad=True)\n",
      "Train loss at 337: tensor(2.4445, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3820],\n",
      "        [1.3795]], requires_grad=True)\n",
      "Train loss at 338: tensor(2.4440, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3831],\n",
      "        [1.3791]], requires_grad=True)\n",
      "Train loss at 339: tensor(2.4435, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3841],\n",
      "        [1.3787]], requires_grad=True)\n",
      "Train loss at 340: tensor(2.4430, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3852],\n",
      "        [1.3782]], requires_grad=True)\n",
      "Train loss at 341: tensor(2.4424, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3863],\n",
      "        [1.3778]], requires_grad=True)\n",
      "Train loss at 342: tensor(2.4419, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3874],\n",
      "        [1.3774]], requires_grad=True)\n",
      "Train loss at 343: tensor(2.4414, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3884],\n",
      "        [1.3770]], requires_grad=True)\n",
      "Train loss at 344: tensor(2.4409, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3895],\n",
      "        [1.3766]], requires_grad=True)\n",
      "Train loss at 345: tensor(2.4405, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3905],\n",
      "        [1.3761]], requires_grad=True)\n",
      "Train loss at 346: tensor(2.4400, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3916],\n",
      "        [1.3757]], requires_grad=True)\n",
      "Train loss at 347: tensor(2.4395, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3926],\n",
      "        [1.3753]], requires_grad=True)\n",
      "Train loss at 348: tensor(2.4390, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3936],\n",
      "        [1.3749]], requires_grad=True)\n",
      "Train loss at 349: tensor(2.4385, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3947],\n",
      "        [1.3745]], requires_grad=True)\n",
      "Train loss at 350: tensor(2.4381, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3957],\n",
      "        [1.3740]], requires_grad=True)\n",
      "Train loss at 351: tensor(2.4376, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3967],\n",
      "        [1.3736]], requires_grad=True)\n",
      "Train loss at 352: tensor(2.4371, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3977],\n",
      "        [1.3732]], requires_grad=True)\n",
      "Train loss at 353: tensor(2.4367, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3988],\n",
      "        [1.3728]], requires_grad=True)\n",
      "Train loss at 354: tensor(2.4362, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3998],\n",
      "        [1.3723]], requires_grad=True)\n",
      "Train loss at 355: tensor(2.4358, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4008],\n",
      "        [1.3719]], requires_grad=True)\n",
      "Train loss at 356: tensor(2.4353, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4018],\n",
      "        [1.3715]], requires_grad=True)\n",
      "Train loss at 357: tensor(2.4349, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4028],\n",
      "        [1.3711]], requires_grad=True)\n",
      "Train loss at 358: tensor(2.4344, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4038],\n",
      "        [1.3707]], requires_grad=True)\n",
      "Train loss at 359: tensor(2.4340, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4047],\n",
      "        [1.3702]], requires_grad=True)\n",
      "Train loss at 360: tensor(2.4335, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4057],\n",
      "        [1.3698]], requires_grad=True)\n",
      "Train loss at 361: tensor(2.4331, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4067],\n",
      "        [1.3694]], requires_grad=True)\n",
      "Train loss at 362: tensor(2.4327, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4077],\n",
      "        [1.3690]], requires_grad=True)\n",
      "Train loss at 363: tensor(2.4322, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4086],\n",
      "        [1.3685]], requires_grad=True)\n",
      "Train loss at 364: tensor(2.4318, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4096],\n",
      "        [1.3681]], requires_grad=True)\n",
      "Train loss at 365: tensor(2.4314, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4106],\n",
      "        [1.3677]], requires_grad=True)\n",
      "Train loss at 366: tensor(2.4310, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4115],\n",
      "        [1.3673]], requires_grad=True)\n",
      "Train loss at 367: tensor(2.4306, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4125],\n",
      "        [1.3669]], requires_grad=True)\n",
      "Train loss at 368: tensor(2.4302, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4134],\n",
      "        [1.3664]], requires_grad=True)\n",
      "Train loss at 369: tensor(2.4298, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4144],\n",
      "        [1.3660]], requires_grad=True)\n",
      "Train loss at 370: tensor(2.4294, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4153],\n",
      "        [1.3656]], requires_grad=True)\n",
      "Train loss at 371: tensor(2.4290, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4162],\n",
      "        [1.3652]], requires_grad=True)\n",
      "Train loss at 372: tensor(2.4286, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4172],\n",
      "        [1.3647]], requires_grad=True)\n",
      "Train loss at 373: tensor(2.4282, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4181],\n",
      "        [1.3643]], requires_grad=True)\n",
      "Train loss at 374: tensor(2.4278, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4190],\n",
      "        [1.3639]], requires_grad=True)\n",
      "Train loss at 375: tensor(2.4274, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4199],\n",
      "        [1.3635]], requires_grad=True)\n",
      "Train loss at 376: tensor(2.4270, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4209],\n",
      "        [1.3631]], requires_grad=True)\n",
      "Train loss at 377: tensor(2.4266, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4218],\n",
      "        [1.3626]], requires_grad=True)\n",
      "Train loss at 378: tensor(2.4262, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4227],\n",
      "        [1.3622]], requires_grad=True)\n",
      "Train loss at 379: tensor(2.4259, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4236],\n",
      "        [1.3618]], requires_grad=True)\n",
      "Train loss at 380: tensor(2.4255, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4245],\n",
      "        [1.3614]], requires_grad=True)\n",
      "Train loss at 381: tensor(2.4251, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4254],\n",
      "        [1.3610]], requires_grad=True)\n",
      "Train loss at 382: tensor(2.4248, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4263],\n",
      "        [1.3605]], requires_grad=True)\n",
      "Train loss at 383: tensor(2.4244, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4272],\n",
      "        [1.3601]], requires_grad=True)\n",
      "Train loss at 384: tensor(2.4240, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4280],\n",
      "        [1.3597]], requires_grad=True)\n",
      "Train loss at 385: tensor(2.4237, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4289],\n",
      "        [1.3593]], requires_grad=True)\n",
      "Train loss at 386: tensor(2.4233, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4298],\n",
      "        [1.3589]], requires_grad=True)\n",
      "Train loss at 387: tensor(2.4230, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4307],\n",
      "        [1.3584]], requires_grad=True)\n",
      "Train loss at 388: tensor(2.4226, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4315],\n",
      "        [1.3580]], requires_grad=True)\n",
      "Train loss at 389: tensor(2.4223, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4324],\n",
      "        [1.3576]], requires_grad=True)\n",
      "Train loss at 390: tensor(2.4219, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4333],\n",
      "        [1.3572]], requires_grad=True)\n",
      "Train loss at 391: tensor(2.4216, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4341],\n",
      "        [1.3568]], requires_grad=True)\n",
      "Train loss at 392: tensor(2.4212, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4350],\n",
      "        [1.3563]], requires_grad=True)\n",
      "Train loss at 393: tensor(2.4209, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4358],\n",
      "        [1.3559]], requires_grad=True)\n",
      "Train loss at 394: tensor(2.4206, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4367],\n",
      "        [1.3555]], requires_grad=True)\n",
      "Train loss at 395: tensor(2.4202, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4375],\n",
      "        [1.3551]], requires_grad=True)\n",
      "Train loss at 396: tensor(2.4199, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4383],\n",
      "        [1.3547]], requires_grad=True)\n",
      "Train loss at 397: tensor(2.4196, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4392],\n",
      "        [1.3543]], requires_grad=True)\n",
      "Train loss at 398: tensor(2.4193, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4400],\n",
      "        [1.3539]], requires_grad=True)\n",
      "Train loss at 399: tensor(2.4189, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4408],\n",
      "        [1.3534]], requires_grad=True)\n",
      "Train loss at 400: tensor(2.4186, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4417],\n",
      "        [1.3530]], requires_grad=True)\n",
      "Train loss at 401: tensor(2.4183, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4425],\n",
      "        [1.3526]], requires_grad=True)\n",
      "Train loss at 402: tensor(2.4180, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4433],\n",
      "        [1.3522]], requires_grad=True)\n",
      "Train loss at 403: tensor(2.4177, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4441],\n",
      "        [1.3518]], requires_grad=True)\n",
      "Train loss at 404: tensor(2.4174, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4449],\n",
      "        [1.3514]], requires_grad=True)\n",
      "Train loss at 405: tensor(2.4171, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4457],\n",
      "        [1.3510]], requires_grad=True)\n",
      "Train loss at 406: tensor(2.4167, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4465],\n",
      "        [1.3505]], requires_grad=True)\n",
      "Train loss at 407: tensor(2.4164, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4473],\n",
      "        [1.3501]], requires_grad=True)\n",
      "Train loss at 408: tensor(2.4161, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4481],\n",
      "        [1.3497]], requires_grad=True)\n",
      "Train loss at 409: tensor(2.4158, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4489],\n",
      "        [1.3493]], requires_grad=True)\n",
      "Train loss at 410: tensor(2.4156, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4497],\n",
      "        [1.3489]], requires_grad=True)\n",
      "Train loss at 411: tensor(2.4153, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4505],\n",
      "        [1.3485]], requires_grad=True)\n",
      "Train loss at 412: tensor(2.4150, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4513],\n",
      "        [1.3481]], requires_grad=True)\n",
      "Train loss at 413: tensor(2.4147, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4521],\n",
      "        [1.3477]], requires_grad=True)\n",
      "Train loss at 414: tensor(2.4144, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4528],\n",
      "        [1.3473]], requires_grad=True)\n",
      "Train loss at 415: tensor(2.4141, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4536],\n",
      "        [1.3468]], requires_grad=True)\n",
      "Train loss at 416: tensor(2.4138, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4544],\n",
      "        [1.3464]], requires_grad=True)\n",
      "Train loss at 417: tensor(2.4135, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4551],\n",
      "        [1.3460]], requires_grad=True)\n",
      "Train loss at 418: tensor(2.4133, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4559],\n",
      "        [1.3456]], requires_grad=True)\n",
      "Train loss at 419: tensor(2.4130, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4567],\n",
      "        [1.3452]], requires_grad=True)\n",
      "Train loss at 420: tensor(2.4127, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4574],\n",
      "        [1.3448]], requires_grad=True)\n",
      "Train loss at 421: tensor(2.4124, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4582],\n",
      "        [1.3444]], requires_grad=True)\n",
      "Train loss at 422: tensor(2.4122, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4589],\n",
      "        [1.3440]], requires_grad=True)\n",
      "Train loss at 423: tensor(2.4119, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4597],\n",
      "        [1.3436]], requires_grad=True)\n",
      "Train loss at 424: tensor(2.4116, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4604],\n",
      "        [1.3432]], requires_grad=True)\n",
      "Train loss at 425: tensor(2.4114, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4611],\n",
      "        [1.3428]], requires_grad=True)\n",
      "Train loss at 426: tensor(2.4111, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4619],\n",
      "        [1.3424]], requires_grad=True)\n",
      "Train loss at 427: tensor(2.4109, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4626],\n",
      "        [1.3420]], requires_grad=True)\n",
      "Train loss at 428: tensor(2.4106, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4633],\n",
      "        [1.3416]], requires_grad=True)\n",
      "Train loss at 429: tensor(2.4103, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4641],\n",
      "        [1.3412]], requires_grad=True)\n",
      "Train loss at 430: tensor(2.4101, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4648],\n",
      "        [1.3408]], requires_grad=True)\n",
      "Train loss at 431: tensor(2.4098, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4655],\n",
      "        [1.3404]], requires_grad=True)\n",
      "Train loss at 432: tensor(2.4096, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4662],\n",
      "        [1.3400]], requires_grad=True)\n",
      "Train loss at 433: tensor(2.4093, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4669],\n",
      "        [1.3396]], requires_grad=True)\n",
      "Train loss at 434: tensor(2.4091, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4677],\n",
      "        [1.3392]], requires_grad=True)\n",
      "Train loss at 435: tensor(2.4088, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4684],\n",
      "        [1.3388]], requires_grad=True)\n",
      "Train loss at 436: tensor(2.4086, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4691],\n",
      "        [1.3384]], requires_grad=True)\n",
      "Train loss at 437: tensor(2.4084, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4698],\n",
      "        [1.3380]], requires_grad=True)\n",
      "Train loss at 438: tensor(2.4081, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4705],\n",
      "        [1.3376]], requires_grad=True)\n",
      "Train loss at 439: tensor(2.4079, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4712],\n",
      "        [1.3372]], requires_grad=True)\n",
      "Train loss at 440: tensor(2.4076, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4719],\n",
      "        [1.3368]], requires_grad=True)\n",
      "Train loss at 441: tensor(2.4074, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4726],\n",
      "        [1.3364]], requires_grad=True)\n",
      "Train loss at 442: tensor(2.4072, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4733],\n",
      "        [1.3360]], requires_grad=True)\n",
      "Train loss at 443: tensor(2.4069, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4739],\n",
      "        [1.3356]], requires_grad=True)\n",
      "Train loss at 444: tensor(2.4067, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4746],\n",
      "        [1.3352]], requires_grad=True)\n",
      "Train loss at 445: tensor(2.4065, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4753],\n",
      "        [1.3348]], requires_grad=True)\n",
      "Train loss at 446: tensor(2.4063, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4760],\n",
      "        [1.3344]], requires_grad=True)\n",
      "Train loss at 447: tensor(2.4060, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4767],\n",
      "        [1.3340]], requires_grad=True)\n",
      "Train loss at 448: tensor(2.4058, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4773],\n",
      "        [1.3337]], requires_grad=True)\n",
      "Train loss at 449: tensor(2.4056, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4780],\n",
      "        [1.3333]], requires_grad=True)\n",
      "Train loss at 450: tensor(2.4054, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4787],\n",
      "        [1.3329]], requires_grad=True)\n",
      "Train loss at 451: tensor(2.4052, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4793],\n",
      "        [1.3325]], requires_grad=True)\n",
      "Train loss at 452: tensor(2.4049, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4800],\n",
      "        [1.3321]], requires_grad=True)\n",
      "Train loss at 453: tensor(2.4047, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4806],\n",
      "        [1.3317]], requires_grad=True)\n",
      "Train loss at 454: tensor(2.4045, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4813],\n",
      "        [1.3313]], requires_grad=True)\n",
      "Train loss at 455: tensor(2.4043, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4819],\n",
      "        [1.3309]], requires_grad=True)\n",
      "Train loss at 456: tensor(2.4041, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4826],\n",
      "        [1.3306]], requires_grad=True)\n",
      "Train loss at 457: tensor(2.4039, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4832],\n",
      "        [1.3302]], requires_grad=True)\n",
      "Train loss at 458: tensor(2.4037, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4839],\n",
      "        [1.3298]], requires_grad=True)\n",
      "Train loss at 459: tensor(2.4035, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4845],\n",
      "        [1.3294]], requires_grad=True)\n",
      "Train loss at 460: tensor(2.4033, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4852],\n",
      "        [1.3290]], requires_grad=True)\n",
      "Train loss at 461: tensor(2.4031, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4858],\n",
      "        [1.3286]], requires_grad=True)\n",
      "Train loss at 462: tensor(2.4029, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4864],\n",
      "        [1.3283]], requires_grad=True)\n",
      "Train loss at 463: tensor(2.4027, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4871],\n",
      "        [1.3279]], requires_grad=True)\n",
      "Train loss at 464: tensor(2.4025, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4877],\n",
      "        [1.3275]], requires_grad=True)\n",
      "Train loss at 465: tensor(2.4023, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4883],\n",
      "        [1.3271]], requires_grad=True)\n",
      "Train loss at 466: tensor(2.4021, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4889],\n",
      "        [1.3267]], requires_grad=True)\n",
      "Train loss at 467: tensor(2.4019, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4896],\n",
      "        [1.3264]], requires_grad=True)\n",
      "Train loss at 468: tensor(2.4017, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4902],\n",
      "        [1.3260]], requires_grad=True)\n",
      "Train loss at 469: tensor(2.4015, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4908],\n",
      "        [1.3256]], requires_grad=True)\n",
      "Train loss at 470: tensor(2.4013, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4914],\n",
      "        [1.3252]], requires_grad=True)\n",
      "Train loss at 471: tensor(2.4011, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4920],\n",
      "        [1.3249]], requires_grad=True)\n",
      "Train loss at 472: tensor(2.4010, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4926],\n",
      "        [1.3245]], requires_grad=True)\n",
      "Train loss at 473: tensor(2.4008, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4932],\n",
      "        [1.3241]], requires_grad=True)\n",
      "Train loss at 474: tensor(2.4006, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4938],\n",
      "        [1.3237]], requires_grad=True)\n",
      "Train loss at 475: tensor(2.4004, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4944],\n",
      "        [1.3234]], requires_grad=True)\n",
      "Train loss at 476: tensor(2.4002, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4950],\n",
      "        [1.3230]], requires_grad=True)\n",
      "Train loss at 477: tensor(2.4000, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4956],\n",
      "        [1.3226]], requires_grad=True)\n",
      "Train loss at 478: tensor(2.3999, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4962],\n",
      "        [1.3223]], requires_grad=True)\n",
      "Train loss at 479: tensor(2.3997, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4968],\n",
      "        [1.3219]], requires_grad=True)\n",
      "Train loss at 480: tensor(2.3995, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4974],\n",
      "        [1.3215]], requires_grad=True)\n",
      "Train loss at 481: tensor(2.3993, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4980],\n",
      "        [1.3212]], requires_grad=True)\n",
      "Train loss at 482: tensor(2.3992, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4986],\n",
      "        [1.3208]], requires_grad=True)\n",
      "Train loss at 483: tensor(2.3990, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4991],\n",
      "        [1.3204]], requires_grad=True)\n",
      "Train loss at 484: tensor(2.3988, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4997],\n",
      "        [1.3201]], requires_grad=True)\n",
      "Train loss at 485: tensor(2.3987, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5003],\n",
      "        [1.3197]], requires_grad=True)\n",
      "Train loss at 486: tensor(2.3985, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5009],\n",
      "        [1.3193]], requires_grad=True)\n",
      "Train loss at 487: tensor(2.3983, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5014],\n",
      "        [1.3190]], requires_grad=True)\n",
      "Train loss at 488: tensor(2.3982, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5020],\n",
      "        [1.3186]], requires_grad=True)\n",
      "Train loss at 489: tensor(2.3980, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5026],\n",
      "        [1.3182]], requires_grad=True)\n",
      "Train loss at 490: tensor(2.3978, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5031],\n",
      "        [1.3179]], requires_grad=True)\n",
      "Train loss at 491: tensor(2.3977, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5037],\n",
      "        [1.3175]], requires_grad=True)\n",
      "Train loss at 492: tensor(2.3975, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5043],\n",
      "        [1.3172]], requires_grad=True)\n",
      "Train loss at 493: tensor(2.3973, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5048],\n",
      "        [1.3168]], requires_grad=True)\n",
      "Train loss at 494: tensor(2.3972, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5054],\n",
      "        [1.3164]], requires_grad=True)\n",
      "Train loss at 495: tensor(2.3970, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5059],\n",
      "        [1.3161]], requires_grad=True)\n",
      "Train loss at 496: tensor(2.3969, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5065],\n",
      "        [1.3157]], requires_grad=True)\n",
      "Train loss at 497: tensor(2.3967, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5070],\n",
      "        [1.3154]], requires_grad=True)\n",
      "Train loss at 498: tensor(2.3966, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5076],\n",
      "        [1.3150]], requires_grad=True)\n",
      "Train loss at 499: tensor(2.3964, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5081],\n",
      "        [1.3147]], requires_grad=True)\n",
      "Train loss at 500: tensor(2.3963, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5087],\n",
      "        [1.3143]], requires_grad=True)\n",
      "Train loss at 501: tensor(2.3961, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5092],\n",
      "        [1.3140]], requires_grad=True)\n",
      "Train loss at 502: tensor(2.3960, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5097],\n",
      "        [1.3136]], requires_grad=True)\n",
      "Train loss at 503: tensor(2.3958, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5103],\n",
      "        [1.3133]], requires_grad=True)\n",
      "Train loss at 504: tensor(2.3957, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5108],\n",
      "        [1.3129]], requires_grad=True)\n",
      "Train loss at 505: tensor(2.3955, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5113],\n",
      "        [1.3126]], requires_grad=True)\n",
      "Train loss at 506: tensor(2.3954, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5119],\n",
      "        [1.3122]], requires_grad=True)\n",
      "Train loss at 507: tensor(2.3952, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5124],\n",
      "        [1.3119]], requires_grad=True)\n",
      "Train loss at 508: tensor(2.3951, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5129],\n",
      "        [1.3115]], requires_grad=True)\n",
      "Train loss at 509: tensor(2.3949, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5135],\n",
      "        [1.3112]], requires_grad=True)\n",
      "Train loss at 510: tensor(2.3948, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5140],\n",
      "        [1.3108]], requires_grad=True)\n",
      "Train loss at 511: tensor(2.3947, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5145],\n",
      "        [1.3105]], requires_grad=True)\n",
      "Train loss at 512: tensor(2.3945, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5150],\n",
      "        [1.3101]], requires_grad=True)\n",
      "Train loss at 513: tensor(2.3944, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5155],\n",
      "        [1.3098]], requires_grad=True)\n",
      "Train loss at 514: tensor(2.3942, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5160],\n",
      "        [1.3094]], requires_grad=True)\n",
      "Train loss at 515: tensor(2.3941, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5165],\n",
      "        [1.3091]], requires_grad=True)\n",
      "Train loss at 516: tensor(2.3940, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5171],\n",
      "        [1.3088]], requires_grad=True)\n",
      "Train loss at 517: tensor(2.3938, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5176],\n",
      "        [1.3084]], requires_grad=True)\n",
      "Train loss at 518: tensor(2.3937, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5181],\n",
      "        [1.3081]], requires_grad=True)\n",
      "Train loss at 519: tensor(2.3936, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5186],\n",
      "        [1.3078]], requires_grad=True)\n",
      "Train loss at 520: tensor(2.3934, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5191],\n",
      "        [1.3074]], requires_grad=True)\n",
      "Train loss at 521: tensor(2.3933, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5196],\n",
      "        [1.3071]], requires_grad=True)\n",
      "Train loss at 522: tensor(2.3932, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5201],\n",
      "        [1.3067]], requires_grad=True)\n",
      "Train loss at 523: tensor(2.3931, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5206],\n",
      "        [1.3064]], requires_grad=True)\n",
      "Train loss at 524: tensor(2.3929, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5210],\n",
      "        [1.3061]], requires_grad=True)\n",
      "Train loss at 525: tensor(2.3928, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5215],\n",
      "        [1.3057]], requires_grad=True)\n",
      "Train loss at 526: tensor(2.3927, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5220],\n",
      "        [1.3054]], requires_grad=True)\n",
      "Train loss at 527: tensor(2.3926, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5225],\n",
      "        [1.3051]], requires_grad=True)\n",
      "Train loss at 528: tensor(2.3924, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5230],\n",
      "        [1.3047]], requires_grad=True)\n",
      "Train loss at 529: tensor(2.3923, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5235],\n",
      "        [1.3044]], requires_grad=True)\n",
      "Train loss at 530: tensor(2.3922, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5240],\n",
      "        [1.3041]], requires_grad=True)\n",
      "Train loss at 531: tensor(2.3921, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5244],\n",
      "        [1.3038]], requires_grad=True)\n",
      "Train loss at 532: tensor(2.3919, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5249],\n",
      "        [1.3034]], requires_grad=True)\n",
      "Train loss at 533: tensor(2.3918, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5254],\n",
      "        [1.3031]], requires_grad=True)\n",
      "Train loss at 534: tensor(2.3917, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5259],\n",
      "        [1.3028]], requires_grad=True)\n",
      "Train loss at 535: tensor(2.3916, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5263],\n",
      "        [1.3024]], requires_grad=True)\n",
      "Train loss at 536: tensor(2.3915, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5268],\n",
      "        [1.3021]], requires_grad=True)\n",
      "Train loss at 537: tensor(2.3914, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5273],\n",
      "        [1.3018]], requires_grad=True)\n",
      "Train loss at 538: tensor(2.3912, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5277],\n",
      "        [1.3015]], requires_grad=True)\n",
      "Train loss at 539: tensor(2.3911, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5282],\n",
      "        [1.3012]], requires_grad=True)\n",
      "Train loss at 540: tensor(2.3910, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5287],\n",
      "        [1.3008]], requires_grad=True)\n",
      "Train loss at 541: tensor(2.3909, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5291],\n",
      "        [1.3005]], requires_grad=True)\n",
      "Train loss at 542: tensor(2.3908, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5296],\n",
      "        [1.3002]], requires_grad=True)\n",
      "Train loss at 543: tensor(2.3907, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5300],\n",
      "        [1.2999]], requires_grad=True)\n",
      "Train loss at 544: tensor(2.3906, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5305],\n",
      "        [1.2996]], requires_grad=True)\n",
      "Train loss at 545: tensor(2.3905, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5310],\n",
      "        [1.2992]], requires_grad=True)\n",
      "Train loss at 546: tensor(2.3903, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5314],\n",
      "        [1.2989]], requires_grad=True)\n",
      "Train loss at 547: tensor(2.3902, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5319],\n",
      "        [1.2986]], requires_grad=True)\n",
      "Train loss at 548: tensor(2.3901, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5323],\n",
      "        [1.2983]], requires_grad=True)\n",
      "Train loss at 549: tensor(2.3900, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5327],\n",
      "        [1.2980]], requires_grad=True)\n",
      "Train loss at 550: tensor(2.3899, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5332],\n",
      "        [1.2977]], requires_grad=True)\n",
      "Train loss at 551: tensor(2.3898, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5336],\n",
      "        [1.2973]], requires_grad=True)\n",
      "Train loss at 552: tensor(2.3897, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5341],\n",
      "        [1.2970]], requires_grad=True)\n",
      "Train loss at 553: tensor(2.3896, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5345],\n",
      "        [1.2967]], requires_grad=True)\n",
      "Train loss at 554: tensor(2.3895, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5350],\n",
      "        [1.2964]], requires_grad=True)\n",
      "Train loss at 555: tensor(2.3894, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5354],\n",
      "        [1.2961]], requires_grad=True)\n",
      "Train loss at 556: tensor(2.3893, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5358],\n",
      "        [1.2958]], requires_grad=True)\n",
      "Train loss at 557: tensor(2.3892, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5363],\n",
      "        [1.2955]], requires_grad=True)\n",
      "Train loss at 558: tensor(2.3891, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5367],\n",
      "        [1.2952]], requires_grad=True)\n",
      "Train loss at 559: tensor(2.3890, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5371],\n",
      "        [1.2949]], requires_grad=True)\n",
      "Train loss at 560: tensor(2.3889, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5375],\n",
      "        [1.2946]], requires_grad=True)\n",
      "Train loss at 561: tensor(2.3888, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5380],\n",
      "        [1.2943]], requires_grad=True)\n",
      "Train loss at 562: tensor(2.3887, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5384],\n",
      "        [1.2940]], requires_grad=True)\n",
      "Train loss at 563: tensor(2.3886, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5388],\n",
      "        [1.2937]], requires_grad=True)\n",
      "Train loss at 564: tensor(2.3885, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5392],\n",
      "        [1.2934]], requires_grad=True)\n",
      "Train loss at 565: tensor(2.3884, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5397],\n",
      "        [1.2931]], requires_grad=True)\n",
      "Train loss at 566: tensor(2.3883, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5401],\n",
      "        [1.2928]], requires_grad=True)\n",
      "Train loss at 567: tensor(2.3882, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5405],\n",
      "        [1.2925]], requires_grad=True)\n",
      "Train loss at 568: tensor(2.3881, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5409],\n",
      "        [1.2922]], requires_grad=True)\n",
      "Train loss at 569: tensor(2.3880, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5413],\n",
      "        [1.2919]], requires_grad=True)\n",
      "Train loss at 570: tensor(2.3880, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5417],\n",
      "        [1.2916]], requires_grad=True)\n",
      "Train loss at 571: tensor(2.3879, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5421],\n",
      "        [1.2913]], requires_grad=True)\n",
      "Train loss at 572: tensor(2.3878, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5425],\n",
      "        [1.2910]], requires_grad=True)\n",
      "Train loss at 573: tensor(2.3877, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5430],\n",
      "        [1.2907]], requires_grad=True)\n",
      "Train loss at 574: tensor(2.3876, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5434],\n",
      "        [1.2904]], requires_grad=True)\n",
      "Train loss at 575: tensor(2.3875, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5438],\n",
      "        [1.2901]], requires_grad=True)\n",
      "Train loss at 576: tensor(2.3874, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5442],\n",
      "        [1.2898]], requires_grad=True)\n",
      "Train loss at 577: tensor(2.3873, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5446],\n",
      "        [1.2895]], requires_grad=True)\n",
      "Train loss at 578: tensor(2.3872, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5450],\n",
      "        [1.2892]], requires_grad=True)\n",
      "Train loss at 579: tensor(2.3872, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5454],\n",
      "        [1.2889]], requires_grad=True)\n",
      "Train loss at 580: tensor(2.3871, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5458],\n",
      "        [1.2886]], requires_grad=True)\n",
      "Train loss at 581: tensor(2.3870, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5462],\n",
      "        [1.2883]], requires_grad=True)\n",
      "Train loss at 582: tensor(2.3869, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5465],\n",
      "        [1.2880]], requires_grad=True)\n",
      "Train loss at 583: tensor(2.3868, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5469],\n",
      "        [1.2878]], requires_grad=True)\n",
      "Train loss at 584: tensor(2.3867, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5473],\n",
      "        [1.2875]], requires_grad=True)\n",
      "Train loss at 585: tensor(2.3867, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5477],\n",
      "        [1.2872]], requires_grad=True)\n",
      "Train loss at 586: tensor(2.3866, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5481],\n",
      "        [1.2869]], requires_grad=True)\n",
      "Train loss at 587: tensor(2.3865, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5485],\n",
      "        [1.2866]], requires_grad=True)\n",
      "Train loss at 588: tensor(2.3864, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5489],\n",
      "        [1.2863]], requires_grad=True)\n",
      "Train loss at 589: tensor(2.3863, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5493],\n",
      "        [1.2861]], requires_grad=True)\n",
      "Train loss at 590: tensor(2.3863, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5496],\n",
      "        [1.2858]], requires_grad=True)\n",
      "Train loss at 591: tensor(2.3862, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5500],\n",
      "        [1.2855]], requires_grad=True)\n",
      "Train loss at 592: tensor(2.3861, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5504],\n",
      "        [1.2852]], requires_grad=True)\n",
      "Train loss at 593: tensor(2.3860, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5508],\n",
      "        [1.2849]], requires_grad=True)\n",
      "Train loss at 594: tensor(2.3859, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5511],\n",
      "        [1.2847]], requires_grad=True)\n",
      "Train loss at 595: tensor(2.3859, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5515],\n",
      "        [1.2844]], requires_grad=True)\n",
      "Train loss at 596: tensor(2.3858, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5519],\n",
      "        [1.2841]], requires_grad=True)\n",
      "Train loss at 597: tensor(2.3857, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5523],\n",
      "        [1.2838]], requires_grad=True)\n",
      "Train loss at 598: tensor(2.3856, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5526],\n",
      "        [1.2835]], requires_grad=True)\n",
      "Train loss at 599: tensor(2.3856, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5530],\n",
      "        [1.2833]], requires_grad=True)\n",
      "Train loss at 600: tensor(2.3855, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5534],\n",
      "        [1.2830]], requires_grad=True)\n",
      "Train loss at 601: tensor(2.3854, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5537],\n",
      "        [1.2827]], requires_grad=True)\n",
      "Train loss at 602: tensor(2.3853, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5541],\n",
      "        [1.2825]], requires_grad=True)\n",
      "Train loss at 603: tensor(2.3853, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5545],\n",
      "        [1.2822]], requires_grad=True)\n",
      "Train loss at 604: tensor(2.3852, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5548],\n",
      "        [1.2819]], requires_grad=True)\n",
      "Train loss at 605: tensor(2.3851, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5552],\n",
      "        [1.2816]], requires_grad=True)\n",
      "Train loss at 606: tensor(2.3851, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5555],\n",
      "        [1.2814]], requires_grad=True)\n",
      "Train loss at 607: tensor(2.3850, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5559],\n",
      "        [1.2811]], requires_grad=True)\n",
      "Train loss at 608: tensor(2.3849, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5562],\n",
      "        [1.2808]], requires_grad=True)\n",
      "Train loss at 609: tensor(2.3848, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5566],\n",
      "        [1.2806]], requires_grad=True)\n",
      "Train loss at 610: tensor(2.3848, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5570],\n",
      "        [1.2803]], requires_grad=True)\n",
      "Train loss at 611: tensor(2.3847, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5573],\n",
      "        [1.2800]], requires_grad=True)\n",
      "Train loss at 612: tensor(2.3846, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5577],\n",
      "        [1.2798]], requires_grad=True)\n",
      "Train loss at 613: tensor(2.3846, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5580],\n",
      "        [1.2795]], requires_grad=True)\n",
      "Train loss at 614: tensor(2.3845, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5584],\n",
      "        [1.2792]], requires_grad=True)\n",
      "Train loss at 615: tensor(2.3844, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5587],\n",
      "        [1.2790]], requires_grad=True)\n",
      "Train loss at 616: tensor(2.3844, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5590],\n",
      "        [1.2787]], requires_grad=True)\n",
      "Train loss at 617: tensor(2.3843, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5594],\n",
      "        [1.2785]], requires_grad=True)\n",
      "Train loss at 618: tensor(2.3842, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5597],\n",
      "        [1.2782]], requires_grad=True)\n",
      "Train loss at 619: tensor(2.3842, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5601],\n",
      "        [1.2779]], requires_grad=True)\n",
      "Train loss at 620: tensor(2.3841, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5604],\n",
      "        [1.2777]], requires_grad=True)\n",
      "Train loss at 621: tensor(2.3840, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5608],\n",
      "        [1.2774]], requires_grad=True)\n",
      "Train loss at 622: tensor(2.3840, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5611],\n",
      "        [1.2772]], requires_grad=True)\n",
      "Train loss at 623: tensor(2.3839, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5614],\n",
      "        [1.2769]], requires_grad=True)\n",
      "Train loss at 624: tensor(2.3839, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5618],\n",
      "        [1.2767]], requires_grad=True)\n",
      "Train loss at 625: tensor(2.3838, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5621],\n",
      "        [1.2764]], requires_grad=True)\n",
      "Train loss at 626: tensor(2.3837, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5624],\n",
      "        [1.2762]], requires_grad=True)\n",
      "Train loss at 627: tensor(2.3837, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5628],\n",
      "        [1.2759]], requires_grad=True)\n",
      "Train loss at 628: tensor(2.3836, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5631],\n",
      "        [1.2756]], requires_grad=True)\n",
      "Train loss at 629: tensor(2.3835, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5634],\n",
      "        [1.2754]], requires_grad=True)\n",
      "Train loss at 630: tensor(2.3835, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5637],\n",
      "        [1.2751]], requires_grad=True)\n",
      "Train loss at 631: tensor(2.3834, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5641],\n",
      "        [1.2749]], requires_grad=True)\n",
      "Train loss at 632: tensor(2.3834, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5644],\n",
      "        [1.2746]], requires_grad=True)\n",
      "Train loss at 633: tensor(2.3833, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5647],\n",
      "        [1.2744]], requires_grad=True)\n",
      "Train loss at 634: tensor(2.3833, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5650],\n",
      "        [1.2741]], requires_grad=True)\n",
      "Train loss at 635: tensor(2.3832, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5654],\n",
      "        [1.2739]], requires_grad=True)\n",
      "Train loss at 636: tensor(2.3831, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5657],\n",
      "        [1.2737]], requires_grad=True)\n",
      "Train loss at 637: tensor(2.3831, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5660],\n",
      "        [1.2734]], requires_grad=True)\n",
      "Train loss at 638: tensor(2.3830, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5663],\n",
      "        [1.2732]], requires_grad=True)\n",
      "Train loss at 639: tensor(2.3830, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5666],\n",
      "        [1.2729]], requires_grad=True)\n",
      "Train loss at 640: tensor(2.3829, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5669],\n",
      "        [1.2727]], requires_grad=True)\n",
      "Train loss at 641: tensor(2.3829, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5673],\n",
      "        [1.2724]], requires_grad=True)\n",
      "Train loss at 642: tensor(2.3828, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5676],\n",
      "        [1.2722]], requires_grad=True)\n",
      "Train loss at 643: tensor(2.3827, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5679],\n",
      "        [1.2719]], requires_grad=True)\n",
      "Train loss at 644: tensor(2.3827, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5682],\n",
      "        [1.2717]], requires_grad=True)\n",
      "Train loss at 645: tensor(2.3826, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5685],\n",
      "        [1.2715]], requires_grad=True)\n",
      "Train loss at 646: tensor(2.3826, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5688],\n",
      "        [1.2712]], requires_grad=True)\n",
      "Train loss at 647: tensor(2.3825, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5691],\n",
      "        [1.2710]], requires_grad=True)\n",
      "Train loss at 648: tensor(2.3825, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5694],\n",
      "        [1.2708]], requires_grad=True)\n",
      "Train loss at 649: tensor(2.3824, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5697],\n",
      "        [1.2705]], requires_grad=True)\n",
      "Train loss at 650: tensor(2.3824, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5700],\n",
      "        [1.2703]], requires_grad=True)\n",
      "Train loss at 651: tensor(2.3823, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5703],\n",
      "        [1.2700]], requires_grad=True)\n",
      "Train loss at 652: tensor(2.3823, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5706],\n",
      "        [1.2698]], requires_grad=True)\n",
      "Train loss at 653: tensor(2.3822, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5709],\n",
      "        [1.2696]], requires_grad=True)\n",
      "Train loss at 654: tensor(2.3822, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5712],\n",
      "        [1.2693]], requires_grad=True)\n",
      "Train loss at 655: tensor(2.3821, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5715],\n",
      "        [1.2691]], requires_grad=True)\n",
      "Train loss at 656: tensor(2.3821, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5718],\n",
      "        [1.2689]], requires_grad=True)\n",
      "Train loss at 657: tensor(2.3820, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5721],\n",
      "        [1.2686]], requires_grad=True)\n",
      "Train loss at 658: tensor(2.3820, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5724],\n",
      "        [1.2684]], requires_grad=True)\n",
      "Train loss at 659: tensor(2.3819, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5727],\n",
      "        [1.2682]], requires_grad=True)\n",
      "Train loss at 660: tensor(2.3819, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5730],\n",
      "        [1.2680]], requires_grad=True)\n",
      "Train loss at 661: tensor(2.3818, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5733],\n",
      "        [1.2677]], requires_grad=True)\n",
      "Train loss at 662: tensor(2.3818, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5736],\n",
      "        [1.2675]], requires_grad=True)\n",
      "Train loss at 663: tensor(2.3817, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5739],\n",
      "        [1.2673]], requires_grad=True)\n",
      "Train loss at 664: tensor(2.3817, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5742],\n",
      "        [1.2670]], requires_grad=True)\n",
      "Train loss at 665: tensor(2.3816, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5745],\n",
      "        [1.2668]], requires_grad=True)\n",
      "Train loss at 666: tensor(2.3816, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5748],\n",
      "        [1.2666]], requires_grad=True)\n",
      "Train loss at 667: tensor(2.3815, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5750],\n",
      "        [1.2664]], requires_grad=True)\n",
      "Train loss at 668: tensor(2.3815, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5753],\n",
      "        [1.2661]], requires_grad=True)\n",
      "Train loss at 669: tensor(2.3814, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5756],\n",
      "        [1.2659]], requires_grad=True)\n",
      "Train loss at 670: tensor(2.3814, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5759],\n",
      "        [1.2657]], requires_grad=True)\n",
      "Train loss at 671: tensor(2.3814, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5762],\n",
      "        [1.2655]], requires_grad=True)\n",
      "Train loss at 672: tensor(2.3813, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5765],\n",
      "        [1.2653]], requires_grad=True)\n",
      "Train loss at 673: tensor(2.3813, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5767],\n",
      "        [1.2650]], requires_grad=True)\n",
      "Train loss at 674: tensor(2.3812, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5770],\n",
      "        [1.2648]], requires_grad=True)\n",
      "Train loss at 675: tensor(2.3812, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5773],\n",
      "        [1.2646]], requires_grad=True)\n",
      "Train loss at 676: tensor(2.3811, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5776],\n",
      "        [1.2644]], requires_grad=True)\n",
      "Train loss at 677: tensor(2.3811, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5778],\n",
      "        [1.2642]], requires_grad=True)\n",
      "Train loss at 678: tensor(2.3811, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5781],\n",
      "        [1.2640]], requires_grad=True)\n",
      "Train loss at 679: tensor(2.3810, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5784],\n",
      "        [1.2637]], requires_grad=True)\n",
      "Train loss at 680: tensor(2.3810, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5787],\n",
      "        [1.2635]], requires_grad=True)\n",
      "Train loss at 681: tensor(2.3809, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5789],\n",
      "        [1.2633]], requires_grad=True)\n",
      "Train loss at 682: tensor(2.3809, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5792],\n",
      "        [1.2631]], requires_grad=True)\n",
      "Train loss at 683: tensor(2.3808, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5795],\n",
      "        [1.2629]], requires_grad=True)\n",
      "Train loss at 684: tensor(2.3808, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5797],\n",
      "        [1.2627]], requires_grad=True)\n",
      "Train loss at 685: tensor(2.3808, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5800],\n",
      "        [1.2625]], requires_grad=True)\n",
      "Train loss at 686: tensor(2.3807, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5803],\n",
      "        [1.2622]], requires_grad=True)\n",
      "Train loss at 687: tensor(2.3807, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5805],\n",
      "        [1.2620]], requires_grad=True)\n",
      "Train loss at 688: tensor(2.3806, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5808],\n",
      "        [1.2618]], requires_grad=True)\n",
      "Train loss at 689: tensor(2.3806, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5811],\n",
      "        [1.2616]], requires_grad=True)\n",
      "Train loss at 690: tensor(2.3806, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5813],\n",
      "        [1.2614]], requires_grad=True)\n",
      "Train loss at 691: tensor(2.3805, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5816],\n",
      "        [1.2612]], requires_grad=True)\n",
      "Train loss at 692: tensor(2.3805, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5818],\n",
      "        [1.2610]], requires_grad=True)\n",
      "Train loss at 693: tensor(2.3804, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5821],\n",
      "        [1.2608]], requires_grad=True)\n",
      "Train loss at 694: tensor(2.3804, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5824],\n",
      "        [1.2606]], requires_grad=True)\n",
      "Train loss at 695: tensor(2.3804, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5826],\n",
      "        [1.2604]], requires_grad=True)\n",
      "Train loss at 696: tensor(2.3803, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5829],\n",
      "        [1.2602]], requires_grad=True)\n",
      "Train loss at 697: tensor(2.3803, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5831],\n",
      "        [1.2600]], requires_grad=True)\n",
      "Train loss at 698: tensor(2.3803, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5834],\n",
      "        [1.2598]], requires_grad=True)\n",
      "Train loss at 699: tensor(2.3802, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5836],\n",
      "        [1.2596]], requires_grad=True)\n",
      "Train loss at 700: tensor(2.3802, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5839],\n",
      "        [1.2594]], requires_grad=True)\n",
      "Train loss at 701: tensor(2.3801, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5841],\n",
      "        [1.2592]], requires_grad=True)\n",
      "Train loss at 702: tensor(2.3801, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5844],\n",
      "        [1.2590]], requires_grad=True)\n",
      "Train loss at 703: tensor(2.3801, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5846],\n",
      "        [1.2588]], requires_grad=True)\n",
      "Train loss at 704: tensor(2.3800, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5849],\n",
      "        [1.2586]], requires_grad=True)\n",
      "Train loss at 705: tensor(2.3800, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5851],\n",
      "        [1.2584]], requires_grad=True)\n",
      "Train loss at 706: tensor(2.3800, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5854],\n",
      "        [1.2582]], requires_grad=True)\n",
      "Train loss at 707: tensor(2.3799, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5856],\n",
      "        [1.2580]], requires_grad=True)\n",
      "Train loss at 708: tensor(2.3799, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5859],\n",
      "        [1.2578]], requires_grad=True)\n",
      "Train loss at 709: tensor(2.3799, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5861],\n",
      "        [1.2576]], requires_grad=True)\n",
      "Train loss at 710: tensor(2.3798, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5864],\n",
      "        [1.2574]], requires_grad=True)\n",
      "Train loss at 711: tensor(2.3798, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5866],\n",
      "        [1.2572]], requires_grad=True)\n",
      "Train loss at 712: tensor(2.3798, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5869],\n",
      "        [1.2570]], requires_grad=True)\n",
      "Train loss at 713: tensor(2.3797, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5871],\n",
      "        [1.2568]], requires_grad=True)\n",
      "Train loss at 714: tensor(2.3797, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5873],\n",
      "        [1.2566]], requires_grad=True)\n",
      "Train loss at 715: tensor(2.3797, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5876],\n",
      "        [1.2564]], requires_grad=True)\n",
      "Train loss at 716: tensor(2.3796, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5878],\n",
      "        [1.2562]], requires_grad=True)\n",
      "Train loss at 717: tensor(2.3796, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5881],\n",
      "        [1.2560]], requires_grad=True)\n",
      "Train loss at 718: tensor(2.3796, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5883],\n",
      "        [1.2558]], requires_grad=True)\n",
      "Train loss at 719: tensor(2.3795, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5885],\n",
      "        [1.2556]], requires_grad=True)\n",
      "Train loss at 720: tensor(2.3795, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5888],\n",
      "        [1.2554]], requires_grad=True)\n",
      "Train loss at 721: tensor(2.3795, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5890],\n",
      "        [1.2553]], requires_grad=True)\n",
      "Train loss at 722: tensor(2.3794, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5892],\n",
      "        [1.2551]], requires_grad=True)\n",
      "Train loss at 723: tensor(2.3794, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5895],\n",
      "        [1.2549]], requires_grad=True)\n",
      "Train loss at 724: tensor(2.3794, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5897],\n",
      "        [1.2547]], requires_grad=True)\n",
      "Train loss at 725: tensor(2.3793, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5899],\n",
      "        [1.2545]], requires_grad=True)\n",
      "Train loss at 726: tensor(2.3793, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5902],\n",
      "        [1.2543]], requires_grad=True)\n",
      "Train loss at 727: tensor(2.3793, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5904],\n",
      "        [1.2541]], requires_grad=True)\n",
      "Train loss at 728: tensor(2.3793, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5906],\n",
      "        [1.2540]], requires_grad=True)\n",
      "Train loss at 729: tensor(2.3792, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5908],\n",
      "        [1.2538]], requires_grad=True)\n",
      "Train loss at 730: tensor(2.3792, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5911],\n",
      "        [1.2536]], requires_grad=True)\n",
      "Train loss at 731: tensor(2.3792, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5913],\n",
      "        [1.2534]], requires_grad=True)\n",
      "Train loss at 732: tensor(2.3791, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5915],\n",
      "        [1.2532]], requires_grad=True)\n",
      "Train loss at 733: tensor(2.3791, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5918],\n",
      "        [1.2530]], requires_grad=True)\n",
      "Train loss at 734: tensor(2.3791, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5920],\n",
      "        [1.2529]], requires_grad=True)\n",
      "Train loss at 735: tensor(2.3790, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5922],\n",
      "        [1.2527]], requires_grad=True)\n",
      "Train loss at 736: tensor(2.3790, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5924],\n",
      "        [1.2525]], requires_grad=True)\n",
      "Train loss at 737: tensor(2.3790, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5926],\n",
      "        [1.2523]], requires_grad=True)\n",
      "Train loss at 738: tensor(2.3790, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5929],\n",
      "        [1.2521]], requires_grad=True)\n",
      "Train loss at 739: tensor(2.3789, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5931],\n",
      "        [1.2520]], requires_grad=True)\n",
      "Train loss at 740: tensor(2.3789, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5933],\n",
      "        [1.2518]], requires_grad=True)\n",
      "Train loss at 741: tensor(2.3789, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5935],\n",
      "        [1.2516]], requires_grad=True)\n",
      "Train loss at 742: tensor(2.3788, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5937],\n",
      "        [1.2514]], requires_grad=True)\n",
      "Train loss at 743: tensor(2.3788, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5940],\n",
      "        [1.2513]], requires_grad=True)\n",
      "Train loss at 744: tensor(2.3788, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5942],\n",
      "        [1.2511]], requires_grad=True)\n",
      "Train loss at 745: tensor(2.3788, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5944],\n",
      "        [1.2509]], requires_grad=True)\n",
      "Train loss at 746: tensor(2.3787, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5946],\n",
      "        [1.2507]], requires_grad=True)\n",
      "Train loss at 747: tensor(2.3787, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5948],\n",
      "        [1.2506]], requires_grad=True)\n",
      "Train loss at 748: tensor(2.3787, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5950],\n",
      "        [1.2504]], requires_grad=True)\n",
      "Train loss at 749: tensor(2.3787, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5952],\n",
      "        [1.2502]], requires_grad=True)\n",
      "Train loss at 750: tensor(2.3786, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5954],\n",
      "        [1.2500]], requires_grad=True)\n",
      "Train loss at 751: tensor(2.3786, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5957],\n",
      "        [1.2499]], requires_grad=True)\n",
      "Train loss at 752: tensor(2.3786, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5959],\n",
      "        [1.2497]], requires_grad=True)\n",
      "Train loss at 753: tensor(2.3786, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5961],\n",
      "        [1.2495]], requires_grad=True)\n",
      "Train loss at 754: tensor(2.3785, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5963],\n",
      "        [1.2494]], requires_grad=True)\n",
      "Train loss at 755: tensor(2.3785, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5965],\n",
      "        [1.2492]], requires_grad=True)\n",
      "Train loss at 756: tensor(2.3785, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5967],\n",
      "        [1.2490]], requires_grad=True)\n",
      "Train loss at 757: tensor(2.3785, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5969],\n",
      "        [1.2489]], requires_grad=True)\n",
      "Train loss at 758: tensor(2.3784, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5971],\n",
      "        [1.2487]], requires_grad=True)\n",
      "Train loss at 759: tensor(2.3784, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5973],\n",
      "        [1.2485]], requires_grad=True)\n",
      "Train loss at 760: tensor(2.3784, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5975],\n",
      "        [1.2484]], requires_grad=True)\n",
      "Train loss at 761: tensor(2.3784, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5977],\n",
      "        [1.2482]], requires_grad=True)\n",
      "Train loss at 762: tensor(2.3783, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5979],\n",
      "        [1.2480]], requires_grad=True)\n",
      "Train loss at 763: tensor(2.3783, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5981],\n",
      "        [1.2479]], requires_grad=True)\n",
      "Train loss at 764: tensor(2.3783, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5983],\n",
      "        [1.2477]], requires_grad=True)\n",
      "Train loss at 765: tensor(2.3783, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5985],\n",
      "        [1.2475]], requires_grad=True)\n",
      "Train loss at 766: tensor(2.3782, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5987],\n",
      "        [1.2474]], requires_grad=True)\n",
      "Train loss at 767: tensor(2.3782, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5989],\n",
      "        [1.2472]], requires_grad=True)\n",
      "Train loss at 768: tensor(2.3782, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5991],\n",
      "        [1.2470]], requires_grad=True)\n",
      "Train loss at 769: tensor(2.3782, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5993],\n",
      "        [1.2469]], requires_grad=True)\n",
      "Train loss at 770: tensor(2.3782, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5995],\n",
      "        [1.2467]], requires_grad=True)\n",
      "Train loss at 771: tensor(2.3781, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5997],\n",
      "        [1.2466]], requires_grad=True)\n",
      "Train loss at 772: tensor(2.3781, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5999],\n",
      "        [1.2464]], requires_grad=True)\n",
      "Train loss at 773: tensor(2.3781, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6001],\n",
      "        [1.2462]], requires_grad=True)\n",
      "Train loss at 774: tensor(2.3781, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6003],\n",
      "        [1.2461]], requires_grad=True)\n",
      "Train loss at 775: tensor(2.3780, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6005],\n",
      "        [1.2459]], requires_grad=True)\n",
      "Train loss at 776: tensor(2.3780, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6007],\n",
      "        [1.2458]], requires_grad=True)\n",
      "Train loss at 777: tensor(2.3780, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6009],\n",
      "        [1.2456]], requires_grad=True)\n",
      "Train loss at 778: tensor(2.3780, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6011],\n",
      "        [1.2455]], requires_grad=True)\n",
      "Train loss at 779: tensor(2.3780, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6013],\n",
      "        [1.2453]], requires_grad=True)\n",
      "Train loss at 780: tensor(2.3779, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6015],\n",
      "        [1.2452]], requires_grad=True)\n",
      "Train loss at 781: tensor(2.3779, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6016],\n",
      "        [1.2450]], requires_grad=True)\n",
      "Train loss at 782: tensor(2.3779, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6018],\n",
      "        [1.2448]], requires_grad=True)\n",
      "Train loss at 783: tensor(2.3779, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6020],\n",
      "        [1.2447]], requires_grad=True)\n",
      "Train loss at 784: tensor(2.3779, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6022],\n",
      "        [1.2445]], requires_grad=True)\n",
      "Train loss at 785: tensor(2.3778, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6024],\n",
      "        [1.2444]], requires_grad=True)\n",
      "Train loss at 786: tensor(2.3778, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6026],\n",
      "        [1.2442]], requires_grad=True)\n",
      "Train loss at 787: tensor(2.3778, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6028],\n",
      "        [1.2441]], requires_grad=True)\n",
      "Train loss at 788: tensor(2.3778, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6029],\n",
      "        [1.2439]], requires_grad=True)\n",
      "Train loss at 789: tensor(2.3778, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6031],\n",
      "        [1.2438]], requires_grad=True)\n",
      "Train loss at 790: tensor(2.3777, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6033],\n",
      "        [1.2436]], requires_grad=True)\n",
      "Train loss at 791: tensor(2.3777, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6035],\n",
      "        [1.2435]], requires_grad=True)\n",
      "Train loss at 792: tensor(2.3777, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6037],\n",
      "        [1.2433]], requires_grad=True)\n",
      "Train loss at 793: tensor(2.3777, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6039],\n",
      "        [1.2432]], requires_grad=True)\n",
      "Train loss at 794: tensor(2.3777, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6040],\n",
      "        [1.2430]], requires_grad=True)\n",
      "Train loss at 795: tensor(2.3776, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6042],\n",
      "        [1.2429]], requires_grad=True)\n",
      "Train loss at 796: tensor(2.3776, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6044],\n",
      "        [1.2427]], requires_grad=True)\n",
      "Train loss at 797: tensor(2.3776, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6046],\n",
      "        [1.2426]], requires_grad=True)\n",
      "Train loss at 798: tensor(2.3776, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6048],\n",
      "        [1.2424]], requires_grad=True)\n",
      "Train loss at 799: tensor(2.3776, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6049],\n",
      "        [1.2423]], requires_grad=True)\n",
      "Train loss at 800: tensor(2.3775, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6051],\n",
      "        [1.2422]], requires_grad=True)\n",
      "Train loss at 801: tensor(2.3775, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6053],\n",
      "        [1.2420]], requires_grad=True)\n",
      "Train loss at 802: tensor(2.3775, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6055],\n",
      "        [1.2419]], requires_grad=True)\n",
      "Train loss at 803: tensor(2.3775, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6056],\n",
      "        [1.2417]], requires_grad=True)\n",
      "Train loss at 804: tensor(2.3775, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6058],\n",
      "        [1.2416]], requires_grad=True)\n",
      "Train loss at 805: tensor(2.3775, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6060],\n",
      "        [1.2414]], requires_grad=True)\n",
      "Train loss at 806: tensor(2.3774, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6062],\n",
      "        [1.2413]], requires_grad=True)\n",
      "Train loss at 807: tensor(2.3774, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6063],\n",
      "        [1.2411]], requires_grad=True)\n",
      "Train loss at 808: tensor(2.3774, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6065],\n",
      "        [1.2410]], requires_grad=True)\n",
      "Train loss at 809: tensor(2.3774, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6067],\n",
      "        [1.2409]], requires_grad=True)\n",
      "Train loss at 810: tensor(2.3774, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6068],\n",
      "        [1.2407]], requires_grad=True)\n",
      "Train loss at 811: tensor(2.3773, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6070],\n",
      "        [1.2406]], requires_grad=True)\n",
      "Train loss at 812: tensor(2.3773, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6072],\n",
      "        [1.2404]], requires_grad=True)\n",
      "Train loss at 813: tensor(2.3773, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6074],\n",
      "        [1.2403]], requires_grad=True)\n",
      "Train loss at 814: tensor(2.3773, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6075],\n",
      "        [1.2402]], requires_grad=True)\n",
      "Train loss at 815: tensor(2.3773, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6077],\n",
      "        [1.2400]], requires_grad=True)\n",
      "Train loss at 816: tensor(2.3773, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6079],\n",
      "        [1.2399]], requires_grad=True)\n",
      "Train loss at 817: tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6080],\n",
      "        [1.2398]], requires_grad=True)\n",
      "Train loss at 818: tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6082],\n",
      "        [1.2396]], requires_grad=True)\n",
      "Train loss at 819: tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6084],\n",
      "        [1.2395]], requires_grad=True)\n",
      "Train loss at 820: tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6085],\n",
      "        [1.2393]], requires_grad=True)\n",
      "Train loss at 821: tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6087],\n",
      "        [1.2392]], requires_grad=True)\n",
      "Train loss at 822: tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6089],\n",
      "        [1.2391]], requires_grad=True)\n",
      "Train loss at 823: tensor(2.3772, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6090],\n",
      "        [1.2389]], requires_grad=True)\n",
      "Train loss at 824: tensor(2.3771, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6092],\n",
      "        [1.2388]], requires_grad=True)\n",
      "Train loss at 825: tensor(2.3771, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6093],\n",
      "        [1.2387]], requires_grad=True)\n",
      "Train loss at 826: tensor(2.3771, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6095],\n",
      "        [1.2385]], requires_grad=True)\n",
      "Train loss at 827: tensor(2.3771, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6097],\n",
      "        [1.2384]], requires_grad=True)\n",
      "Train loss at 828: tensor(2.3771, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6098],\n",
      "        [1.2383]], requires_grad=True)\n",
      "Train loss at 829: tensor(2.3771, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6100],\n",
      "        [1.2381]], requires_grad=True)\n",
      "Train loss at 830: tensor(2.3770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6101],\n",
      "        [1.2380]], requires_grad=True)\n",
      "Train loss at 831: tensor(2.3770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6103],\n",
      "        [1.2379]], requires_grad=True)\n",
      "Train loss at 832: tensor(2.3770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6105],\n",
      "        [1.2378]], requires_grad=True)\n",
      "Train loss at 833: tensor(2.3770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6106],\n",
      "        [1.2376]], requires_grad=True)\n",
      "Train loss at 834: tensor(2.3770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6108],\n",
      "        [1.2375]], requires_grad=True)\n",
      "Train loss at 835: tensor(2.3770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6109],\n",
      "        [1.2374]], requires_grad=True)\n",
      "Train loss at 836: tensor(2.3770, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6111],\n",
      "        [1.2372]], requires_grad=True)\n",
      "Train loss at 837: tensor(2.3769, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6112],\n",
      "        [1.2371]], requires_grad=True)\n",
      "Train loss at 838: tensor(2.3769, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6114],\n",
      "        [1.2370]], requires_grad=True)\n",
      "Train loss at 839: tensor(2.3769, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6116],\n",
      "        [1.2369]], requires_grad=True)\n",
      "Train loss at 840: tensor(2.3769, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6117],\n",
      "        [1.2367]], requires_grad=True)\n",
      "Train loss at 841: tensor(2.3769, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6119],\n",
      "        [1.2366]], requires_grad=True)\n",
      "Train loss at 842: tensor(2.3769, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6120],\n",
      "        [1.2365]], requires_grad=True)\n",
      "Train loss at 843: tensor(2.3769, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6122],\n",
      "        [1.2364]], requires_grad=True)\n",
      "Train loss at 844: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6123],\n",
      "        [1.2362]], requires_grad=True)\n",
      "Train loss at 845: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6125],\n",
      "        [1.2361]], requires_grad=True)\n",
      "Train loss at 846: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6126],\n",
      "        [1.2360]], requires_grad=True)\n",
      "Train loss at 847: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6128],\n",
      "        [1.2359]], requires_grad=True)\n",
      "Train loss at 848: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6129],\n",
      "        [1.2357]], requires_grad=True)\n",
      "Train loss at 849: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6131],\n",
      "        [1.2356]], requires_grad=True)\n",
      "Train loss at 850: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6132],\n",
      "        [1.2355]], requires_grad=True)\n",
      "Train loss at 851: tensor(2.3768, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6134],\n",
      "        [1.2354]], requires_grad=True)\n",
      "Train loss at 852: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6135],\n",
      "        [1.2352]], requires_grad=True)\n",
      "Train loss at 853: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6137],\n",
      "        [1.2351]], requires_grad=True)\n",
      "Train loss at 854: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6138],\n",
      "        [1.2350]], requires_grad=True)\n",
      "Train loss at 855: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6140],\n",
      "        [1.2349]], requires_grad=True)\n",
      "Train loss at 856: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6141],\n",
      "        [1.2348]], requires_grad=True)\n",
      "Train loss at 857: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6142],\n",
      "        [1.2346]], requires_grad=True)\n",
      "Train loss at 858: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6144],\n",
      "        [1.2345]], requires_grad=True)\n",
      "Train loss at 859: tensor(2.3767, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6145],\n",
      "        [1.2344]], requires_grad=True)\n",
      "Train loss at 860: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6147],\n",
      "        [1.2343]], requires_grad=True)\n",
      "Train loss at 861: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6148],\n",
      "        [1.2342]], requires_grad=True)\n",
      "Train loss at 862: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6150],\n",
      "        [1.2341]], requires_grad=True)\n",
      "Train loss at 863: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6151],\n",
      "        [1.2339]], requires_grad=True)\n",
      "Train loss at 864: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6152],\n",
      "        [1.2338]], requires_grad=True)\n",
      "Train loss at 865: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6154],\n",
      "        [1.2337]], requires_grad=True)\n",
      "Train loss at 866: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6155],\n",
      "        [1.2336]], requires_grad=True)\n",
      "Train loss at 867: tensor(2.3766, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6157],\n",
      "        [1.2335]], requires_grad=True)\n",
      "Train loss at 868: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6158],\n",
      "        [1.2334]], requires_grad=True)\n",
      "Train loss at 869: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6159],\n",
      "        [1.2332]], requires_grad=True)\n",
      "Train loss at 870: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6161],\n",
      "        [1.2331]], requires_grad=True)\n",
      "Train loss at 871: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6162],\n",
      "        [1.2330]], requires_grad=True)\n",
      "Train loss at 872: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6164],\n",
      "        [1.2329]], requires_grad=True)\n",
      "Train loss at 873: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6165],\n",
      "        [1.2328]], requires_grad=True)\n",
      "Train loss at 874: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6166],\n",
      "        [1.2327]], requires_grad=True)\n",
      "Train loss at 875: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6168],\n",
      "        [1.2326]], requires_grad=True)\n",
      "Train loss at 876: tensor(2.3765, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6169],\n",
      "        [1.2325]], requires_grad=True)\n",
      "Train loss at 877: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6170],\n",
      "        [1.2323]], requires_grad=True)\n",
      "Train loss at 878: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6172],\n",
      "        [1.2322]], requires_grad=True)\n",
      "Train loss at 879: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6173],\n",
      "        [1.2321]], requires_grad=True)\n",
      "Train loss at 880: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6174],\n",
      "        [1.2320]], requires_grad=True)\n",
      "Train loss at 881: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6176],\n",
      "        [1.2319]], requires_grad=True)\n",
      "Train loss at 882: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6177],\n",
      "        [1.2318]], requires_grad=True)\n",
      "Train loss at 883: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6178],\n",
      "        [1.2317]], requires_grad=True)\n",
      "Train loss at 884: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6180],\n",
      "        [1.2316]], requires_grad=True)\n",
      "Train loss at 885: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6181],\n",
      "        [1.2315]], requires_grad=True)\n",
      "Train loss at 886: tensor(2.3764, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6182],\n",
      "        [1.2314]], requires_grad=True)\n",
      "Train loss at 887: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6184],\n",
      "        [1.2312]], requires_grad=True)\n",
      "Train loss at 888: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6185],\n",
      "        [1.2311]], requires_grad=True)\n",
      "Train loss at 889: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6186],\n",
      "        [1.2310]], requires_grad=True)\n",
      "Train loss at 890: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6188],\n",
      "        [1.2309]], requires_grad=True)\n",
      "Train loss at 891: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6189],\n",
      "        [1.2308]], requires_grad=True)\n",
      "Train loss at 892: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6190],\n",
      "        [1.2307]], requires_grad=True)\n",
      "Train loss at 893: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6191],\n",
      "        [1.2306]], requires_grad=True)\n",
      "Train loss at 894: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6193],\n",
      "        [1.2305]], requires_grad=True)\n",
      "Train loss at 895: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6194],\n",
      "        [1.2304]], requires_grad=True)\n",
      "Train loss at 896: tensor(2.3763, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6195],\n",
      "        [1.2303]], requires_grad=True)\n",
      "Train loss at 897: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6196],\n",
      "        [1.2302]], requires_grad=True)\n",
      "Train loss at 898: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6198],\n",
      "        [1.2301]], requires_grad=True)\n",
      "Train loss at 899: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6199],\n",
      "        [1.2300]], requires_grad=True)\n",
      "Train loss at 900: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6200],\n",
      "        [1.2299]], requires_grad=True)\n",
      "Train loss at 901: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6201],\n",
      "        [1.2298]], requires_grad=True)\n",
      "Train loss at 902: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6203],\n",
      "        [1.2297]], requires_grad=True)\n",
      "Train loss at 903: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6204],\n",
      "        [1.2296]], requires_grad=True)\n",
      "Train loss at 904: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6205],\n",
      "        [1.2295]], requires_grad=True)\n",
      "Train loss at 905: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6206],\n",
      "        [1.2294]], requires_grad=True)\n",
      "Train loss at 906: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6208],\n",
      "        [1.2293]], requires_grad=True)\n",
      "Train loss at 907: tensor(2.3762, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6209],\n",
      "        [1.2292]], requires_grad=True)\n",
      "Train loss at 908: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6210],\n",
      "        [1.2291]], requires_grad=True)\n",
      "Train loss at 909: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6211],\n",
      "        [1.2290]], requires_grad=True)\n",
      "Train loss at 910: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6212],\n",
      "        [1.2289]], requires_grad=True)\n",
      "Train loss at 911: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6214],\n",
      "        [1.2288]], requires_grad=True)\n",
      "Train loss at 912: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6215],\n",
      "        [1.2287]], requires_grad=True)\n",
      "Train loss at 913: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6216],\n",
      "        [1.2286]], requires_grad=True)\n",
      "Train loss at 914: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6217],\n",
      "        [1.2285]], requires_grad=True)\n",
      "Train loss at 915: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6218],\n",
      "        [1.2284]], requires_grad=True)\n",
      "Train loss at 916: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6220],\n",
      "        [1.2283]], requires_grad=True)\n",
      "Train loss at 917: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6221],\n",
      "        [1.2282]], requires_grad=True)\n",
      "Train loss at 918: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6222],\n",
      "        [1.2281]], requires_grad=True)\n",
      "Train loss at 919: tensor(2.3761, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6223],\n",
      "        [1.2280]], requires_grad=True)\n",
      "Train loss at 920: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6224],\n",
      "        [1.2279]], requires_grad=True)\n",
      "Train loss at 921: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6225],\n",
      "        [1.2278]], requires_grad=True)\n",
      "Train loss at 922: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6227],\n",
      "        [1.2277]], requires_grad=True)\n",
      "Train loss at 923: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6228],\n",
      "        [1.2276]], requires_grad=True)\n",
      "Train loss at 924: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6229],\n",
      "        [1.2275]], requires_grad=True)\n",
      "Train loss at 925: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6230],\n",
      "        [1.2274]], requires_grad=True)\n",
      "Train loss at 926: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6231],\n",
      "        [1.2273]], requires_grad=True)\n",
      "Train loss at 927: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6232],\n",
      "        [1.2272]], requires_grad=True)\n",
      "Train loss at 928: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6233],\n",
      "        [1.2271]], requires_grad=True)\n",
      "Train loss at 929: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6235],\n",
      "        [1.2270]], requires_grad=True)\n",
      "Train loss at 930: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6236],\n",
      "        [1.2270]], requires_grad=True)\n",
      "Train loss at 931: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6237],\n",
      "        [1.2269]], requires_grad=True)\n",
      "Train loss at 932: tensor(2.3760, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6238],\n",
      "        [1.2268]], requires_grad=True)\n",
      "Train loss at 933: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6239],\n",
      "        [1.2267]], requires_grad=True)\n",
      "Train loss at 934: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6240],\n",
      "        [1.2266]], requires_grad=True)\n",
      "Train loss at 935: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6241],\n",
      "        [1.2265]], requires_grad=True)\n",
      "Train loss at 936: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6242],\n",
      "        [1.2264]], requires_grad=True)\n",
      "Train loss at 937: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6243],\n",
      "        [1.2263]], requires_grad=True)\n",
      "Train loss at 938: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6245],\n",
      "        [1.2262]], requires_grad=True)\n",
      "Train loss at 939: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6246],\n",
      "        [1.2261]], requires_grad=True)\n",
      "Train loss at 940: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6247],\n",
      "        [1.2260]], requires_grad=True)\n",
      "Train loss at 941: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6248],\n",
      "        [1.2260]], requires_grad=True)\n",
      "Train loss at 942: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6249],\n",
      "        [1.2259]], requires_grad=True)\n",
      "Train loss at 943: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6250],\n",
      "        [1.2258]], requires_grad=True)\n",
      "Train loss at 944: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6251],\n",
      "        [1.2257]], requires_grad=True)\n",
      "Train loss at 945: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6252],\n",
      "        [1.2256]], requires_grad=True)\n",
      "Train loss at 946: tensor(2.3759, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6253],\n",
      "        [1.2255]], requires_grad=True)\n",
      "Train loss at 947: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6254],\n",
      "        [1.2254]], requires_grad=True)\n",
      "Train loss at 948: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6255],\n",
      "        [1.2253]], requires_grad=True)\n",
      "Train loss at 949: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6256],\n",
      "        [1.2253]], requires_grad=True)\n",
      "Train loss at 950: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6257],\n",
      "        [1.2252]], requires_grad=True)\n",
      "Train loss at 951: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6258],\n",
      "        [1.2251]], requires_grad=True)\n",
      "Train loss at 952: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6260],\n",
      "        [1.2250]], requires_grad=True)\n",
      "Train loss at 953: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6261],\n",
      "        [1.2249]], requires_grad=True)\n",
      "Train loss at 954: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6262],\n",
      "        [1.2248]], requires_grad=True)\n",
      "Train loss at 955: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6263],\n",
      "        [1.2247]], requires_grad=True)\n",
      "Train loss at 956: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6264],\n",
      "        [1.2247]], requires_grad=True)\n",
      "Train loss at 957: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6265],\n",
      "        [1.2246]], requires_grad=True)\n",
      "Train loss at 958: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6266],\n",
      "        [1.2245]], requires_grad=True)\n",
      "Train loss at 959: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6267],\n",
      "        [1.2244]], requires_grad=True)\n",
      "Train loss at 960: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6268],\n",
      "        [1.2243]], requires_grad=True)\n",
      "Train loss at 961: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6269],\n",
      "        [1.2242]], requires_grad=True)\n",
      "Train loss at 962: tensor(2.3758, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6270],\n",
      "        [1.2242]], requires_grad=True)\n",
      "Train loss at 963: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6271],\n",
      "        [1.2241]], requires_grad=True)\n",
      "Train loss at 964: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6272],\n",
      "        [1.2240]], requires_grad=True)\n",
      "Train loss at 965: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6273],\n",
      "        [1.2239]], requires_grad=True)\n",
      "Train loss at 966: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6274],\n",
      "        [1.2238]], requires_grad=True)\n",
      "Train loss at 967: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6275],\n",
      "        [1.2237]], requires_grad=True)\n",
      "Train loss at 968: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6276],\n",
      "        [1.2237]], requires_grad=True)\n",
      "Train loss at 969: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6277],\n",
      "        [1.2236]], requires_grad=True)\n",
      "Train loss at 970: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6278],\n",
      "        [1.2235]], requires_grad=True)\n",
      "Train loss at 971: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6279],\n",
      "        [1.2234]], requires_grad=True)\n",
      "Train loss at 972: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6280],\n",
      "        [1.2233]], requires_grad=True)\n",
      "Train loss at 973: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6281],\n",
      "        [1.2233]], requires_grad=True)\n",
      "Train loss at 974: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6282],\n",
      "        [1.2232]], requires_grad=True)\n",
      "Train loss at 975: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6283],\n",
      "        [1.2231]], requires_grad=True)\n",
      "Train loss at 976: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6284],\n",
      "        [1.2230]], requires_grad=True)\n",
      "Train loss at 977: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6284],\n",
      "        [1.2229]], requires_grad=True)\n",
      "Train loss at 978: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6285],\n",
      "        [1.2229]], requires_grad=True)\n",
      "Train loss at 979: tensor(2.3757, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6286],\n",
      "        [1.2228]], requires_grad=True)\n",
      "Train loss at 980: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6287],\n",
      "        [1.2227]], requires_grad=True)\n",
      "Train loss at 981: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6288],\n",
      "        [1.2226]], requires_grad=True)\n",
      "Train loss at 982: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6289],\n",
      "        [1.2226]], requires_grad=True)\n",
      "Train loss at 983: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6290],\n",
      "        [1.2225]], requires_grad=True)\n",
      "Train loss at 984: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6291],\n",
      "        [1.2224]], requires_grad=True)\n",
      "Train loss at 985: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6292],\n",
      "        [1.2223]], requires_grad=True)\n",
      "Train loss at 986: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6293],\n",
      "        [1.2222]], requires_grad=True)\n",
      "Train loss at 987: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6294],\n",
      "        [1.2222]], requires_grad=True)\n",
      "Train loss at 988: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6295],\n",
      "        [1.2221]], requires_grad=True)\n",
      "Train loss at 989: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6296],\n",
      "        [1.2220]], requires_grad=True)\n",
      "Train loss at 990: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6297],\n",
      "        [1.2219]], requires_grad=True)\n",
      "Train loss at 991: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6298],\n",
      "        [1.2219]], requires_grad=True)\n",
      "Train loss at 992: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6298],\n",
      "        [1.2218]], requires_grad=True)\n",
      "Train loss at 993: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6299],\n",
      "        [1.2217]], requires_grad=True)\n",
      "Train loss at 994: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6300],\n",
      "        [1.2216]], requires_grad=True)\n",
      "Train loss at 995: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6301],\n",
      "        [1.2216]], requires_grad=True)\n",
      "Train loss at 996: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6302],\n",
      "        [1.2215]], requires_grad=True)\n",
      "Train loss at 997: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6303],\n",
      "        [1.2214]], requires_grad=True)\n",
      "Train loss at 998: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6304],\n",
      "        [1.2214]], requires_grad=True)\n",
      "Train loss at 999: tensor(2.3756, grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.6305],\n",
      "        [1.2213]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# THIS BLOCK SERVES AS THE SANITY CHECK FOR THE MAIN TRAINING PROCESS\n",
    "torch.manual_seed(1)\n",
    "h_epoch = 10000  # Hyperparameter epoch\n",
    "epoch = 1000  # Epoch for training\n",
    "\n",
    "#Create underlying linear function\n",
    "x = torch.randn((10, 2))\n",
    "true_w = torch.tensor([[3.], [1.]])\n",
    "y = torch.matmul(x, true_w) + torch.randn((10, 1))\n",
    "\n",
    "# Split train_valid\n",
    "x_train = x[:8, ]\n",
    "y_train = y[:8, ]\n",
    "\n",
    "x_valid = x[8:, ]\n",
    "y_valid = y[8:, ]\n",
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[2.5], [1.3]], requires_grad=True)\n",
    "lamb = torch.tensor([3.], requires_grad=True)  #Intentionally high value\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr = 0.001)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])\n",
    "for ep in range(epoch):\n",
    "    total_train_loss = 0\n",
    "    for i in range(len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i])\n",
    "        total_train_loss += train_loss\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / len(x_train)))\n",
    "    print('w: ', w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This Section is using L2 regularization\n",
    "Observations:\n",
    "- The higher the lamb value, the higher the training loss and the more difference between closed-form weight solution and SGD weight solution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat:  tensor([[3.6521],\n",
      "        [1.2001]], grad_fn=<MmBackward>)\n",
      "y_predicted:  tensor([[-6.4720],\n",
      "        [-4.3200],\n",
      "        [ 3.4549],\n",
      "        [-1.4521],\n",
      "        [ 3.0094],\n",
      "        [ 6.3403],\n",
      "        [-0.5180],\n",
      "        [-3.0726]], grad_fn=<MmBackward>)\n",
      "loss:  tensor(2.3745, grad_fn=<MseLossBackward>)\n",
      "true_loss:  tensor(2.9038)\n"
     ]
    }
   ],
   "source": [
    "w_hat = torch.matmul(torch.matmul(torch.inverse(torch.matmul(x_train.T, x_train) + lamb * torch.eye(2)), x_train.T), y_train)\n",
    "print('w_hat: ', w_hat)\n",
    "y_train_predicted = torch.matmul(x_train, w_hat)\n",
    "print('y_predicted: ', y_train_predicted)\n",
    "loss = torch.nn.functional.mse_loss(y_train_predicted, y_train)\n",
    "print('loss: ', loss)\n",
    "true_loss = torch.nn.functional.mse_loss(torch.matmul(x_train, true_w), y_train)\n",
    "print('true_loss: ', true_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 0: tensor([3.7132], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5056],\n",
      "        [1.3033]], requires_grad=True)\n",
      "Train loss at 1: tensor([3.7002], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5104],\n",
      "        [1.3051]], requires_grad=True)\n",
      "Train loss at 2: tensor([3.6889], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5152],\n",
      "        [1.3069]], requires_grad=True)\n",
      "Train loss at 3: tensor([3.6779], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5199],\n",
      "        [1.3086]], requires_grad=True)\n",
      "Train loss at 4: tensor([3.6671], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5246],\n",
      "        [1.3103]], requires_grad=True)\n",
      "Train loss at 5: tensor([3.6563], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5293],\n",
      "        [1.3119]], requires_grad=True)\n",
      "Train loss at 6: tensor([3.6456], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5340],\n",
      "        [1.3136]], requires_grad=True)\n",
      "Train loss at 7: tensor([3.6350], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5387],\n",
      "        [1.3152]], requires_grad=True)\n",
      "Train loss at 8: tensor([3.6244], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5433],\n",
      "        [1.3168]], requires_grad=True)\n",
      "Train loss at 9: tensor([3.6140], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5480],\n",
      "        [1.3184]], requires_grad=True)\n",
      "Train loss at 10: tensor([3.6036], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5527],\n",
      "        [1.3200]], requires_grad=True)\n",
      "Train loss at 11: tensor([3.5933], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5573],\n",
      "        [1.3216]], requires_grad=True)\n",
      "Train loss at 12: tensor([3.5831], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5619],\n",
      "        [1.3231]], requires_grad=True)\n",
      "Train loss at 13: tensor([3.5730], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5665],\n",
      "        [1.3246]], requires_grad=True)\n",
      "Train loss at 14: tensor([3.5630], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5711],\n",
      "        [1.3262]], requires_grad=True)\n",
      "Train loss at 15: tensor([3.5531], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5757],\n",
      "        [1.3277]], requires_grad=True)\n",
      "Train loss at 16: tensor([3.5433], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5803],\n",
      "        [1.3292]], requires_grad=True)\n",
      "Train loss at 17: tensor([3.5335], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5848],\n",
      "        [1.3306]], requires_grad=True)\n",
      "Train loss at 18: tensor([3.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5894],\n",
      "        [1.3321]], requires_grad=True)\n",
      "Train loss at 19: tensor([3.5143], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5939],\n",
      "        [1.3335]], requires_grad=True)\n",
      "Train loss at 20: tensor([3.5048], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.5984],\n",
      "        [1.3350]], requires_grad=True)\n",
      "Train loss at 21: tensor([3.4955], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6029],\n",
      "        [1.3364]], requires_grad=True)\n",
      "Train loss at 22: tensor([3.4862], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6073],\n",
      "        [1.3378]], requires_grad=True)\n",
      "Train loss at 23: tensor([3.4770], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6118],\n",
      "        [1.3392]], requires_grad=True)\n",
      "Train loss at 24: tensor([3.4678], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6162],\n",
      "        [1.3405]], requires_grad=True)\n",
      "Train loss at 25: tensor([3.4588], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6206],\n",
      "        [1.3419]], requires_grad=True)\n",
      "Train loss at 26: tensor([3.4499], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6250],\n",
      "        [1.3432]], requires_grad=True)\n",
      "Train loss at 27: tensor([3.4410], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6294],\n",
      "        [1.3445]], requires_grad=True)\n",
      "Train loss at 28: tensor([3.4323], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6338],\n",
      "        [1.3458]], requires_grad=True)\n",
      "Train loss at 29: tensor([3.4236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6381],\n",
      "        [1.3471]], requires_grad=True)\n",
      "Train loss at 30: tensor([3.4150], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6425],\n",
      "        [1.3484]], requires_grad=True)\n",
      "Train loss at 31: tensor([3.4065], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6468],\n",
      "        [1.3497]], requires_grad=True)\n",
      "Train loss at 32: tensor([3.3980], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6511],\n",
      "        [1.3509]], requires_grad=True)\n",
      "Train loss at 33: tensor([3.3897], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6553],\n",
      "        [1.3522]], requires_grad=True)\n",
      "Train loss at 34: tensor([3.3814], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6596],\n",
      "        [1.3534]], requires_grad=True)\n",
      "Train loss at 35: tensor([3.3732], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6638],\n",
      "        [1.3546]], requires_grad=True)\n",
      "Train loss at 36: tensor([3.3651], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6681],\n",
      "        [1.3558]], requires_grad=True)\n",
      "Train loss at 37: tensor([3.3571], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6723],\n",
      "        [1.3570]], requires_grad=True)\n",
      "Train loss at 38: tensor([3.3491], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6765],\n",
      "        [1.3581]], requires_grad=True)\n",
      "Train loss at 39: tensor([3.3413], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6806],\n",
      "        [1.3593]], requires_grad=True)\n",
      "Train loss at 40: tensor([3.3335], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6848],\n",
      "        [1.3604]], requires_grad=True)\n",
      "Train loss at 41: tensor([3.3257], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6889],\n",
      "        [1.3616]], requires_grad=True)\n",
      "Train loss at 42: tensor([3.3181], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6931],\n",
      "        [1.3627]], requires_grad=True)\n",
      "Train loss at 43: tensor([3.3105], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.6972],\n",
      "        [1.3638]], requires_grad=True)\n",
      "Train loss at 44: tensor([3.3030], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7012],\n",
      "        [1.3648]], requires_grad=True)\n",
      "Train loss at 45: tensor([3.2956], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7053],\n",
      "        [1.3659]], requires_grad=True)\n",
      "Train loss at 46: tensor([3.2883], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7094],\n",
      "        [1.3670]], requires_grad=True)\n",
      "Train loss at 47: tensor([3.2810], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7134],\n",
      "        [1.3680]], requires_grad=True)\n",
      "Train loss at 48: tensor([3.2738], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7174],\n",
      "        [1.3690]], requires_grad=True)\n",
      "Train loss at 49: tensor([3.2666], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7214],\n",
      "        [1.3701]], requires_grad=True)\n",
      "Train loss at 50: tensor([3.2596], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7254],\n",
      "        [1.3711]], requires_grad=True)\n",
      "Train loss at 51: tensor([3.2526], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7294],\n",
      "        [1.3721]], requires_grad=True)\n",
      "Train loss at 52: tensor([3.2457], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7334],\n",
      "        [1.3730]], requires_grad=True)\n",
      "Train loss at 53: tensor([3.2388], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7373],\n",
      "        [1.3740]], requires_grad=True)\n",
      "Train loss at 54: tensor([3.2320], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7412],\n",
      "        [1.3750]], requires_grad=True)\n",
      "Train loss at 55: tensor([3.2253], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7451],\n",
      "        [1.3759]], requires_grad=True)\n",
      "Train loss at 56: tensor([3.2187], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7490],\n",
      "        [1.3768]], requires_grad=True)\n",
      "Train loss at 57: tensor([3.2121], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7529],\n",
      "        [1.3777]], requires_grad=True)\n",
      "Train loss at 58: tensor([3.2055], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7567],\n",
      "        [1.3786]], requires_grad=True)\n",
      "Train loss at 59: tensor([3.1991], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7606],\n",
      "        [1.3795]], requires_grad=True)\n",
      "Train loss at 60: tensor([3.1927], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7644],\n",
      "        [1.3804]], requires_grad=True)\n",
      "Train loss at 61: tensor([3.1864], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7682],\n",
      "        [1.3813]], requires_grad=True)\n",
      "Train loss at 62: tensor([3.1801], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7720],\n",
      "        [1.3821]], requires_grad=True)\n",
      "Train loss at 63: tensor([3.1739], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7757],\n",
      "        [1.3830]], requires_grad=True)\n",
      "Train loss at 64: tensor([3.1677], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7795],\n",
      "        [1.3838]], requires_grad=True)\n",
      "Train loss at 65: tensor([3.1617], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7832],\n",
      "        [1.3846]], requires_grad=True)\n",
      "Train loss at 66: tensor([3.1556], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7870],\n",
      "        [1.3854]], requires_grad=True)\n",
      "Train loss at 67: tensor([3.1497], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7907],\n",
      "        [1.3862]], requires_grad=True)\n",
      "Train loss at 68: tensor([3.1438], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7944],\n",
      "        [1.3870]], requires_grad=True)\n",
      "Train loss at 69: tensor([3.1379], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.7980],\n",
      "        [1.3878]], requires_grad=True)\n",
      "Train loss at 70: tensor([3.1321], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8017],\n",
      "        [1.3885]], requires_grad=True)\n",
      "Train loss at 71: tensor([3.1264], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8053],\n",
      "        [1.3893]], requires_grad=True)\n",
      "Train loss at 72: tensor([3.1207], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8090],\n",
      "        [1.3900]], requires_grad=True)\n",
      "Train loss at 73: tensor([3.1151], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8126],\n",
      "        [1.3907]], requires_grad=True)\n",
      "Train loss at 74: tensor([3.1096], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8162],\n",
      "        [1.3915]], requires_grad=True)\n",
      "Train loss at 75: tensor([3.1041], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8198],\n",
      "        [1.3922]], requires_grad=True)\n",
      "Train loss at 76: tensor([3.0986], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8233],\n",
      "        [1.3928]], requires_grad=True)\n",
      "Train loss at 77: tensor([3.0932], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8269],\n",
      "        [1.3935]], requires_grad=True)\n",
      "Train loss at 78: tensor([3.0879], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8304],\n",
      "        [1.3942]], requires_grad=True)\n",
      "Train loss at 79: tensor([3.0826], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8339],\n",
      "        [1.3949]], requires_grad=True)\n",
      "Train loss at 80: tensor([3.0773], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8374],\n",
      "        [1.3955]], requires_grad=True)\n",
      "Train loss at 81: tensor([3.0722], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8409],\n",
      "        [1.3961]], requires_grad=True)\n",
      "Train loss at 82: tensor([3.0670], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8444],\n",
      "        [1.3968]], requires_grad=True)\n",
      "Train loss at 83: tensor([3.0619], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8479],\n",
      "        [1.3974]], requires_grad=True)\n",
      "Train loss at 84: tensor([3.0569], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8513],\n",
      "        [1.3980]], requires_grad=True)\n",
      "Train loss at 85: tensor([3.0519], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8547],\n",
      "        [1.3986]], requires_grad=True)\n",
      "Train loss at 86: tensor([3.0470], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8581],\n",
      "        [1.3992]], requires_grad=True)\n",
      "Train loss at 87: tensor([3.0421], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8615],\n",
      "        [1.3997]], requires_grad=True)\n",
      "Train loss at 88: tensor([3.0373], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8649],\n",
      "        [1.4003]], requires_grad=True)\n",
      "Train loss at 89: tensor([3.0325], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8683],\n",
      "        [1.4009]], requires_grad=True)\n",
      "Train loss at 90: tensor([3.0277], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8716],\n",
      "        [1.4014]], requires_grad=True)\n",
      "Train loss at 91: tensor([3.0231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8750],\n",
      "        [1.4019]], requires_grad=True)\n",
      "Train loss at 92: tensor([3.0184], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8783],\n",
      "        [1.4025]], requires_grad=True)\n",
      "Train loss at 93: tensor([3.0138], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8816],\n",
      "        [1.4030]], requires_grad=True)\n",
      "Train loss at 94: tensor([3.0093], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8849],\n",
      "        [1.4035]], requires_grad=True)\n",
      "Train loss at 95: tensor([3.0047], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8881],\n",
      "        [1.4040]], requires_grad=True)\n",
      "Train loss at 96: tensor([3.0003], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8914],\n",
      "        [1.4045]], requires_grad=True)\n",
      "Train loss at 97: tensor([2.9959], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8947],\n",
      "        [1.4049]], requires_grad=True)\n",
      "Train loss at 98: tensor([2.9915], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.8979],\n",
      "        [1.4054]], requires_grad=True)\n",
      "Train loss at 99: tensor([2.9872], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9011],\n",
      "        [1.4059]], requires_grad=True)\n",
      "Train loss at 100: tensor([2.9829], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9043],\n",
      "        [1.4063]], requires_grad=True)\n",
      "Train loss at 101: tensor([2.9786], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9075],\n",
      "        [1.4067]], requires_grad=True)\n",
      "Train loss at 102: tensor([2.9744], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9107],\n",
      "        [1.4072]], requires_grad=True)\n",
      "Train loss at 103: tensor([2.9703], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9138],\n",
      "        [1.4076]], requires_grad=True)\n",
      "Train loss at 104: tensor([2.9661], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9170],\n",
      "        [1.4080]], requires_grad=True)\n",
      "Train loss at 105: tensor([2.9621], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9201],\n",
      "        [1.4084]], requires_grad=True)\n",
      "Train loss at 106: tensor([2.9580], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9232],\n",
      "        [1.4088]], requires_grad=True)\n",
      "Train loss at 107: tensor([2.9540], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9263],\n",
      "        [1.4092]], requires_grad=True)\n",
      "Train loss at 108: tensor([2.9500], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9294],\n",
      "        [1.4096]], requires_grad=True)\n",
      "Train loss at 109: tensor([2.9461], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9325],\n",
      "        [1.4099]], requires_grad=True)\n",
      "Train loss at 110: tensor([2.9422], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9356],\n",
      "        [1.4103]], requires_grad=True)\n",
      "Train loss at 111: tensor([2.9384], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9386],\n",
      "        [1.4106]], requires_grad=True)\n",
      "Train loss at 112: tensor([2.9346], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9416],\n",
      "        [1.4110]], requires_grad=True)\n",
      "Train loss at 113: tensor([2.9308], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9447],\n",
      "        [1.4113]], requires_grad=True)\n",
      "Train loss at 114: tensor([2.9271], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9477],\n",
      "        [1.4116]], requires_grad=True)\n",
      "Train loss at 115: tensor([2.9234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9506],\n",
      "        [1.4119]], requires_grad=True)\n",
      "Train loss at 116: tensor([2.9197], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9536],\n",
      "        [1.4122]], requires_grad=True)\n",
      "Train loss at 117: tensor([2.9161], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9566],\n",
      "        [1.4125]], requires_grad=True)\n",
      "Train loss at 118: tensor([2.9125], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9595],\n",
      "        [1.4128]], requires_grad=True)\n",
      "Train loss at 119: tensor([2.9090], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9625],\n",
      "        [1.4131]], requires_grad=True)\n",
      "Train loss at 120: tensor([2.9055], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9654],\n",
      "        [1.4134]], requires_grad=True)\n",
      "Train loss at 121: tensor([2.9020], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9683],\n",
      "        [1.4137]], requires_grad=True)\n",
      "Train loss at 122: tensor([2.8985], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9712],\n",
      "        [1.4139]], requires_grad=True)\n",
      "Train loss at 123: tensor([2.8951], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9741],\n",
      "        [1.4142]], requires_grad=True)\n",
      "Train loss at 124: tensor([2.8918], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9770],\n",
      "        [1.4144]], requires_grad=True)\n",
      "Train loss at 125: tensor([2.8884], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9798],\n",
      "        [1.4146]], requires_grad=True)\n",
      "Train loss at 126: tensor([2.8851], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9827],\n",
      "        [1.4149]], requires_grad=True)\n",
      "Train loss at 127: tensor([2.8818], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9855],\n",
      "        [1.4151]], requires_grad=True)\n",
      "Train loss at 128: tensor([2.8786], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9883],\n",
      "        [1.4153]], requires_grad=True)\n",
      "Train loss at 129: tensor([2.8754], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9911],\n",
      "        [1.4155]], requires_grad=True)\n",
      "Train loss at 130: tensor([2.8722], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9939],\n",
      "        [1.4157]], requires_grad=True)\n",
      "Train loss at 131: tensor([2.8690], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9967],\n",
      "        [1.4159]], requires_grad=True)\n",
      "Train loss at 132: tensor([2.8659], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[2.9994],\n",
      "        [1.4161]], requires_grad=True)\n",
      "Train loss at 133: tensor([2.8628], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0022],\n",
      "        [1.4163]], requires_grad=True)\n",
      "Train loss at 134: tensor([2.8598], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0049],\n",
      "        [1.4164]], requires_grad=True)\n",
      "Train loss at 135: tensor([2.8567], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0077],\n",
      "        [1.4166]], requires_grad=True)\n",
      "Train loss at 136: tensor([2.8537], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0104],\n",
      "        [1.4167]], requires_grad=True)\n",
      "Train loss at 137: tensor([2.8507], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0131],\n",
      "        [1.4169]], requires_grad=True)\n",
      "Train loss at 138: tensor([2.8478], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0158],\n",
      "        [1.4170]], requires_grad=True)\n",
      "Train loss at 139: tensor([2.8449], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0184],\n",
      "        [1.4172]], requires_grad=True)\n",
      "Train loss at 140: tensor([2.8420], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0211],\n",
      "        [1.4173]], requires_grad=True)\n",
      "Train loss at 141: tensor([2.8392], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0238],\n",
      "        [1.4174]], requires_grad=True)\n",
      "Train loss at 142: tensor([2.8363], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0264],\n",
      "        [1.4175]], requires_grad=True)\n",
      "Train loss at 143: tensor([2.8335], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0290],\n",
      "        [1.4176]], requires_grad=True)\n",
      "Train loss at 144: tensor([2.8308], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0316],\n",
      "        [1.4177]], requires_grad=True)\n",
      "Train loss at 145: tensor([2.8280], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0342],\n",
      "        [1.4178]], requires_grad=True)\n",
      "Train loss at 146: tensor([2.8253], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0368],\n",
      "        [1.4179]], requires_grad=True)\n",
      "Train loss at 147: tensor([2.8226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0394],\n",
      "        [1.4180]], requires_grad=True)\n",
      "Train loss at 148: tensor([2.8199], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0420],\n",
      "        [1.4181]], requires_grad=True)\n",
      "Train loss at 149: tensor([2.8173], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0445],\n",
      "        [1.4181]], requires_grad=True)\n",
      "Train loss at 150: tensor([2.8147], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0471],\n",
      "        [1.4182]], requires_grad=True)\n",
      "Train loss at 151: tensor([2.8121], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0496],\n",
      "        [1.4182]], requires_grad=True)\n",
      "Train loss at 152: tensor([2.8095], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0521],\n",
      "        [1.4183]], requires_grad=True)\n",
      "Train loss at 153: tensor([2.8070], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0546],\n",
      "        [1.4183]], requires_grad=True)\n",
      "Train loss at 154: tensor([2.8045], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0571],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 155: tensor([2.8020], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0596],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 156: tensor([2.7995], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0621],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 157: tensor([2.7971], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0645],\n",
      "        [1.4185]], requires_grad=True)\n",
      "Train loss at 158: tensor([2.7946], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0670],\n",
      "        [1.4185]], requires_grad=True)\n",
      "Train loss at 159: tensor([2.7922], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0694],\n",
      "        [1.4185]], requires_grad=True)\n",
      "Train loss at 160: tensor([2.7899], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0718],\n",
      "        [1.4185]], requires_grad=True)\n",
      "Train loss at 161: tensor([2.7875], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0742],\n",
      "        [1.4185]], requires_grad=True)\n",
      "Train loss at 162: tensor([2.7852], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0766],\n",
      "        [1.4185]], requires_grad=True)\n",
      "Train loss at 163: tensor([2.7829], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0790],\n",
      "        [1.4185]], requires_grad=True)\n",
      "Train loss at 164: tensor([2.7806], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0814],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 165: tensor([2.7783], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0838],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 166: tensor([2.7761], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0861],\n",
      "        [1.4184]], requires_grad=True)\n",
      "Train loss at 167: tensor([2.7739], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0885],\n",
      "        [1.4183]], requires_grad=True)\n",
      "Train loss at 168: tensor([2.7717], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0908],\n",
      "        [1.4183]], requires_grad=True)\n",
      "Train loss at 169: tensor([2.7695], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0931],\n",
      "        [1.4183]], requires_grad=True)\n",
      "Train loss at 170: tensor([2.7674], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0954],\n",
      "        [1.4182]], requires_grad=True)\n",
      "Train loss at 171: tensor([2.7652], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.0977],\n",
      "        [1.4181]], requires_grad=True)\n",
      "Train loss at 172: tensor([2.7631], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1000],\n",
      "        [1.4181]], requires_grad=True)\n",
      "Train loss at 173: tensor([2.7610], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1023],\n",
      "        [1.4180]], requires_grad=True)\n",
      "Train loss at 174: tensor([2.7590], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1046],\n",
      "        [1.4179]], requires_grad=True)\n",
      "Train loss at 175: tensor([2.7569], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1068],\n",
      "        [1.4179]], requires_grad=True)\n",
      "Train loss at 176: tensor([2.7549], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1091],\n",
      "        [1.4178]], requires_grad=True)\n",
      "Train loss at 177: tensor([2.7529], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1113],\n",
      "        [1.4177]], requires_grad=True)\n",
      "Train loss at 178: tensor([2.7509], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1136],\n",
      "        [1.4176]], requires_grad=True)\n",
      "Train loss at 179: tensor([2.7489], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1158],\n",
      "        [1.4175]], requires_grad=True)\n",
      "Train loss at 180: tensor([2.7469], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1180],\n",
      "        [1.4174]], requires_grad=True)\n",
      "Train loss at 181: tensor([2.7450], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1202],\n",
      "        [1.4173]], requires_grad=True)\n",
      "Train loss at 182: tensor([2.7431], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1223],\n",
      "        [1.4172]], requires_grad=True)\n",
      "Train loss at 183: tensor([2.7412], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1245],\n",
      "        [1.4171]], requires_grad=True)\n",
      "Train loss at 184: tensor([2.7393], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1267],\n",
      "        [1.4170]], requires_grad=True)\n",
      "Train loss at 185: tensor([2.7374], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1288],\n",
      "        [1.4168]], requires_grad=True)\n",
      "Train loss at 186: tensor([2.7356], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1310],\n",
      "        [1.4167]], requires_grad=True)\n",
      "Train loss at 187: tensor([2.7338], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1331],\n",
      "        [1.4166]], requires_grad=True)\n",
      "Train loss at 188: tensor([2.7319], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1352],\n",
      "        [1.4164]], requires_grad=True)\n",
      "Train loss at 189: tensor([2.7301], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1373],\n",
      "        [1.4163]], requires_grad=True)\n",
      "Train loss at 190: tensor([2.7284], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1394],\n",
      "        [1.4162]], requires_grad=True)\n",
      "Train loss at 191: tensor([2.7266], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1415],\n",
      "        [1.4160]], requires_grad=True)\n",
      "Train loss at 192: tensor([2.7249], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1436],\n",
      "        [1.4159]], requires_grad=True)\n",
      "Train loss at 193: tensor([2.7231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1457],\n",
      "        [1.4157]], requires_grad=True)\n",
      "Train loss at 194: tensor([2.7214], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1477],\n",
      "        [1.4155]], requires_grad=True)\n",
      "Train loss at 195: tensor([2.7197], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1498],\n",
      "        [1.4154]], requires_grad=True)\n",
      "Train loss at 196: tensor([2.7180], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1518],\n",
      "        [1.4152]], requires_grad=True)\n",
      "Train loss at 197: tensor([2.7164], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1539],\n",
      "        [1.4150]], requires_grad=True)\n",
      "Train loss at 198: tensor([2.7147], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1559],\n",
      "        [1.4148]], requires_grad=True)\n",
      "Train loss at 199: tensor([2.7131], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1579],\n",
      "        [1.4147]], requires_grad=True)\n",
      "Train loss at 200: tensor([2.7115], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1599],\n",
      "        [1.4145]], requires_grad=True)\n",
      "Train loss at 201: tensor([2.7099], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1619],\n",
      "        [1.4143]], requires_grad=True)\n",
      "Train loss at 202: tensor([2.7083], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1639],\n",
      "        [1.4141]], requires_grad=True)\n",
      "Train loss at 203: tensor([2.7067], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1659],\n",
      "        [1.4139]], requires_grad=True)\n",
      "Train loss at 204: tensor([2.7052], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1678],\n",
      "        [1.4137]], requires_grad=True)\n",
      "Train loss at 205: tensor([2.7036], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1698],\n",
      "        [1.4135]], requires_grad=True)\n",
      "Train loss at 206: tensor([2.7021], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1717],\n",
      "        [1.4133]], requires_grad=True)\n",
      "Train loss at 207: tensor([2.7006], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1737],\n",
      "        [1.4131]], requires_grad=True)\n",
      "Train loss at 208: tensor([2.6991], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1756],\n",
      "        [1.4129]], requires_grad=True)\n",
      "Train loss at 209: tensor([2.6976], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1775],\n",
      "        [1.4127]], requires_grad=True)\n",
      "Train loss at 210: tensor([2.6961], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1794],\n",
      "        [1.4124]], requires_grad=True)\n",
      "Train loss at 211: tensor([2.6946], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1813],\n",
      "        [1.4122]], requires_grad=True)\n",
      "Train loss at 212: tensor([2.6932], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1832],\n",
      "        [1.4120]], requires_grad=True)\n",
      "Train loss at 213: tensor([2.6918], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1851],\n",
      "        [1.4118]], requires_grad=True)\n",
      "Train loss at 214: tensor([2.6904], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1870],\n",
      "        [1.4115]], requires_grad=True)\n",
      "Train loss at 215: tensor([2.6889], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1888],\n",
      "        [1.4113]], requires_grad=True)\n",
      "Train loss at 216: tensor([2.6876], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1907],\n",
      "        [1.4110]], requires_grad=True)\n",
      "Train loss at 217: tensor([2.6862], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1925],\n",
      "        [1.4108]], requires_grad=True)\n",
      "Train loss at 218: tensor([2.6848], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1944],\n",
      "        [1.4106]], requires_grad=True)\n",
      "Train loss at 219: tensor([2.6834], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1962],\n",
      "        [1.4103]], requires_grad=True)\n",
      "Train loss at 220: tensor([2.6821], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1980],\n",
      "        [1.4100]], requires_grad=True)\n",
      "Train loss at 221: tensor([2.6808], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.1998],\n",
      "        [1.4098]], requires_grad=True)\n",
      "Train loss at 222: tensor([2.6795], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2016],\n",
      "        [1.4095]], requires_grad=True)\n",
      "Train loss at 223: tensor([2.6781], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2034],\n",
      "        [1.4093]], requires_grad=True)\n",
      "Train loss at 224: tensor([2.6769], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2052],\n",
      "        [1.4090]], requires_grad=True)\n",
      "Train loss at 225: tensor([2.6756], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2070],\n",
      "        [1.4087]], requires_grad=True)\n",
      "Train loss at 226: tensor([2.6743], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2088],\n",
      "        [1.4085]], requires_grad=True)\n",
      "Train loss at 227: tensor([2.6730], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2105],\n",
      "        [1.4082]], requires_grad=True)\n",
      "Train loss at 228: tensor([2.6718], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2123],\n",
      "        [1.4079]], requires_grad=True)\n",
      "Train loss at 229: tensor([2.6706], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2140],\n",
      "        [1.4076]], requires_grad=True)\n",
      "Train loss at 230: tensor([2.6693], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2158],\n",
      "        [1.4074]], requires_grad=True)\n",
      "Train loss at 231: tensor([2.6681], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2175],\n",
      "        [1.4071]], requires_grad=True)\n",
      "Train loss at 232: tensor([2.6669], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2192],\n",
      "        [1.4068]], requires_grad=True)\n",
      "Train loss at 233: tensor([2.6657], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2209],\n",
      "        [1.4065]], requires_grad=True)\n",
      "Train loss at 234: tensor([2.6645], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2226],\n",
      "        [1.4062]], requires_grad=True)\n",
      "Train loss at 235: tensor([2.6634], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2243],\n",
      "        [1.4059]], requires_grad=True)\n",
      "Train loss at 236: tensor([2.6622], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2260],\n",
      "        [1.4056]], requires_grad=True)\n",
      "Train loss at 237: tensor([2.6611], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2277],\n",
      "        [1.4053]], requires_grad=True)\n",
      "Train loss at 238: tensor([2.6599], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2293],\n",
      "        [1.4050]], requires_grad=True)\n",
      "Train loss at 239: tensor([2.6588], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2310],\n",
      "        [1.4047]], requires_grad=True)\n",
      "Train loss at 240: tensor([2.6577], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2327],\n",
      "        [1.4044]], requires_grad=True)\n",
      "Train loss at 241: tensor([2.6566], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2343],\n",
      "        [1.4041]], requires_grad=True)\n",
      "Train loss at 242: tensor([2.6555], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2359],\n",
      "        [1.4038]], requires_grad=True)\n",
      "Train loss at 243: tensor([2.6544], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2376],\n",
      "        [1.4035]], requires_grad=True)\n",
      "Train loss at 244: tensor([2.6533], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2392],\n",
      "        [1.4032]], requires_grad=True)\n",
      "Train loss at 245: tensor([2.6522], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2408],\n",
      "        [1.4029]], requires_grad=True)\n",
      "Train loss at 246: tensor([2.6512], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2424],\n",
      "        [1.4025]], requires_grad=True)\n",
      "Train loss at 247: tensor([2.6501], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2440],\n",
      "        [1.4022]], requires_grad=True)\n",
      "Train loss at 248: tensor([2.6491], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2456],\n",
      "        [1.4019]], requires_grad=True)\n",
      "Train loss at 249: tensor([2.6481], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2472],\n",
      "        [1.4016]], requires_grad=True)\n",
      "Train loss at 250: tensor([2.6470], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2488],\n",
      "        [1.4012]], requires_grad=True)\n",
      "Train loss at 251: tensor([2.6460], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2503],\n",
      "        [1.4009]], requires_grad=True)\n",
      "Train loss at 252: tensor([2.6450], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2519],\n",
      "        [1.4006]], requires_grad=True)\n",
      "Train loss at 253: tensor([2.6440], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2535],\n",
      "        [1.4003]], requires_grad=True)\n",
      "Train loss at 254: tensor([2.6430], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2550],\n",
      "        [1.3999]], requires_grad=True)\n",
      "Train loss at 255: tensor([2.6421], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2565],\n",
      "        [1.3996]], requires_grad=True)\n",
      "Train loss at 256: tensor([2.6411], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2581],\n",
      "        [1.3992]], requires_grad=True)\n",
      "Train loss at 257: tensor([2.6401], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2596],\n",
      "        [1.3989]], requires_grad=True)\n",
      "Train loss at 258: tensor([2.6392], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2611],\n",
      "        [1.3986]], requires_grad=True)\n",
      "Train loss at 259: tensor([2.6382], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2626],\n",
      "        [1.3982]], requires_grad=True)\n",
      "Train loss at 260: tensor([2.6373], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2641],\n",
      "        [1.3979]], requires_grad=True)\n",
      "Train loss at 261: tensor([2.6364], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2656],\n",
      "        [1.3975]], requires_grad=True)\n",
      "Train loss at 262: tensor([2.6354], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2671],\n",
      "        [1.3972]], requires_grad=True)\n",
      "Train loss at 263: tensor([2.6345], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2686],\n",
      "        [1.3968]], requires_grad=True)\n",
      "Train loss at 264: tensor([2.6336], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2701],\n",
      "        [1.3965]], requires_grad=True)\n",
      "Train loss at 265: tensor([2.6327], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2715],\n",
      "        [1.3961]], requires_grad=True)\n",
      "Train loss at 266: tensor([2.6319], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2730],\n",
      "        [1.3958]], requires_grad=True)\n",
      "Train loss at 267: tensor([2.6310], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2745],\n",
      "        [1.3954]], requires_grad=True)\n",
      "Train loss at 268: tensor([2.6301], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2759],\n",
      "        [1.3950]], requires_grad=True)\n",
      "Train loss at 269: tensor([2.6292], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2773],\n",
      "        [1.3947]], requires_grad=True)\n",
      "Train loss at 270: tensor([2.6284], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2788],\n",
      "        [1.3943]], requires_grad=True)\n",
      "Train loss at 271: tensor([2.6275], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2802],\n",
      "        [1.3940]], requires_grad=True)\n",
      "Train loss at 272: tensor([2.6267], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2816],\n",
      "        [1.3936]], requires_grad=True)\n",
      "Train loss at 273: tensor([2.6259], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2830],\n",
      "        [1.3932]], requires_grad=True)\n",
      "Train loss at 274: tensor([2.6250], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2844],\n",
      "        [1.3929]], requires_grad=True)\n",
      "Train loss at 275: tensor([2.6242], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2858],\n",
      "        [1.3925]], requires_grad=True)\n",
      "Train loss at 276: tensor([2.6234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2872],\n",
      "        [1.3921]], requires_grad=True)\n",
      "Train loss at 277: tensor([2.6226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2886],\n",
      "        [1.3917]], requires_grad=True)\n",
      "Train loss at 278: tensor([2.6218], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2900],\n",
      "        [1.3914]], requires_grad=True)\n",
      "Train loss at 279: tensor([2.6210], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2914],\n",
      "        [1.3910]], requires_grad=True)\n",
      "Train loss at 280: tensor([2.6202], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2927],\n",
      "        [1.3906]], requires_grad=True)\n",
      "Train loss at 281: tensor([2.6194], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2941],\n",
      "        [1.3902]], requires_grad=True)\n",
      "Train loss at 282: tensor([2.6187], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2955],\n",
      "        [1.3899]], requires_grad=True)\n",
      "Train loss at 283: tensor([2.6179], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2968],\n",
      "        [1.3895]], requires_grad=True)\n",
      "Train loss at 284: tensor([2.6171], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2982],\n",
      "        [1.3891]], requires_grad=True)\n",
      "Train loss at 285: tensor([2.6164], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.2995],\n",
      "        [1.3887]], requires_grad=True)\n",
      "Train loss at 286: tensor([2.6156], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3008],\n",
      "        [1.3883]], requires_grad=True)\n",
      "Train loss at 287: tensor([2.6149], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3021],\n",
      "        [1.3880]], requires_grad=True)\n",
      "Train loss at 288: tensor([2.6142], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3035],\n",
      "        [1.3876]], requires_grad=True)\n",
      "Train loss at 289: tensor([2.6134], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3048],\n",
      "        [1.3872]], requires_grad=True)\n",
      "Train loss at 290: tensor([2.6127], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3061],\n",
      "        [1.3868]], requires_grad=True)\n",
      "Train loss at 291: tensor([2.6120], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3074],\n",
      "        [1.3864]], requires_grad=True)\n",
      "Train loss at 292: tensor([2.6113], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3087],\n",
      "        [1.3860]], requires_grad=True)\n",
      "Train loss at 293: tensor([2.6106], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3100],\n",
      "        [1.3856]], requires_grad=True)\n",
      "Train loss at 294: tensor([2.6099], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3112],\n",
      "        [1.3852]], requires_grad=True)\n",
      "Train loss at 295: tensor([2.6092], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3125],\n",
      "        [1.3849]], requires_grad=True)\n",
      "Train loss at 296: tensor([2.6085], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3138],\n",
      "        [1.3845]], requires_grad=True)\n",
      "Train loss at 297: tensor([2.6079], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3150],\n",
      "        [1.3841]], requires_grad=True)\n",
      "Train loss at 298: tensor([2.6072], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3163],\n",
      "        [1.3837]], requires_grad=True)\n",
      "Train loss at 299: tensor([2.6065], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3175],\n",
      "        [1.3833]], requires_grad=True)\n",
      "Train loss at 300: tensor([2.6059], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3188],\n",
      "        [1.3829]], requires_grad=True)\n",
      "Train loss at 301: tensor([2.6052], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3200],\n",
      "        [1.3825]], requires_grad=True)\n",
      "Train loss at 302: tensor([2.6046], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3213],\n",
      "        [1.3821]], requires_grad=True)\n",
      "Train loss at 303: tensor([2.6039], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3225],\n",
      "        [1.3817]], requires_grad=True)\n",
      "Train loss at 304: tensor([2.6033], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3237],\n",
      "        [1.3813]], requires_grad=True)\n",
      "Train loss at 305: tensor([2.6026], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3249],\n",
      "        [1.3809]], requires_grad=True)\n",
      "Train loss at 306: tensor([2.6020], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3261],\n",
      "        [1.3805]], requires_grad=True)\n",
      "Train loss at 307: tensor([2.6014], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3273],\n",
      "        [1.3801]], requires_grad=True)\n",
      "Train loss at 308: tensor([2.6008], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3285],\n",
      "        [1.3797]], requires_grad=True)\n",
      "Train loss at 309: tensor([2.6002], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3297],\n",
      "        [1.3793]], requires_grad=True)\n",
      "Train loss at 310: tensor([2.5995], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3309],\n",
      "        [1.3789]], requires_grad=True)\n",
      "Train loss at 311: tensor([2.5989], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3321],\n",
      "        [1.3785]], requires_grad=True)\n",
      "Train loss at 312: tensor([2.5983], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3333],\n",
      "        [1.3781]], requires_grad=True)\n",
      "Train loss at 313: tensor([2.5978], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3345],\n",
      "        [1.3777]], requires_grad=True)\n",
      "Train loss at 314: tensor([2.5972], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3356],\n",
      "        [1.3773]], requires_grad=True)\n",
      "Train loss at 315: tensor([2.5966], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3368],\n",
      "        [1.3769]], requires_grad=True)\n",
      "Train loss at 316: tensor([2.5960], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3379],\n",
      "        [1.3765]], requires_grad=True)\n",
      "Train loss at 317: tensor([2.5954], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3391],\n",
      "        [1.3760]], requires_grad=True)\n",
      "Train loss at 318: tensor([2.5949], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3402],\n",
      "        [1.3756]], requires_grad=True)\n",
      "Train loss at 319: tensor([2.5943], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3414],\n",
      "        [1.3752]], requires_grad=True)\n",
      "Train loss at 320: tensor([2.5937], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3425],\n",
      "        [1.3748]], requires_grad=True)\n",
      "Train loss at 321: tensor([2.5932], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3436],\n",
      "        [1.3744]], requires_grad=True)\n",
      "Train loss at 322: tensor([2.5926], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3448],\n",
      "        [1.3740]], requires_grad=True)\n",
      "Train loss at 323: tensor([2.5921], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3459],\n",
      "        [1.3736]], requires_grad=True)\n",
      "Train loss at 324: tensor([2.5916], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3470],\n",
      "        [1.3732]], requires_grad=True)\n",
      "Train loss at 325: tensor([2.5910], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3481],\n",
      "        [1.3728]], requires_grad=True)\n",
      "Train loss at 326: tensor([2.5905], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3492],\n",
      "        [1.3724]], requires_grad=True)\n",
      "Train loss at 327: tensor([2.5900], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3503],\n",
      "        [1.3720]], requires_grad=True)\n",
      "Train loss at 328: tensor([2.5894], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3514],\n",
      "        [1.3715]], requires_grad=True)\n",
      "Train loss at 329: tensor([2.5889], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3525],\n",
      "        [1.3711]], requires_grad=True)\n",
      "Train loss at 330: tensor([2.5884], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3536],\n",
      "        [1.3707]], requires_grad=True)\n",
      "Train loss at 331: tensor([2.5879], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3546],\n",
      "        [1.3703]], requires_grad=True)\n",
      "Train loss at 332: tensor([2.5874], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3557],\n",
      "        [1.3699]], requires_grad=True)\n",
      "Train loss at 333: tensor([2.5869], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3568],\n",
      "        [1.3695]], requires_grad=True)\n",
      "Train loss at 334: tensor([2.5864], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3578],\n",
      "        [1.3691]], requires_grad=True)\n",
      "Train loss at 335: tensor([2.5859], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3589],\n",
      "        [1.3687]], requires_grad=True)\n",
      "Train loss at 336: tensor([2.5854], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3600],\n",
      "        [1.3682]], requires_grad=True)\n",
      "Train loss at 337: tensor([2.5849], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3610],\n",
      "        [1.3678]], requires_grad=True)\n",
      "Train loss at 338: tensor([2.5844], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3620],\n",
      "        [1.3674]], requires_grad=True)\n",
      "Train loss at 339: tensor([2.5840], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3631],\n",
      "        [1.3670]], requires_grad=True)\n",
      "Train loss at 340: tensor([2.5835], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3641],\n",
      "        [1.3666]], requires_grad=True)\n",
      "Train loss at 341: tensor([2.5830], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3652],\n",
      "        [1.3662]], requires_grad=True)\n",
      "Train loss at 342: tensor([2.5826], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3662],\n",
      "        [1.3658]], requires_grad=True)\n",
      "Train loss at 343: tensor([2.5821], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3672],\n",
      "        [1.3653]], requires_grad=True)\n",
      "Train loss at 344: tensor([2.5816], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3682],\n",
      "        [1.3649]], requires_grad=True)\n",
      "Train loss at 345: tensor([2.5812], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3692],\n",
      "        [1.3645]], requires_grad=True)\n",
      "Train loss at 346: tensor([2.5807], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3702],\n",
      "        [1.3641]], requires_grad=True)\n",
      "Train loss at 347: tensor([2.5803], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3712],\n",
      "        [1.3637]], requires_grad=True)\n",
      "Train loss at 348: tensor([2.5798], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3722],\n",
      "        [1.3633]], requires_grad=True)\n",
      "Train loss at 349: tensor([2.5794], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3732],\n",
      "        [1.3628]], requires_grad=True)\n",
      "Train loss at 350: tensor([2.5790], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3742],\n",
      "        [1.3624]], requires_grad=True)\n",
      "Train loss at 351: tensor([2.5785], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3752],\n",
      "        [1.3620]], requires_grad=True)\n",
      "Train loss at 352: tensor([2.5781], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3762],\n",
      "        [1.3616]], requires_grad=True)\n",
      "Train loss at 353: tensor([2.5777], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3771],\n",
      "        [1.3612]], requires_grad=True)\n",
      "Train loss at 354: tensor([2.5773], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3781],\n",
      "        [1.3608]], requires_grad=True)\n",
      "Train loss at 355: tensor([2.5768], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3791],\n",
      "        [1.3604]], requires_grad=True)\n",
      "Train loss at 356: tensor([2.5764], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3800],\n",
      "        [1.3599]], requires_grad=True)\n",
      "Train loss at 357: tensor([2.5760], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3810],\n",
      "        [1.3595]], requires_grad=True)\n",
      "Train loss at 358: tensor([2.5756], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3820],\n",
      "        [1.3591]], requires_grad=True)\n",
      "Train loss at 359: tensor([2.5752], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3829],\n",
      "        [1.3587]], requires_grad=True)\n",
      "Train loss at 360: tensor([2.5748], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3839],\n",
      "        [1.3583]], requires_grad=True)\n",
      "Train loss at 361: tensor([2.5744], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3848],\n",
      "        [1.3579]], requires_grad=True)\n",
      "Train loss at 362: tensor([2.5740], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3857],\n",
      "        [1.3574]], requires_grad=True)\n",
      "Train loss at 363: tensor([2.5736], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3867],\n",
      "        [1.3570]], requires_grad=True)\n",
      "Train loss at 364: tensor([2.5732], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3876],\n",
      "        [1.3566]], requires_grad=True)\n",
      "Train loss at 365: tensor([2.5728], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3885],\n",
      "        [1.3562]], requires_grad=True)\n",
      "Train loss at 366: tensor([2.5724], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3894],\n",
      "        [1.3558]], requires_grad=True)\n",
      "Train loss at 367: tensor([2.5721], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3903],\n",
      "        [1.3554]], requires_grad=True)\n",
      "Train loss at 368: tensor([2.5717], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3913],\n",
      "        [1.3550]], requires_grad=True)\n",
      "Train loss at 369: tensor([2.5713], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3922],\n",
      "        [1.3545]], requires_grad=True)\n",
      "Train loss at 370: tensor([2.5709], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3931],\n",
      "        [1.3541]], requires_grad=True)\n",
      "Train loss at 371: tensor([2.5706], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3940],\n",
      "        [1.3537]], requires_grad=True)\n",
      "Train loss at 372: tensor([2.5702], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3949],\n",
      "        [1.3533]], requires_grad=True)\n",
      "Train loss at 373: tensor([2.5698], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3957],\n",
      "        [1.3529]], requires_grad=True)\n",
      "Train loss at 374: tensor([2.5695], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3966],\n",
      "        [1.3525]], requires_grad=True)\n",
      "Train loss at 375: tensor([2.5691], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3975],\n",
      "        [1.3521]], requires_grad=True)\n",
      "Train loss at 376: tensor([2.5688], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3984],\n",
      "        [1.3516]], requires_grad=True)\n",
      "Train loss at 377: tensor([2.5684], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.3993],\n",
      "        [1.3512]], requires_grad=True)\n",
      "Train loss at 378: tensor([2.5681], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4001],\n",
      "        [1.3508]], requires_grad=True)\n",
      "Train loss at 379: tensor([2.5677], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4010],\n",
      "        [1.3504]], requires_grad=True)\n",
      "Train loss at 380: tensor([2.5674], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4019],\n",
      "        [1.3500]], requires_grad=True)\n",
      "Train loss at 381: tensor([2.5670], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4027],\n",
      "        [1.3496]], requires_grad=True)\n",
      "Train loss at 382: tensor([2.5667], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4036],\n",
      "        [1.3492]], requires_grad=True)\n",
      "Train loss at 383: tensor([2.5664], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4044],\n",
      "        [1.3488]], requires_grad=True)\n",
      "Train loss at 384: tensor([2.5660], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4053],\n",
      "        [1.3484]], requires_grad=True)\n",
      "Train loss at 385: tensor([2.5657], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4061],\n",
      "        [1.3479]], requires_grad=True)\n",
      "Train loss at 386: tensor([2.5654], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4070],\n",
      "        [1.3475]], requires_grad=True)\n",
      "Train loss at 387: tensor([2.5650], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4078],\n",
      "        [1.3471]], requires_grad=True)\n",
      "Train loss at 388: tensor([2.5647], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4086],\n",
      "        [1.3467]], requires_grad=True)\n",
      "Train loss at 389: tensor([2.5644], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4095],\n",
      "        [1.3463]], requires_grad=True)\n",
      "Train loss at 390: tensor([2.5641], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4103],\n",
      "        [1.3459]], requires_grad=True)\n",
      "Train loss at 391: tensor([2.5638], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4111],\n",
      "        [1.3455]], requires_grad=True)\n",
      "Train loss at 392: tensor([2.5635], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4119],\n",
      "        [1.3451]], requires_grad=True)\n",
      "Train loss at 393: tensor([2.5631], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4128],\n",
      "        [1.3447]], requires_grad=True)\n",
      "Train loss at 394: tensor([2.5628], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4136],\n",
      "        [1.3443]], requires_grad=True)\n",
      "Train loss at 395: tensor([2.5625], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4144],\n",
      "        [1.3439]], requires_grad=True)\n",
      "Train loss at 396: tensor([2.5622], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4152],\n",
      "        [1.3434]], requires_grad=True)\n",
      "Train loss at 397: tensor([2.5619], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4160],\n",
      "        [1.3430]], requires_grad=True)\n",
      "Train loss at 398: tensor([2.5616], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4168],\n",
      "        [1.3426]], requires_grad=True)\n",
      "Train loss at 399: tensor([2.5613], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4176],\n",
      "        [1.3422]], requires_grad=True)\n",
      "Train loss at 400: tensor([2.5610], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4184],\n",
      "        [1.3418]], requires_grad=True)\n",
      "Train loss at 401: tensor([2.5608], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4191],\n",
      "        [1.3414]], requires_grad=True)\n",
      "Train loss at 402: tensor([2.5605], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4199],\n",
      "        [1.3410]], requires_grad=True)\n",
      "Train loss at 403: tensor([2.5602], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4207],\n",
      "        [1.3406]], requires_grad=True)\n",
      "Train loss at 404: tensor([2.5599], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4215],\n",
      "        [1.3402]], requires_grad=True)\n",
      "Train loss at 405: tensor([2.5596], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4223],\n",
      "        [1.3398]], requires_grad=True)\n",
      "Train loss at 406: tensor([2.5593], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4230],\n",
      "        [1.3394]], requires_grad=True)\n",
      "Train loss at 407: tensor([2.5591], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4238],\n",
      "        [1.3390]], requires_grad=True)\n",
      "Train loss at 408: tensor([2.5588], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4246],\n",
      "        [1.3386]], requires_grad=True)\n",
      "Train loss at 409: tensor([2.5585], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4253],\n",
      "        [1.3382]], requires_grad=True)\n",
      "Train loss at 410: tensor([2.5582], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4261],\n",
      "        [1.3378]], requires_grad=True)\n",
      "Train loss at 411: tensor([2.5580], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4268],\n",
      "        [1.3374]], requires_grad=True)\n",
      "Train loss at 412: tensor([2.5577], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4276],\n",
      "        [1.3370]], requires_grad=True)\n",
      "Train loss at 413: tensor([2.5574], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4283],\n",
      "        [1.3366]], requires_grad=True)\n",
      "Train loss at 414: tensor([2.5572], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4291],\n",
      "        [1.3362]], requires_grad=True)\n",
      "Train loss at 415: tensor([2.5569], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4298],\n",
      "        [1.3358]], requires_grad=True)\n",
      "Train loss at 416: tensor([2.5566], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4305],\n",
      "        [1.3354]], requires_grad=True)\n",
      "Train loss at 417: tensor([2.5564], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4313],\n",
      "        [1.3350]], requires_grad=True)\n",
      "Train loss at 418: tensor([2.5561], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4320],\n",
      "        [1.3346]], requires_grad=True)\n",
      "Train loss at 419: tensor([2.5559], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4327],\n",
      "        [1.3342]], requires_grad=True)\n",
      "Train loss at 420: tensor([2.5556], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4335],\n",
      "        [1.3338]], requires_grad=True)\n",
      "Train loss at 421: tensor([2.5554], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4342],\n",
      "        [1.3334]], requires_grad=True)\n",
      "Train loss at 422: tensor([2.5551], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4349],\n",
      "        [1.3330]], requires_grad=True)\n",
      "Train loss at 423: tensor([2.5549], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4356],\n",
      "        [1.3326]], requires_grad=True)\n",
      "Train loss at 424: tensor([2.5546], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4363],\n",
      "        [1.3323]], requires_grad=True)\n",
      "Train loss at 425: tensor([2.5544], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4370],\n",
      "        [1.3319]], requires_grad=True)\n",
      "Train loss at 426: tensor([2.5542], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4377],\n",
      "        [1.3315]], requires_grad=True)\n",
      "Train loss at 427: tensor([2.5539], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4384],\n",
      "        [1.3311]], requires_grad=True)\n",
      "Train loss at 428: tensor([2.5537], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4391],\n",
      "        [1.3307]], requires_grad=True)\n",
      "Train loss at 429: tensor([2.5535], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4398],\n",
      "        [1.3303]], requires_grad=True)\n",
      "Train loss at 430: tensor([2.5532], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4405],\n",
      "        [1.3299]], requires_grad=True)\n",
      "Train loss at 431: tensor([2.5530], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4412],\n",
      "        [1.3295]], requires_grad=True)\n",
      "Train loss at 432: tensor([2.5528], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4419],\n",
      "        [1.3291]], requires_grad=True)\n",
      "Train loss at 433: tensor([2.5525], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4426],\n",
      "        [1.3287]], requires_grad=True)\n",
      "Train loss at 434: tensor([2.5523], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4433],\n",
      "        [1.3284]], requires_grad=True)\n",
      "Train loss at 435: tensor([2.5521], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4440],\n",
      "        [1.3280]], requires_grad=True)\n",
      "Train loss at 436: tensor([2.5519], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4446],\n",
      "        [1.3276]], requires_grad=True)\n",
      "Train loss at 437: tensor([2.5516], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4453],\n",
      "        [1.3272]], requires_grad=True)\n",
      "Train loss at 438: tensor([2.5514], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4460],\n",
      "        [1.3268]], requires_grad=True)\n",
      "Train loss at 439: tensor([2.5512], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4466],\n",
      "        [1.3264]], requires_grad=True)\n",
      "Train loss at 440: tensor([2.5510], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4473],\n",
      "        [1.3261]], requires_grad=True)\n",
      "Train loss at 441: tensor([2.5508], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4480],\n",
      "        [1.3257]], requires_grad=True)\n",
      "Train loss at 442: tensor([2.5506], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4486],\n",
      "        [1.3253]], requires_grad=True)\n",
      "Train loss at 443: tensor([2.5503], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4493],\n",
      "        [1.3249]], requires_grad=True)\n",
      "Train loss at 444: tensor([2.5501], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4499],\n",
      "        [1.3245]], requires_grad=True)\n",
      "Train loss at 445: tensor([2.5499], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4506],\n",
      "        [1.3241]], requires_grad=True)\n",
      "Train loss at 446: tensor([2.5497], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4512],\n",
      "        [1.3238]], requires_grad=True)\n",
      "Train loss at 447: tensor([2.5495], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4519],\n",
      "        [1.3234]], requires_grad=True)\n",
      "Train loss at 448: tensor([2.5493], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4525],\n",
      "        [1.3230]], requires_grad=True)\n",
      "Train loss at 449: tensor([2.5491], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4532],\n",
      "        [1.3226]], requires_grad=True)\n",
      "Train loss at 450: tensor([2.5489], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4538],\n",
      "        [1.3223]], requires_grad=True)\n",
      "Train loss at 451: tensor([2.5487], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4544],\n",
      "        [1.3219]], requires_grad=True)\n",
      "Train loss at 452: tensor([2.5485], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4551],\n",
      "        [1.3215]], requires_grad=True)\n",
      "Train loss at 453: tensor([2.5483], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4557],\n",
      "        [1.3211]], requires_grad=True)\n",
      "Train loss at 454: tensor([2.5481], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4563],\n",
      "        [1.3208]], requires_grad=True)\n",
      "Train loss at 455: tensor([2.5479], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4569],\n",
      "        [1.3204]], requires_grad=True)\n",
      "Train loss at 456: tensor([2.5477], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4576],\n",
      "        [1.3200]], requires_grad=True)\n",
      "Train loss at 457: tensor([2.5476], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4582],\n",
      "        [1.3196]], requires_grad=True)\n",
      "Train loss at 458: tensor([2.5474], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4588],\n",
      "        [1.3193]], requires_grad=True)\n",
      "Train loss at 459: tensor([2.5472], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4594],\n",
      "        [1.3189]], requires_grad=True)\n",
      "Train loss at 460: tensor([2.5470], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4600],\n",
      "        [1.3185]], requires_grad=True)\n",
      "Train loss at 461: tensor([2.5468], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4606],\n",
      "        [1.3182]], requires_grad=True)\n",
      "Train loss at 462: tensor([2.5466], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4612],\n",
      "        [1.3178]], requires_grad=True)\n",
      "Train loss at 463: tensor([2.5465], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4618],\n",
      "        [1.3174]], requires_grad=True)\n",
      "Train loss at 464: tensor([2.5463], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4624],\n",
      "        [1.3171]], requires_grad=True)\n",
      "Train loss at 465: tensor([2.5461], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4630],\n",
      "        [1.3167]], requires_grad=True)\n",
      "Train loss at 466: tensor([2.5459], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4636],\n",
      "        [1.3163]], requires_grad=True)\n",
      "Train loss at 467: tensor([2.5457], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4642],\n",
      "        [1.3160]], requires_grad=True)\n",
      "Train loss at 468: tensor([2.5456], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4648],\n",
      "        [1.3156]], requires_grad=True)\n",
      "Train loss at 469: tensor([2.5454], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4654],\n",
      "        [1.3152]], requires_grad=True)\n",
      "Train loss at 470: tensor([2.5452], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4660],\n",
      "        [1.3149]], requires_grad=True)\n",
      "Train loss at 471: tensor([2.5450], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4666],\n",
      "        [1.3145]], requires_grad=True)\n",
      "Train loss at 472: tensor([2.5449], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4672],\n",
      "        [1.3141]], requires_grad=True)\n",
      "Train loss at 473: tensor([2.5447], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4677],\n",
      "        [1.3138]], requires_grad=True)\n",
      "Train loss at 474: tensor([2.5445], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4683],\n",
      "        [1.3134]], requires_grad=True)\n",
      "Train loss at 475: tensor([2.5444], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4689],\n",
      "        [1.3131]], requires_grad=True)\n",
      "Train loss at 476: tensor([2.5442], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4695],\n",
      "        [1.3127]], requires_grad=True)\n",
      "Train loss at 477: tensor([2.5441], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4700],\n",
      "        [1.3124]], requires_grad=True)\n",
      "Train loss at 478: tensor([2.5439], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4706],\n",
      "        [1.3120]], requires_grad=True)\n",
      "Train loss at 479: tensor([2.5437], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4712],\n",
      "        [1.3116]], requires_grad=True)\n",
      "Train loss at 480: tensor([2.5436], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4717],\n",
      "        [1.3113]], requires_grad=True)\n",
      "Train loss at 481: tensor([2.5434], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4723],\n",
      "        [1.3109]], requires_grad=True)\n",
      "Train loss at 482: tensor([2.5433], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4728],\n",
      "        [1.3106]], requires_grad=True)\n",
      "Train loss at 483: tensor([2.5431], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4734],\n",
      "        [1.3102]], requires_grad=True)\n",
      "Train loss at 484: tensor([2.5429], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4739],\n",
      "        [1.3099]], requires_grad=True)\n",
      "Train loss at 485: tensor([2.5428], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4745],\n",
      "        [1.3095]], requires_grad=True)\n",
      "Train loss at 486: tensor([2.5426], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4750],\n",
      "        [1.3092]], requires_grad=True)\n",
      "Train loss at 487: tensor([2.5425], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4756],\n",
      "        [1.3088]], requires_grad=True)\n",
      "Train loss at 488: tensor([2.5423], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4761],\n",
      "        [1.3085]], requires_grad=True)\n",
      "Train loss at 489: tensor([2.5422], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4767],\n",
      "        [1.3081]], requires_grad=True)\n",
      "Train loss at 490: tensor([2.5420], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4772],\n",
      "        [1.3078]], requires_grad=True)\n",
      "Train loss at 491: tensor([2.5419], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4777],\n",
      "        [1.3074]], requires_grad=True)\n",
      "Train loss at 492: tensor([2.5417], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4783],\n",
      "        [1.3071]], requires_grad=True)\n",
      "Train loss at 493: tensor([2.5416], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4788],\n",
      "        [1.3067]], requires_grad=True)\n",
      "Train loss at 494: tensor([2.5415], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4793],\n",
      "        [1.3064]], requires_grad=True)\n",
      "Train loss at 495: tensor([2.5413], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4799],\n",
      "        [1.3061]], requires_grad=True)\n",
      "Train loss at 496: tensor([2.5412], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4804],\n",
      "        [1.3057]], requires_grad=True)\n",
      "Train loss at 497: tensor([2.5410], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4809],\n",
      "        [1.3054]], requires_grad=True)\n",
      "Train loss at 498: tensor([2.5409], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4814],\n",
      "        [1.3050]], requires_grad=True)\n",
      "Train loss at 499: tensor([2.5408], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4820],\n",
      "        [1.3047]], requires_grad=True)\n",
      "Train loss at 500: tensor([2.5406], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4825],\n",
      "        [1.3044]], requires_grad=True)\n",
      "Train loss at 501: tensor([2.5405], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4830],\n",
      "        [1.3040]], requires_grad=True)\n",
      "Train loss at 502: tensor([2.5403], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4835],\n",
      "        [1.3037]], requires_grad=True)\n",
      "Train loss at 503: tensor([2.5402], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4840],\n",
      "        [1.3033]], requires_grad=True)\n",
      "Train loss at 504: tensor([2.5401], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4845],\n",
      "        [1.3030]], requires_grad=True)\n",
      "Train loss at 505: tensor([2.5399], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4850],\n",
      "        [1.3027]], requires_grad=True)\n",
      "Train loss at 506: tensor([2.5398], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4855],\n",
      "        [1.3023]], requires_grad=True)\n",
      "Train loss at 507: tensor([2.5397], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4860],\n",
      "        [1.3020]], requires_grad=True)\n",
      "Train loss at 508: tensor([2.5396], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4865],\n",
      "        [1.3017]], requires_grad=True)\n",
      "Train loss at 509: tensor([2.5394], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4870],\n",
      "        [1.3013]], requires_grad=True)\n",
      "Train loss at 510: tensor([2.5393], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4875],\n",
      "        [1.3010]], requires_grad=True)\n",
      "Train loss at 511: tensor([2.5392], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4880],\n",
      "        [1.3007]], requires_grad=True)\n",
      "Train loss at 512: tensor([2.5390], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4885],\n",
      "        [1.3003]], requires_grad=True)\n",
      "Train loss at 513: tensor([2.5389], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4890],\n",
      "        [1.3000]], requires_grad=True)\n",
      "Train loss at 514: tensor([2.5388], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4895],\n",
      "        [1.2997]], requires_grad=True)\n",
      "Train loss at 515: tensor([2.5387], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4900],\n",
      "        [1.2994]], requires_grad=True)\n",
      "Train loss at 516: tensor([2.5385], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4905],\n",
      "        [1.2990]], requires_grad=True)\n",
      "Train loss at 517: tensor([2.5384], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4910],\n",
      "        [1.2987]], requires_grad=True)\n",
      "Train loss at 518: tensor([2.5383], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4914],\n",
      "        [1.2984]], requires_grad=True)\n",
      "Train loss at 519: tensor([2.5382], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4919],\n",
      "        [1.2980]], requires_grad=True)\n",
      "Train loss at 520: tensor([2.5381], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4924],\n",
      "        [1.2977]], requires_grad=True)\n",
      "Train loss at 521: tensor([2.5380], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4929],\n",
      "        [1.2974]], requires_grad=True)\n",
      "Train loss at 522: tensor([2.5378], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4933],\n",
      "        [1.2971]], requires_grad=True)\n",
      "Train loss at 523: tensor([2.5377], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4938],\n",
      "        [1.2968]], requires_grad=True)\n",
      "Train loss at 524: tensor([2.5376], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4943],\n",
      "        [1.2964]], requires_grad=True)\n",
      "Train loss at 525: tensor([2.5375], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4947],\n",
      "        [1.2961]], requires_grad=True)\n",
      "Train loss at 526: tensor([2.5374], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4952],\n",
      "        [1.2958]], requires_grad=True)\n",
      "Train loss at 527: tensor([2.5373], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4957],\n",
      "        [1.2955]], requires_grad=True)\n",
      "Train loss at 528: tensor([2.5371], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4961],\n",
      "        [1.2952]], requires_grad=True)\n",
      "Train loss at 529: tensor([2.5370], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4966],\n",
      "        [1.2948]], requires_grad=True)\n",
      "Train loss at 530: tensor([2.5369], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4971],\n",
      "        [1.2945]], requires_grad=True)\n",
      "Train loss at 531: tensor([2.5368], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4975],\n",
      "        [1.2942]], requires_grad=True)\n",
      "Train loss at 532: tensor([2.5367], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4980],\n",
      "        [1.2939]], requires_grad=True)\n",
      "Train loss at 533: tensor([2.5366], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4984],\n",
      "        [1.2936]], requires_grad=True)\n",
      "Train loss at 534: tensor([2.5365], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4989],\n",
      "        [1.2933]], requires_grad=True)\n",
      "Train loss at 535: tensor([2.5364], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4993],\n",
      "        [1.2930]], requires_grad=True)\n",
      "Train loss at 536: tensor([2.5363], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.4998],\n",
      "        [1.2927]], requires_grad=True)\n",
      "Train loss at 537: tensor([2.5362], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5002],\n",
      "        [1.2923]], requires_grad=True)\n",
      "Train loss at 538: tensor([2.5361], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5006],\n",
      "        [1.2920]], requires_grad=True)\n",
      "Train loss at 539: tensor([2.5360], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5011],\n",
      "        [1.2917]], requires_grad=True)\n",
      "Train loss at 540: tensor([2.5359], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5015],\n",
      "        [1.2914]], requires_grad=True)\n",
      "Train loss at 541: tensor([2.5358], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5020],\n",
      "        [1.2911]], requires_grad=True)\n",
      "Train loss at 542: tensor([2.5357], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5024],\n",
      "        [1.2908]], requires_grad=True)\n",
      "Train loss at 543: tensor([2.5356], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5028],\n",
      "        [1.2905]], requires_grad=True)\n",
      "Train loss at 544: tensor([2.5355], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5033],\n",
      "        [1.2902]], requires_grad=True)\n",
      "Train loss at 545: tensor([2.5354], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5037],\n",
      "        [1.2899]], requires_grad=True)\n",
      "Train loss at 546: tensor([2.5353], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5041],\n",
      "        [1.2896]], requires_grad=True)\n",
      "Train loss at 547: tensor([2.5352], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5046],\n",
      "        [1.2893]], requires_grad=True)\n",
      "Train loss at 548: tensor([2.5351], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5050],\n",
      "        [1.2890]], requires_grad=True)\n",
      "Train loss at 549: tensor([2.5350], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5054],\n",
      "        [1.2887]], requires_grad=True)\n",
      "Train loss at 550: tensor([2.5349], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5058],\n",
      "        [1.2884]], requires_grad=True)\n",
      "Train loss at 551: tensor([2.5348], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5063],\n",
      "        [1.2881]], requires_grad=True)\n",
      "Train loss at 552: tensor([2.5347], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5067],\n",
      "        [1.2878]], requires_grad=True)\n",
      "Train loss at 553: tensor([2.5346], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5071],\n",
      "        [1.2875]], requires_grad=True)\n",
      "Train loss at 554: tensor([2.5345], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5075],\n",
      "        [1.2872]], requires_grad=True)\n",
      "Train loss at 555: tensor([2.5344], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5079],\n",
      "        [1.2869]], requires_grad=True)\n",
      "Train loss at 556: tensor([2.5343], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5083],\n",
      "        [1.2866]], requires_grad=True)\n",
      "Train loss at 557: tensor([2.5342], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5087],\n",
      "        [1.2863]], requires_grad=True)\n",
      "Train loss at 558: tensor([2.5341], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5092],\n",
      "        [1.2860]], requires_grad=True)\n",
      "Train loss at 559: tensor([2.5341], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5096],\n",
      "        [1.2857]], requires_grad=True)\n",
      "Train loss at 560: tensor([2.5340], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5100],\n",
      "        [1.2854]], requires_grad=True)\n",
      "Train loss at 561: tensor([2.5339], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5104],\n",
      "        [1.2851]], requires_grad=True)\n",
      "Train loss at 562: tensor([2.5338], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5108],\n",
      "        [1.2849]], requires_grad=True)\n",
      "Train loss at 563: tensor([2.5337], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5112],\n",
      "        [1.2846]], requires_grad=True)\n",
      "Train loss at 564: tensor([2.5336], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5116],\n",
      "        [1.2843]], requires_grad=True)\n",
      "Train loss at 565: tensor([2.5335], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5120],\n",
      "        [1.2840]], requires_grad=True)\n",
      "Train loss at 566: tensor([2.5335], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5124],\n",
      "        [1.2837]], requires_grad=True)\n",
      "Train loss at 567: tensor([2.5334], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5128],\n",
      "        [1.2834]], requires_grad=True)\n",
      "Train loss at 568: tensor([2.5333], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5132],\n",
      "        [1.2831]], requires_grad=True)\n",
      "Train loss at 569: tensor([2.5332], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5135],\n",
      "        [1.2829]], requires_grad=True)\n",
      "Train loss at 570: tensor([2.5331], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5139],\n",
      "        [1.2826]], requires_grad=True)\n",
      "Train loss at 571: tensor([2.5330], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5143],\n",
      "        [1.2823]], requires_grad=True)\n",
      "Train loss at 572: tensor([2.5330], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5147],\n",
      "        [1.2820]], requires_grad=True)\n",
      "Train loss at 573: tensor([2.5329], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5151],\n",
      "        [1.2817]], requires_grad=True)\n",
      "Train loss at 574: tensor([2.5328], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5155],\n",
      "        [1.2814]], requires_grad=True)\n",
      "Train loss at 575: tensor([2.5327], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5159],\n",
      "        [1.2812]], requires_grad=True)\n",
      "Train loss at 576: tensor([2.5326], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5163],\n",
      "        [1.2809]], requires_grad=True)\n",
      "Train loss at 577: tensor([2.5326], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5166],\n",
      "        [1.2806]], requires_grad=True)\n",
      "Train loss at 578: tensor([2.5325], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5170],\n",
      "        [1.2803]], requires_grad=True)\n",
      "Train loss at 579: tensor([2.5324], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5174],\n",
      "        [1.2800]], requires_grad=True)\n",
      "Train loss at 580: tensor([2.5323], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5178],\n",
      "        [1.2798]], requires_grad=True)\n",
      "Train loss at 581: tensor([2.5323], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5181],\n",
      "        [1.2795]], requires_grad=True)\n",
      "Train loss at 582: tensor([2.5322], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5185],\n",
      "        [1.2792]], requires_grad=True)\n",
      "Train loss at 583: tensor([2.5321], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5189],\n",
      "        [1.2790]], requires_grad=True)\n",
      "Train loss at 584: tensor([2.5320], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5192],\n",
      "        [1.2787]], requires_grad=True)\n",
      "Train loss at 585: tensor([2.5320], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5196],\n",
      "        [1.2784]], requires_grad=True)\n",
      "Train loss at 586: tensor([2.5319], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5200],\n",
      "        [1.2781]], requires_grad=True)\n",
      "Train loss at 587: tensor([2.5318], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5203],\n",
      "        [1.2779]], requires_grad=True)\n",
      "Train loss at 588: tensor([2.5317], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5207],\n",
      "        [1.2776]], requires_grad=True)\n",
      "Train loss at 589: tensor([2.5317], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5211],\n",
      "        [1.2773]], requires_grad=True)\n",
      "Train loss at 590: tensor([2.5316], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5214],\n",
      "        [1.2771]], requires_grad=True)\n",
      "Train loss at 591: tensor([2.5315], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5218],\n",
      "        [1.2768]], requires_grad=True)\n",
      "Train loss at 592: tensor([2.5315], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5222],\n",
      "        [1.2765]], requires_grad=True)\n",
      "Train loss at 593: tensor([2.5314], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5225],\n",
      "        [1.2763]], requires_grad=True)\n",
      "Train loss at 594: tensor([2.5313], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5229],\n",
      "        [1.2760]], requires_grad=True)\n",
      "Train loss at 595: tensor([2.5312], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5232],\n",
      "        [1.2757]], requires_grad=True)\n",
      "Train loss at 596: tensor([2.5312], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5236],\n",
      "        [1.2755]], requires_grad=True)\n",
      "Train loss at 597: tensor([2.5311], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5239],\n",
      "        [1.2752]], requires_grad=True)\n",
      "Train loss at 598: tensor([2.5310], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5243],\n",
      "        [1.2749]], requires_grad=True)\n",
      "Train loss at 599: tensor([2.5310], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5246],\n",
      "        [1.2747]], requires_grad=True)\n",
      "Train loss at 600: tensor([2.5309], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5250],\n",
      "        [1.2744]], requires_grad=True)\n",
      "Train loss at 601: tensor([2.5308], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5253],\n",
      "        [1.2742]], requires_grad=True)\n",
      "Train loss at 602: tensor([2.5308], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5257],\n",
      "        [1.2739]], requires_grad=True)\n",
      "Train loss at 603: tensor([2.5307], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5260],\n",
      "        [1.2736]], requires_grad=True)\n",
      "Train loss at 604: tensor([2.5307], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5263],\n",
      "        [1.2734]], requires_grad=True)\n",
      "Train loss at 605: tensor([2.5306], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5267],\n",
      "        [1.2731]], requires_grad=True)\n",
      "Train loss at 606: tensor([2.5305], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5270],\n",
      "        [1.2729]], requires_grad=True)\n",
      "Train loss at 607: tensor([2.5305], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5274],\n",
      "        [1.2726]], requires_grad=True)\n",
      "Train loss at 608: tensor([2.5304], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5277],\n",
      "        [1.2724]], requires_grad=True)\n",
      "Train loss at 609: tensor([2.5303], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5280],\n",
      "        [1.2721]], requires_grad=True)\n",
      "Train loss at 610: tensor([2.5303], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5284],\n",
      "        [1.2719]], requires_grad=True)\n",
      "Train loss at 611: tensor([2.5302], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5287],\n",
      "        [1.2716]], requires_grad=True)\n",
      "Train loss at 612: tensor([2.5302], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5290],\n",
      "        [1.2714]], requires_grad=True)\n",
      "Train loss at 613: tensor([2.5301], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5294],\n",
      "        [1.2711]], requires_grad=True)\n",
      "Train loss at 614: tensor([2.5300], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5297],\n",
      "        [1.2709]], requires_grad=True)\n",
      "Train loss at 615: tensor([2.5300], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5300],\n",
      "        [1.2706]], requires_grad=True)\n",
      "Train loss at 616: tensor([2.5299], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5304],\n",
      "        [1.2704]], requires_grad=True)\n",
      "Train loss at 617: tensor([2.5299], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5307],\n",
      "        [1.2701]], requires_grad=True)\n",
      "Train loss at 618: tensor([2.5298], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5310],\n",
      "        [1.2699]], requires_grad=True)\n",
      "Train loss at 619: tensor([2.5297], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5313],\n",
      "        [1.2696]], requires_grad=True)\n",
      "Train loss at 620: tensor([2.5297], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5316],\n",
      "        [1.2694]], requires_grad=True)\n",
      "Train loss at 621: tensor([2.5296], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5320],\n",
      "        [1.2691]], requires_grad=True)\n",
      "Train loss at 622: tensor([2.5296], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5323],\n",
      "        [1.2689]], requires_grad=True)\n",
      "Train loss at 623: tensor([2.5295], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5326],\n",
      "        [1.2687]], requires_grad=True)\n",
      "Train loss at 624: tensor([2.5295], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5329],\n",
      "        [1.2684]], requires_grad=True)\n",
      "Train loss at 625: tensor([2.5294], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5332],\n",
      "        [1.2682]], requires_grad=True)\n",
      "Train loss at 626: tensor([2.5293], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5336],\n",
      "        [1.2679]], requires_grad=True)\n",
      "Train loss at 627: tensor([2.5293], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5339],\n",
      "        [1.2677]], requires_grad=True)\n",
      "Train loss at 628: tensor([2.5292], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5342],\n",
      "        [1.2674]], requires_grad=True)\n",
      "Train loss at 629: tensor([2.5292], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5345],\n",
      "        [1.2672]], requires_grad=True)\n",
      "Train loss at 630: tensor([2.5291], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5348],\n",
      "        [1.2670]], requires_grad=True)\n",
      "Train loss at 631: tensor([2.5291], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5351],\n",
      "        [1.2667]], requires_grad=True)\n",
      "Train loss at 632: tensor([2.5290], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5354],\n",
      "        [1.2665]], requires_grad=True)\n",
      "Train loss at 633: tensor([2.5290], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5357],\n",
      "        [1.2663]], requires_grad=True)\n",
      "Train loss at 634: tensor([2.5289], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5360],\n",
      "        [1.2660]], requires_grad=True)\n",
      "Train loss at 635: tensor([2.5289], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5363],\n",
      "        [1.2658]], requires_grad=True)\n",
      "Train loss at 636: tensor([2.5288], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5366],\n",
      "        [1.2656]], requires_grad=True)\n",
      "Train loss at 637: tensor([2.5288], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5369],\n",
      "        [1.2653]], requires_grad=True)\n",
      "Train loss at 638: tensor([2.5287], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5372],\n",
      "        [1.2651]], requires_grad=True)\n",
      "Train loss at 639: tensor([2.5287], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5375],\n",
      "        [1.2649]], requires_grad=True)\n",
      "Train loss at 640: tensor([2.5286], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5378],\n",
      "        [1.2646]], requires_grad=True)\n",
      "Train loss at 641: tensor([2.5286], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5381],\n",
      "        [1.2644]], requires_grad=True)\n",
      "Train loss at 642: tensor([2.5285], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5384],\n",
      "        [1.2642]], requires_grad=True)\n",
      "Train loss at 643: tensor([2.5285], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5387],\n",
      "        [1.2639]], requires_grad=True)\n",
      "Train loss at 644: tensor([2.5284], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5390],\n",
      "        [1.2637]], requires_grad=True)\n",
      "Train loss at 645: tensor([2.5284], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5393],\n",
      "        [1.2635]], requires_grad=True)\n",
      "Train loss at 646: tensor([2.5283], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5396],\n",
      "        [1.2633]], requires_grad=True)\n",
      "Train loss at 647: tensor([2.5283], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5399],\n",
      "        [1.2630]], requires_grad=True)\n",
      "Train loss at 648: tensor([2.5282], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5402],\n",
      "        [1.2628]], requires_grad=True)\n",
      "Train loss at 649: tensor([2.5282], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5405],\n",
      "        [1.2626]], requires_grad=True)\n",
      "Train loss at 650: tensor([2.5281], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5407],\n",
      "        [1.2624]], requires_grad=True)\n",
      "Train loss at 651: tensor([2.5281], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5410],\n",
      "        [1.2621]], requires_grad=True)\n",
      "Train loss at 652: tensor([2.5280], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5413],\n",
      "        [1.2619]], requires_grad=True)\n",
      "Train loss at 653: tensor([2.5280], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5416],\n",
      "        [1.2617]], requires_grad=True)\n",
      "Train loss at 654: tensor([2.5280], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5419],\n",
      "        [1.2615]], requires_grad=True)\n",
      "Train loss at 655: tensor([2.5279], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5422],\n",
      "        [1.2613]], requires_grad=True)\n",
      "Train loss at 656: tensor([2.5279], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5424],\n",
      "        [1.2610]], requires_grad=True)\n",
      "Train loss at 657: tensor([2.5278], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5427],\n",
      "        [1.2608]], requires_grad=True)\n",
      "Train loss at 658: tensor([2.5278], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5430],\n",
      "        [1.2606]], requires_grad=True)\n",
      "Train loss at 659: tensor([2.5277], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5433],\n",
      "        [1.2604]], requires_grad=True)\n",
      "Train loss at 660: tensor([2.5277], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5436],\n",
      "        [1.2602]], requires_grad=True)\n",
      "Train loss at 661: tensor([2.5277], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5438],\n",
      "        [1.2600]], requires_grad=True)\n",
      "Train loss at 662: tensor([2.5276], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5441],\n",
      "        [1.2597]], requires_grad=True)\n",
      "Train loss at 663: tensor([2.5276], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5444],\n",
      "        [1.2595]], requires_grad=True)\n",
      "Train loss at 664: tensor([2.5275], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5447],\n",
      "        [1.2593]], requires_grad=True)\n",
      "Train loss at 665: tensor([2.5275], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5449],\n",
      "        [1.2591]], requires_grad=True)\n",
      "Train loss at 666: tensor([2.5274], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5452],\n",
      "        [1.2589]], requires_grad=True)\n",
      "Train loss at 667: tensor([2.5274], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5455],\n",
      "        [1.2587]], requires_grad=True)\n",
      "Train loss at 668: tensor([2.5274], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5457],\n",
      "        [1.2585]], requires_grad=True)\n",
      "Train loss at 669: tensor([2.5273], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5460],\n",
      "        [1.2583]], requires_grad=True)\n",
      "Train loss at 670: tensor([2.5273], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5463],\n",
      "        [1.2581]], requires_grad=True)\n",
      "Train loss at 671: tensor([2.5272], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5465],\n",
      "        [1.2578]], requires_grad=True)\n",
      "Train loss at 672: tensor([2.5272], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5468],\n",
      "        [1.2576]], requires_grad=True)\n",
      "Train loss at 673: tensor([2.5272], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5471],\n",
      "        [1.2574]], requires_grad=True)\n",
      "Train loss at 674: tensor([2.5271], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5473],\n",
      "        [1.2572]], requires_grad=True)\n",
      "Train loss at 675: tensor([2.5271], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5476],\n",
      "        [1.2570]], requires_grad=True)\n",
      "Train loss at 676: tensor([2.5270], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5478],\n",
      "        [1.2568]], requires_grad=True)\n",
      "Train loss at 677: tensor([2.5270], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5481],\n",
      "        [1.2566]], requires_grad=True)\n",
      "Train loss at 678: tensor([2.5270], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5484],\n",
      "        [1.2564]], requires_grad=True)\n",
      "Train loss at 679: tensor([2.5269], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5486],\n",
      "        [1.2562]], requires_grad=True)\n",
      "Train loss at 680: tensor([2.5269], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5489],\n",
      "        [1.2560]], requires_grad=True)\n",
      "Train loss at 681: tensor([2.5269], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5491],\n",
      "        [1.2558]], requires_grad=True)\n",
      "Train loss at 682: tensor([2.5268], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5494],\n",
      "        [1.2556]], requires_grad=True)\n",
      "Train loss at 683: tensor([2.5268], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5496],\n",
      "        [1.2554]], requires_grad=True)\n",
      "Train loss at 684: tensor([2.5268], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5499],\n",
      "        [1.2552]], requires_grad=True)\n",
      "Train loss at 685: tensor([2.5267], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5501],\n",
      "        [1.2550]], requires_grad=True)\n",
      "Train loss at 686: tensor([2.5267], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5504],\n",
      "        [1.2548]], requires_grad=True)\n",
      "Train loss at 687: tensor([2.5266], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5506],\n",
      "        [1.2546]], requires_grad=True)\n",
      "Train loss at 688: tensor([2.5266], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5509],\n",
      "        [1.2544]], requires_grad=True)\n",
      "Train loss at 689: tensor([2.5266], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5511],\n",
      "        [1.2542]], requires_grad=True)\n",
      "Train loss at 690: tensor([2.5265], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5514],\n",
      "        [1.2540]], requires_grad=True)\n",
      "Train loss at 691: tensor([2.5265], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5516],\n",
      "        [1.2538]], requires_grad=True)\n",
      "Train loss at 692: tensor([2.5265], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5519],\n",
      "        [1.2536]], requires_grad=True)\n",
      "Train loss at 693: tensor([2.5264], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5521],\n",
      "        [1.2534]], requires_grad=True)\n",
      "Train loss at 694: tensor([2.5264], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5524],\n",
      "        [1.2532]], requires_grad=True)\n",
      "Train loss at 695: tensor([2.5264], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5526],\n",
      "        [1.2530]], requires_grad=True)\n",
      "Train loss at 696: tensor([2.5263], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5529],\n",
      "        [1.2528]], requires_grad=True)\n",
      "Train loss at 697: tensor([2.5263], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5531],\n",
      "        [1.2526]], requires_grad=True)\n",
      "Train loss at 698: tensor([2.5263], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5533],\n",
      "        [1.2525]], requires_grad=True)\n",
      "Train loss at 699: tensor([2.5262], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5536],\n",
      "        [1.2523]], requires_grad=True)\n",
      "Train loss at 700: tensor([2.5262], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5538],\n",
      "        [1.2521]], requires_grad=True)\n",
      "Train loss at 701: tensor([2.5262], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5540],\n",
      "        [1.2519]], requires_grad=True)\n",
      "Train loss at 702: tensor([2.5261], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5543],\n",
      "        [1.2517]], requires_grad=True)\n",
      "Train loss at 703: tensor([2.5261], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5545],\n",
      "        [1.2515]], requires_grad=True)\n",
      "Train loss at 704: tensor([2.5261], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5548],\n",
      "        [1.2513]], requires_grad=True)\n",
      "Train loss at 705: tensor([2.5260], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5550],\n",
      "        [1.2511]], requires_grad=True)\n",
      "Train loss at 706: tensor([2.5260], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5552],\n",
      "        [1.2509]], requires_grad=True)\n",
      "Train loss at 707: tensor([2.5260], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5555],\n",
      "        [1.2508]], requires_grad=True)\n",
      "Train loss at 708: tensor([2.5260], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5557],\n",
      "        [1.2506]], requires_grad=True)\n",
      "Train loss at 709: tensor([2.5259], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5559],\n",
      "        [1.2504]], requires_grad=True)\n",
      "Train loss at 710: tensor([2.5259], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5561],\n",
      "        [1.2502]], requires_grad=True)\n",
      "Train loss at 711: tensor([2.5259], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5564],\n",
      "        [1.2500]], requires_grad=True)\n",
      "Train loss at 712: tensor([2.5258], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5566],\n",
      "        [1.2498]], requires_grad=True)\n",
      "Train loss at 713: tensor([2.5258], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5568],\n",
      "        [1.2497]], requires_grad=True)\n",
      "Train loss at 714: tensor([2.5258], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5571],\n",
      "        [1.2495]], requires_grad=True)\n",
      "Train loss at 715: tensor([2.5257], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5573],\n",
      "        [1.2493]], requires_grad=True)\n",
      "Train loss at 716: tensor([2.5257], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5575],\n",
      "        [1.2491]], requires_grad=True)\n",
      "Train loss at 717: tensor([2.5257], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5577],\n",
      "        [1.2489]], requires_grad=True)\n",
      "Train loss at 718: tensor([2.5257], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5579],\n",
      "        [1.2488]], requires_grad=True)\n",
      "Train loss at 719: tensor([2.5256], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5582],\n",
      "        [1.2486]], requires_grad=True)\n",
      "Train loss at 720: tensor([2.5256], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5584],\n",
      "        [1.2484]], requires_grad=True)\n",
      "Train loss at 721: tensor([2.5256], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5586],\n",
      "        [1.2482]], requires_grad=True)\n",
      "Train loss at 722: tensor([2.5255], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5588],\n",
      "        [1.2480]], requires_grad=True)\n",
      "Train loss at 723: tensor([2.5255], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5590],\n",
      "        [1.2479]], requires_grad=True)\n",
      "Train loss at 724: tensor([2.5255], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5593],\n",
      "        [1.2477]], requires_grad=True)\n",
      "Train loss at 725: tensor([2.5255], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5595],\n",
      "        [1.2475]], requires_grad=True)\n",
      "Train loss at 726: tensor([2.5254], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5597],\n",
      "        [1.2473]], requires_grad=True)\n",
      "Train loss at 727: tensor([2.5254], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5599],\n",
      "        [1.2472]], requires_grad=True)\n",
      "Train loss at 728: tensor([2.5254], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5601],\n",
      "        [1.2470]], requires_grad=True)\n",
      "Train loss at 729: tensor([2.5254], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5603],\n",
      "        [1.2468]], requires_grad=True)\n",
      "Train loss at 730: tensor([2.5253], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5606],\n",
      "        [1.2467]], requires_grad=True)\n",
      "Train loss at 731: tensor([2.5253], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5608],\n",
      "        [1.2465]], requires_grad=True)\n",
      "Train loss at 732: tensor([2.5253], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5610],\n",
      "        [1.2463]], requires_grad=True)\n",
      "Train loss at 733: tensor([2.5253], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5612],\n",
      "        [1.2461]], requires_grad=True)\n",
      "Train loss at 734: tensor([2.5252], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5614],\n",
      "        [1.2460]], requires_grad=True)\n",
      "Train loss at 735: tensor([2.5252], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5616],\n",
      "        [1.2458]], requires_grad=True)\n",
      "Train loss at 736: tensor([2.5252], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5618],\n",
      "        [1.2456]], requires_grad=True)\n",
      "Train loss at 737: tensor([2.5252], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5620],\n",
      "        [1.2455]], requires_grad=True)\n",
      "Train loss at 738: tensor([2.5251], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5622],\n",
      "        [1.2453]], requires_grad=True)\n",
      "Train loss at 739: tensor([2.5251], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5624],\n",
      "        [1.2451]], requires_grad=True)\n",
      "Train loss at 740: tensor([2.5251], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5626],\n",
      "        [1.2450]], requires_grad=True)\n",
      "Train loss at 741: tensor([2.5251], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5629],\n",
      "        [1.2448]], requires_grad=True)\n",
      "Train loss at 742: tensor([2.5250], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5631],\n",
      "        [1.2446]], requires_grad=True)\n",
      "Train loss at 743: tensor([2.5250], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5633],\n",
      "        [1.2445]], requires_grad=True)\n",
      "Train loss at 744: tensor([2.5250], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5635],\n",
      "        [1.2443]], requires_grad=True)\n",
      "Train loss at 745: tensor([2.5250], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5637],\n",
      "        [1.2441]], requires_grad=True)\n",
      "Train loss at 746: tensor([2.5249], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5639],\n",
      "        [1.2440]], requires_grad=True)\n",
      "Train loss at 747: tensor([2.5249], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5641],\n",
      "        [1.2438]], requires_grad=True)\n",
      "Train loss at 748: tensor([2.5249], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5643],\n",
      "        [1.2437]], requires_grad=True)\n",
      "Train loss at 749: tensor([2.5249], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5645],\n",
      "        [1.2435]], requires_grad=True)\n",
      "Train loss at 750: tensor([2.5249], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5647],\n",
      "        [1.2433]], requires_grad=True)\n",
      "Train loss at 751: tensor([2.5248], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5649],\n",
      "        [1.2432]], requires_grad=True)\n",
      "Train loss at 752: tensor([2.5248], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5651],\n",
      "        [1.2430]], requires_grad=True)\n",
      "Train loss at 753: tensor([2.5248], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5652],\n",
      "        [1.2429]], requires_grad=True)\n",
      "Train loss at 754: tensor([2.5248], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5654],\n",
      "        [1.2427]], requires_grad=True)\n",
      "Train loss at 755: tensor([2.5247], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5656],\n",
      "        [1.2425]], requires_grad=True)\n",
      "Train loss at 756: tensor([2.5247], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5658],\n",
      "        [1.2424]], requires_grad=True)\n",
      "Train loss at 757: tensor([2.5247], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5660],\n",
      "        [1.2422]], requires_grad=True)\n",
      "Train loss at 758: tensor([2.5247], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5662],\n",
      "        [1.2421]], requires_grad=True)\n",
      "Train loss at 759: tensor([2.5247], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5664],\n",
      "        [1.2419]], requires_grad=True)\n",
      "Train loss at 760: tensor([2.5246], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5666],\n",
      "        [1.2418]], requires_grad=True)\n",
      "Train loss at 761: tensor([2.5246], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5668],\n",
      "        [1.2416]], requires_grad=True)\n",
      "Train loss at 762: tensor([2.5246], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5670],\n",
      "        [1.2415]], requires_grad=True)\n",
      "Train loss at 763: tensor([2.5246], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5672],\n",
      "        [1.2413]], requires_grad=True)\n",
      "Train loss at 764: tensor([2.5245], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5674],\n",
      "        [1.2411]], requires_grad=True)\n",
      "Train loss at 765: tensor([2.5245], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5675],\n",
      "        [1.2410]], requires_grad=True)\n",
      "Train loss at 766: tensor([2.5245], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5677],\n",
      "        [1.2408]], requires_grad=True)\n",
      "Train loss at 767: tensor([2.5245], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5679],\n",
      "        [1.2407]], requires_grad=True)\n",
      "Train loss at 768: tensor([2.5245], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5681],\n",
      "        [1.2405]], requires_grad=True)\n",
      "Train loss at 769: tensor([2.5244], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5683],\n",
      "        [1.2404]], requires_grad=True)\n",
      "Train loss at 770: tensor([2.5244], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5685],\n",
      "        [1.2402]], requires_grad=True)\n",
      "Train loss at 771: tensor([2.5244], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5687],\n",
      "        [1.2401]], requires_grad=True)\n",
      "Train loss at 772: tensor([2.5244], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5688],\n",
      "        [1.2399]], requires_grad=True)\n",
      "Train loss at 773: tensor([2.5244], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5690],\n",
      "        [1.2398]], requires_grad=True)\n",
      "Train loss at 774: tensor([2.5244], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5692],\n",
      "        [1.2396]], requires_grad=True)\n",
      "Train loss at 775: tensor([2.5243], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5694],\n",
      "        [1.2395]], requires_grad=True)\n",
      "Train loss at 776: tensor([2.5243], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5696],\n",
      "        [1.2393]], requires_grad=True)\n",
      "Train loss at 777: tensor([2.5243], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5697],\n",
      "        [1.2392]], requires_grad=True)\n",
      "Train loss at 778: tensor([2.5243], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5699],\n",
      "        [1.2391]], requires_grad=True)\n",
      "Train loss at 779: tensor([2.5243], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5701],\n",
      "        [1.2389]], requires_grad=True)\n",
      "Train loss at 780: tensor([2.5242], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5703],\n",
      "        [1.2388]], requires_grad=True)\n",
      "Train loss at 781: tensor([2.5242], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5705],\n",
      "        [1.2386]], requires_grad=True)\n",
      "Train loss at 782: tensor([2.5242], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5706],\n",
      "        [1.2385]], requires_grad=True)\n",
      "Train loss at 783: tensor([2.5242], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5708],\n",
      "        [1.2383]], requires_grad=True)\n",
      "Train loss at 784: tensor([2.5242], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5710],\n",
      "        [1.2382]], requires_grad=True)\n",
      "Train loss at 785: tensor([2.5242], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5712],\n",
      "        [1.2380]], requires_grad=True)\n",
      "Train loss at 786: tensor([2.5241], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5713],\n",
      "        [1.2379]], requires_grad=True)\n",
      "Train loss at 787: tensor([2.5241], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5715],\n",
      "        [1.2378]], requires_grad=True)\n",
      "Train loss at 788: tensor([2.5241], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5717],\n",
      "        [1.2376]], requires_grad=True)\n",
      "Train loss at 789: tensor([2.5241], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5719],\n",
      "        [1.2375]], requires_grad=True)\n",
      "Train loss at 790: tensor([2.5241], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5720],\n",
      "        [1.2373]], requires_grad=True)\n",
      "Train loss at 791: tensor([2.5240], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5722],\n",
      "        [1.2372]], requires_grad=True)\n",
      "Train loss at 792: tensor([2.5240], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5724],\n",
      "        [1.2371]], requires_grad=True)\n",
      "Train loss at 793: tensor([2.5240], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5725],\n",
      "        [1.2369]], requires_grad=True)\n",
      "Train loss at 794: tensor([2.5240], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5727],\n",
      "        [1.2368]], requires_grad=True)\n",
      "Train loss at 795: tensor([2.5240], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5729],\n",
      "        [1.2366]], requires_grad=True)\n",
      "Train loss at 796: tensor([2.5240], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5730],\n",
      "        [1.2365]], requires_grad=True)\n",
      "Train loss at 797: tensor([2.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5732],\n",
      "        [1.2364]], requires_grad=True)\n",
      "Train loss at 798: tensor([2.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5734],\n",
      "        [1.2362]], requires_grad=True)\n",
      "Train loss at 799: tensor([2.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5735],\n",
      "        [1.2361]], requires_grad=True)\n",
      "Train loss at 800: tensor([2.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5737],\n",
      "        [1.2360]], requires_grad=True)\n",
      "Train loss at 801: tensor([2.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5739],\n",
      "        [1.2358]], requires_grad=True)\n",
      "Train loss at 802: tensor([2.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5740],\n",
      "        [1.2357]], requires_grad=True)\n",
      "Train loss at 803: tensor([2.5239], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5742],\n",
      "        [1.2356]], requires_grad=True)\n",
      "Train loss at 804: tensor([2.5238], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5744],\n",
      "        [1.2354]], requires_grad=True)\n",
      "Train loss at 805: tensor([2.5238], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5745],\n",
      "        [1.2353]], requires_grad=True)\n",
      "Train loss at 806: tensor([2.5238], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5747],\n",
      "        [1.2352]], requires_grad=True)\n",
      "Train loss at 807: tensor([2.5238], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5748],\n",
      "        [1.2350]], requires_grad=True)\n",
      "Train loss at 808: tensor([2.5238], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5750],\n",
      "        [1.2349]], requires_grad=True)\n",
      "Train loss at 809: tensor([2.5238], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5752],\n",
      "        [1.2348]], requires_grad=True)\n",
      "Train loss at 810: tensor([2.5237], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5753],\n",
      "        [1.2346]], requires_grad=True)\n",
      "Train loss at 811: tensor([2.5237], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5755],\n",
      "        [1.2345]], requires_grad=True)\n",
      "Train loss at 812: tensor([2.5237], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5756],\n",
      "        [1.2344]], requires_grad=True)\n",
      "Train loss at 813: tensor([2.5237], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5758],\n",
      "        [1.2343]], requires_grad=True)\n",
      "Train loss at 814: tensor([2.5237], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5760],\n",
      "        [1.2341]], requires_grad=True)\n",
      "Train loss at 815: tensor([2.5237], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5761],\n",
      "        [1.2340]], requires_grad=True)\n",
      "Train loss at 816: tensor([2.5237], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5763],\n",
      "        [1.2339]], requires_grad=True)\n",
      "Train loss at 817: tensor([2.5236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5764],\n",
      "        [1.2337]], requires_grad=True)\n",
      "Train loss at 818: tensor([2.5236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5766],\n",
      "        [1.2336]], requires_grad=True)\n",
      "Train loss at 819: tensor([2.5236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5767],\n",
      "        [1.2335]], requires_grad=True)\n",
      "Train loss at 820: tensor([2.5236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5769],\n",
      "        [1.2334]], requires_grad=True)\n",
      "Train loss at 821: tensor([2.5236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5770],\n",
      "        [1.2332]], requires_grad=True)\n",
      "Train loss at 822: tensor([2.5236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5772],\n",
      "        [1.2331]], requires_grad=True)\n",
      "Train loss at 823: tensor([2.5236], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5773],\n",
      "        [1.2330]], requires_grad=True)\n",
      "Train loss at 824: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5775],\n",
      "        [1.2329]], requires_grad=True)\n",
      "Train loss at 825: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5776],\n",
      "        [1.2327]], requires_grad=True)\n",
      "Train loss at 826: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5778],\n",
      "        [1.2326]], requires_grad=True)\n",
      "Train loss at 827: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5779],\n",
      "        [1.2325]], requires_grad=True)\n",
      "Train loss at 828: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5781],\n",
      "        [1.2324]], requires_grad=True)\n",
      "Train loss at 829: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5782],\n",
      "        [1.2322]], requires_grad=True)\n",
      "Train loss at 830: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5784],\n",
      "        [1.2321]], requires_grad=True)\n",
      "Train loss at 831: tensor([2.5235], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5785],\n",
      "        [1.2320]], requires_grad=True)\n",
      "Train loss at 832: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5787],\n",
      "        [1.2319]], requires_grad=True)\n",
      "Train loss at 833: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5788],\n",
      "        [1.2318]], requires_grad=True)\n",
      "Train loss at 834: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5790],\n",
      "        [1.2316]], requires_grad=True)\n",
      "Train loss at 835: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5791],\n",
      "        [1.2315]], requires_grad=True)\n",
      "Train loss at 836: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5793],\n",
      "        [1.2314]], requires_grad=True)\n",
      "Train loss at 837: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5794],\n",
      "        [1.2313]], requires_grad=True)\n",
      "Train loss at 838: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5796],\n",
      "        [1.2312]], requires_grad=True)\n",
      "Train loss at 839: tensor([2.5234], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5797],\n",
      "        [1.2310]], requires_grad=True)\n",
      "Train loss at 840: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5798],\n",
      "        [1.2309]], requires_grad=True)\n",
      "Train loss at 841: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5800],\n",
      "        [1.2308]], requires_grad=True)\n",
      "Train loss at 842: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5801],\n",
      "        [1.2307]], requires_grad=True)\n",
      "Train loss at 843: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5803],\n",
      "        [1.2306]], requires_grad=True)\n",
      "Train loss at 844: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5804],\n",
      "        [1.2305]], requires_grad=True)\n",
      "Train loss at 845: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5806],\n",
      "        [1.2303]], requires_grad=True)\n",
      "Train loss at 846: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5807],\n",
      "        [1.2302]], requires_grad=True)\n",
      "Train loss at 847: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5808],\n",
      "        [1.2301]], requires_grad=True)\n",
      "Train loss at 848: tensor([2.5233], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5810],\n",
      "        [1.2300]], requires_grad=True)\n",
      "Train loss at 849: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5811],\n",
      "        [1.2299]], requires_grad=True)\n",
      "Train loss at 850: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5813],\n",
      "        [1.2298]], requires_grad=True)\n",
      "Train loss at 851: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5814],\n",
      "        [1.2297]], requires_grad=True)\n",
      "Train loss at 852: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5815],\n",
      "        [1.2295]], requires_grad=True)\n",
      "Train loss at 853: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5817],\n",
      "        [1.2294]], requires_grad=True)\n",
      "Train loss at 854: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5818],\n",
      "        [1.2293]], requires_grad=True)\n",
      "Train loss at 855: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5819],\n",
      "        [1.2292]], requires_grad=True)\n",
      "Train loss at 856: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5821],\n",
      "        [1.2291]], requires_grad=True)\n",
      "Train loss at 857: tensor([2.5232], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5822],\n",
      "        [1.2290]], requires_grad=True)\n",
      "Train loss at 858: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5823],\n",
      "        [1.2289]], requires_grad=True)\n",
      "Train loss at 859: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5825],\n",
      "        [1.2288]], requires_grad=True)\n",
      "Train loss at 860: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5826],\n",
      "        [1.2287]], requires_grad=True)\n",
      "Train loss at 861: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5827],\n",
      "        [1.2286]], requires_grad=True)\n",
      "Train loss at 862: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5829],\n",
      "        [1.2284]], requires_grad=True)\n",
      "Train loss at 863: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5830],\n",
      "        [1.2283]], requires_grad=True)\n",
      "Train loss at 864: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5831],\n",
      "        [1.2282]], requires_grad=True)\n",
      "Train loss at 865: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5833],\n",
      "        [1.2281]], requires_grad=True)\n",
      "Train loss at 866: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5834],\n",
      "        [1.2280]], requires_grad=True)\n",
      "Train loss at 867: tensor([2.5231], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5835],\n",
      "        [1.2279]], requires_grad=True)\n",
      "Train loss at 868: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5837],\n",
      "        [1.2278]], requires_grad=True)\n",
      "Train loss at 869: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5838],\n",
      "        [1.2277]], requires_grad=True)\n",
      "Train loss at 870: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5839],\n",
      "        [1.2276]], requires_grad=True)\n",
      "Train loss at 871: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5840],\n",
      "        [1.2275]], requires_grad=True)\n",
      "Train loss at 872: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5842],\n",
      "        [1.2274]], requires_grad=True)\n",
      "Train loss at 873: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5843],\n",
      "        [1.2273]], requires_grad=True)\n",
      "Train loss at 874: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5844],\n",
      "        [1.2272]], requires_grad=True)\n",
      "Train loss at 875: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5845],\n",
      "        [1.2271]], requires_grad=True)\n",
      "Train loss at 876: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5847],\n",
      "        [1.2270]], requires_grad=True)\n",
      "Train loss at 877: tensor([2.5230], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5848],\n",
      "        [1.2269]], requires_grad=True)\n",
      "Train loss at 878: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5849],\n",
      "        [1.2268]], requires_grad=True)\n",
      "Train loss at 879: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5851],\n",
      "        [1.2267]], requires_grad=True)\n",
      "Train loss at 880: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5852],\n",
      "        [1.2265]], requires_grad=True)\n",
      "Train loss at 881: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5853],\n",
      "        [1.2264]], requires_grad=True)\n",
      "Train loss at 882: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5854],\n",
      "        [1.2263]], requires_grad=True)\n",
      "Train loss at 883: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5855],\n",
      "        [1.2262]], requires_grad=True)\n",
      "Train loss at 884: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5857],\n",
      "        [1.2261]], requires_grad=True)\n",
      "Train loss at 885: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5858],\n",
      "        [1.2260]], requires_grad=True)\n",
      "Train loss at 886: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5859],\n",
      "        [1.2259]], requires_grad=True)\n",
      "Train loss at 887: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5860],\n",
      "        [1.2258]], requires_grad=True)\n",
      "Train loss at 888: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5862],\n",
      "        [1.2257]], requires_grad=True)\n",
      "Train loss at 889: tensor([2.5229], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5863],\n",
      "        [1.2256]], requires_grad=True)\n",
      "Train loss at 890: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5864],\n",
      "        [1.2255]], requires_grad=True)\n",
      "Train loss at 891: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5865],\n",
      "        [1.2255]], requires_grad=True)\n",
      "Train loss at 892: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5866],\n",
      "        [1.2254]], requires_grad=True)\n",
      "Train loss at 893: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5868],\n",
      "        [1.2253]], requires_grad=True)\n",
      "Train loss at 894: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5869],\n",
      "        [1.2252]], requires_grad=True)\n",
      "Train loss at 895: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5870],\n",
      "        [1.2251]], requires_grad=True)\n",
      "Train loss at 896: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5871],\n",
      "        [1.2250]], requires_grad=True)\n",
      "Train loss at 897: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5872],\n",
      "        [1.2249]], requires_grad=True)\n",
      "Train loss at 898: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5873],\n",
      "        [1.2248]], requires_grad=True)\n",
      "Train loss at 899: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5875],\n",
      "        [1.2247]], requires_grad=True)\n",
      "Train loss at 900: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5876],\n",
      "        [1.2246]], requires_grad=True)\n",
      "Train loss at 901: tensor([2.5228], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5877],\n",
      "        [1.2245]], requires_grad=True)\n",
      "Train loss at 902: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5878],\n",
      "        [1.2244]], requires_grad=True)\n",
      "Train loss at 903: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5879],\n",
      "        [1.2243]], requires_grad=True)\n",
      "Train loss at 904: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5880],\n",
      "        [1.2242]], requires_grad=True)\n",
      "Train loss at 905: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5881],\n",
      "        [1.2241]], requires_grad=True)\n",
      "Train loss at 906: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5883],\n",
      "        [1.2240]], requires_grad=True)\n",
      "Train loss at 907: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5884],\n",
      "        [1.2239]], requires_grad=True)\n",
      "Train loss at 908: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5885],\n",
      "        [1.2238]], requires_grad=True)\n",
      "Train loss at 909: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5886],\n",
      "        [1.2237]], requires_grad=True)\n",
      "Train loss at 910: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5887],\n",
      "        [1.2237]], requires_grad=True)\n",
      "Train loss at 911: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5888],\n",
      "        [1.2236]], requires_grad=True)\n",
      "Train loss at 912: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5889],\n",
      "        [1.2235]], requires_grad=True)\n",
      "Train loss at 913: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5890],\n",
      "        [1.2234]], requires_grad=True)\n",
      "Train loss at 914: tensor([2.5227], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5891],\n",
      "        [1.2233]], requires_grad=True)\n",
      "Train loss at 915: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5893],\n",
      "        [1.2232]], requires_grad=True)\n",
      "Train loss at 916: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5894],\n",
      "        [1.2231]], requires_grad=True)\n",
      "Train loss at 917: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5895],\n",
      "        [1.2230]], requires_grad=True)\n",
      "Train loss at 918: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5896],\n",
      "        [1.2229]], requires_grad=True)\n",
      "Train loss at 919: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5897],\n",
      "        [1.2228]], requires_grad=True)\n",
      "Train loss at 920: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5898],\n",
      "        [1.2228]], requires_grad=True)\n",
      "Train loss at 921: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5899],\n",
      "        [1.2227]], requires_grad=True)\n",
      "Train loss at 922: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5900],\n",
      "        [1.2226]], requires_grad=True)\n",
      "Train loss at 923: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5901],\n",
      "        [1.2225]], requires_grad=True)\n",
      "Train loss at 924: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5902],\n",
      "        [1.2224]], requires_grad=True)\n",
      "Train loss at 925: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5903],\n",
      "        [1.2223]], requires_grad=True)\n",
      "Train loss at 926: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5904],\n",
      "        [1.2222]], requires_grad=True)\n",
      "Train loss at 927: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5905],\n",
      "        [1.2221]], requires_grad=True)\n",
      "Train loss at 928: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5906],\n",
      "        [1.2221]], requires_grad=True)\n",
      "Train loss at 929: tensor([2.5226], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5907],\n",
      "        [1.2220]], requires_grad=True)\n",
      "Train loss at 930: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5909],\n",
      "        [1.2219]], requires_grad=True)\n",
      "Train loss at 931: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5910],\n",
      "        [1.2218]], requires_grad=True)\n",
      "Train loss at 932: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5911],\n",
      "        [1.2217]], requires_grad=True)\n",
      "Train loss at 933: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5912],\n",
      "        [1.2216]], requires_grad=True)\n",
      "Train loss at 934: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5913],\n",
      "        [1.2215]], requires_grad=True)\n",
      "Train loss at 935: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5914],\n",
      "        [1.2215]], requires_grad=True)\n",
      "Train loss at 936: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5915],\n",
      "        [1.2214]], requires_grad=True)\n",
      "Train loss at 937: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5916],\n",
      "        [1.2213]], requires_grad=True)\n",
      "Train loss at 938: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5917],\n",
      "        [1.2212]], requires_grad=True)\n",
      "Train loss at 939: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5918],\n",
      "        [1.2211]], requires_grad=True)\n",
      "Train loss at 940: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5919],\n",
      "        [1.2210]], requires_grad=True)\n",
      "Train loss at 941: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5920],\n",
      "        [1.2210]], requires_grad=True)\n",
      "Train loss at 942: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5921],\n",
      "        [1.2209]], requires_grad=True)\n",
      "Train loss at 943: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5922],\n",
      "        [1.2208]], requires_grad=True)\n",
      "Train loss at 944: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5923],\n",
      "        [1.2207]], requires_grad=True)\n",
      "Train loss at 945: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5924],\n",
      "        [1.2206]], requires_grad=True)\n",
      "Train loss at 946: tensor([2.5225], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5925],\n",
      "        [1.2206]], requires_grad=True)\n",
      "Train loss at 947: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5926],\n",
      "        [1.2205]], requires_grad=True)\n",
      "Train loss at 948: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5927],\n",
      "        [1.2204]], requires_grad=True)\n",
      "Train loss at 949: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5928],\n",
      "        [1.2203]], requires_grad=True)\n",
      "Train loss at 950: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5929],\n",
      "        [1.2202]], requires_grad=True)\n",
      "Train loss at 951: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5930],\n",
      "        [1.2202]], requires_grad=True)\n",
      "Train loss at 952: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5931],\n",
      "        [1.2201]], requires_grad=True)\n",
      "Train loss at 953: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5931],\n",
      "        [1.2200]], requires_grad=True)\n",
      "Train loss at 954: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5932],\n",
      "        [1.2199]], requires_grad=True)\n",
      "Train loss at 955: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5933],\n",
      "        [1.2198]], requires_grad=True)\n",
      "Train loss at 956: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5934],\n",
      "        [1.2198]], requires_grad=True)\n",
      "Train loss at 957: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5935],\n",
      "        [1.2197]], requires_grad=True)\n",
      "Train loss at 958: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5936],\n",
      "        [1.2196]], requires_grad=True)\n",
      "Train loss at 959: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5937],\n",
      "        [1.2195]], requires_grad=True)\n",
      "Train loss at 960: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5938],\n",
      "        [1.2195]], requires_grad=True)\n",
      "Train loss at 961: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5939],\n",
      "        [1.2194]], requires_grad=True)\n",
      "Train loss at 962: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5940],\n",
      "        [1.2193]], requires_grad=True)\n",
      "Train loss at 963: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5941],\n",
      "        [1.2192]], requires_grad=True)\n",
      "Train loss at 964: tensor([2.5224], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5942],\n",
      "        [1.2192]], requires_grad=True)\n",
      "Train loss at 965: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5943],\n",
      "        [1.2191]], requires_grad=True)\n",
      "Train loss at 966: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5944],\n",
      "        [1.2190]], requires_grad=True)\n",
      "Train loss at 967: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5945],\n",
      "        [1.2189]], requires_grad=True)\n",
      "Train loss at 968: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5945],\n",
      "        [1.2189]], requires_grad=True)\n",
      "Train loss at 969: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5946],\n",
      "        [1.2188]], requires_grad=True)\n",
      "Train loss at 970: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5947],\n",
      "        [1.2187]], requires_grad=True)\n",
      "Train loss at 971: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5948],\n",
      "        [1.2186]], requires_grad=True)\n",
      "Train loss at 972: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5949],\n",
      "        [1.2186]], requires_grad=True)\n",
      "Train loss at 973: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5950],\n",
      "        [1.2185]], requires_grad=True)\n",
      "Train loss at 974: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5951],\n",
      "        [1.2184]], requires_grad=True)\n",
      "Train loss at 975: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5952],\n",
      "        [1.2183]], requires_grad=True)\n",
      "Train loss at 976: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5953],\n",
      "        [1.2183]], requires_grad=True)\n",
      "Train loss at 977: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5954],\n",
      "        [1.2182]], requires_grad=True)\n",
      "Train loss at 978: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5954],\n",
      "        [1.2181]], requires_grad=True)\n",
      "Train loss at 979: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5955],\n",
      "        [1.2181]], requires_grad=True)\n",
      "Train loss at 980: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5956],\n",
      "        [1.2180]], requires_grad=True)\n",
      "Train loss at 981: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5957],\n",
      "        [1.2179]], requires_grad=True)\n",
      "Train loss at 982: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5958],\n",
      "        [1.2178]], requires_grad=True)\n",
      "Train loss at 983: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5959],\n",
      "        [1.2178]], requires_grad=True)\n",
      "Train loss at 984: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5960],\n",
      "        [1.2177]], requires_grad=True)\n",
      "Train loss at 985: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5960],\n",
      "        [1.2176]], requires_grad=True)\n",
      "Train loss at 986: tensor([2.5223], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5961],\n",
      "        [1.2176]], requires_grad=True)\n",
      "Train loss at 987: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5962],\n",
      "        [1.2175]], requires_grad=True)\n",
      "Train loss at 988: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5963],\n",
      "        [1.2174]], requires_grad=True)\n",
      "Train loss at 989: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5964],\n",
      "        [1.2174]], requires_grad=True)\n",
      "Train loss at 990: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5965],\n",
      "        [1.2173]], requires_grad=True)\n",
      "Train loss at 991: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5966],\n",
      "        [1.2172]], requires_grad=True)\n",
      "Train loss at 992: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5966],\n",
      "        [1.2171]], requires_grad=True)\n",
      "Train loss at 993: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5967],\n",
      "        [1.2171]], requires_grad=True)\n",
      "Train loss at 994: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5968],\n",
      "        [1.2170]], requires_grad=True)\n",
      "Train loss at 995: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5969],\n",
      "        [1.2169]], requires_grad=True)\n",
      "Train loss at 996: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5970],\n",
      "        [1.2169]], requires_grad=True)\n",
      "Train loss at 997: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5971],\n",
      "        [1.2168]], requires_grad=True)\n",
      "Train loss at 998: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5971],\n",
      "        [1.2167]], requires_grad=True)\n",
      "Train loss at 999: tensor([2.5222], grad_fn=<DivBackward0>)\n",
      "w:  tensor([[3.5972],\n",
      "        [1.2167]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# THIS BLOCK SERVES AS THE SANITY CHECK FOR THE MAIN TRAINING PROCESS\n",
    "torch.manual_seed(1)\n",
    "h_epoch = 10000  # Hyperparameter epoch\n",
    "epoch = 1000  # Epoch for training\n",
    "\n",
    "#Create underlying linear function\n",
    "x = torch.randn((10, 2))\n",
    "true_w = torch.tensor([[3.], [1.]])\n",
    "y = torch.matmul(x, true_w) + torch.randn((10, 1))\n",
    "\n",
    "# Split train_valid\n",
    "x_train = x[:8, ]\n",
    "y_train = y[:8, ]\n",
    "\n",
    "x_valid = x[8:, ]\n",
    "y_valid = y[8:, ]\n",
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[2.5], [1.3]], requires_grad=True)\n",
    "lamb = torch.tensor([0.01], requires_grad=True)  #Change the value form 3 to 0.1 and 0.01 and observe the behavior\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr = 0.001)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])\n",
    "for ep in range(epoch):\n",
    "    total_train_loss = 0\n",
    "    for i in range(len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb  * torch.sum(w ** 2)\n",
    "        total_train_loss += train_loss\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / len(x_train)))\n",
    "    print('w: ', w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following block combines the optimization of weight and hyperparameter together\n",
    "Observations:\n",
    "    - The training loss still decreases even when the hyperparameter is wrong, but after hyperparameter adjusts, the training loss seems to decreases compared to previous hepoch, but not during epoch.\n",
    "        - Question: Does this mean the loss already converge? Does this mean optimizing lambda is not important?\n",
    "        - Thought: Which hyperparameter should we optimize?\n",
    "    - After epoch 200, the hyperparameter becomes negative, w starts to increase?\n",
    "        - Question:\n",
    "    - Training epoch vs Hyperparam epoch\n",
    "    - Note: that we have individual lambda for each weight\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28.2738], grad_fn=<AddBackward0>)\n",
      "tensor([23.7981], grad_fn=<AddBackward0>)\n",
      "tensor([39.3672], grad_fn=<AddBackward0>)\n",
      "tensor([29.5345], grad_fn=<AddBackward0>)\n",
      "tensor([25.9012], grad_fn=<AddBackward0>)\n",
      "tensor([23.8358], grad_fn=<AddBackward0>)\n",
      "tensor([23.9760], grad_fn=<AddBackward0>)\n",
      "tensor([24.3781], grad_fn=<AddBackward0>)\n",
      "tensor([28.1708], grad_fn=<AddBackward0>)\n",
      "tensor([23.6195], grad_fn=<AddBackward0>)\n",
      "tensor([39.2583], grad_fn=<AddBackward0>)\n",
      "tensor([29.3523], grad_fn=<AddBackward0>)\n",
      "tensor([25.7606], grad_fn=<AddBackward0>)\n",
      "tensor([23.6493], grad_fn=<AddBackward0>)\n",
      "tensor([23.7943], grad_fn=<AddBackward0>)\n",
      "tensor([24.2133], grad_fn=<AddBackward0>)\n",
      "tensor([28.0699], grad_fn=<AddBackward0>)\n",
      "tensor([23.4425], grad_fn=<AddBackward0>)\n",
      "tensor([39.1504], grad_fn=<AddBackward0>)\n",
      "tensor([29.1711], grad_fn=<AddBackward0>)\n",
      "tensor([25.6213], grad_fn=<AddBackward0>)\n",
      "tensor([23.4642], grad_fn=<AddBackward0>)\n",
      "tensor([23.6137], grad_fn=<AddBackward0>)\n",
      "tensor([24.0499], grad_fn=<AddBackward0>)\n",
      "tensor([27.9705], grad_fn=<AddBackward0>)\n",
      "tensor([23.2673], grad_fn=<AddBackward0>)\n",
      "tensor([39.0438], grad_fn=<AddBackward0>)\n",
      "tensor([28.9912], grad_fn=<AddBackward0>)\n",
      "tensor([25.4834], grad_fn=<AddBackward0>)\n",
      "tensor([23.2807], grad_fn=<AddBackward0>)\n",
      "tensor([23.4344], grad_fn=<AddBackward0>)\n",
      "tensor([23.8878], grad_fn=<AddBackward0>)\n",
      "tensor([27.8728], grad_fn=<AddBackward0>)\n",
      "tensor([23.0941], grad_fn=<AddBackward0>)\n",
      "tensor([38.9384], grad_fn=<AddBackward0>)\n",
      "tensor([28.8127], grad_fn=<AddBackward0>)\n",
      "tensor([25.3471], grad_fn=<AddBackward0>)\n",
      "tensor([23.0991], grad_fn=<AddBackward0>)\n",
      "tensor([23.2566], grad_fn=<AddBackward0>)\n",
      "tensor([23.7272], grad_fn=<AddBackward0>)\n",
      "tensor([27.7769], grad_fn=<AddBackward0>)\n",
      "tensor([22.9229], grad_fn=<AddBackward0>)\n",
      "tensor([38.8343], grad_fn=<AddBackward0>)\n",
      "tensor([28.6357], grad_fn=<AddBackward0>)\n",
      "tensor([25.2123], grad_fn=<AddBackward0>)\n",
      "tensor([22.9191], grad_fn=<AddBackward0>)\n",
      "tensor([23.0803], grad_fn=<AddBackward0>)\n",
      "tensor([23.5680], grad_fn=<AddBackward0>)\n",
      "tensor([27.6826], grad_fn=<AddBackward0>)\n",
      "tensor([22.7536], grad_fn=<AddBackward0>)\n",
      "tensor([38.7315], grad_fn=<AddBackward0>)\n",
      "tensor([28.4601], grad_fn=<AddBackward0>)\n",
      "tensor([25.0791], grad_fn=<AddBackward0>)\n",
      "tensor([22.7409], grad_fn=<AddBackward0>)\n",
      "tensor([22.9054], grad_fn=<AddBackward0>)\n",
      "tensor([23.4102], grad_fn=<AddBackward0>)\n",
      "tensor([27.5900], grad_fn=<AddBackward0>)\n",
      "tensor([22.5863], grad_fn=<AddBackward0>)\n",
      "tensor([38.6300], grad_fn=<AddBackward0>)\n",
      "tensor([28.2859], grad_fn=<AddBackward0>)\n",
      "tensor([24.9473], grad_fn=<AddBackward0>)\n",
      "tensor([22.5645], grad_fn=<AddBackward0>)\n",
      "tensor([22.7319], grad_fn=<AddBackward0>)\n",
      "tensor([23.2539], grad_fn=<AddBackward0>)\n",
      "tensor([27.4991], grad_fn=<AddBackward0>)\n",
      "tensor([22.4209], grad_fn=<AddBackward0>)\n",
      "tensor([38.5297], grad_fn=<AddBackward0>)\n",
      "tensor([28.1131], grad_fn=<AddBackward0>)\n",
      "tensor([24.8170], grad_fn=<AddBackward0>)\n",
      "tensor([22.3897], grad_fn=<AddBackward0>)\n",
      "tensor([22.5598], grad_fn=<AddBackward0>)\n",
      "tensor([23.0989], grad_fn=<AddBackward0>)\n",
      "tensor([27.4098], grad_fn=<AddBackward0>)\n",
      "tensor([22.2575], grad_fn=<AddBackward0>)\n",
      "tensor([38.4306], grad_fn=<AddBackward0>)\n",
      "tensor([27.9418], grad_fn=<AddBackward0>)\n",
      "tensor([24.6881], grad_fn=<AddBackward0>)\n",
      "tensor([22.2166], grad_fn=<AddBackward0>)\n",
      "tensor([22.3890], grad_fn=<AddBackward0>)\n",
      "tensor([22.9454], grad_fn=<AddBackward0>)\n",
      "tensor([27.3222], grad_fn=<AddBackward0>)\n",
      "tensor([22.0959], grad_fn=<AddBackward0>)\n",
      "tensor([38.3328], grad_fn=<AddBackward0>)\n",
      "tensor([27.7718], grad_fn=<AddBackward0>)\n",
      "tensor([24.5607], grad_fn=<AddBackward0>)\n",
      "tensor([22.0452], grad_fn=<AddBackward0>)\n",
      "tensor([22.2197], grad_fn=<AddBackward0>)\n",
      "tensor([22.7932], grad_fn=<AddBackward0>)\n",
      "tensor([27.2362], grad_fn=<AddBackward0>)\n",
      "tensor([21.9362], grad_fn=<AddBackward0>)\n",
      "tensor([38.2361], grad_fn=<AddBackward0>)\n",
      "tensor([27.6032], grad_fn=<AddBackward0>)\n",
      "tensor([24.4347], grad_fn=<AddBackward0>)\n",
      "tensor([21.8755], grad_fn=<AddBackward0>)\n",
      "tensor([22.0517], grad_fn=<AddBackward0>)\n",
      "tensor([22.6424], grad_fn=<AddBackward0>)\n",
      "tensor([27.1517], grad_fn=<AddBackward0>)\n",
      "tensor([21.7783], grad_fn=<AddBackward0>)\n",
      "tensor([38.1407], grad_fn=<AddBackward0>)\n",
      "tensor([27.4360], grad_fn=<AddBackward0>)\n",
      "tensor([24.3100], grad_fn=<AddBackward0>)\n",
      "tensor([21.7073], grad_fn=<AddBackward0>)\n",
      "tensor([21.8851], grad_fn=<AddBackward0>)\n",
      "tensor([22.4929], grad_fn=<AddBackward0>)\n",
      "tensor([27.0689], grad_fn=<AddBackward0>)\n",
      "tensor([21.6222], grad_fn=<AddBackward0>)\n",
      "tensor([38.0464], grad_fn=<AddBackward0>)\n",
      "tensor([27.2701], grad_fn=<AddBackward0>)\n",
      "tensor([24.1868], grad_fn=<AddBackward0>)\n",
      "tensor([21.5408], grad_fn=<AddBackward0>)\n",
      "tensor([21.7197], grad_fn=<AddBackward0>)\n",
      "tensor([22.3447], grad_fn=<AddBackward0>)\n",
      "tensor([26.9875], grad_fn=<AddBackward0>)\n",
      "tensor([21.4679], grad_fn=<AddBackward0>)\n",
      "tensor([37.9533], grad_fn=<AddBackward0>)\n",
      "tensor([27.1056], grad_fn=<AddBackward0>)\n",
      "tensor([24.0649], grad_fn=<AddBackward0>)\n",
      "tensor([21.3759], grad_fn=<AddBackward0>)\n",
      "tensor([21.5557], grad_fn=<AddBackward0>)\n",
      "tensor([22.1979], grad_fn=<AddBackward0>)\n",
      "tensor([26.9078], grad_fn=<AddBackward0>)\n",
      "tensor([21.3154], grad_fn=<AddBackward0>)\n",
      "tensor([37.8614], grad_fn=<AddBackward0>)\n",
      "tensor([26.9424], grad_fn=<AddBackward0>)\n",
      "tensor([23.9444], grad_fn=<AddBackward0>)\n",
      "tensor([21.2125], grad_fn=<AddBackward0>)\n",
      "tensor([21.3930], grad_fn=<AddBackward0>)\n",
      "tensor([22.0523], grad_fn=<AddBackward0>)\n",
      "tensor([26.8295], grad_fn=<AddBackward0>)\n",
      "tensor([21.1647], grad_fn=<AddBackward0>)\n",
      "tensor([37.7706], grad_fn=<AddBackward0>)\n",
      "tensor([26.7805], grad_fn=<AddBackward0>)\n",
      "tensor([23.8252], grad_fn=<AddBackward0>)\n",
      "tensor([21.0508], grad_fn=<AddBackward0>)\n",
      "tensor([21.2316], grad_fn=<AddBackward0>)\n",
      "tensor([21.9081], grad_fn=<AddBackward0>)\n",
      "tensor([26.7527], grad_fn=<AddBackward0>)\n",
      "tensor([21.0157], grad_fn=<AddBackward0>)\n",
      "tensor([37.6809], grad_fn=<AddBackward0>)\n",
      "tensor([26.6199], grad_fn=<AddBackward0>)\n",
      "tensor([23.7074], grad_fn=<AddBackward0>)\n",
      "tensor([20.8906], grad_fn=<AddBackward0>)\n",
      "tensor([21.0715], grad_fn=<AddBackward0>)\n",
      "tensor([21.7651], grad_fn=<AddBackward0>)\n",
      "tensor([26.6775], grad_fn=<AddBackward0>)\n",
      "tensor([20.8684], grad_fn=<AddBackward0>)\n",
      "tensor([37.5924], grad_fn=<AddBackward0>)\n",
      "tensor([26.4607], grad_fn=<AddBackward0>)\n",
      "tensor([23.5908], grad_fn=<AddBackward0>)\n",
      "tensor([20.7319], grad_fn=<AddBackward0>)\n",
      "tensor([20.9126], grad_fn=<AddBackward0>)\n",
      "tensor([21.6234], grad_fn=<AddBackward0>)\n",
      "tensor([26.6037], grad_fn=<AddBackward0>)\n",
      "tensor([20.7228], grad_fn=<AddBackward0>)\n",
      "tensor([37.5050], grad_fn=<AddBackward0>)\n",
      "tensor([26.3027], grad_fn=<AddBackward0>)\n",
      "tensor([23.4756], grad_fn=<AddBackward0>)\n",
      "tensor([20.5748], grad_fn=<AddBackward0>)\n",
      "tensor([20.7550], grad_fn=<AddBackward0>)\n",
      "tensor([21.4830], grad_fn=<AddBackward0>)\n",
      "tensor([26.5313], grad_fn=<AddBackward0>)\n",
      "tensor([20.5789], grad_fn=<AddBackward0>)\n",
      "tensor([37.4187], grad_fn=<AddBackward0>)\n",
      "tensor([26.1460], grad_fn=<AddBackward0>)\n",
      "tensor([23.3616], grad_fn=<AddBackward0>)\n",
      "tensor([20.4192], grad_fn=<AddBackward0>)\n",
      "tensor([20.5987], grad_fn=<AddBackward0>)\n",
      "tensor([21.3438], grad_fn=<AddBackward0>)\n",
      "tensor([26.4604], grad_fn=<AddBackward0>)\n",
      "tensor([20.4367], grad_fn=<AddBackward0>)\n",
      "tensor([37.3335], grad_fn=<AddBackward0>)\n",
      "tensor([25.9906], grad_fn=<AddBackward0>)\n",
      "tensor([23.2489], grad_fn=<AddBackward0>)\n",
      "tensor([20.2651], grad_fn=<AddBackward0>)\n",
      "tensor([20.4436], grad_fn=<AddBackward0>)\n",
      "tensor([21.2058], grad_fn=<AddBackward0>)\n",
      "tensor([26.3910], grad_fn=<AddBackward0>)\n",
      "tensor([20.2961], grad_fn=<AddBackward0>)\n",
      "tensor([37.2493], grad_fn=<AddBackward0>)\n",
      "tensor([25.8364], grad_fn=<AddBackward0>)\n",
      "tensor([23.1375], grad_fn=<AddBackward0>)\n",
      "tensor([20.1125], grad_fn=<AddBackward0>)\n",
      "tensor([20.2897], grad_fn=<AddBackward0>)\n",
      "tensor([21.0691], grad_fn=<AddBackward0>)\n",
      "tensor([26.3230], grad_fn=<AddBackward0>)\n",
      "tensor([20.1572], grad_fn=<AddBackward0>)\n",
      "tensor([37.1663], grad_fn=<AddBackward0>)\n",
      "tensor([25.6835], grad_fn=<AddBackward0>)\n",
      "tensor([23.0273], grad_fn=<AddBackward0>)\n",
      "tensor([19.9615], grad_fn=<AddBackward0>)\n",
      "tensor([20.1371], grad_fn=<AddBackward0>)\n",
      "tensor([20.9336], grad_fn=<AddBackward0>)\n",
      "tensor([26.2563], grad_fn=<AddBackward0>)\n",
      "tensor([20.0200], grad_fn=<AddBackward0>)\n",
      "tensor([37.0843], grad_fn=<AddBackward0>)\n",
      "tensor([25.5319], grad_fn=<AddBackward0>)\n",
      "tensor([22.9184], grad_fn=<AddBackward0>)\n",
      "tensor([19.8119], grad_fn=<AddBackward0>)\n",
      "tensor([19.9857], grad_fn=<AddBackward0>)\n",
      "tensor([20.7994], grad_fn=<AddBackward0>)\n",
      "tensor([26.1911], grad_fn=<AddBackward0>)\n",
      "tensor([19.8843], grad_fn=<AddBackward0>)\n",
      "tensor([37.0034], grad_fn=<AddBackward0>)\n",
      "tensor([25.3815], grad_fn=<AddBackward0>)\n",
      "tensor([22.8107], grad_fn=<AddBackward0>)\n",
      "tensor([19.6637], grad_fn=<AddBackward0>)\n",
      "tensor([19.8355], grad_fn=<AddBackward0>)\n",
      "tensor([20.6663], grad_fn=<AddBackward0>)\n",
      "tensor([26.1272], grad_fn=<AddBackward0>)\n",
      "tensor([19.7502], grad_fn=<AddBackward0>)\n",
      "tensor([36.9235], grad_fn=<AddBackward0>)\n",
      "tensor([25.2323], grad_fn=<AddBackward0>)\n",
      "tensor([22.7042], grad_fn=<AddBackward0>)\n",
      "tensor([19.5171], grad_fn=<AddBackward0>)\n",
      "tensor([19.6865], grad_fn=<AddBackward0>)\n",
      "tensor([20.5345], grad_fn=<AddBackward0>)\n",
      "tensor([26.0647], grad_fn=<AddBackward0>)\n",
      "tensor([19.6177], grad_fn=<AddBackward0>)\n",
      "tensor([36.8447], grad_fn=<AddBackward0>)\n",
      "tensor([25.0844], grad_fn=<AddBackward0>)\n",
      "tensor([22.5989], grad_fn=<AddBackward0>)\n",
      "tensor([19.3718], grad_fn=<AddBackward0>)\n",
      "tensor([19.5387], grad_fn=<AddBackward0>)\n",
      "tensor([20.4038], grad_fn=<AddBackward0>)\n",
      "tensor([26.0036], grad_fn=<AddBackward0>)\n",
      "tensor([19.4868], grad_fn=<AddBackward0>)\n",
      "tensor([36.7669], grad_fn=<AddBackward0>)\n",
      "tensor([24.9377], grad_fn=<AddBackward0>)\n",
      "tensor([22.4948], grad_fn=<AddBackward0>)\n",
      "tensor([19.2281], grad_fn=<AddBackward0>)\n",
      "tensor([19.3921], grad_fn=<AddBackward0>)\n",
      "tensor([20.2743], grad_fn=<AddBackward0>)\n",
      "tensor([25.9437], grad_fn=<AddBackward0>)\n",
      "tensor([19.3574], grad_fn=<AddBackward0>)\n",
      "tensor([36.6902], grad_fn=<AddBackward0>)\n",
      "tensor([24.7922], grad_fn=<AddBackward0>)\n",
      "tensor([22.3919], grad_fn=<AddBackward0>)\n",
      "tensor([19.0857], grad_fn=<AddBackward0>)\n",
      "tensor([19.2467], grad_fn=<AddBackward0>)\n",
      "tensor([20.1460], grad_fn=<AddBackward0>)\n",
      "tensor([25.8852], grad_fn=<AddBackward0>)\n",
      "tensor([19.2296], grad_fn=<AddBackward0>)\n",
      "tensor([36.6144], grad_fn=<AddBackward0>)\n",
      "tensor([24.6479], grad_fn=<AddBackward0>)\n",
      "tensor([22.2902], grad_fn=<AddBackward0>)\n",
      "tensor([18.9448], grad_fn=<AddBackward0>)\n",
      "tensor([19.1024], grad_fn=<AddBackward0>)\n",
      "tensor([20.0189], grad_fn=<AddBackward0>)\n",
      "tensor([25.8281], grad_fn=<AddBackward0>)\n",
      "tensor([19.1033], grad_fn=<AddBackward0>)\n",
      "tensor([36.5397], grad_fn=<AddBackward0>)\n",
      "tensor([24.5048], grad_fn=<AddBackward0>)\n",
      "tensor([22.1896], grad_fn=<AddBackward0>)\n",
      "tensor([18.8053], grad_fn=<AddBackward0>)\n",
      "tensor([18.9593], grad_fn=<AddBackward0>)\n",
      "tensor([19.8929], grad_fn=<AddBackward0>)\n",
      "tensor([25.7722], grad_fn=<AddBackward0>)\n",
      "tensor([18.9785], grad_fn=<AddBackward0>)\n",
      "tensor([36.4660], grad_fn=<AddBackward0>)\n",
      "tensor([24.3629], grad_fn=<AddBackward0>)\n",
      "tensor([22.0901], grad_fn=<AddBackward0>)\n",
      "tensor([18.6672], grad_fn=<AddBackward0>)\n",
      "tensor([18.8174], grad_fn=<AddBackward0>)\n",
      "tensor([19.7680], grad_fn=<AddBackward0>)\n",
      "tensor([25.7176], grad_fn=<AddBackward0>)\n",
      "tensor([18.8552], grad_fn=<AddBackward0>)\n",
      "tensor([36.3932], grad_fn=<AddBackward0>)\n",
      "tensor([24.2222], grad_fn=<AddBackward0>)\n",
      "tensor([21.9918], grad_fn=<AddBackward0>)\n",
      "tensor([18.5305], grad_fn=<AddBackward0>)\n",
      "tensor([18.6766], grad_fn=<AddBackward0>)\n",
      "tensor([19.6443], grad_fn=<AddBackward0>)\n",
      "tensor([25.6642], grad_fn=<AddBackward0>)\n",
      "tensor([18.7334], grad_fn=<AddBackward0>)\n",
      "tensor([36.3214], grad_fn=<AddBackward0>)\n",
      "tensor([24.0826], grad_fn=<AddBackward0>)\n",
      "tensor([21.8947], grad_fn=<AddBackward0>)\n",
      "tensor([18.3951], grad_fn=<AddBackward0>)\n",
      "tensor([18.5369], grad_fn=<AddBackward0>)\n",
      "tensor([19.5218], grad_fn=<AddBackward0>)\n",
      "tensor([25.6121], grad_fn=<AddBackward0>)\n",
      "tensor([18.6131], grad_fn=<AddBackward0>)\n",
      "tensor([36.2507], grad_fn=<AddBackward0>)\n",
      "tensor([23.9443], grad_fn=<AddBackward0>)\n",
      "tensor([21.7986], grad_fn=<AddBackward0>)\n",
      "tensor([18.2611], grad_fn=<AddBackward0>)\n",
      "tensor([18.3984], grad_fn=<AddBackward0>)\n",
      "tensor([19.4003], grad_fn=<AddBackward0>)\n",
      "tensor([25.5613], grad_fn=<AddBackward0>)\n",
      "tensor([18.4942], grad_fn=<AddBackward0>)\n",
      "tensor([36.1808], grad_fn=<AddBackward0>)\n",
      "tensor([23.8071], grad_fn=<AddBackward0>)\n",
      "tensor([21.7037], grad_fn=<AddBackward0>)\n",
      "tensor([18.1285], grad_fn=<AddBackward0>)\n",
      "tensor([18.2610], grad_fn=<AddBackward0>)\n",
      "tensor([19.2800], grad_fn=<AddBackward0>)\n",
      "tensor([25.5117], grad_fn=<AddBackward0>)\n",
      "tensor([18.3767], grad_fn=<AddBackward0>)\n",
      "tensor([36.1119], grad_fn=<AddBackward0>)\n",
      "tensor([23.6710], grad_fn=<AddBackward0>)\n",
      "tensor([21.6098], grad_fn=<AddBackward0>)\n",
      "tensor([17.9973], grad_fn=<AddBackward0>)\n",
      "tensor([18.1248], grad_fn=<AddBackward0>)\n",
      "tensor([19.1608], grad_fn=<AddBackward0>)\n",
      "tensor([25.4633], grad_fn=<AddBackward0>)\n",
      "tensor([18.2607], grad_fn=<AddBackward0>)\n",
      "tensor([36.0440], grad_fn=<AddBackward0>)\n",
      "tensor([23.5361], grad_fn=<AddBackward0>)\n",
      "tensor([21.5170], grad_fn=<AddBackward0>)\n",
      "tensor([17.8674], grad_fn=<AddBackward0>)\n",
      "tensor([17.9896], grad_fn=<AddBackward0>)\n",
      "tensor([19.0427], grad_fn=<AddBackward0>)\n",
      "tensor([25.4161], grad_fn=<AddBackward0>)\n",
      "tensor([18.1461], grad_fn=<AddBackward0>)\n",
      "tensor([35.9770], grad_fn=<AddBackward0>)\n",
      "tensor([23.4024], grad_fn=<AddBackward0>)\n",
      "tensor([21.4253], grad_fn=<AddBackward0>)\n",
      "tensor([17.7388], grad_fn=<AddBackward0>)\n",
      "tensor([17.8555], grad_fn=<AddBackward0>)\n",
      "tensor([18.9257], grad_fn=<AddBackward0>)\n",
      "tensor([25.3701], grad_fn=<AddBackward0>)\n",
      "tensor([18.0329], grad_fn=<AddBackward0>)\n",
      "tensor([35.9110], grad_fn=<AddBackward0>)\n",
      "tensor([23.2698], grad_fn=<AddBackward0>)\n",
      "tensor([21.3347], grad_fn=<AddBackward0>)\n",
      "tensor([17.6115], grad_fn=<AddBackward0>)\n",
      "tensor([17.7226], grad_fn=<AddBackward0>)\n",
      "tensor([18.8098], grad_fn=<AddBackward0>)\n",
      "tensor([25.3252], grad_fn=<AddBackward0>)\n",
      "tensor([17.9211], grad_fn=<AddBackward0>)\n",
      "tensor([35.8459], grad_fn=<AddBackward0>)\n",
      "tensor([23.1383], grad_fn=<AddBackward0>)\n",
      "tensor([21.2451], grad_fn=<AddBackward0>)\n",
      "tensor([17.4856], grad_fn=<AddBackward0>)\n",
      "tensor([17.5907], grad_fn=<AddBackward0>)\n",
      "tensor([18.6949], grad_fn=<AddBackward0>)\n",
      "tensor([25.2816], grad_fn=<AddBackward0>)\n",
      "tensor([17.8106], grad_fn=<AddBackward0>)\n",
      "tensor([35.7816], grad_fn=<AddBackward0>)\n",
      "tensor([23.0079], grad_fn=<AddBackward0>)\n",
      "tensor([21.1566], grad_fn=<AddBackward0>)\n",
      "tensor([17.3610], grad_fn=<AddBackward0>)\n",
      "tensor([17.4599], grad_fn=<AddBackward0>)\n",
      "tensor([18.5811], grad_fn=<AddBackward0>)\n",
      "tensor([25.2391], grad_fn=<AddBackward0>)\n",
      "tensor([17.7015], grad_fn=<AddBackward0>)\n",
      "tensor([35.7183], grad_fn=<AddBackward0>)\n",
      "tensor([22.8787], grad_fn=<AddBackward0>)\n",
      "tensor([21.0690], grad_fn=<AddBackward0>)\n",
      "tensor([17.2376], grad_fn=<AddBackward0>)\n",
      "tensor([17.3302], grad_fn=<AddBackward0>)\n",
      "tensor([18.4684], grad_fn=<AddBackward0>)\n",
      "tensor([25.1977], grad_fn=<AddBackward0>)\n",
      "tensor([17.5938], grad_fn=<AddBackward0>)\n",
      "tensor([35.6559], grad_fn=<AddBackward0>)\n",
      "tensor([22.7505], grad_fn=<AddBackward0>)\n",
      "tensor([20.9826], grad_fn=<AddBackward0>)\n",
      "tensor([17.1156], grad_fn=<AddBackward0>)\n",
      "tensor([17.2015], grad_fn=<AddBackward0>)\n",
      "tensor([18.3568], grad_fn=<AddBackward0>)\n",
      "tensor([25.1575], grad_fn=<AddBackward0>)\n",
      "tensor([17.4874], grad_fn=<AddBackward0>)\n",
      "tensor([35.5944], grad_fn=<AddBackward0>)\n",
      "tensor([22.6235], grad_fn=<AddBackward0>)\n",
      "tensor([20.8971], grad_fn=<AddBackward0>)\n",
      "tensor([16.9948], grad_fn=<AddBackward0>)\n",
      "tensor([17.0740], grad_fn=<AddBackward0>)\n",
      "tensor([18.2461], grad_fn=<AddBackward0>)\n",
      "tensor([25.1183], grad_fn=<AddBackward0>)\n",
      "tensor([17.3822], grad_fn=<AddBackward0>)\n",
      "tensor([35.5338], grad_fn=<AddBackward0>)\n",
      "tensor([22.4976], grad_fn=<AddBackward0>)\n",
      "tensor([20.8126], grad_fn=<AddBackward0>)\n",
      "tensor([16.8753], grad_fn=<AddBackward0>)\n",
      "tensor([16.9474], grad_fn=<AddBackward0>)\n",
      "tensor([18.1366], grad_fn=<AddBackward0>)\n",
      "tensor([25.0803], grad_fn=<AddBackward0>)\n",
      "tensor([17.2785], grad_fn=<AddBackward0>)\n",
      "tensor([35.4740], grad_fn=<AddBackward0>)\n",
      "tensor([22.3727], grad_fn=<AddBackward0>)\n",
      "tensor([20.7291], grad_fn=<AddBackward0>)\n",
      "tensor([16.7571], grad_fn=<AddBackward0>)\n",
      "tensor([16.8219], grad_fn=<AddBackward0>)\n",
      "tensor([18.0280], grad_fn=<AddBackward0>)\n",
      "tensor([25.0434], grad_fn=<AddBackward0>)\n",
      "tensor([17.1759], grad_fn=<AddBackward0>)\n",
      "tensor([35.4151], grad_fn=<AddBackward0>)\n",
      "tensor([22.2490], grad_fn=<AddBackward0>)\n",
      "tensor([20.6466], grad_fn=<AddBackward0>)\n",
      "tensor([16.6401], grad_fn=<AddBackward0>)\n",
      "tensor([16.6975], grad_fn=<AddBackward0>)\n",
      "tensor([17.9205], grad_fn=<AddBackward0>)\n",
      "tensor([25.0075], grad_fn=<AddBackward0>)\n",
      "tensor([17.0747], grad_fn=<AddBackward0>)\n",
      "tensor([35.3571], grad_fn=<AddBackward0>)\n",
      "tensor([22.1263], grad_fn=<AddBackward0>)\n",
      "tensor([20.5651], grad_fn=<AddBackward0>)\n",
      "tensor([16.5243], grad_fn=<AddBackward0>)\n",
      "tensor([16.5740], grad_fn=<AddBackward0>)\n",
      "tensor([17.8140], grad_fn=<AddBackward0>)\n",
      "tensor([24.9727], grad_fn=<AddBackward0>)\n",
      "tensor([16.9747], grad_fn=<AddBackward0>)\n",
      "tensor([35.2999], grad_fn=<AddBackward0>)\n",
      "tensor([22.0047], grad_fn=<AddBackward0>)\n",
      "tensor([20.4845], grad_fn=<AddBackward0>)\n",
      "tensor([16.4098], grad_fn=<AddBackward0>)\n",
      "tensor([16.4516], grad_fn=<AddBackward0>)\n",
      "tensor([17.7085], grad_fn=<AddBackward0>)\n",
      "tensor([24.9390], grad_fn=<AddBackward0>)\n",
      "tensor([16.8760], grad_fn=<AddBackward0>)\n",
      "tensor([35.2435], grad_fn=<AddBackward0>)\n",
      "tensor([21.8841], grad_fn=<AddBackward0>)\n",
      "tensor([20.4048], grad_fn=<AddBackward0>)\n",
      "tensor([16.2965], grad_fn=<AddBackward0>)\n",
      "tensor([16.3303], grad_fn=<AddBackward0>)\n",
      "tensor([17.6040], grad_fn=<AddBackward0>)\n",
      "tensor([24.9063], grad_fn=<AddBackward0>)\n",
      "tensor([16.7785], grad_fn=<AddBackward0>)\n",
      "tensor([35.1880], grad_fn=<AddBackward0>)\n",
      "tensor([21.7646], grad_fn=<AddBackward0>)\n",
      "tensor([20.3262], grad_fn=<AddBackward0>)\n",
      "tensor([16.1844], grad_fn=<AddBackward0>)\n",
      "tensor([16.2099], grad_fn=<AddBackward0>)\n",
      "tensor([17.5005], grad_fn=<AddBackward0>)\n",
      "tensor([24.8746], grad_fn=<AddBackward0>)\n",
      "tensor([16.6823], grad_fn=<AddBackward0>)\n",
      "tensor([35.1333], grad_fn=<AddBackward0>)\n",
      "tensor([21.6462], grad_fn=<AddBackward0>)\n",
      "tensor([20.2484], grad_fn=<AddBackward0>)\n",
      "tensor([16.0736], grad_fn=<AddBackward0>)\n",
      "tensor([16.0905], grad_fn=<AddBackward0>)\n",
      "tensor([17.3980], grad_fn=<AddBackward0>)\n",
      "tensor([24.8440], grad_fn=<AddBackward0>)\n",
      "tensor([16.5873], grad_fn=<AddBackward0>)\n",
      "tensor([35.0795], grad_fn=<AddBackward0>)\n",
      "tensor([21.5288], grad_fn=<AddBackward0>)\n",
      "tensor([20.1716], grad_fn=<AddBackward0>)\n",
      "tensor([15.9639], grad_fn=<AddBackward0>)\n",
      "tensor([15.9721], grad_fn=<AddBackward0>)\n",
      "tensor([17.2965], grad_fn=<AddBackward0>)\n",
      "tensor([24.8143], grad_fn=<AddBackward0>)\n",
      "tensor([16.4934], grad_fn=<AddBackward0>)\n",
      "tensor([35.0264], grad_fn=<AddBackward0>)\n",
      "tensor([21.4124], grad_fn=<AddBackward0>)\n",
      "tensor([20.0956], grad_fn=<AddBackward0>)\n",
      "tensor([15.8554], grad_fn=<AddBackward0>)\n",
      "tensor([15.8548], grad_fn=<AddBackward0>)\n",
      "tensor([17.1959], grad_fn=<AddBackward0>)\n",
      "tensor([24.7856], grad_fn=<AddBackward0>)\n",
      "tensor([16.4007], grad_fn=<AddBackward0>)\n",
      "tensor([34.9741], grad_fn=<AddBackward0>)\n",
      "tensor([21.2971], grad_fn=<AddBackward0>)\n",
      "tensor([20.0206], grad_fn=<AddBackward0>)\n",
      "tensor([15.7481], grad_fn=<AddBackward0>)\n",
      "tensor([15.7383], grad_fn=<AddBackward0>)\n",
      "tensor([17.0963], grad_fn=<AddBackward0>)\n",
      "tensor([24.7580], grad_fn=<AddBackward0>)\n",
      "tensor([16.3093], grad_fn=<AddBackward0>)\n",
      "tensor([34.9227], grad_fn=<AddBackward0>)\n",
      "tensor([21.1828], grad_fn=<AddBackward0>)\n",
      "tensor([19.9465], grad_fn=<AddBackward0>)\n",
      "tensor([15.6419], grad_fn=<AddBackward0>)\n",
      "tensor([15.6229], grad_fn=<AddBackward0>)\n",
      "tensor([16.9977], grad_fn=<AddBackward0>)\n",
      "tensor([24.7313], grad_fn=<AddBackward0>)\n",
      "tensor([16.2189], grad_fn=<AddBackward0>)\n",
      "tensor([34.8720], grad_fn=<AddBackward0>)\n",
      "tensor([21.0695], grad_fn=<AddBackward0>)\n",
      "tensor([19.8732], grad_fn=<AddBackward0>)\n",
      "tensor([15.5369], grad_fn=<AddBackward0>)\n",
      "tensor([15.5085], grad_fn=<AddBackward0>)\n",
      "tensor([16.9000], grad_fn=<AddBackward0>)\n",
      "tensor([24.7055], grad_fn=<AddBackward0>)\n",
      "tensor([16.1298], grad_fn=<AddBackward0>)\n",
      "tensor([34.8221], grad_fn=<AddBackward0>)\n",
      "tensor([20.9572], grad_fn=<AddBackward0>)\n",
      "tensor([19.8008], grad_fn=<AddBackward0>)\n",
      "tensor([15.4331], grad_fn=<AddBackward0>)\n",
      "tensor([15.3949], grad_fn=<AddBackward0>)\n",
      "tensor([16.8033], grad_fn=<AddBackward0>)\n",
      "tensor([24.6807], grad_fn=<AddBackward0>)\n",
      "tensor([16.0417], grad_fn=<AddBackward0>)\n",
      "tensor([34.7730], grad_fn=<AddBackward0>)\n",
      "tensor([20.8459], grad_fn=<AddBackward0>)\n",
      "tensor([19.7293], grad_fn=<AddBackward0>)\n",
      "tensor([15.3304], grad_fn=<AddBackward0>)\n",
      "tensor([15.2824], grad_fn=<AddBackward0>)\n",
      "tensor([16.7075], grad_fn=<AddBackward0>)\n",
      "tensor([24.6568], grad_fn=<AddBackward0>)\n",
      "tensor([15.9548], grad_fn=<AddBackward0>)\n",
      "tensor([34.7246], grad_fn=<AddBackward0>)\n",
      "tensor([20.7356], grad_fn=<AddBackward0>)\n",
      "tensor([19.6586], grad_fn=<AddBackward0>)\n",
      "tensor([15.2289], grad_fn=<AddBackward0>)\n",
      "tensor([15.1708], grad_fn=<AddBackward0>)\n",
      "tensor([16.6126], grad_fn=<AddBackward0>)\n",
      "tensor([24.6338], grad_fn=<AddBackward0>)\n",
      "tensor([15.8690], grad_fn=<AddBackward0>)\n",
      "tensor([34.6770], grad_fn=<AddBackward0>)\n",
      "tensor([20.6263], grad_fn=<AddBackward0>)\n",
      "tensor([19.5888], grad_fn=<AddBackward0>)\n",
      "tensor([15.1284], grad_fn=<AddBackward0>)\n",
      "tensor([15.0602], grad_fn=<AddBackward0>)\n",
      "tensor([16.5187], grad_fn=<AddBackward0>)\n",
      "tensor([24.6118], grad_fn=<AddBackward0>)\n",
      "tensor([15.7843], grad_fn=<AddBackward0>)\n",
      "tensor([34.6302], grad_fn=<AddBackward0>)\n",
      "tensor([20.5180], grad_fn=<AddBackward0>)\n",
      "tensor([19.5198], grad_fn=<AddBackward0>)\n",
      "tensor([15.0291], grad_fn=<AddBackward0>)\n",
      "tensor([14.9504], grad_fn=<AddBackward0>)\n",
      "tensor([16.4256], grad_fn=<AddBackward0>)\n",
      "tensor([24.5906], grad_fn=<AddBackward0>)\n",
      "tensor([15.7007], grad_fn=<AddBackward0>)\n",
      "tensor([34.5841], grad_fn=<AddBackward0>)\n",
      "tensor([20.4107], grad_fn=<AddBackward0>)\n",
      "tensor([19.4516], grad_fn=<AddBackward0>)\n",
      "tensor([14.9309], grad_fn=<AddBackward0>)\n",
      "tensor([14.8416], grad_fn=<AddBackward0>)\n",
      "tensor([16.3335], grad_fn=<AddBackward0>)\n",
      "tensor([24.5704], grad_fn=<AddBackward0>)\n",
      "tensor([15.6182], grad_fn=<AddBackward0>)\n",
      "tensor([34.5387], grad_fn=<AddBackward0>)\n",
      "tensor([20.3044], grad_fn=<AddBackward0>)\n",
      "tensor([19.3843], grad_fn=<AddBackward0>)\n",
      "tensor([14.8338], grad_fn=<AddBackward0>)\n",
      "tensor([14.7338], grad_fn=<AddBackward0>)\n",
      "tensor([16.2423], grad_fn=<AddBackward0>)\n",
      "tensor([24.5510], grad_fn=<AddBackward0>)\n",
      "tensor([15.5367], grad_fn=<AddBackward0>)\n",
      "tensor([34.4941], grad_fn=<AddBackward0>)\n",
      "tensor([20.1990], grad_fn=<AddBackward0>)\n",
      "tensor([19.3177], grad_fn=<AddBackward0>)\n",
      "tensor([14.7378], grad_fn=<AddBackward0>)\n",
      "tensor([14.6268], grad_fn=<AddBackward0>)\n",
      "tensor([16.1519], grad_fn=<AddBackward0>)\n",
      "tensor([24.5325], grad_fn=<AddBackward0>)\n",
      "tensor([15.4563], grad_fn=<AddBackward0>)\n",
      "tensor([34.4501], grad_fn=<AddBackward0>)\n",
      "tensor([20.0946], grad_fn=<AddBackward0>)\n",
      "tensor([19.2520], grad_fn=<AddBackward0>)\n",
      "tensor([14.6429], grad_fn=<AddBackward0>)\n",
      "tensor([14.5208], grad_fn=<AddBackward0>)\n",
      "tensor([16.0625], grad_fn=<AddBackward0>)\n",
      "tensor([24.5148], grad_fn=<AddBackward0>)\n",
      "tensor([15.3769], grad_fn=<AddBackward0>)\n",
      "tensor([34.4069], grad_fn=<AddBackward0>)\n",
      "tensor([19.9911], grad_fn=<AddBackward0>)\n",
      "tensor([19.1870], grad_fn=<AddBackward0>)\n",
      "tensor([14.5490], grad_fn=<AddBackward0>)\n",
      "tensor([14.4156], grad_fn=<AddBackward0>)\n",
      "tensor([15.9739], grad_fn=<AddBackward0>)\n",
      "tensor([24.4980], grad_fn=<AddBackward0>)\n",
      "tensor([15.2985], grad_fn=<AddBackward0>)\n",
      "tensor([34.3644], grad_fn=<AddBackward0>)\n",
      "tensor([19.8886], grad_fn=<AddBackward0>)\n",
      "tensor([19.1229], grad_fn=<AddBackward0>)\n",
      "tensor([14.4562], grad_fn=<AddBackward0>)\n",
      "tensor([14.3113], grad_fn=<AddBackward0>)\n",
      "tensor([15.8862], grad_fn=<AddBackward0>)\n",
      "tensor([24.4819], grad_fn=<AddBackward0>)\n",
      "tensor([15.2212], grad_fn=<AddBackward0>)\n",
      "tensor([34.3226], grad_fn=<AddBackward0>)\n",
      "tensor([19.7870], grad_fn=<AddBackward0>)\n",
      "tensor([19.0595], grad_fn=<AddBackward0>)\n",
      "tensor([14.3645], grad_fn=<AddBackward0>)\n",
      "tensor([14.2080], grad_fn=<AddBackward0>)\n",
      "tensor([15.7994], grad_fn=<AddBackward0>)\n",
      "tensor([24.4668], grad_fn=<AddBackward0>)\n",
      "tensor([15.1448], grad_fn=<AddBackward0>)\n",
      "tensor([34.2815], grad_fn=<AddBackward0>)\n",
      "tensor([19.6863], grad_fn=<AddBackward0>)\n",
      "tensor([18.9968], grad_fn=<AddBackward0>)\n",
      "tensor([14.2738], grad_fn=<AddBackward0>)\n",
      "tensor([14.1055], grad_fn=<AddBackward0>)\n",
      "tensor([15.7134], grad_fn=<AddBackward0>)\n",
      "tensor([24.4524], grad_fn=<AddBackward0>)\n",
      "tensor([15.0695], grad_fn=<AddBackward0>)\n",
      "tensor([34.2410], grad_fn=<AddBackward0>)\n",
      "tensor([19.5866], grad_fn=<AddBackward0>)\n",
      "tensor([18.9349], grad_fn=<AddBackward0>)\n",
      "tensor([14.1842], grad_fn=<AddBackward0>)\n",
      "tensor([14.0039], grad_fn=<AddBackward0>)\n",
      "tensor([15.6283], grad_fn=<AddBackward0>)\n",
      "tensor([24.4388], grad_fn=<AddBackward0>)\n",
      "tensor([14.9951], grad_fn=<AddBackward0>)\n",
      "tensor([34.2013], grad_fn=<AddBackward0>)\n",
      "tensor([19.4879], grad_fn=<AddBackward0>)\n",
      "tensor([18.8738], grad_fn=<AddBackward0>)\n",
      "tensor([14.0956], grad_fn=<AddBackward0>)\n",
      "tensor([13.9032], grad_fn=<AddBackward0>)\n",
      "tensor([15.5441], grad_fn=<AddBackward0>)\n",
      "tensor([24.4260], grad_fn=<AddBackward0>)\n",
      "tensor([14.9217], grad_fn=<AddBackward0>)\n",
      "tensor([34.1622], grad_fn=<AddBackward0>)\n",
      "tensor([19.3900], grad_fn=<AddBackward0>)\n",
      "tensor([18.8134], grad_fn=<AddBackward0>)\n",
      "tensor([14.0080], grad_fn=<AddBackward0>)\n",
      "tensor([13.8033], grad_fn=<AddBackward0>)\n",
      "tensor([15.4606], grad_fn=<AddBackward0>)\n",
      "tensor([24.4140], grad_fn=<AddBackward0>)\n",
      "tensor([14.8492], grad_fn=<AddBackward0>)\n",
      "tensor([34.1238], grad_fn=<AddBackward0>)\n",
      "tensor([19.2930], grad_fn=<AddBackward0>)\n",
      "tensor([18.7537], grad_fn=<AddBackward0>)\n",
      "tensor([13.9214], grad_fn=<AddBackward0>)\n",
      "tensor([13.7043], grad_fn=<AddBackward0>)\n",
      "tensor([15.3780], grad_fn=<AddBackward0>)\n",
      "tensor([24.4027], grad_fn=<AddBackward0>)\n",
      "tensor([14.7777], grad_fn=<AddBackward0>)\n",
      "tensor([34.0860], grad_fn=<AddBackward0>)\n",
      "tensor([19.1970], grad_fn=<AddBackward0>)\n",
      "tensor([18.6947], grad_fn=<AddBackward0>)\n",
      "tensor([13.8359], grad_fn=<AddBackward0>)\n",
      "tensor([13.6061], grad_fn=<AddBackward0>)\n",
      "tensor([15.2963], grad_fn=<AddBackward0>)\n",
      "tensor([24.3922], grad_fn=<AddBackward0>)\n",
      "tensor([14.7072], grad_fn=<AddBackward0>)\n",
      "tensor([34.0488], grad_fn=<AddBackward0>)\n",
      "tensor([19.1018], grad_fn=<AddBackward0>)\n",
      "tensor([18.6365], grad_fn=<AddBackward0>)\n",
      "tensor([13.7513], grad_fn=<AddBackward0>)\n",
      "tensor([13.5088], grad_fn=<AddBackward0>)\n",
      "tensor([15.2153], grad_fn=<AddBackward0>)\n",
      "tensor([24.3824], grad_fn=<AddBackward0>)\n",
      "tensor([14.6375], grad_fn=<AddBackward0>)\n",
      "tensor([34.0124], grad_fn=<AddBackward0>)\n",
      "tensor([19.0075], grad_fn=<AddBackward0>)\n",
      "tensor([18.5789], grad_fn=<AddBackward0>)\n",
      "tensor([13.6677], grad_fn=<AddBackward0>)\n",
      "tensor([13.4123], grad_fn=<AddBackward0>)\n",
      "tensor([15.1352], grad_fn=<AddBackward0>)\n",
      "tensor([24.3734], grad_fn=<AddBackward0>)\n",
      "tensor([14.5688], grad_fn=<AddBackward0>)\n",
      "tensor([33.9765], grad_fn=<AddBackward0>)\n",
      "tensor([18.9142], grad_fn=<AddBackward0>)\n",
      "tensor([18.5220], grad_fn=<AddBackward0>)\n",
      "tensor([13.5852], grad_fn=<AddBackward0>)\n",
      "tensor([13.3166], grad_fn=<AddBackward0>)\n",
      "tensor([15.0558], grad_fn=<AddBackward0>)\n",
      "tensor([24.3651], grad_fn=<AddBackward0>)\n",
      "tensor([14.5009], grad_fn=<AddBackward0>)\n",
      "tensor([33.9413], grad_fn=<AddBackward0>)\n",
      "tensor([18.8217], grad_fn=<AddBackward0>)\n",
      "tensor([18.4658], grad_fn=<AddBackward0>)\n",
      "tensor([13.5036], grad_fn=<AddBackward0>)\n",
      "tensor([13.2218], grad_fn=<AddBackward0>)\n",
      "tensor([14.9773], grad_fn=<AddBackward0>)\n",
      "tensor([24.3575], grad_fn=<AddBackward0>)\n",
      "tensor([14.4340], grad_fn=<AddBackward0>)\n",
      "tensor([33.9067], grad_fn=<AddBackward0>)\n",
      "tensor([18.7301], grad_fn=<AddBackward0>)\n",
      "tensor([18.4103], grad_fn=<AddBackward0>)\n",
      "tensor([13.4229], grad_fn=<AddBackward0>)\n",
      "tensor([13.1278], grad_fn=<AddBackward0>)\n",
      "tensor([14.8995], grad_fn=<AddBackward0>)\n",
      "tensor([24.3506], grad_fn=<AddBackward0>)\n",
      "tensor([14.3679], grad_fn=<AddBackward0>)\n",
      "tensor([33.8727], grad_fn=<AddBackward0>)\n",
      "tensor([18.6393], grad_fn=<AddBackward0>)\n",
      "tensor([18.3555], grad_fn=<AddBackward0>)\n",
      "tensor([13.3432], grad_fn=<AddBackward0>)\n",
      "tensor([13.0346], grad_fn=<AddBackward0>)\n",
      "tensor([14.8226], grad_fn=<AddBackward0>)\n",
      "tensor([24.3444], grad_fn=<AddBackward0>)\n",
      "tensor([14.3027], grad_fn=<AddBackward0>)\n",
      "tensor([33.8393], grad_fn=<AddBackward0>)\n",
      "tensor([18.5494], grad_fn=<AddBackward0>)\n",
      "tensor([18.3013], grad_fn=<AddBackward0>)\n",
      "tensor([13.2645], grad_fn=<AddBackward0>)\n",
      "tensor([12.9422], grad_fn=<AddBackward0>)\n",
      "tensor([14.7464], grad_fn=<AddBackward0>)\n",
      "tensor([24.3388], grad_fn=<AddBackward0>)\n",
      "tensor([14.2384], grad_fn=<AddBackward0>)\n",
      "tensor([33.8065], grad_fn=<AddBackward0>)\n",
      "tensor([18.4604], grad_fn=<AddBackward0>)\n",
      "tensor([18.2478], grad_fn=<AddBackward0>)\n",
      "tensor([13.1867], grad_fn=<AddBackward0>)\n",
      "tensor([12.8506], grad_fn=<AddBackward0>)\n",
      "tensor([14.6710], grad_fn=<AddBackward0>)\n",
      "tensor([24.3339], grad_fn=<AddBackward0>)\n",
      "tensor([14.1749], grad_fn=<AddBackward0>)\n",
      "tensor([33.7743], grad_fn=<AddBackward0>)\n",
      "tensor([18.3722], grad_fn=<AddBackward0>)\n",
      "tensor([18.1949], grad_fn=<AddBackward0>)\n",
      "tensor([13.1099], grad_fn=<AddBackward0>)\n",
      "tensor([12.7598], grad_fn=<AddBackward0>)\n",
      "tensor([14.5963], grad_fn=<AddBackward0>)\n",
      "tensor([24.3297], grad_fn=<AddBackward0>)\n",
      "tensor([14.1122], grad_fn=<AddBackward0>)\n",
      "tensor([33.7427], grad_fn=<AddBackward0>)\n",
      "tensor([18.2849], grad_fn=<AddBackward0>)\n",
      "tensor([18.1427], grad_fn=<AddBackward0>)\n",
      "tensor([13.0339], grad_fn=<AddBackward0>)\n",
      "tensor([12.6698], grad_fn=<AddBackward0>)\n",
      "tensor([14.5224], grad_fn=<AddBackward0>)\n",
      "tensor([24.3262], grad_fn=<AddBackward0>)\n",
      "tensor([14.0504], grad_fn=<AddBackward0>)\n",
      "tensor([33.7116], grad_fn=<AddBackward0>)\n",
      "tensor([18.1984], grad_fn=<AddBackward0>)\n",
      "tensor([18.0910], grad_fn=<AddBackward0>)\n",
      "tensor([12.9589], grad_fn=<AddBackward0>)\n",
      "tensor([12.5805], grad_fn=<AddBackward0>)\n",
      "tensor([14.4493], grad_fn=<AddBackward0>)\n",
      "tensor([24.3232], grad_fn=<AddBackward0>)\n",
      "tensor([13.9894], grad_fn=<AddBackward0>)\n",
      "tensor([33.6812], grad_fn=<AddBackward0>)\n",
      "tensor([18.1127], grad_fn=<AddBackward0>)\n",
      "tensor([18.0400], grad_fn=<AddBackward0>)\n",
      "tensor([12.8848], grad_fn=<AddBackward0>)\n",
      "tensor([12.4921], grad_fn=<AddBackward0>)\n",
      "tensor([14.3769], grad_fn=<AddBackward0>)\n",
      "tensor([24.3210], grad_fn=<AddBackward0>)\n",
      "tensor([13.9292], grad_fn=<AddBackward0>)\n",
      "tensor([33.6513], grad_fn=<AddBackward0>)\n",
      "tensor([18.0279], grad_fn=<AddBackward0>)\n",
      "tensor([17.9896], grad_fn=<AddBackward0>)\n",
      "tensor([12.8117], grad_fn=<AddBackward0>)\n",
      "tensor([12.4044], grad_fn=<AddBackward0>)\n",
      "tensor([14.3052], grad_fn=<AddBackward0>)\n",
      "tensor([24.3193], grad_fn=<AddBackward0>)\n",
      "tensor([13.8697], grad_fn=<AddBackward0>)\n",
      "tensor([33.6219], grad_fn=<AddBackward0>)\n",
      "tensor([17.9439], grad_fn=<AddBackward0>)\n",
      "tensor([17.9398], grad_fn=<AddBackward0>)\n",
      "tensor([12.7394], grad_fn=<AddBackward0>)\n",
      "tensor([12.3175], grad_fn=<AddBackward0>)\n",
      "tensor([14.2343], grad_fn=<AddBackward0>)\n",
      "tensor([24.3182], grad_fn=<AddBackward0>)\n",
      "tensor([13.8111], grad_fn=<AddBackward0>)\n",
      "tensor([33.5932], grad_fn=<AddBackward0>)\n",
      "tensor([17.8607], grad_fn=<AddBackward0>)\n",
      "tensor([17.8907], grad_fn=<AddBackward0>)\n",
      "tensor([12.6680], grad_fn=<AddBackward0>)\n",
      "tensor([12.2313], grad_fn=<AddBackward0>)\n",
      "tensor([14.1641], grad_fn=<AddBackward0>)\n",
      "tensor([24.3177], grad_fn=<AddBackward0>)\n",
      "tensor([13.7532], grad_fn=<AddBackward0>)\n",
      "tensor([33.5649], grad_fn=<AddBackward0>)\n",
      "tensor([17.7783], grad_fn=<AddBackward0>)\n",
      "tensor([17.8421], grad_fn=<AddBackward0>)\n",
      "tensor([12.5975], grad_fn=<AddBackward0>)\n",
      "tensor([12.1459], grad_fn=<AddBackward0>)\n",
      "tensor([14.0947], grad_fn=<AddBackward0>)\n",
      "tensor([24.3179], grad_fn=<AddBackward0>)\n",
      "tensor([13.6961], grad_fn=<AddBackward0>)\n",
      "tensor([33.5372], grad_fn=<AddBackward0>)\n",
      "tensor([17.6967], grad_fn=<AddBackward0>)\n",
      "tensor([17.7940], grad_fn=<AddBackward0>)\n",
      "tensor([12.5278], grad_fn=<AddBackward0>)\n",
      "tensor([12.0612], grad_fn=<AddBackward0>)\n",
      "tensor([14.0259], grad_fn=<AddBackward0>)\n",
      "tensor([24.3186], grad_fn=<AddBackward0>)\n",
      "tensor([13.6398], grad_fn=<AddBackward0>)\n",
      "tensor([33.5101], grad_fn=<AddBackward0>)\n",
      "tensor([17.6159], grad_fn=<AddBackward0>)\n",
      "tensor([17.7466], grad_fn=<AddBackward0>)\n",
      "tensor([12.4590], grad_fn=<AddBackward0>)\n",
      "tensor([11.9773], grad_fn=<AddBackward0>)\n",
      "tensor([13.9579], grad_fn=<AddBackward0>)\n",
      "tensor([24.3198], grad_fn=<AddBackward0>)\n",
      "tensor([13.5842], grad_fn=<AddBackward0>)\n",
      "tensor([33.4834], grad_fn=<AddBackward0>)\n",
      "tensor([17.5359], grad_fn=<AddBackward0>)\n",
      "tensor([17.6997], grad_fn=<AddBackward0>)\n",
      "tensor([12.3911], grad_fn=<AddBackward0>)\n",
      "tensor([11.8941], grad_fn=<AddBackward0>)\n",
      "tensor([13.8906], grad_fn=<AddBackward0>)\n",
      "tensor([24.3216], grad_fn=<AddBackward0>)\n",
      "tensor([13.5293], grad_fn=<AddBackward0>)\n",
      "tensor([33.4573], grad_fn=<AddBackward0>)\n",
      "tensor([17.4567], grad_fn=<AddBackward0>)\n",
      "tensor([17.6534], grad_fn=<AddBackward0>)\n",
      "tensor([12.3240], grad_fn=<AddBackward0>)\n",
      "tensor([11.8117], grad_fn=<AddBackward0>)\n",
      "tensor([13.8239], grad_fn=<AddBackward0>)\n",
      "tensor([24.3240], grad_fn=<AddBackward0>)\n",
      "tensor([13.4752], grad_fn=<AddBackward0>)\n",
      "tensor([33.4317], grad_fn=<AddBackward0>)\n",
      "tensor([17.3783], grad_fn=<AddBackward0>)\n",
      "tensor([17.6076], grad_fn=<AddBackward0>)\n",
      "tensor([12.2578], grad_fn=<AddBackward0>)\n",
      "tensor([11.7300], grad_fn=<AddBackward0>)\n",
      "tensor([13.7580], grad_fn=<AddBackward0>)\n",
      "tensor([24.3269], grad_fn=<AddBackward0>)\n",
      "tensor([13.4217], grad_fn=<AddBackward0>)\n",
      "tensor([33.4066], grad_fn=<AddBackward0>)\n",
      "tensor([17.3006], grad_fn=<AddBackward0>)\n",
      "tensor([17.5624], grad_fn=<AddBackward0>)\n",
      "tensor([12.1924], grad_fn=<AddBackward0>)\n",
      "tensor([11.6489], grad_fn=<AddBackward0>)\n",
      "tensor([13.6927], grad_fn=<AddBackward0>)\n",
      "tensor([24.3304], grad_fn=<AddBackward0>)\n",
      "tensor([13.3690], grad_fn=<AddBackward0>)\n",
      "tensor([33.3820], grad_fn=<AddBackward0>)\n",
      "tensor([17.2238], grad_fn=<AddBackward0>)\n",
      "tensor([17.5177], grad_fn=<AddBackward0>)\n",
      "tensor([12.1278], grad_fn=<AddBackward0>)\n",
      "tensor([11.5686], grad_fn=<AddBackward0>)\n",
      "tensor([13.6281], grad_fn=<AddBackward0>)\n",
      "tensor([24.3344], grad_fn=<AddBackward0>)\n",
      "tensor([13.3170], grad_fn=<AddBackward0>)\n",
      "tensor([33.3579], grad_fn=<AddBackward0>)\n",
      "tensor([17.1476], grad_fn=<AddBackward0>)\n",
      "tensor([17.4735], grad_fn=<AddBackward0>)\n",
      "tensor([12.0641], grad_fn=<AddBackward0>)\n",
      "tensor([11.4890], grad_fn=<AddBackward0>)\n",
      "tensor([13.5642], grad_fn=<AddBackward0>)\n",
      "tensor([24.3389], grad_fn=<AddBackward0>)\n",
      "tensor([13.2656], grad_fn=<AddBackward0>)\n",
      "tensor([33.3343], grad_fn=<AddBackward0>)\n",
      "tensor([17.0723], grad_fn=<AddBackward0>)\n",
      "tensor([17.4299], grad_fn=<AddBackward0>)\n",
      "tensor([12.0012], grad_fn=<AddBackward0>)\n",
      "tensor([11.4102], grad_fn=<AddBackward0>)\n",
      "tensor([13.5010], grad_fn=<AddBackward0>)\n",
      "tensor([24.3439], grad_fn=<AddBackward0>)\n",
      "tensor([13.2150], grad_fn=<AddBackward0>)\n",
      "tensor([33.3112], grad_fn=<AddBackward0>)\n",
      "tensor([16.9977], grad_fn=<AddBackward0>)\n",
      "tensor([17.3868], grad_fn=<AddBackward0>)\n",
      "tensor([11.9390], grad_fn=<AddBackward0>)\n",
      "tensor([11.3320], grad_fn=<AddBackward0>)\n",
      "tensor([13.4384], grad_fn=<AddBackward0>)\n",
      "tensor([24.3493], grad_fn=<AddBackward0>)\n",
      "tensor([13.1650], grad_fn=<AddBackward0>)\n",
      "tensor([33.2886], grad_fn=<AddBackward0>)\n",
      "tensor([16.9238], grad_fn=<AddBackward0>)\n",
      "tensor([17.3442], grad_fn=<AddBackward0>)\n",
      "tensor([11.8777], grad_fn=<AddBackward0>)\n",
      "tensor([11.2544], grad_fn=<AddBackward0>)\n",
      "tensor([13.3765], grad_fn=<AddBackward0>)\n",
      "tensor([24.3553], grad_fn=<AddBackward0>)\n",
      "tensor([13.1156], grad_fn=<AddBackward0>)\n",
      "tensor([33.2664], grad_fn=<AddBackward0>)\n",
      "tensor([16.8507], grad_fn=<AddBackward0>)\n",
      "tensor([17.3021], grad_fn=<AddBackward0>)\n",
      "tensor([11.8172], grad_fn=<AddBackward0>)\n",
      "tensor([11.1776], grad_fn=<AddBackward0>)\n",
      "tensor([13.3152], grad_fn=<AddBackward0>)\n",
      "tensor([24.3618], grad_fn=<AddBackward0>)\n",
      "tensor([13.0669], grad_fn=<AddBackward0>)\n",
      "tensor([33.2447], grad_fn=<AddBackward0>)\n",
      "tensor([16.7783], grad_fn=<AddBackward0>)\n",
      "tensor([17.2605], grad_fn=<AddBackward0>)\n",
      "tensor([11.7574], grad_fn=<AddBackward0>)\n",
      "tensor([11.1015], grad_fn=<AddBackward0>)\n",
      "tensor([13.2546], grad_fn=<AddBackward0>)\n",
      "tensor([24.3687], grad_fn=<AddBackward0>)\n",
      "tensor([13.0189], grad_fn=<AddBackward0>)\n",
      "tensor([33.2235], grad_fn=<AddBackward0>)\n",
      "tensor([16.7067], grad_fn=<AddBackward0>)\n",
      "tensor([17.2193], grad_fn=<AddBackward0>)\n",
      "tensor([11.6985], grad_fn=<AddBackward0>)\n",
      "tensor([11.0260], grad_fn=<AddBackward0>)\n",
      "tensor([13.1946], grad_fn=<AddBackward0>)\n",
      "tensor([24.3761], grad_fn=<AddBackward0>)\n",
      "tensor([12.9715], grad_fn=<AddBackward0>)\n",
      "tensor([33.2027], grad_fn=<AddBackward0>)\n",
      "tensor([16.6358], grad_fn=<AddBackward0>)\n",
      "tensor([17.1787], grad_fn=<AddBackward0>)\n",
      "tensor([11.6403], grad_fn=<AddBackward0>)\n",
      "tensor([10.9512], grad_fn=<AddBackward0>)\n",
      "tensor([13.1352], grad_fn=<AddBackward0>)\n",
      "tensor([24.3839], grad_fn=<AddBackward0>)\n",
      "tensor([12.9247], grad_fn=<AddBackward0>)\n",
      "tensor([33.1823], grad_fn=<AddBackward0>)\n",
      "tensor([16.5656], grad_fn=<AddBackward0>)\n",
      "tensor([17.1385], grad_fn=<AddBackward0>)\n",
      "tensor([11.5828], grad_fn=<AddBackward0>)\n",
      "tensor([10.8771], grad_fn=<AddBackward0>)\n",
      "tensor([13.0765], grad_fn=<AddBackward0>)\n",
      "tensor([24.3922], grad_fn=<AddBackward0>)\n",
      "tensor([12.8785], grad_fn=<AddBackward0>)\n",
      "tensor([33.1624], grad_fn=<AddBackward0>)\n",
      "tensor([16.4961], grad_fn=<AddBackward0>)\n",
      "tensor([17.0989], grad_fn=<AddBackward0>)\n",
      "tensor([11.5261], grad_fn=<AddBackward0>)\n",
      "tensor([10.8036], grad_fn=<AddBackward0>)\n",
      "tensor([13.0184], grad_fn=<AddBackward0>)\n",
      "tensor([24.4009], grad_fn=<AddBackward0>)\n",
      "tensor([12.8329], grad_fn=<AddBackward0>)\n",
      "tensor([33.1430], grad_fn=<AddBackward0>)\n",
      "tensor([16.4273], grad_fn=<AddBackward0>)\n",
      "tensor([17.0596], grad_fn=<AddBackward0>)\n",
      "tensor([11.4702], grad_fn=<AddBackward0>)\n",
      "tensor([10.7308], grad_fn=<AddBackward0>)\n",
      "tensor([12.9609], grad_fn=<AddBackward0>)\n",
      "tensor([24.4101], grad_fn=<AddBackward0>)\n",
      "tensor([12.7880], grad_fn=<AddBackward0>)\n",
      "tensor([33.1240], grad_fn=<AddBackward0>)\n",
      "tensor([16.3592], grad_fn=<AddBackward0>)\n",
      "tensor([17.0209], grad_fn=<AddBackward0>)\n",
      "tensor([11.4150], grad_fn=<AddBackward0>)\n",
      "tensor([10.6586], grad_fn=<AddBackward0>)\n",
      "tensor([12.9040], grad_fn=<AddBackward0>)\n",
      "tensor([24.4197], grad_fn=<AddBackward0>)\n",
      "tensor([12.7436], grad_fn=<AddBackward0>)\n",
      "tensor([33.1054], grad_fn=<AddBackward0>)\n",
      "tensor([16.2918], grad_fn=<AddBackward0>)\n",
      "tensor([16.9825], grad_fn=<AddBackward0>)\n",
      "tensor([11.3605], grad_fn=<AddBackward0>)\n",
      "tensor([10.5871], grad_fn=<AddBackward0>)\n",
      "tensor([12.8477], grad_fn=<AddBackward0>)\n",
      "tensor([24.4296], grad_fn=<AddBackward0>)\n",
      "tensor([12.6998], grad_fn=<AddBackward0>)\n",
      "tensor([33.0872], grad_fn=<AddBackward0>)\n",
      "tensor([16.2252], grad_fn=<AddBackward0>)\n",
      "tensor([16.9447], grad_fn=<AddBackward0>)\n",
      "tensor([11.3068], grad_fn=<AddBackward0>)\n",
      "tensor([10.5162], grad_fn=<AddBackward0>)\n",
      "tensor([12.7920], grad_fn=<AddBackward0>)\n",
      "tensor([24.4400], grad_fn=<AddBackward0>)\n",
      "tensor([12.6565], grad_fn=<AddBackward0>)\n",
      "tensor([33.0694], grad_fn=<AddBackward0>)\n",
      "tensor([16.1592], grad_fn=<AddBackward0>)\n",
      "tensor([16.9072], grad_fn=<AddBackward0>)\n",
      "tensor([11.2538], grad_fn=<AddBackward0>)\n",
      "tensor([10.4459], grad_fn=<AddBackward0>)\n",
      "tensor([12.7369], grad_fn=<AddBackward0>)\n",
      "tensor([24.4508], grad_fn=<AddBackward0>)\n",
      "tensor([12.6139], grad_fn=<AddBackward0>)\n",
      "tensor([33.0520], grad_fn=<AddBackward0>)\n",
      "tensor([16.0939], grad_fn=<AddBackward0>)\n",
      "tensor([16.8702], grad_fn=<AddBackward0>)\n",
      "tensor([11.2015], grad_fn=<AddBackward0>)\n",
      "tensor([10.3762], grad_fn=<AddBackward0>)\n",
      "tensor([12.6824], grad_fn=<AddBackward0>)\n",
      "tensor([24.4619], grad_fn=<AddBackward0>)\n",
      "tensor([12.5718], grad_fn=<AddBackward0>)\n",
      "tensor([33.0351], grad_fn=<AddBackward0>)\n",
      "tensor([16.0292], grad_fn=<AddBackward0>)\n",
      "tensor([16.8337], grad_fn=<AddBackward0>)\n",
      "tensor([11.1499], grad_fn=<AddBackward0>)\n",
      "tensor([10.3072], grad_fn=<AddBackward0>)\n",
      "tensor([12.6285], grad_fn=<AddBackward0>)\n",
      "tensor([24.4735], grad_fn=<AddBackward0>)\n",
      "tensor([12.5302], grad_fn=<AddBackward0>)\n",
      "tensor([33.0185], grad_fn=<AddBackward0>)\n",
      "tensor([15.9652], grad_fn=<AddBackward0>)\n",
      "tensor([16.7975], grad_fn=<AddBackward0>)\n",
      "tensor([11.0990], grad_fn=<AddBackward0>)\n",
      "tensor([10.2388], grad_fn=<AddBackward0>)\n",
      "tensor([12.5751], grad_fn=<AddBackward0>)\n",
      "tensor([24.4854], grad_fn=<AddBackward0>)\n",
      "tensor([12.4892], grad_fn=<AddBackward0>)\n",
      "tensor([33.0024], grad_fn=<AddBackward0>)\n",
      "tensor([15.9019], grad_fn=<AddBackward0>)\n",
      "tensor([16.7618], grad_fn=<AddBackward0>)\n",
      "tensor([11.0488], grad_fn=<AddBackward0>)\n",
      "tensor([10.1710], grad_fn=<AddBackward0>)\n",
      "tensor([12.5224], grad_fn=<AddBackward0>)\n",
      "tensor([24.4977], grad_fn=<AddBackward0>)\n",
      "tensor([12.4487], grad_fn=<AddBackward0>)\n",
      "tensor([32.9866], grad_fn=<AddBackward0>)\n",
      "tensor([15.8393], grad_fn=<AddBackward0>)\n",
      "tensor([16.7265], grad_fn=<AddBackward0>)\n",
      "tensor([10.9993], grad_fn=<AddBackward0>)\n",
      "tensor([10.1038], grad_fn=<AddBackward0>)\n",
      "tensor([12.4701], grad_fn=<AddBackward0>)\n",
      "tensor([24.5103], grad_fn=<AddBackward0>)\n",
      "tensor([12.4087], grad_fn=<AddBackward0>)\n",
      "tensor([32.9712], grad_fn=<AddBackward0>)\n",
      "tensor([15.7773], grad_fn=<AddBackward0>)\n",
      "tensor([16.6915], grad_fn=<AddBackward0>)\n",
      "tensor([10.9505], grad_fn=<AddBackward0>)\n",
      "tensor([10.0373], grad_fn=<AddBackward0>)\n",
      "tensor([12.4185], grad_fn=<AddBackward0>)\n",
      "tensor([24.5233], grad_fn=<AddBackward0>)\n",
      "tensor([12.3693], grad_fn=<AddBackward0>)\n",
      "tensor([32.9562], grad_fn=<AddBackward0>)\n",
      "tensor([15.7160], grad_fn=<AddBackward0>)\n",
      "tensor([16.6570], grad_fn=<AddBackward0>)\n",
      "tensor([10.9023], grad_fn=<AddBackward0>)\n",
      "tensor([9.9713], grad_fn=<AddBackward0>)\n",
      "tensor([12.3674], grad_fn=<AddBackward0>)\n",
      "tensor([24.5366], grad_fn=<AddBackward0>)\n",
      "tensor([12.3304], grad_fn=<AddBackward0>)\n",
      "tensor([32.9416], grad_fn=<AddBackward0>)\n",
      "tensor([15.6553], grad_fn=<AddBackward0>)\n",
      "tensor([16.6229], grad_fn=<AddBackward0>)\n",
      "tensor([10.8549], grad_fn=<AddBackward0>)\n",
      "tensor([9.9059], grad_fn=<AddBackward0>)\n",
      "tensor([12.3169], grad_fn=<AddBackward0>)\n",
      "tensor([24.5502], grad_fn=<AddBackward0>)\n",
      "tensor([12.2919], grad_fn=<AddBackward0>)\n",
      "tensor([32.9273], grad_fn=<AddBackward0>)\n",
      "tensor([15.5952], grad_fn=<AddBackward0>)\n",
      "tensor([16.5892], grad_fn=<AddBackward0>)\n",
      "tensor([10.8080], grad_fn=<AddBackward0>)\n",
      "tensor([9.8411], grad_fn=<AddBackward0>)\n",
      "tensor([12.2669], grad_fn=<AddBackward0>)\n",
      "tensor([24.5642], grad_fn=<AddBackward0>)\n",
      "tensor([12.2540], grad_fn=<AddBackward0>)\n",
      "tensor([32.9134], grad_fn=<AddBackward0>)\n",
      "tensor([15.5358], grad_fn=<AddBackward0>)\n",
      "tensor([16.5558], grad_fn=<AddBackward0>)\n",
      "tensor([10.7619], grad_fn=<AddBackward0>)\n",
      "tensor([9.7768], grad_fn=<AddBackward0>)\n",
      "tensor([12.2174], grad_fn=<AddBackward0>)\n",
      "tensor([24.5785], grad_fn=<AddBackward0>)\n",
      "tensor([12.2166], grad_fn=<AddBackward0>)\n",
      "tensor([32.8998], grad_fn=<AddBackward0>)\n",
      "tensor([15.4770], grad_fn=<AddBackward0>)\n",
      "tensor([16.5228], grad_fn=<AddBackward0>)\n",
      "tensor([10.7163], grad_fn=<AddBackward0>)\n",
      "tensor([9.7132], grad_fn=<AddBackward0>)\n",
      "tensor([12.1685], grad_fn=<AddBackward0>)\n",
      "tensor([24.5931], grad_fn=<AddBackward0>)\n",
      "tensor([12.1796], grad_fn=<AddBackward0>)\n",
      "tensor([32.8866], grad_fn=<AddBackward0>)\n",
      "tensor([15.4189], grad_fn=<AddBackward0>)\n",
      "tensor([16.4902], grad_fn=<AddBackward0>)\n",
      "tensor([10.6714], grad_fn=<AddBackward0>)\n",
      "tensor([9.6501], grad_fn=<AddBackward0>)\n",
      "tensor([12.1201], grad_fn=<AddBackward0>)\n",
      "tensor([24.6080], grad_fn=<AddBackward0>)\n",
      "tensor([12.1431], grad_fn=<AddBackward0>)\n",
      "tensor([32.8738], grad_fn=<AddBackward0>)\n",
      "tensor([15.3613], grad_fn=<AddBackward0>)\n",
      "tensor([16.4580], grad_fn=<AddBackward0>)\n",
      "tensor([10.6272], grad_fn=<AddBackward0>)\n",
      "tensor([9.5876], grad_fn=<AddBackward0>)\n",
      "tensor([12.0722], grad_fn=<AddBackward0>)\n",
      "tensor([24.6232], grad_fn=<AddBackward0>)\n",
      "tensor([12.1071], grad_fn=<AddBackward0>)\n",
      "tensor([32.8613], grad_fn=<AddBackward0>)\n",
      "tensor([15.3044], grad_fn=<AddBackward0>)\n",
      "tensor([16.4261], grad_fn=<AddBackward0>)\n",
      "tensor([10.5836], grad_fn=<AddBackward0>)\n",
      "tensor([9.5257], grad_fn=<AddBackward0>)\n",
      "tensor([12.0249], grad_fn=<AddBackward0>)\n",
      "tensor([24.6387], grad_fn=<AddBackward0>)\n",
      "tensor([12.0715], grad_fn=<AddBackward0>)\n",
      "tensor([32.8491], grad_fn=<AddBackward0>)\n",
      "tensor([15.2481], grad_fn=<AddBackward0>)\n",
      "tensor([16.3946], grad_fn=<AddBackward0>)\n",
      "tensor([10.5406], grad_fn=<AddBackward0>)\n",
      "tensor([9.4643], grad_fn=<AddBackward0>)\n",
      "tensor([11.9781], grad_fn=<AddBackward0>)\n",
      "tensor([24.6545], grad_fn=<AddBackward0>)\n",
      "tensor([12.0364], grad_fn=<AddBackward0>)\n",
      "tensor([32.8373], grad_fn=<AddBackward0>)\n",
      "tensor([15.1924], grad_fn=<AddBackward0>)\n",
      "tensor([16.3634], grad_fn=<AddBackward0>)\n",
      "tensor([10.4982], grad_fn=<AddBackward0>)\n",
      "tensor([9.4035], grad_fn=<AddBackward0>)\n",
      "tensor([11.9318], grad_fn=<AddBackward0>)\n",
      "tensor([24.6706], grad_fn=<AddBackward0>)\n",
      "tensor([12.0018], grad_fn=<AddBackward0>)\n",
      "tensor([32.8258], grad_fn=<AddBackward0>)\n",
      "tensor([15.1373], grad_fn=<AddBackward0>)\n",
      "tensor([16.3326], grad_fn=<AddBackward0>)\n",
      "tensor([10.4564], grad_fn=<AddBackward0>)\n",
      "tensor([9.3432], grad_fn=<AddBackward0>)\n",
      "tensor([11.8860], grad_fn=<AddBackward0>)\n",
      "tensor([24.6869], grad_fn=<AddBackward0>)\n",
      "tensor([11.9676], grad_fn=<AddBackward0>)\n",
      "tensor([32.8146], grad_fn=<AddBackward0>)\n",
      "tensor([15.0827], grad_fn=<AddBackward0>)\n",
      "tensor([16.3021], grad_fn=<AddBackward0>)\n",
      "tensor([10.4153], grad_fn=<AddBackward0>)\n",
      "tensor([9.2834], grad_fn=<AddBackward0>)\n",
      "tensor([11.8406], grad_fn=<AddBackward0>)\n",
      "tensor([24.7035], grad_fn=<AddBackward0>)\n",
      "tensor([11.9338], grad_fn=<AddBackward0>)\n",
      "tensor([32.8037], grad_fn=<AddBackward0>)\n",
      "tensor([15.0288], grad_fn=<AddBackward0>)\n",
      "tensor([16.2720], grad_fn=<AddBackward0>)\n",
      "tensor([10.3747], grad_fn=<AddBackward0>)\n",
      "tensor([9.2242], grad_fn=<AddBackward0>)\n",
      "tensor([11.7958], grad_fn=<AddBackward0>)\n",
      "tensor([24.7204], grad_fn=<AddBackward0>)\n",
      "tensor([11.9004], grad_fn=<AddBackward0>)\n",
      "tensor([32.7931], grad_fn=<AddBackward0>)\n",
      "tensor([14.9755], grad_fn=<AddBackward0>)\n",
      "tensor([16.2421], grad_fn=<AddBackward0>)\n",
      "tensor([10.3347], grad_fn=<AddBackward0>)\n",
      "tensor([9.1656], grad_fn=<AddBackward0>)\n",
      "tensor([11.7515], grad_fn=<AddBackward0>)\n",
      "tensor([24.7374], grad_fn=<AddBackward0>)\n",
      "tensor([11.8675], grad_fn=<AddBackward0>)\n",
      "tensor([32.7829], grad_fn=<AddBackward0>)\n",
      "tensor([14.9227], grad_fn=<AddBackward0>)\n",
      "tensor([16.2127], grad_fn=<AddBackward0>)\n",
      "tensor([10.2953], grad_fn=<AddBackward0>)\n",
      "tensor([9.1074], grad_fn=<AddBackward0>)\n",
      "tensor([11.7077], grad_fn=<AddBackward0>)\n",
      "tensor([24.7548], grad_fn=<AddBackward0>)\n",
      "tensor([11.8350], grad_fn=<AddBackward0>)\n",
      "tensor([32.7729], grad_fn=<AddBackward0>)\n",
      "tensor([14.8705], grad_fn=<AddBackward0>)\n",
      "tensor([16.1835], grad_fn=<AddBackward0>)\n",
      "tensor([10.2565], grad_fn=<AddBackward0>)\n",
      "tensor([9.0498], grad_fn=<AddBackward0>)\n",
      "tensor([11.6643], grad_fn=<AddBackward0>)\n",
      "tensor([24.7724], grad_fn=<AddBackward0>)\n",
      "tensor([11.8029], grad_fn=<AddBackward0>)\n",
      "tensor([32.7633], grad_fn=<AddBackward0>)\n",
      "tensor([14.8189], grad_fn=<AddBackward0>)\n",
      "tensor([16.1547], grad_fn=<AddBackward0>)\n",
      "tensor([10.2182], grad_fn=<AddBackward0>)\n",
      "tensor([8.9928], grad_fn=<AddBackward0>)\n",
      "tensor([11.6214], grad_fn=<AddBackward0>)\n",
      "tensor([24.7902], grad_fn=<AddBackward0>)\n",
      "tensor([11.7712], grad_fn=<AddBackward0>)\n",
      "tensor([32.7539], grad_fn=<AddBackward0>)\n",
      "tensor([14.7678], grad_fn=<AddBackward0>)\n",
      "tensor([16.1261], grad_fn=<AddBackward0>)\n",
      "tensor([10.1805], grad_fn=<AddBackward0>)\n",
      "tensor([8.9362], grad_fn=<AddBackward0>)\n",
      "tensor([11.5790], grad_fn=<AddBackward0>)\n",
      "tensor([24.8082], grad_fn=<AddBackward0>)\n",
      "tensor([11.7399], grad_fn=<AddBackward0>)\n",
      "tensor([32.7449], grad_fn=<AddBackward0>)\n",
      "tensor([14.7173], grad_fn=<AddBackward0>)\n",
      "tensor([16.0979], grad_fn=<AddBackward0>)\n",
      "tensor([10.1434], grad_fn=<AddBackward0>)\n",
      "tensor([8.8802], grad_fn=<AddBackward0>)\n",
      "tensor([11.5371], grad_fn=<AddBackward0>)\n",
      "tensor([24.8265], grad_fn=<AddBackward0>)\n",
      "tensor([11.7089], grad_fn=<AddBackward0>)\n",
      "tensor([32.7361], grad_fn=<AddBackward0>)\n",
      "tensor([14.6674], grad_fn=<AddBackward0>)\n",
      "tensor([16.0700], grad_fn=<AddBackward0>)\n",
      "tensor([10.1069], grad_fn=<AddBackward0>)\n",
      "tensor([8.8246], grad_fn=<AddBackward0>)\n",
      "tensor([11.4956], grad_fn=<AddBackward0>)\n",
      "tensor([24.8450], grad_fn=<AddBackward0>)\n",
      "tensor([11.6784], grad_fn=<AddBackward0>)\n",
      "tensor([32.7276], grad_fn=<AddBackward0>)\n",
      "tensor([14.6180], grad_fn=<AddBackward0>)\n",
      "tensor([16.0424], grad_fn=<AddBackward0>)\n",
      "tensor([10.0708], grad_fn=<AddBackward0>)\n",
      "tensor([8.7696], grad_fn=<AddBackward0>)\n",
      "tensor([11.4546], grad_fn=<AddBackward0>)\n",
      "tensor([24.8636], grad_fn=<AddBackward0>)\n",
      "tensor([11.6483], grad_fn=<AddBackward0>)\n",
      "tensor([32.7194], grad_fn=<AddBackward0>)\n",
      "tensor([14.5691], grad_fn=<AddBackward0>)\n",
      "tensor([16.0151], grad_fn=<AddBackward0>)\n",
      "tensor([10.0354], grad_fn=<AddBackward0>)\n",
      "tensor([8.7150], grad_fn=<AddBackward0>)\n",
      "tensor([11.4141], grad_fn=<AddBackward0>)\n",
      "tensor([24.8825], grad_fn=<AddBackward0>)\n",
      "tensor([11.6185], grad_fn=<AddBackward0>)\n",
      "tensor([32.7115], grad_fn=<AddBackward0>)\n",
      "tensor([14.5208], grad_fn=<AddBackward0>)\n",
      "tensor([15.9881], grad_fn=<AddBackward0>)\n",
      "tensor([10.0004], grad_fn=<AddBackward0>)\n",
      "tensor([8.6610], grad_fn=<AddBackward0>)\n",
      "tensor([11.3739], grad_fn=<AddBackward0>)\n",
      "tensor([24.9016], grad_fn=<AddBackward0>)\n",
      "tensor([11.5891], grad_fn=<AddBackward0>)\n",
      "tensor([32.7038], grad_fn=<AddBackward0>)\n",
      "tensor([14.4730], grad_fn=<AddBackward0>)\n",
      "tensor([15.9613], grad_fn=<AddBackward0>)\n",
      "tensor([9.9660], grad_fn=<AddBackward0>)\n",
      "tensor([8.6075], grad_fn=<AddBackward0>)\n",
      "tensor([11.3343], grad_fn=<AddBackward0>)\n",
      "tensor([24.9209], grad_fn=<AddBackward0>)\n",
      "tensor([11.5600], grad_fn=<AddBackward0>)\n",
      "tensor([32.6964], grad_fn=<AddBackward0>)\n",
      "tensor([14.4258], grad_fn=<AddBackward0>)\n",
      "tensor([15.9349], grad_fn=<AddBackward0>)\n",
      "tensor([9.9321], grad_fn=<AddBackward0>)\n",
      "tensor([8.5544], grad_fn=<AddBackward0>)\n",
      "tensor([11.2951], grad_fn=<AddBackward0>)\n",
      "tensor([24.9403], grad_fn=<AddBackward0>)\n",
      "tensor([11.5313], grad_fn=<AddBackward0>)\n",
      "tensor([32.6893], grad_fn=<AddBackward0>)\n",
      "tensor([14.3791], grad_fn=<AddBackward0>)\n",
      "tensor([15.9087], grad_fn=<AddBackward0>)\n",
      "tensor([9.8988], grad_fn=<AddBackward0>)\n",
      "tensor([8.5018], grad_fn=<AddBackward0>)\n",
      "tensor([11.2563], grad_fn=<AddBackward0>)\n",
      "tensor([24.9599], grad_fn=<AddBackward0>)\n",
      "tensor([11.5030], grad_fn=<AddBackward0>)\n",
      "tensor([32.6824], grad_fn=<AddBackward0>)\n",
      "tensor([14.3329], grad_fn=<AddBackward0>)\n",
      "tensor([15.8829], grad_fn=<AddBackward0>)\n",
      "tensor([9.8659], grad_fn=<AddBackward0>)\n",
      "tensor([8.4498], grad_fn=<AddBackward0>)\n",
      "tensor([11.2179], grad_fn=<AddBackward0>)\n",
      "tensor([24.9797], grad_fn=<AddBackward0>)\n",
      "tensor([11.4750], grad_fn=<AddBackward0>)\n",
      "tensor([32.6758], grad_fn=<AddBackward0>)\n",
      "tensor([14.2872], grad_fn=<AddBackward0>)\n",
      "tensor([15.8572], grad_fn=<AddBackward0>)\n",
      "tensor([9.8336], grad_fn=<AddBackward0>)\n",
      "tensor([8.3981], grad_fn=<AddBackward0>)\n",
      "tensor([11.1800], grad_fn=<AddBackward0>)\n",
      "tensor([24.9997], grad_fn=<AddBackward0>)\n",
      "tensor([11.4474], grad_fn=<AddBackward0>)\n",
      "tensor([32.6694], grad_fn=<AddBackward0>)\n",
      "tensor([14.2420], grad_fn=<AddBackward0>)\n",
      "tensor([15.8319], grad_fn=<AddBackward0>)\n",
      "tensor([9.8018], grad_fn=<AddBackward0>)\n",
      "tensor([8.3470], grad_fn=<AddBackward0>)\n",
      "tensor([11.1425], grad_fn=<AddBackward0>)\n",
      "tensor([25.0198], grad_fn=<AddBackward0>)\n",
      "tensor([11.4201], grad_fn=<AddBackward0>)\n",
      "tensor([32.6633], grad_fn=<AddBackward0>)\n",
      "tensor([14.1974], grad_fn=<AddBackward0>)\n",
      "tensor([15.8068], grad_fn=<AddBackward0>)\n",
      "tensor([9.7704], grad_fn=<AddBackward0>)\n",
      "tensor([8.2963], grad_fn=<AddBackward0>)\n",
      "tensor([11.1054], grad_fn=<AddBackward0>)\n",
      "tensor([25.0401], grad_fn=<AddBackward0>)\n",
      "tensor([11.3931], grad_fn=<AddBackward0>)\n",
      "tensor([32.6574], grad_fn=<AddBackward0>)\n",
      "tensor([14.1532], grad_fn=<AddBackward0>)\n",
      "tensor([15.7820], grad_fn=<AddBackward0>)\n",
      "tensor([9.7396], grad_fn=<AddBackward0>)\n",
      "tensor([8.2461], grad_fn=<AddBackward0>)\n",
      "tensor([11.0687], grad_fn=<AddBackward0>)\n",
      "tensor([25.0606], grad_fn=<AddBackward0>)\n",
      "tensor([11.3664], grad_fn=<AddBackward0>)\n",
      "tensor([32.6517], grad_fn=<AddBackward0>)\n",
      "tensor([14.1096], grad_fn=<AddBackward0>)\n",
      "tensor([15.7575], grad_fn=<AddBackward0>)\n",
      "tensor([9.7092], grad_fn=<AddBackward0>)\n",
      "tensor([8.1964], grad_fn=<AddBackward0>)\n",
      "tensor([11.0325], grad_fn=<AddBackward0>)\n",
      "tensor([25.0812], grad_fn=<AddBackward0>)\n",
      "tensor([11.3401], grad_fn=<AddBackward0>)\n",
      "tensor([32.6463], grad_fn=<AddBackward0>)\n",
      "tensor([14.0664], grad_fn=<AddBackward0>)\n",
      "tensor([15.7332], grad_fn=<AddBackward0>)\n",
      "tensor([9.6793], grad_fn=<AddBackward0>)\n",
      "tensor([8.1471], grad_fn=<AddBackward0>)\n",
      "tensor([10.9966], grad_fn=<AddBackward0>)\n",
      "tensor([25.1019], grad_fn=<AddBackward0>)\n",
      "tensor([11.3141], grad_fn=<AddBackward0>)\n",
      "tensor([32.6411], grad_fn=<AddBackward0>)\n",
      "tensor([14.0237], grad_fn=<AddBackward0>)\n",
      "tensor([15.7092], grad_fn=<AddBackward0>)\n",
      "tensor([9.6499], grad_fn=<AddBackward0>)\n",
      "tensor([8.0983], grad_fn=<AddBackward0>)\n",
      "tensor([10.9612], grad_fn=<AddBackward0>)\n",
      "tensor([25.1228], grad_fn=<AddBackward0>)\n",
      "tensor([11.2884], grad_fn=<AddBackward0>)\n",
      "tensor([32.6361], grad_fn=<AddBackward0>)\n",
      "tensor([13.9816], grad_fn=<AddBackward0>)\n",
      "tensor([15.6854], grad_fn=<AddBackward0>)\n",
      "tensor([9.6210], grad_fn=<AddBackward0>)\n",
      "tensor([8.0499], grad_fn=<AddBackward0>)\n",
      "tensor([10.9262], grad_fn=<AddBackward0>)\n",
      "tensor([25.1438], grad_fn=<AddBackward0>)\n",
      "tensor([11.2630], grad_fn=<AddBackward0>)\n",
      "tensor([32.6314], grad_fn=<AddBackward0>)\n",
      "tensor([13.9399], grad_fn=<AddBackward0>)\n",
      "tensor([15.6618], grad_fn=<AddBackward0>)\n",
      "tensor([9.5925], grad_fn=<AddBackward0>)\n",
      "tensor([8.0019], grad_fn=<AddBackward0>)\n",
      "tensor([10.8915], grad_fn=<AddBackward0>)\n",
      "tensor([25.1649], grad_fn=<AddBackward0>)\n",
      "tensor([11.2379], grad_fn=<AddBackward0>)\n",
      "tensor([32.6269], grad_fn=<AddBackward0>)\n",
      "tensor([13.8986], grad_fn=<AddBackward0>)\n",
      "tensor([15.6385], grad_fn=<AddBackward0>)\n",
      "tensor([9.5645], grad_fn=<AddBackward0>)\n",
      "tensor([7.9544], grad_fn=<AddBackward0>)\n",
      "tensor([10.8572], grad_fn=<AddBackward0>)\n",
      "tensor([25.1861], grad_fn=<AddBackward0>)\n",
      "tensor([11.2131], grad_fn=<AddBackward0>)\n",
      "tensor([32.6226], grad_fn=<AddBackward0>)\n",
      "tensor([13.8579], grad_fn=<AddBackward0>)\n",
      "tensor([15.6155], grad_fn=<AddBackward0>)\n",
      "tensor([9.5370], grad_fn=<AddBackward0>)\n",
      "tensor([7.9074], grad_fn=<AddBackward0>)\n",
      "tensor([10.8234], grad_fn=<AddBackward0>)\n",
      "tensor([25.2075], grad_fn=<AddBackward0>)\n",
      "tensor([11.1887], grad_fn=<AddBackward0>)\n",
      "tensor([32.6185], grad_fn=<AddBackward0>)\n",
      "tensor([13.8176], grad_fn=<AddBackward0>)\n",
      "tensor([15.5926], grad_fn=<AddBackward0>)\n",
      "tensor([9.5099], grad_fn=<AddBackward0>)\n",
      "tensor([7.8607], grad_fn=<AddBackward0>)\n",
      "tensor([10.7899], grad_fn=<AddBackward0>)\n",
      "tensor([25.2290], grad_fn=<AddBackward0>)\n",
      "tensor([11.1645], grad_fn=<AddBackward0>)\n",
      "tensor([32.6146], grad_fn=<AddBackward0>)\n",
      "tensor([13.7778], grad_fn=<AddBackward0>)\n",
      "tensor([15.5700], grad_fn=<AddBackward0>)\n",
      "tensor([9.4832], grad_fn=<AddBackward0>)\n",
      "tensor([7.8145], grad_fn=<AddBackward0>)\n",
      "tensor([10.7568], grad_fn=<AddBackward0>)\n",
      "tensor([25.2506], grad_fn=<AddBackward0>)\n",
      "tensor([11.1405], grad_fn=<AddBackward0>)\n",
      "tensor([32.6109], grad_fn=<AddBackward0>)\n",
      "tensor([13.7384], grad_fn=<AddBackward0>)\n",
      "tensor([15.5477], grad_fn=<AddBackward0>)\n",
      "tensor([9.4570], grad_fn=<AddBackward0>)\n",
      "tensor([7.7688], grad_fn=<AddBackward0>)\n",
      "tensor([10.7241], grad_fn=<AddBackward0>)\n",
      "tensor([25.2723], grad_fn=<AddBackward0>)\n",
      "tensor([11.1169], grad_fn=<AddBackward0>)\n",
      "tensor([32.6074], grad_fn=<AddBackward0>)\n",
      "tensor([13.6995], grad_fn=<AddBackward0>)\n",
      "tensor([15.5255], grad_fn=<AddBackward0>)\n",
      "tensor([9.4312], grad_fn=<AddBackward0>)\n",
      "tensor([7.7234], grad_fn=<AddBackward0>)\n",
      "tensor([10.6917], grad_fn=<AddBackward0>)\n",
      "tensor([25.2941], grad_fn=<AddBackward0>)\n",
      "tensor([11.0936], grad_fn=<AddBackward0>)\n",
      "tensor([32.6041], grad_fn=<AddBackward0>)\n",
      "tensor([13.6611], grad_fn=<AddBackward0>)\n",
      "tensor([15.5036], grad_fn=<AddBackward0>)\n",
      "tensor([9.4058], grad_fn=<AddBackward0>)\n",
      "tensor([7.6785], grad_fn=<AddBackward0>)\n",
      "tensor([10.6597], grad_fn=<AddBackward0>)\n",
      "tensor([25.3160], grad_fn=<AddBackward0>)\n",
      "tensor([11.0705], grad_fn=<AddBackward0>)\n",
      "tensor([32.6010], grad_fn=<AddBackward0>)\n",
      "tensor([13.6231], grad_fn=<AddBackward0>)\n",
      "tensor([15.4819], grad_fn=<AddBackward0>)\n",
      "tensor([9.3809], grad_fn=<AddBackward0>)\n",
      "tensor([7.6340], grad_fn=<AddBackward0>)\n",
      "tensor([10.6281], grad_fn=<AddBackward0>)\n",
      "tensor([25.3379], grad_fn=<AddBackward0>)\n",
      "tensor([11.0477], grad_fn=<AddBackward0>)\n",
      "tensor([32.5981], grad_fn=<AddBackward0>)\n",
      "tensor([13.5855], grad_fn=<AddBackward0>)\n",
      "tensor([15.4605], grad_fn=<AddBackward0>)\n",
      "tensor([9.3564], grad_fn=<AddBackward0>)\n",
      "tensor([7.5899], grad_fn=<AddBackward0>)\n",
      "tensor([10.5968], grad_fn=<AddBackward0>)\n",
      "tensor([25.3600], grad_fn=<AddBackward0>)\n",
      "tensor([11.0251], grad_fn=<AddBackward0>)\n",
      "tensor([32.5954], grad_fn=<AddBackward0>)\n",
      "tensor([13.5484], grad_fn=<AddBackward0>)\n",
      "tensor([15.4392], grad_fn=<AddBackward0>)\n",
      "tensor([9.3323], grad_fn=<AddBackward0>)\n",
      "tensor([7.5462], grad_fn=<AddBackward0>)\n",
      "tensor([10.5659], grad_fn=<AddBackward0>)\n",
      "tensor([25.3821], grad_fn=<AddBackward0>)\n",
      "tensor([11.0028], grad_fn=<AddBackward0>)\n",
      "tensor([32.5929], grad_fn=<AddBackward0>)\n",
      "tensor([13.5117], grad_fn=<AddBackward0>)\n",
      "tensor([15.4182], grad_fn=<AddBackward0>)\n",
      "tensor([9.3086], grad_fn=<AddBackward0>)\n",
      "tensor([7.5029], grad_fn=<AddBackward0>)\n",
      "tensor([10.5354], grad_fn=<AddBackward0>)\n",
      "tensor([25.4044], grad_fn=<AddBackward0>)\n",
      "tensor([10.9808], grad_fn=<AddBackward0>)\n",
      "tensor([32.5906], grad_fn=<AddBackward0>)\n",
      "tensor([13.4754], grad_fn=<AddBackward0>)\n",
      "tensor([15.3973], grad_fn=<AddBackward0>)\n",
      "tensor([9.2853], grad_fn=<AddBackward0>)\n",
      "tensor([7.4600], grad_fn=<AddBackward0>)\n",
      "tensor([10.5051], grad_fn=<AddBackward0>)\n",
      "tensor([25.4267], grad_fn=<AddBackward0>)\n",
      "tensor([10.9590], grad_fn=<AddBackward0>)\n",
      "tensor([32.5884], grad_fn=<AddBackward0>)\n",
      "tensor([13.4396], grad_fn=<AddBackward0>)\n",
      "tensor([15.3767], grad_fn=<AddBackward0>)\n",
      "tensor([9.2624], grad_fn=<AddBackward0>)\n",
      "tensor([7.4176], grad_fn=<AddBackward0>)\n",
      "tensor([10.4753], grad_fn=<AddBackward0>)\n",
      "tensor([25.4490], grad_fn=<AddBackward0>)\n",
      "tensor([10.9375], grad_fn=<AddBackward0>)\n",
      "tensor([32.5864], grad_fn=<AddBackward0>)\n",
      "tensor([13.4042], grad_fn=<AddBackward0>)\n",
      "tensor([15.3563], grad_fn=<AddBackward0>)\n",
      "tensor([9.2399], grad_fn=<AddBackward0>)\n",
      "tensor([7.3755], grad_fn=<AddBackward0>)\n",
      "tensor([10.4458], grad_fn=<AddBackward0>)\n",
      "tensor([25.4715], grad_fn=<AddBackward0>)\n",
      "tensor([10.9162], grad_fn=<AddBackward0>)\n",
      "tensor([32.5846], grad_fn=<AddBackward0>)\n",
      "tensor([13.3692], grad_fn=<AddBackward0>)\n",
      "tensor([15.3361], grad_fn=<AddBackward0>)\n",
      "tensor([9.2178], grad_fn=<AddBackward0>)\n",
      "tensor([7.3338], grad_fn=<AddBackward0>)\n",
      "tensor([10.4166], grad_fn=<AddBackward0>)\n",
      "tensor([25.4940], grad_fn=<AddBackward0>)\n",
      "tensor([10.8952], grad_fn=<AddBackward0>)\n",
      "tensor([32.5829], grad_fn=<AddBackward0>)\n",
      "tensor([13.3346], grad_fn=<AddBackward0>)\n",
      "tensor([15.3160], grad_fn=<AddBackward0>)\n",
      "tensor([9.1961], grad_fn=<AddBackward0>)\n",
      "tensor([7.2925], grad_fn=<AddBackward0>)\n",
      "tensor([10.3878], grad_fn=<AddBackward0>)\n",
      "tensor([25.5165], grad_fn=<AddBackward0>)\n",
      "tensor([10.8744], grad_fn=<AddBackward0>)\n",
      "tensor([32.5814], grad_fn=<AddBackward0>)\n",
      "tensor([13.3004], grad_fn=<AddBackward0>)\n",
      "tensor([15.2962], grad_fn=<AddBackward0>)\n",
      "tensor([9.1747], grad_fn=<AddBackward0>)\n",
      "tensor([7.2515], grad_fn=<AddBackward0>)\n",
      "tensor([10.3592], grad_fn=<AddBackward0>)\n",
      "tensor([25.5391], grad_fn=<AddBackward0>)\n",
      "tensor([10.8539], grad_fn=<AddBackward0>)\n",
      "tensor([32.5801], grad_fn=<AddBackward0>)\n",
      "tensor([13.2667], grad_fn=<AddBackward0>)\n",
      "tensor([15.2766], grad_fn=<AddBackward0>)\n",
      "tensor([9.1538], grad_fn=<AddBackward0>)\n",
      "tensor([7.2110], grad_fn=<AddBackward0>)\n",
      "tensor([10.3311], grad_fn=<AddBackward0>)\n",
      "tensor([25.5618], grad_fn=<AddBackward0>)\n",
      "tensor([10.8336], grad_fn=<AddBackward0>)\n",
      "tensor([32.5789], grad_fn=<AddBackward0>)\n",
      "tensor([13.2333], grad_fn=<AddBackward0>)\n",
      "tensor([15.2571], grad_fn=<AddBackward0>)\n",
      "tensor([9.1332], grad_fn=<AddBackward0>)\n",
      "tensor([7.1709], grad_fn=<AddBackward0>)\n",
      "tensor([10.3032], grad_fn=<AddBackward0>)\n",
      "tensor([25.5845], grad_fn=<AddBackward0>)\n",
      "tensor([10.8135], grad_fn=<AddBackward0>)\n",
      "tensor([32.5779], grad_fn=<AddBackward0>)\n",
      "tensor([13.2003], grad_fn=<AddBackward0>)\n",
      "tensor([15.2379], grad_fn=<AddBackward0>)\n",
      "tensor([9.1130], grad_fn=<AddBackward0>)\n",
      "tensor([7.1311], grad_fn=<AddBackward0>)\n",
      "tensor([10.2757], grad_fn=<AddBackward0>)\n",
      "tensor([25.6073], grad_fn=<AddBackward0>)\n",
      "tensor([10.7936], grad_fn=<AddBackward0>)\n",
      "tensor([32.5771], grad_fn=<AddBackward0>)\n",
      "tensor([13.1678], grad_fn=<AddBackward0>)\n",
      "tensor([15.2188], grad_fn=<AddBackward0>)\n",
      "tensor([9.0931], grad_fn=<AddBackward0>)\n",
      "tensor([7.0917], grad_fn=<AddBackward0>)\n",
      "tensor([10.2485], grad_fn=<AddBackward0>)\n",
      "tensor([25.6301], grad_fn=<AddBackward0>)\n",
      "tensor([10.7740], grad_fn=<AddBackward0>)\n",
      "tensor([32.5764], grad_fn=<AddBackward0>)\n",
      "tensor([13.1356], grad_fn=<AddBackward0>)\n",
      "tensor([15.2000], grad_fn=<AddBackward0>)\n",
      "tensor([9.0736], grad_fn=<AddBackward0>)\n",
      "tensor([7.0526], grad_fn=<AddBackward0>)\n",
      "tensor([10.2216], grad_fn=<AddBackward0>)\n",
      "tensor([25.6529], grad_fn=<AddBackward0>)\n",
      "tensor([10.7545], grad_fn=<AddBackward0>)\n",
      "tensor([32.5758], grad_fn=<AddBackward0>)\n",
      "tensor([13.1038], grad_fn=<AddBackward0>)\n",
      "tensor([15.1813], grad_fn=<AddBackward0>)\n",
      "tensor([9.0544], grad_fn=<AddBackward0>)\n",
      "tensor([7.0139], grad_fn=<AddBackward0>)\n",
      "tensor([10.1950], grad_fn=<AddBackward0>)\n",
      "tensor([25.6758], grad_fn=<AddBackward0>)\n",
      "tensor([10.7353], grad_fn=<AddBackward0>)\n",
      "tensor([32.5754], grad_fn=<AddBackward0>)\n",
      "tensor([13.0724], grad_fn=<AddBackward0>)\n",
      "tensor([15.1628], grad_fn=<AddBackward0>)\n",
      "tensor([9.0356], grad_fn=<AddBackward0>)\n",
      "tensor([6.9756], grad_fn=<AddBackward0>)\n",
      "tensor([10.1687], grad_fn=<AddBackward0>)\n",
      "tensor([25.6986], grad_fn=<AddBackward0>)\n",
      "tensor([10.7164], grad_fn=<AddBackward0>)\n",
      "tensor([32.5751], grad_fn=<AddBackward0>)\n",
      "tensor([13.0413], grad_fn=<AddBackward0>)\n",
      "tensor([15.1444], grad_fn=<AddBackward0>)\n",
      "tensor([9.0171], grad_fn=<AddBackward0>)\n",
      "tensor([6.9377], grad_fn=<AddBackward0>)\n",
      "tensor([10.1427], grad_fn=<AddBackward0>)\n",
      "tensor([25.7216], grad_fn=<AddBackward0>)\n",
      "tensor([10.6976], grad_fn=<AddBackward0>)\n",
      "tensor([32.5750], grad_fn=<AddBackward0>)\n",
      "tensor([13.0107], grad_fn=<AddBackward0>)\n",
      "tensor([15.1263], grad_fn=<AddBackward0>)\n",
      "tensor([8.9990], grad_fn=<AddBackward0>)\n",
      "tensor([6.9001], grad_fn=<AddBackward0>)\n",
      "tensor([10.1171], grad_fn=<AddBackward0>)\n",
      "tensor([25.7445], grad_fn=<AddBackward0>)\n",
      "tensor([10.6790], grad_fn=<AddBackward0>)\n",
      "tensor([32.5750], grad_fn=<AddBackward0>)\n",
      "tensor([12.9804], grad_fn=<AddBackward0>)\n",
      "tensor([15.1083], grad_fn=<AddBackward0>)\n",
      "tensor([8.9812], grad_fn=<AddBackward0>)\n",
      "tensor([6.8629], grad_fn=<AddBackward0>)\n",
      "tensor([10.0917], grad_fn=<AddBackward0>)\n",
      "tensor([25.7675], grad_fn=<AddBackward0>)\n",
      "tensor([10.6607], grad_fn=<AddBackward0>)\n",
      "tensor([32.5751], grad_fn=<AddBackward0>)\n",
      "tensor([12.9505], grad_fn=<AddBackward0>)\n",
      "tensor([15.0905], grad_fn=<AddBackward0>)\n",
      "tensor([8.9638], grad_fn=<AddBackward0>)\n",
      "tensor([6.8260], grad_fn=<AddBackward0>)\n",
      "tensor([10.0666], grad_fn=<AddBackward0>)\n",
      "tensor([25.7905], grad_fn=<AddBackward0>)\n",
      "tensor([10.6425], grad_fn=<AddBackward0>)\n",
      "tensor([32.5754], grad_fn=<AddBackward0>)\n",
      "tensor([12.9209], grad_fn=<AddBackward0>)\n",
      "tensor([15.0729], grad_fn=<AddBackward0>)\n",
      "tensor([8.9466], grad_fn=<AddBackward0>)\n",
      "tensor([6.7894], grad_fn=<AddBackward0>)\n",
      "tensor([10.0419], grad_fn=<AddBackward0>)\n",
      "tensor([25.8135], grad_fn=<AddBackward0>)\n",
      "tensor([10.6245], grad_fn=<AddBackward0>)\n",
      "tensor([32.5758], grad_fn=<AddBackward0>)\n",
      "tensor([12.8917], grad_fn=<AddBackward0>)\n",
      "tensor([15.0554], grad_fn=<AddBackward0>)\n",
      "tensor([8.9298], grad_fn=<AddBackward0>)\n",
      "tensor([6.7532], grad_fn=<AddBackward0>)\n",
      "tensor([10.0174], grad_fn=<AddBackward0>)\n",
      "tensor([25.8365], grad_fn=<AddBackward0>)\n",
      "tensor([10.6068], grad_fn=<AddBackward0>)\n",
      "tensor([32.5763], grad_fn=<AddBackward0>)\n",
      "tensor([12.8629], grad_fn=<AddBackward0>)\n",
      "tensor([15.0381], grad_fn=<AddBackward0>)\n",
      "tensor([8.9133], grad_fn=<AddBackward0>)\n",
      "tensor([6.7174], grad_fn=<AddBackward0>)\n",
      "tensor([9.9932], grad_fn=<AddBackward0>)\n",
      "tensor([25.8595], grad_fn=<AddBackward0>)\n",
      "tensor([10.5892], grad_fn=<AddBackward0>)\n",
      "tensor([32.5770], grad_fn=<AddBackward0>)\n",
      "tensor([12.8344], grad_fn=<AddBackward0>)\n",
      "tensor([15.0210], grad_fn=<AddBackward0>)\n",
      "tensor([8.8972], grad_fn=<AddBackward0>)\n",
      "tensor([6.6819], grad_fn=<AddBackward0>)\n",
      "tensor([9.9692], grad_fn=<AddBackward0>)\n",
      "tensor([25.8825], grad_fn=<AddBackward0>)\n",
      "tensor([10.5718], grad_fn=<AddBackward0>)\n",
      "tensor([32.5777], grad_fn=<AddBackward0>)\n",
      "tensor([12.8063], grad_fn=<AddBackward0>)\n",
      "tensor([15.0040], grad_fn=<AddBackward0>)\n",
      "tensor([8.8813], grad_fn=<AddBackward0>)\n",
      "tensor([6.6467], grad_fn=<AddBackward0>)\n",
      "tensor([9.9456], grad_fn=<AddBackward0>)\n",
      "tensor([25.9055], grad_fn=<AddBackward0>)\n",
      "tensor([10.5547], grad_fn=<AddBackward0>)\n",
      "tensor([32.5786], grad_fn=<AddBackward0>)\n",
      "tensor([12.7785], grad_fn=<AddBackward0>)\n",
      "tensor([14.9872], grad_fn=<AddBackward0>)\n",
      "tensor([8.8657], grad_fn=<AddBackward0>)\n",
      "tensor([6.6118], grad_fn=<AddBackward0>)\n",
      "tensor([9.9222], grad_fn=<AddBackward0>)\n",
      "tensor([25.9285], grad_fn=<AddBackward0>)\n",
      "tensor([10.5377], grad_fn=<AddBackward0>)\n",
      "tensor([32.5796], grad_fn=<AddBackward0>)\n",
      "tensor([12.7510], grad_fn=<AddBackward0>)\n",
      "tensor([14.9705], grad_fn=<AddBackward0>)\n",
      "tensor([8.8505], grad_fn=<AddBackward0>)\n",
      "tensor([6.5773], grad_fn=<AddBackward0>)\n",
      "tensor([9.8992], grad_fn=<AddBackward0>)\n",
      "tensor([25.9516], grad_fn=<AddBackward0>)\n",
      "tensor([10.5209], grad_fn=<AddBackward0>)\n",
      "tensor([32.5807], grad_fn=<AddBackward0>)\n",
      "tensor([12.7239], grad_fn=<AddBackward0>)\n",
      "tensor([14.9540], grad_fn=<AddBackward0>)\n",
      "tensor([8.8355], grad_fn=<AddBackward0>)\n",
      "tensor([6.5432], grad_fn=<AddBackward0>)\n",
      "tensor([9.8763], grad_fn=<AddBackward0>)\n",
      "tensor([25.9746], grad_fn=<AddBackward0>)\n",
      "tensor([10.5042], grad_fn=<AddBackward0>)\n",
      "tensor([32.5820], grad_fn=<AddBackward0>)\n",
      "tensor([12.6971], grad_fn=<AddBackward0>)\n",
      "tensor([14.9377], grad_fn=<AddBackward0>)\n",
      "tensor([8.8208], grad_fn=<AddBackward0>)\n",
      "tensor([6.5093], grad_fn=<AddBackward0>)\n",
      "tensor([9.8538], grad_fn=<AddBackward0>)\n",
      "tensor([25.9976], grad_fn=<AddBackward0>)\n",
      "tensor([10.4878], grad_fn=<AddBackward0>)\n",
      "tensor([32.5833], grad_fn=<AddBackward0>)\n",
      "tensor([12.6707], grad_fn=<AddBackward0>)\n",
      "tensor([14.9215], grad_fn=<AddBackward0>)\n",
      "tensor([8.8065], grad_fn=<AddBackward0>)\n",
      "tensor([6.4758], grad_fn=<AddBackward0>)\n",
      "tensor([9.8315], grad_fn=<AddBackward0>)\n",
      "tensor([26.0206], grad_fn=<AddBackward0>)\n",
      "tensor([10.4715], grad_fn=<AddBackward0>)\n",
      "tensor([32.5847], grad_fn=<AddBackward0>)\n",
      "tensor([12.6446], grad_fn=<AddBackward0>)\n",
      "tensor([14.9055], grad_fn=<AddBackward0>)\n",
      "tensor([8.7924], grad_fn=<AddBackward0>)\n",
      "tensor([6.4425], grad_fn=<AddBackward0>)\n",
      "tensor([9.8095], grad_fn=<AddBackward0>)\n",
      "tensor([26.0436], grad_fn=<AddBackward0>)\n",
      "tensor([10.4554], grad_fn=<AddBackward0>)\n",
      "tensor([32.5863], grad_fn=<AddBackward0>)\n",
      "tensor([12.6188], grad_fn=<AddBackward0>)\n",
      "tensor([14.8896], grad_fn=<AddBackward0>)\n",
      "tensor([8.7786], grad_fn=<AddBackward0>)\n",
      "tensor([6.4096], grad_fn=<AddBackward0>)\n",
      "tensor([9.7878], grad_fn=<AddBackward0>)\n",
      "tensor([26.0665], grad_fn=<AddBackward0>)\n",
      "tensor([10.4395], grad_fn=<AddBackward0>)\n",
      "tensor([32.5879], grad_fn=<AddBackward0>)\n",
      "tensor([12.5933], grad_fn=<AddBackward0>)\n",
      "tensor([14.8739], grad_fn=<AddBackward0>)\n",
      "tensor([8.7650], grad_fn=<AddBackward0>)\n",
      "tensor([6.3771], grad_fn=<AddBackward0>)\n",
      "tensor([9.7663], grad_fn=<AddBackward0>)\n",
      "tensor([26.0895], grad_fn=<AddBackward0>)\n",
      "tensor([10.4237], grad_fn=<AddBackward0>)\n",
      "tensor([32.5897], grad_fn=<AddBackward0>)\n",
      "tensor([12.5682], grad_fn=<AddBackward0>)\n",
      "tensor([14.8583], grad_fn=<AddBackward0>)\n",
      "tensor([8.7518], grad_fn=<AddBackward0>)\n",
      "tensor([6.3448], grad_fn=<AddBackward0>)\n",
      "tensor([9.7450], grad_fn=<AddBackward0>)\n",
      "tensor([26.1124], grad_fn=<AddBackward0>)\n",
      "tensor([10.4081], grad_fn=<AddBackward0>)\n",
      "tensor([32.5915], grad_fn=<AddBackward0>)\n",
      "tensor([12.5433], grad_fn=<AddBackward0>)\n",
      "tensor([14.8428], grad_fn=<AddBackward0>)\n",
      "tensor([8.7388], grad_fn=<AddBackward0>)\n",
      "tensor([6.3128], grad_fn=<AddBackward0>)\n",
      "tensor([9.7240], grad_fn=<AddBackward0>)\n",
      "tensor([26.1353], grad_fn=<AddBackward0>)\n",
      "tensor([10.3927], grad_fn=<AddBackward0>)\n",
      "tensor([32.5935], grad_fn=<AddBackward0>)\n",
      "tensor([12.5188], grad_fn=<AddBackward0>)\n",
      "tensor([14.8275], grad_fn=<AddBackward0>)\n",
      "tensor([8.7261], grad_fn=<AddBackward0>)\n",
      "tensor([6.2812], grad_fn=<AddBackward0>)\n",
      "tensor([9.7033], grad_fn=<AddBackward0>)\n",
      "tensor([26.1582], grad_fn=<AddBackward0>)\n",
      "tensor([10.3774], grad_fn=<AddBackward0>)\n",
      "tensor([32.5955], grad_fn=<AddBackward0>)\n",
      "tensor([12.4946], grad_fn=<AddBackward0>)\n",
      "tensor([14.8124], grad_fn=<AddBackward0>)\n",
      "tensor([8.7136], grad_fn=<AddBackward0>)\n",
      "tensor([6.2498], grad_fn=<AddBackward0>)\n",
      "tensor([9.6828], grad_fn=<AddBackward0>)\n",
      "tensor([26.1810], grad_fn=<AddBackward0>)\n",
      "tensor([10.3624], grad_fn=<AddBackward0>)\n",
      "tensor([32.5976], grad_fn=<AddBackward0>)\n",
      "tensor([12.4707], grad_fn=<AddBackward0>)\n",
      "tensor([14.7974], grad_fn=<AddBackward0>)\n",
      "tensor([8.7014], grad_fn=<AddBackward0>)\n",
      "tensor([6.2188], grad_fn=<AddBackward0>)\n",
      "tensor([9.6626], grad_fn=<AddBackward0>)\n",
      "tensor([26.2038], grad_fn=<AddBackward0>)\n",
      "tensor([10.3474], grad_fn=<AddBackward0>)\n",
      "tensor([32.5998], grad_fn=<AddBackward0>)\n",
      "tensor([12.4471], grad_fn=<AddBackward0>)\n",
      "tensor([14.7825], grad_fn=<AddBackward0>)\n",
      "tensor([8.6895], grad_fn=<AddBackward0>)\n",
      "tensor([6.1880], grad_fn=<AddBackward0>)\n",
      "tensor([9.6426], grad_fn=<AddBackward0>)\n",
      "tensor([26.2266], grad_fn=<AddBackward0>)\n",
      "tensor([10.3326], grad_fn=<AddBackward0>)\n",
      "tensor([32.6022], grad_fn=<AddBackward0>)\n",
      "tensor([12.4237], grad_fn=<AddBackward0>)\n",
      "tensor([14.7678], grad_fn=<AddBackward0>)\n",
      "tensor([8.6778], grad_fn=<AddBackward0>)\n",
      "tensor([6.1575], grad_fn=<AddBackward0>)\n",
      "tensor([9.6228], grad_fn=<AddBackward0>)\n",
      "tensor([26.2494], grad_fn=<AddBackward0>)\n",
      "tensor([10.3180], grad_fn=<AddBackward0>)\n",
      "tensor([32.6045], grad_fn=<AddBackward0>)\n",
      "tensor([12.4007], grad_fn=<AddBackward0>)\n",
      "tensor([14.7532], grad_fn=<AddBackward0>)\n",
      "tensor([8.6663], grad_fn=<AddBackward0>)\n",
      "tensor([6.1274], grad_fn=<AddBackward0>)\n",
      "tensor([9.6033], grad_fn=<AddBackward0>)\n",
      "tensor([26.2721], grad_fn=<AddBackward0>)\n",
      "tensor([10.3035], grad_fn=<AddBackward0>)\n",
      "tensor([32.6070], grad_fn=<AddBackward0>)\n",
      "tensor([12.3780], grad_fn=<AddBackward0>)\n",
      "tensor([14.7387], grad_fn=<AddBackward0>)\n",
      "tensor([8.6552], grad_fn=<AddBackward0>)\n",
      "tensor([6.0975], grad_fn=<AddBackward0>)\n",
      "tensor([9.5840], grad_fn=<AddBackward0>)\n",
      "tensor([26.2947], grad_fn=<AddBackward0>)\n",
      "tensor([10.2892], grad_fn=<AddBackward0>)\n",
      "tensor([32.6096], grad_fn=<AddBackward0>)\n",
      "tensor([12.3556], grad_fn=<AddBackward0>)\n",
      "tensor([14.7244], grad_fn=<AddBackward0>)\n",
      "tensor([8.6442], grad_fn=<AddBackward0>)\n",
      "tensor([6.0679], grad_fn=<AddBackward0>)\n",
      "tensor([9.5649], grad_fn=<AddBackward0>)\n",
      "tensor([26.3174], grad_fn=<AddBackward0>)\n",
      "tensor([10.2751], grad_fn=<AddBackward0>)\n",
      "tensor([32.6122], grad_fn=<AddBackward0>)\n",
      "tensor([12.3334], grad_fn=<AddBackward0>)\n",
      "tensor([14.7102], grad_fn=<AddBackward0>)\n",
      "tensor([8.6335], grad_fn=<AddBackward0>)\n",
      "tensor([6.0386], grad_fn=<AddBackward0>)\n",
      "tensor([9.5461], grad_fn=<AddBackward0>)\n",
      "tensor([26.3399], grad_fn=<AddBackward0>)\n",
      "tensor([10.2610], grad_fn=<AddBackward0>)\n",
      "tensor([32.6149], grad_fn=<AddBackward0>)\n",
      "tensor([12.3115], grad_fn=<AddBackward0>)\n",
      "tensor([14.6961], grad_fn=<AddBackward0>)\n",
      "tensor([8.6230], grad_fn=<AddBackward0>)\n",
      "tensor([6.0096], grad_fn=<AddBackward0>)\n",
      "tensor([9.5275], grad_fn=<AddBackward0>)\n",
      "tensor([26.3625], grad_fn=<AddBackward0>)\n",
      "tensor([10.2472], grad_fn=<AddBackward0>)\n",
      "tensor([32.6177], grad_fn=<AddBackward0>)\n",
      "tensor([12.2900], grad_fn=<AddBackward0>)\n",
      "tensor([14.6822], grad_fn=<AddBackward0>)\n",
      "tensor([8.6128], grad_fn=<AddBackward0>)\n",
      "tensor([5.9809], grad_fn=<AddBackward0>)\n",
      "tensor([9.5091], grad_fn=<AddBackward0>)\n",
      "tensor([26.3850], grad_fn=<AddBackward0>)\n",
      "tensor([10.2334], grad_fn=<AddBackward0>)\n",
      "tensor([32.6205], grad_fn=<AddBackward0>)\n",
      "tensor([12.2686], grad_fn=<AddBackward0>)\n",
      "tensor([14.6684], grad_fn=<AddBackward0>)\n",
      "tensor([8.6027], grad_fn=<AddBackward0>)\n",
      "tensor([5.9524], grad_fn=<AddBackward0>)\n",
      "tensor([9.4910], grad_fn=<AddBackward0>)\n",
      "tensor([26.4074], grad_fn=<AddBackward0>)\n",
      "tensor([10.2199], grad_fn=<AddBackward0>)\n",
      "tensor([32.6234], grad_fn=<AddBackward0>)\n",
      "tensor([12.2476], grad_fn=<AddBackward0>)\n",
      "tensor([14.6547], grad_fn=<AddBackward0>)\n",
      "tensor([8.5929], grad_fn=<AddBackward0>)\n",
      "tensor([5.9242], grad_fn=<AddBackward0>)\n",
      "tensor([9.4730], grad_fn=<AddBackward0>)\n",
      "tensor([26.4298], grad_fn=<AddBackward0>)\n",
      "tensor([10.2064], grad_fn=<AddBackward0>)\n",
      "tensor([32.6264], grad_fn=<AddBackward0>)\n",
      "tensor([12.2268], grad_fn=<AddBackward0>)\n",
      "tensor([14.6412], grad_fn=<AddBackward0>)\n",
      "tensor([8.5834], grad_fn=<AddBackward0>)\n",
      "tensor([5.8963], grad_fn=<AddBackward0>)\n",
      "tensor([9.4553], grad_fn=<AddBackward0>)\n",
      "tensor([26.4521], grad_fn=<AddBackward0>)\n",
      "tensor([10.1931], grad_fn=<AddBackward0>)\n",
      "tensor([32.6295], grad_fn=<AddBackward0>)\n",
      "tensor([12.2063], grad_fn=<AddBackward0>)\n",
      "tensor([14.6277], grad_fn=<AddBackward0>)\n",
      "tensor([8.5740], grad_fn=<AddBackward0>)\n",
      "tensor([5.8687], grad_fn=<AddBackward0>)\n",
      "tensor([9.4378], grad_fn=<AddBackward0>)\n",
      "tensor([26.4744], grad_fn=<AddBackward0>)\n",
      "tensor([10.1800], grad_fn=<AddBackward0>)\n",
      "tensor([32.6326], grad_fn=<AddBackward0>)\n",
      "tensor([12.1861], grad_fn=<AddBackward0>)\n",
      "tensor([14.6144], grad_fn=<AddBackward0>)\n",
      "tensor([8.5649], grad_fn=<AddBackward0>)\n",
      "tensor([5.8413], grad_fn=<AddBackward0>)\n",
      "tensor([9.4205], grad_fn=<AddBackward0>)\n",
      "tensor([26.4966], grad_fn=<AddBackward0>)\n",
      "tensor([10.1669], grad_fn=<AddBackward0>)\n",
      "tensor([32.6358], grad_fn=<AddBackward0>)\n",
      "tensor([12.1661], grad_fn=<AddBackward0>)\n",
      "tensor([14.6013], grad_fn=<AddBackward0>)\n",
      "tensor([8.5559], grad_fn=<AddBackward0>)\n",
      "tensor([5.8142], grad_fn=<AddBackward0>)\n",
      "tensor([9.4034], grad_fn=<AddBackward0>)\n",
      "tensor([26.5188], grad_fn=<AddBackward0>)\n",
      "tensor([10.1540], grad_fn=<AddBackward0>)\n",
      "tensor([32.6391], grad_fn=<AddBackward0>)\n",
      "tensor([12.1464], grad_fn=<AddBackward0>)\n",
      "tensor([14.5882], grad_fn=<AddBackward0>)\n",
      "tensor([8.5472], grad_fn=<AddBackward0>)\n",
      "tensor([5.7874], grad_fn=<AddBackward0>)\n",
      "tensor([9.3865], grad_fn=<AddBackward0>)\n",
      "tensor([26.5409], grad_fn=<AddBackward0>)\n",
      "tensor([10.1413], grad_fn=<AddBackward0>)\n",
      "tensor([32.6424], grad_fn=<AddBackward0>)\n",
      "tensor([12.1269], grad_fn=<AddBackward0>)\n",
      "tensor([14.5753], grad_fn=<AddBackward0>)\n",
      "tensor([8.5387], grad_fn=<AddBackward0>)\n",
      "tensor([5.7608], grad_fn=<AddBackward0>)\n",
      "tensor([9.3699], grad_fn=<AddBackward0>)\n",
      "tensor([26.5630], grad_fn=<AddBackward0>)\n",
      "tensor([10.1286], grad_fn=<AddBackward0>)\n",
      "tensor([32.6457], grad_fn=<AddBackward0>)\n",
      "tensor([12.1077], grad_fn=<AddBackward0>)\n",
      "tensor([14.5624], grad_fn=<AddBackward0>)\n",
      "tensor([8.5304], grad_fn=<AddBackward0>)\n",
      "tensor([5.7345], grad_fn=<AddBackward0>)\n",
      "tensor([9.3534], grad_fn=<AddBackward0>)\n",
      "tensor([26.5850], grad_fn=<AddBackward0>)\n",
      "tensor([10.1161], grad_fn=<AddBackward0>)\n",
      "tensor([32.6492], grad_fn=<AddBackward0>)\n",
      "tensor([12.0888], grad_fn=<AddBackward0>)\n",
      "tensor([14.5497], grad_fn=<AddBackward0>)\n",
      "tensor([8.5223], grad_fn=<AddBackward0>)\n",
      "tensor([5.7085], grad_fn=<AddBackward0>)\n",
      "tensor([9.3371], grad_fn=<AddBackward0>)\n",
      "tensor([26.6069], grad_fn=<AddBackward0>)\n",
      "tensor([10.1038], grad_fn=<AddBackward0>)\n",
      "tensor([32.6526], grad_fn=<AddBackward0>)\n",
      "tensor([12.0701], grad_fn=<AddBackward0>)\n",
      "tensor([14.5372], grad_fn=<AddBackward0>)\n",
      "tensor([8.5143], grad_fn=<AddBackward0>)\n",
      "tensor([5.6827], grad_fn=<AddBackward0>)\n",
      "tensor([9.3210], grad_fn=<AddBackward0>)\n",
      "tensor([26.6287], grad_fn=<AddBackward0>)\n",
      "tensor([10.0915], grad_fn=<AddBackward0>)\n",
      "tensor([32.6562], grad_fn=<AddBackward0>)\n",
      "tensor([12.0516], grad_fn=<AddBackward0>)\n",
      "tensor([14.5247], grad_fn=<AddBackward0>)\n",
      "tensor([8.5066], grad_fn=<AddBackward0>)\n",
      "tensor([5.6571], grad_fn=<AddBackward0>)\n",
      "tensor([9.3052], grad_fn=<AddBackward0>)\n",
      "tensor([26.6505], grad_fn=<AddBackward0>)\n",
      "tensor([10.0794], grad_fn=<AddBackward0>)\n",
      "tensor([32.6598], grad_fn=<AddBackward0>)\n",
      "tensor([12.0334], grad_fn=<AddBackward0>)\n",
      "tensor([14.5123], grad_fn=<AddBackward0>)\n",
      "tensor([8.4991], grad_fn=<AddBackward0>)\n",
      "tensor([5.6318], grad_fn=<AddBackward0>)\n",
      "tensor([9.2895], grad_fn=<AddBackward0>)\n",
      "tensor([26.6722], grad_fn=<AddBackward0>)\n",
      "tensor([10.0674], grad_fn=<AddBackward0>)\n",
      "tensor([32.6634], grad_fn=<AddBackward0>)\n",
      "tensor([12.0154], grad_fn=<AddBackward0>)\n",
      "tensor([14.5001], grad_fn=<AddBackward0>)\n",
      "tensor([8.4917], grad_fn=<AddBackward0>)\n",
      "tensor([5.6068], grad_fn=<AddBackward0>)\n",
      "tensor([9.2740], grad_fn=<AddBackward0>)\n",
      "tensor([26.6939], grad_fn=<AddBackward0>)\n",
      "tensor([10.0555], grad_fn=<AddBackward0>)\n",
      "tensor([32.6671], grad_fn=<AddBackward0>)\n",
      "tensor([11.9976], grad_fn=<AddBackward0>)\n",
      "tensor([14.4880], grad_fn=<AddBackward0>)\n",
      "tensor([8.4846], grad_fn=<AddBackward0>)\n",
      "tensor([5.5820], grad_fn=<AddBackward0>)\n",
      "tensor([9.2587], grad_fn=<AddBackward0>)\n",
      "tensor([26.7154], grad_fn=<AddBackward0>)\n",
      "tensor([10.0438], grad_fn=<AddBackward0>)\n",
      "tensor([32.6708], grad_fn=<AddBackward0>)\n",
      "tensor([11.9801], grad_fn=<AddBackward0>)\n",
      "tensor([14.4760], grad_fn=<AddBackward0>)\n",
      "tensor([8.4776], grad_fn=<AddBackward0>)\n",
      "tensor([5.5575], grad_fn=<AddBackward0>)\n",
      "tensor([9.2436], grad_fn=<AddBackward0>)\n",
      "tensor([26.7369], grad_fn=<AddBackward0>)\n",
      "tensor([10.0321], grad_fn=<AddBackward0>)\n",
      "tensor([32.6746], grad_fn=<AddBackward0>)\n",
      "tensor([11.9628], grad_fn=<AddBackward0>)\n",
      "tensor([14.4641], grad_fn=<AddBackward0>)\n",
      "tensor([8.4708], grad_fn=<AddBackward0>)\n",
      "tensor([5.5332], grad_fn=<AddBackward0>)\n",
      "tensor([9.2287], grad_fn=<AddBackward0>)\n",
      "tensor([26.7584], grad_fn=<AddBackward0>)\n",
      "tensor([10.0206], grad_fn=<AddBackward0>)\n",
      "tensor([32.6784], grad_fn=<AddBackward0>)\n",
      "tensor([11.9458], grad_fn=<AddBackward0>)\n",
      "tensor([14.4523], grad_fn=<AddBackward0>)\n",
      "tensor([8.4641], grad_fn=<AddBackward0>)\n",
      "tensor([5.5091], grad_fn=<AddBackward0>)\n",
      "tensor([9.2139], grad_fn=<AddBackward0>)\n",
      "tensor([26.7797], grad_fn=<AddBackward0>)\n",
      "tensor([10.0092], grad_fn=<AddBackward0>)\n",
      "tensor([32.6823], grad_fn=<AddBackward0>)\n",
      "tensor([11.9289], grad_fn=<AddBackward0>)\n",
      "tensor([14.4406], grad_fn=<AddBackward0>)\n",
      "tensor([8.4577], grad_fn=<AddBackward0>)\n",
      "tensor([5.4853], grad_fn=<AddBackward0>)\n",
      "tensor([9.1994], grad_fn=<AddBackward0>)\n",
      "tensor([26.8010], grad_fn=<AddBackward0>)\n",
      "tensor([9.9980], grad_fn=<AddBackward0>)\n",
      "tensor([32.6862], grad_fn=<AddBackward0>)\n",
      "tensor([11.9123], grad_fn=<AddBackward0>)\n",
      "tensor([14.4290], grad_fn=<AddBackward0>)\n",
      "tensor([8.4514], grad_fn=<AddBackward0>)\n",
      "tensor([5.4617], grad_fn=<AddBackward0>)\n",
      "tensor([9.1850], grad_fn=<AddBackward0>)\n",
      "tensor([26.8222], grad_fn=<AddBackward0>)\n",
      "tensor([9.9868], grad_fn=<AddBackward0>)\n",
      "tensor([32.6901], grad_fn=<AddBackward0>)\n",
      "tensor([11.8959], grad_fn=<AddBackward0>)\n",
      "tensor([14.4175], grad_fn=<AddBackward0>)\n",
      "tensor([8.4452], grad_fn=<AddBackward0>)\n",
      "tensor([5.4383], grad_fn=<AddBackward0>)\n",
      "tensor([9.1708], grad_fn=<AddBackward0>)\n",
      "tensor([26.8433], grad_fn=<AddBackward0>)\n",
      "tensor([9.9757], grad_fn=<AddBackward0>)\n",
      "tensor([32.6941], grad_fn=<AddBackward0>)\n",
      "tensor([11.8798], grad_fn=<AddBackward0>)\n",
      "tensor([14.4062], grad_fn=<AddBackward0>)\n",
      "tensor([8.4393], grad_fn=<AddBackward0>)\n",
      "tensor([5.4152], grad_fn=<AddBackward0>)\n",
      "tensor([9.1567], grad_fn=<AddBackward0>)\n",
      "tensor([26.8643], grad_fn=<AddBackward0>)\n",
      "tensor([9.9648], grad_fn=<AddBackward0>)\n",
      "tensor([32.6981], grad_fn=<AddBackward0>)\n",
      "tensor([11.8638], grad_fn=<AddBackward0>)\n",
      "tensor([14.3949], grad_fn=<AddBackward0>)\n",
      "tensor([8.4335], grad_fn=<AddBackward0>)\n",
      "tensor([5.3923], grad_fn=<AddBackward0>)\n",
      "tensor([9.1429], grad_fn=<AddBackward0>)\n",
      "tensor([26.8852], grad_fn=<AddBackward0>)\n",
      "tensor([9.9540], grad_fn=<AddBackward0>)\n",
      "tensor([32.7022], grad_fn=<AddBackward0>)\n",
      "tensor([11.8481], grad_fn=<AddBackward0>)\n",
      "tensor([14.3837], grad_fn=<AddBackward0>)\n",
      "tensor([8.4278], grad_fn=<AddBackward0>)\n",
      "tensor([5.3696], grad_fn=<AddBackward0>)\n",
      "tensor([9.1292], grad_fn=<AddBackward0>)\n",
      "tensor([26.9061], grad_fn=<AddBackward0>)\n",
      "tensor([9.9432], grad_fn=<AddBackward0>)\n",
      "tensor([32.7063], grad_fn=<AddBackward0>)\n",
      "tensor([11.8325], grad_fn=<AddBackward0>)\n",
      "tensor([14.3727], grad_fn=<AddBackward0>)\n",
      "tensor([8.4224], grad_fn=<AddBackward0>)\n",
      "tensor([5.3472], grad_fn=<AddBackward0>)\n",
      "tensor([9.1157], grad_fn=<AddBackward0>)\n",
      "tensor([26.9269], grad_fn=<AddBackward0>)\n",
      "tensor([9.9326], grad_fn=<AddBackward0>)\n",
      "tensor([32.7104], grad_fn=<AddBackward0>)\n",
      "tensor([11.8172], grad_fn=<AddBackward0>)\n",
      "tensor([14.3617], grad_fn=<AddBackward0>)\n",
      "tensor([8.4170], grad_fn=<AddBackward0>)\n",
      "tensor([5.3250], grad_fn=<AddBackward0>)\n",
      "tensor([9.1023], grad_fn=<AddBackward0>)\n",
      "tensor([26.9476], grad_fn=<AddBackward0>)\n",
      "tensor([9.9221], grad_fn=<AddBackward0>)\n",
      "tensor([32.7146], grad_fn=<AddBackward0>)\n",
      "tensor([11.8021], grad_fn=<AddBackward0>)\n",
      "tensor([14.3509], grad_fn=<AddBackward0>)\n",
      "tensor([8.4118], grad_fn=<AddBackward0>)\n",
      "tensor([5.3030], grad_fn=<AddBackward0>)\n",
      "tensor([9.0892], grad_fn=<AddBackward0>)\n",
      "tensor([26.9682], grad_fn=<AddBackward0>)\n",
      "tensor([9.9117], grad_fn=<AddBackward0>)\n",
      "tensor([32.7188], grad_fn=<AddBackward0>)\n",
      "tensor([11.7872], grad_fn=<AddBackward0>)\n",
      "tensor([14.3401], grad_fn=<AddBackward0>)\n",
      "tensor([8.4068], grad_fn=<AddBackward0>)\n",
      "tensor([5.2812], grad_fn=<AddBackward0>)\n",
      "tensor([9.0761], grad_fn=<AddBackward0>)\n",
      "tensor([26.9887], grad_fn=<AddBackward0>)\n",
      "tensor([9.9014], grad_fn=<AddBackward0>)\n",
      "tensor([32.7230], grad_fn=<AddBackward0>)\n",
      "tensor([11.7725], grad_fn=<AddBackward0>)\n",
      "tensor([14.3295], grad_fn=<AddBackward0>)\n",
      "tensor([8.4019], grad_fn=<AddBackward0>)\n",
      "tensor([5.2597], grad_fn=<AddBackward0>)\n",
      "tensor([9.0633], grad_fn=<AddBackward0>)\n",
      "tensor([27.0091], grad_fn=<AddBackward0>)\n",
      "tensor([9.8912], grad_fn=<AddBackward0>)\n",
      "tensor([32.7272], grad_fn=<AddBackward0>)\n",
      "tensor([11.7579], grad_fn=<AddBackward0>)\n",
      "tensor([14.3189], grad_fn=<AddBackward0>)\n",
      "tensor([8.3972], grad_fn=<AddBackward0>)\n",
      "tensor([5.2383], grad_fn=<AddBackward0>)\n",
      "tensor([9.0506], grad_fn=<AddBackward0>)\n",
      "tensor([27.0295], grad_fn=<AddBackward0>)\n",
      "tensor([9.8812], grad_fn=<AddBackward0>)\n",
      "tensor([32.7315], grad_fn=<AddBackward0>)\n",
      "tensor([11.7436], grad_fn=<AddBackward0>)\n",
      "tensor([14.3085], grad_fn=<AddBackward0>)\n",
      "tensor([8.3926], grad_fn=<AddBackward0>)\n",
      "tensor([5.2172], grad_fn=<AddBackward0>)\n",
      "tensor([9.0381], grad_fn=<AddBackward0>)\n",
      "tensor([27.0497], grad_fn=<AddBackward0>)\n",
      "tensor([9.8712], grad_fn=<AddBackward0>)\n",
      "tensor([32.7358], grad_fn=<AddBackward0>)\n",
      "tensor([11.7295], grad_fn=<AddBackward0>)\n",
      "tensor([14.2981], grad_fn=<AddBackward0>)\n",
      "tensor([8.3881], grad_fn=<AddBackward0>)\n",
      "tensor([5.1963], grad_fn=<AddBackward0>)\n",
      "tensor([9.0257], grad_fn=<AddBackward0>)\n",
      "tensor([27.0699], grad_fn=<AddBackward0>)\n",
      "tensor([9.8613], grad_fn=<AddBackward0>)\n",
      "tensor([32.7401], grad_fn=<AddBackward0>)\n",
      "tensor([11.7156], grad_fn=<AddBackward0>)\n",
      "tensor([14.2878], grad_fn=<AddBackward0>)\n",
      "tensor([8.3838], grad_fn=<AddBackward0>)\n",
      "tensor([5.1756], grad_fn=<AddBackward0>)\n",
      "tensor([9.0135], grad_fn=<AddBackward0>)\n",
      "tensor([27.0900], grad_fn=<AddBackward0>)\n",
      "tensor([9.8515], grad_fn=<AddBackward0>)\n",
      "tensor([32.7444], grad_fn=<AddBackward0>)\n",
      "tensor([11.7018], grad_fn=<AddBackward0>)\n",
      "tensor([14.2777], grad_fn=<AddBackward0>)\n",
      "tensor([8.3796], grad_fn=<AddBackward0>)\n",
      "tensor([5.1551], grad_fn=<AddBackward0>)\n",
      "tensor([9.0014], grad_fn=<AddBackward0>)\n",
      "tensor([27.1099], grad_fn=<AddBackward0>)\n",
      "tensor([9.8418], grad_fn=<AddBackward0>)\n",
      "tensor([32.7488], grad_fn=<AddBackward0>)\n",
      "tensor([11.6882], grad_fn=<AddBackward0>)\n",
      "tensor([14.2676], grad_fn=<AddBackward0>)\n",
      "tensor([8.3755], grad_fn=<AddBackward0>)\n",
      "tensor([5.1349], grad_fn=<AddBackward0>)\n",
      "tensor([8.9895], grad_fn=<AddBackward0>)\n",
      "tensor([27.1298], grad_fn=<AddBackward0>)\n",
      "tensor([9.8322], grad_fn=<AddBackward0>)\n",
      "tensor([32.7532], grad_fn=<AddBackward0>)\n",
      "tensor([11.6749], grad_fn=<AddBackward0>)\n",
      "tensor([14.2576], grad_fn=<AddBackward0>)\n",
      "tensor([8.3716], grad_fn=<AddBackward0>)\n",
      "tensor([5.1148], grad_fn=<AddBackward0>)\n",
      "tensor([8.9777], grad_fn=<AddBackward0>)\n",
      "tensor([27.1496], grad_fn=<AddBackward0>)\n",
      "tensor([9.8227], grad_fn=<AddBackward0>)\n",
      "tensor([32.7576], grad_fn=<AddBackward0>)\n",
      "tensor([11.6617], grad_fn=<AddBackward0>)\n",
      "tensor([14.2477], grad_fn=<AddBackward0>)\n",
      "tensor([8.3678], grad_fn=<AddBackward0>)\n",
      "tensor([5.0949], grad_fn=<AddBackward0>)\n",
      "tensor([8.9661], grad_fn=<AddBackward0>)\n",
      "tensor([27.1693], grad_fn=<AddBackward0>)\n",
      "tensor([9.8133], grad_fn=<AddBackward0>)\n",
      "tensor([32.7620], grad_fn=<AddBackward0>)\n",
      "tensor([11.6487], grad_fn=<AddBackward0>)\n",
      "tensor([14.2379], grad_fn=<AddBackward0>)\n",
      "tensor([8.3641], grad_fn=<AddBackward0>)\n",
      "tensor([5.0752], grad_fn=<AddBackward0>)\n",
      "tensor([8.9546], grad_fn=<AddBackward0>)\n",
      "tensor([27.1889], grad_fn=<AddBackward0>)\n",
      "tensor([9.8040], grad_fn=<AddBackward0>)\n",
      "tensor([32.7665], grad_fn=<AddBackward0>)\n",
      "tensor([11.6358], grad_fn=<AddBackward0>)\n",
      "tensor([14.2282], grad_fn=<AddBackward0>)\n",
      "tensor([8.3605], grad_fn=<AddBackward0>)\n",
      "tensor([5.0558], grad_fn=<AddBackward0>)\n",
      "tensor([8.9433], grad_fn=<AddBackward0>)\n",
      "tensor([27.2084], grad_fn=<AddBackward0>)\n",
      "tensor([9.7948], grad_fn=<AddBackward0>)\n",
      "tensor([32.7709], grad_fn=<AddBackward0>)\n",
      "tensor([11.6232], grad_fn=<AddBackward0>)\n",
      "tensor([14.2186], grad_fn=<AddBackward0>)\n",
      "tensor([8.3571], grad_fn=<AddBackward0>)\n",
      "tensor([5.0365], grad_fn=<AddBackward0>)\n",
      "tensor([8.9321], grad_fn=<AddBackward0>)\n",
      "tensor([27.2278], grad_fn=<AddBackward0>)\n",
      "tensor([9.7857], grad_fn=<AddBackward0>)\n",
      "tensor([32.7754], grad_fn=<AddBackward0>)\n",
      "tensor([11.6107], grad_fn=<AddBackward0>)\n",
      "tensor([14.2091], grad_fn=<AddBackward0>)\n",
      "tensor([8.3538], grad_fn=<AddBackward0>)\n",
      "tensor([5.0174], grad_fn=<AddBackward0>)\n",
      "tensor([8.9210], grad_fn=<AddBackward0>)\n",
      "tensor([27.2471], grad_fn=<AddBackward0>)\n",
      "tensor([9.7767], grad_fn=<AddBackward0>)\n",
      "tensor([32.7799], grad_fn=<AddBackward0>)\n",
      "tensor([11.5984], grad_fn=<AddBackward0>)\n",
      "tensor([14.1996], grad_fn=<AddBackward0>)\n",
      "tensor([8.3505], grad_fn=<AddBackward0>)\n",
      "tensor([4.9986], grad_fn=<AddBackward0>)\n",
      "tensor([8.9101], grad_fn=<AddBackward0>)\n",
      "tensor([27.2663], grad_fn=<AddBackward0>)\n",
      "tensor([9.7677], grad_fn=<AddBackward0>)\n",
      "tensor([32.7844], grad_fn=<AddBackward0>)\n",
      "tensor([11.5862], grad_fn=<AddBackward0>)\n",
      "tensor([14.1903], grad_fn=<AddBackward0>)\n",
      "tensor([8.3475], grad_fn=<AddBackward0>)\n",
      "tensor([4.9799], grad_fn=<AddBackward0>)\n",
      "tensor([8.8994], grad_fn=<AddBackward0>)\n",
      "tensor([27.2853], grad_fn=<AddBackward0>)\n",
      "tensor([9.7589], grad_fn=<AddBackward0>)\n",
      "tensor([32.7889], grad_fn=<AddBackward0>)\n",
      "tensor([11.5742], grad_fn=<AddBackward0>)\n",
      "tensor([14.1810], grad_fn=<AddBackward0>)\n",
      "tensor([8.3445], grad_fn=<AddBackward0>)\n",
      "tensor([4.9614], grad_fn=<AddBackward0>)\n",
      "tensor([8.8887], grad_fn=<AddBackward0>)\n",
      "tensor([27.3043], grad_fn=<AddBackward0>)\n",
      "tensor([9.7501], grad_fn=<AddBackward0>)\n",
      "tensor([32.7934], grad_fn=<AddBackward0>)\n",
      "tensor([11.5624], grad_fn=<AddBackward0>)\n",
      "tensor([14.1718], grad_fn=<AddBackward0>)\n",
      "tensor([8.3416], grad_fn=<AddBackward0>)\n",
      "tensor([4.9431], grad_fn=<AddBackward0>)\n",
      "tensor([8.8782], grad_fn=<AddBackward0>)\n",
      "tensor([27.3232], grad_fn=<AddBackward0>)\n",
      "tensor([9.7415], grad_fn=<AddBackward0>)\n",
      "tensor([32.7980], grad_fn=<AddBackward0>)\n",
      "tensor([11.5508], grad_fn=<AddBackward0>)\n",
      "tensor([14.1628], grad_fn=<AddBackward0>)\n",
      "tensor([8.3388], grad_fn=<AddBackward0>)\n",
      "tensor([4.9250], grad_fn=<AddBackward0>)\n",
      "tensor([8.8679], grad_fn=<AddBackward0>)\n",
      "tensor([27.3420], grad_fn=<AddBackward0>)\n",
      "tensor([9.7329], grad_fn=<AddBackward0>)\n",
      "tensor([32.8025], grad_fn=<AddBackward0>)\n",
      "tensor([11.5393], grad_fn=<AddBackward0>)\n",
      "tensor([14.1537], grad_fn=<AddBackward0>)\n",
      "tensor([8.3362], grad_fn=<AddBackward0>)\n",
      "tensor([4.9070], grad_fn=<AddBackward0>)\n",
      "tensor([8.8576], grad_fn=<AddBackward0>)\n",
      "tensor([27.3607], grad_fn=<AddBackward0>)\n",
      "tensor([9.7244], grad_fn=<AddBackward0>)\n",
      "tensor([32.8071], grad_fn=<AddBackward0>)\n",
      "tensor([11.5280], grad_fn=<AddBackward0>)\n",
      "tensor([14.1448], grad_fn=<AddBackward0>)\n",
      "tensor([8.3336], grad_fn=<AddBackward0>)\n",
      "tensor([4.8893], grad_fn=<AddBackward0>)\n",
      "tensor([8.8475], grad_fn=<AddBackward0>)\n",
      "tensor([27.3793], grad_fn=<AddBackward0>)\n",
      "tensor([9.7160], grad_fn=<AddBackward0>)\n",
      "tensor([32.8116], grad_fn=<AddBackward0>)\n",
      "tensor([11.5168], grad_fn=<AddBackward0>)\n",
      "tensor([14.1360], grad_fn=<AddBackward0>)\n",
      "tensor([8.3311], grad_fn=<AddBackward0>)\n",
      "tensor([4.8717], grad_fn=<AddBackward0>)\n",
      "tensor([8.8375], grad_fn=<AddBackward0>)\n",
      "tensor([27.3978], grad_fn=<AddBackward0>)\n",
      "tensor([9.7077], grad_fn=<AddBackward0>)\n",
      "tensor([32.8162], grad_fn=<AddBackward0>)\n",
      "tensor([11.5058], grad_fn=<AddBackward0>)\n",
      "tensor([14.1272], grad_fn=<AddBackward0>)\n",
      "tensor([8.3288], grad_fn=<AddBackward0>)\n",
      "tensor([4.8544], grad_fn=<AddBackward0>)\n",
      "tensor([8.8277], grad_fn=<AddBackward0>)\n",
      "tensor([27.4162], grad_fn=<AddBackward0>)\n",
      "tensor([9.6994], grad_fn=<AddBackward0>)\n",
      "tensor([32.8208], grad_fn=<AddBackward0>)\n",
      "tensor([11.4949], grad_fn=<AddBackward0>)\n",
      "tensor([14.1186], grad_fn=<AddBackward0>)\n",
      "tensor([8.3265], grad_fn=<AddBackward0>)\n",
      "tensor([4.8372], grad_fn=<AddBackward0>)\n",
      "tensor([8.8180], grad_fn=<AddBackward0>)\n",
      "tensor([27.4345], grad_fn=<AddBackward0>)\n",
      "tensor([9.6913], grad_fn=<AddBackward0>)\n",
      "tensor([32.8253], grad_fn=<AddBackward0>)\n",
      "tensor([11.4842], grad_fn=<AddBackward0>)\n",
      "tensor([14.1100], grad_fn=<AddBackward0>)\n",
      "tensor([8.3243], grad_fn=<AddBackward0>)\n",
      "tensor([4.8202], grad_fn=<AddBackward0>)\n",
      "tensor([8.8084], grad_fn=<AddBackward0>)\n",
      "tensor([27.4526], grad_fn=<AddBackward0>)\n",
      "tensor([9.6832], grad_fn=<AddBackward0>)\n",
      "tensor([32.8299], grad_fn=<AddBackward0>)\n",
      "tensor([11.4736], grad_fn=<AddBackward0>)\n",
      "tensor([14.1015], grad_fn=<AddBackward0>)\n",
      "tensor([8.3223], grad_fn=<AddBackward0>)\n",
      "tensor([4.8033], grad_fn=<AddBackward0>)\n",
      "tensor([8.7989], grad_fn=<AddBackward0>)\n",
      "tensor([27.4707], grad_fn=<AddBackward0>)\n",
      "tensor([9.6752], grad_fn=<AddBackward0>)\n",
      "tensor([32.8345], grad_fn=<AddBackward0>)\n",
      "tensor([11.4632], grad_fn=<AddBackward0>)\n",
      "tensor([14.0930], grad_fn=<AddBackward0>)\n",
      "tensor([8.3203], grad_fn=<AddBackward0>)\n",
      "tensor([4.7866], grad_fn=<AddBackward0>)\n",
      "tensor([8.7895], grad_fn=<AddBackward0>)\n",
      "tensor([27.4887], grad_fn=<AddBackward0>)\n",
      "tensor([9.6673], grad_fn=<AddBackward0>)\n",
      "tensor([32.8391], grad_fn=<AddBackward0>)\n",
      "tensor([11.4530], grad_fn=<AddBackward0>)\n",
      "tensor([14.0847], grad_fn=<AddBackward0>)\n",
      "tensor([8.3184], grad_fn=<AddBackward0>)\n",
      "tensor([4.7701], grad_fn=<AddBackward0>)\n",
      "tensor([8.7803], grad_fn=<AddBackward0>)\n",
      "tensor([27.5065], grad_fn=<AddBackward0>)\n",
      "tensor([9.6595], grad_fn=<AddBackward0>)\n",
      "tensor([32.8437], grad_fn=<AddBackward0>)\n",
      "tensor([11.4428], grad_fn=<AddBackward0>)\n",
      "tensor([14.0764], grad_fn=<AddBackward0>)\n",
      "tensor([8.3166], grad_fn=<AddBackward0>)\n",
      "tensor([4.7538], grad_fn=<AddBackward0>)\n",
      "tensor([8.7712], grad_fn=<AddBackward0>)\n",
      "tensor([27.5243], grad_fn=<AddBackward0>)\n",
      "tensor([9.6517], grad_fn=<AddBackward0>)\n",
      "tensor([32.8483], grad_fn=<AddBackward0>)\n",
      "tensor([11.4329], grad_fn=<AddBackward0>)\n",
      "tensor([14.0682], grad_fn=<AddBackward0>)\n",
      "tensor([8.3148], grad_fn=<AddBackward0>)\n",
      "tensor([4.7377], grad_fn=<AddBackward0>)\n",
      "tensor([8.7622], grad_fn=<AddBackward0>)\n",
      "tensor([27.5419], grad_fn=<AddBackward0>)\n",
      "tensor([9.6440], grad_fn=<AddBackward0>)\n",
      "tensor([32.8529], grad_fn=<AddBackward0>)\n",
      "tensor([11.4230], grad_fn=<AddBackward0>)\n",
      "tensor([14.0601], grad_fn=<AddBackward0>)\n",
      "tensor([8.3132], grad_fn=<AddBackward0>)\n",
      "tensor([4.7217], grad_fn=<AddBackward0>)\n",
      "tensor([8.7533], grad_fn=<AddBackward0>)\n",
      "tensor([27.5595], grad_fn=<AddBackward0>)\n",
      "tensor([9.6364], grad_fn=<AddBackward0>)\n",
      "tensor([32.8575], grad_fn=<AddBackward0>)\n",
      "tensor([11.4133], grad_fn=<AddBackward0>)\n",
      "tensor([14.0521], grad_fn=<AddBackward0>)\n",
      "tensor([8.3116], grad_fn=<AddBackward0>)\n",
      "tensor([4.7059], grad_fn=<AddBackward0>)\n",
      "tensor([8.7445], grad_fn=<AddBackward0>)\n",
      "tensor([27.5769], grad_fn=<AddBackward0>)\n",
      "tensor([9.6289], grad_fn=<AddBackward0>)\n",
      "tensor([32.8620], grad_fn=<AddBackward0>)\n",
      "tensor([11.4037], grad_fn=<AddBackward0>)\n",
      "tensor([14.0441], grad_fn=<AddBackward0>)\n",
      "tensor([8.3102], grad_fn=<AddBackward0>)\n",
      "tensor([4.6902], grad_fn=<AddBackward0>)\n",
      "tensor([8.7358], grad_fn=<AddBackward0>)\n",
      "tensor([27.5942], grad_fn=<AddBackward0>)\n",
      "tensor([9.6214], grad_fn=<AddBackward0>)\n",
      "tensor([32.8666], grad_fn=<AddBackward0>)\n",
      "tensor([11.3943], grad_fn=<AddBackward0>)\n",
      "tensor([14.0362], grad_fn=<AddBackward0>)\n",
      "tensor([8.3088], grad_fn=<AddBackward0>)\n",
      "tensor([4.6747], grad_fn=<AddBackward0>)\n",
      "tensor([8.7273], grad_fn=<AddBackward0>)\n",
      "tensor([27.6115], grad_fn=<AddBackward0>)\n",
      "tensor([9.6141], grad_fn=<AddBackward0>)\n",
      "tensor([32.8712], grad_fn=<AddBackward0>)\n",
      "tensor([11.3850], grad_fn=<AddBackward0>)\n",
      "tensor([14.0284], grad_fn=<AddBackward0>)\n",
      "tensor([8.3075], grad_fn=<AddBackward0>)\n",
      "tensor([4.6594], grad_fn=<AddBackward0>)\n",
      "tensor([8.7189], grad_fn=<AddBackward0>)\n",
      "tensor([27.6286], grad_fn=<AddBackward0>)\n",
      "tensor([9.6068], grad_fn=<AddBackward0>)\n",
      "tensor([32.8758], grad_fn=<AddBackward0>)\n",
      "tensor([11.3759], grad_fn=<AddBackward0>)\n",
      "tensor([14.0207], grad_fn=<AddBackward0>)\n",
      "tensor([8.3062], grad_fn=<AddBackward0>)\n",
      "tensor([4.6443], grad_fn=<AddBackward0>)\n",
      "tensor([8.7105], grad_fn=<AddBackward0>)\n",
      "tensor([27.6456], grad_fn=<AddBackward0>)\n",
      "tensor([9.5995], grad_fn=<AddBackward0>)\n",
      "tensor([32.8804], grad_fn=<AddBackward0>)\n",
      "tensor([11.3668], grad_fn=<AddBackward0>)\n",
      "tensor([14.0130], grad_fn=<AddBackward0>)\n",
      "tensor([8.3050], grad_fn=<AddBackward0>)\n",
      "tensor([4.6292], grad_fn=<AddBackward0>)\n",
      "tensor([8.7023], grad_fn=<AddBackward0>)\n",
      "tensor([27.6625], grad_fn=<AddBackward0>)\n",
      "tensor([9.5924], grad_fn=<AddBackward0>)\n",
      "tensor([32.8850], grad_fn=<AddBackward0>)\n",
      "tensor([11.3579], grad_fn=<AddBackward0>)\n",
      "tensor([14.0054], grad_fn=<AddBackward0>)\n",
      "tensor([8.3039], grad_fn=<AddBackward0>)\n",
      "tensor([4.6144], grad_fn=<AddBackward0>)\n",
      "tensor([8.6942], grad_fn=<AddBackward0>)\n",
      "tensor([27.6792], grad_fn=<AddBackward0>)\n",
      "tensor([9.5853], grad_fn=<AddBackward0>)\n",
      "tensor([32.8895], grad_fn=<AddBackward0>)\n",
      "tensor([11.3491], grad_fn=<AddBackward0>)\n",
      "tensor([13.9979], grad_fn=<AddBackward0>)\n",
      "tensor([8.3029], grad_fn=<AddBackward0>)\n",
      "tensor([4.5997], grad_fn=<AddBackward0>)\n",
      "tensor([8.6862], grad_fn=<AddBackward0>)\n",
      "tensor([27.6959], grad_fn=<AddBackward0>)\n",
      "tensor([9.5783], grad_fn=<AddBackward0>)\n",
      "tensor([32.8941], grad_fn=<AddBackward0>)\n",
      "tensor([11.3405], grad_fn=<AddBackward0>)\n",
      "tensor([13.9904], grad_fn=<AddBackward0>)\n",
      "tensor([8.3020], grad_fn=<AddBackward0>)\n",
      "tensor([4.5852], grad_fn=<AddBackward0>)\n",
      "tensor([8.6783], grad_fn=<AddBackward0>)\n",
      "tensor([27.7125], grad_fn=<AddBackward0>)\n",
      "tensor([9.5714], grad_fn=<AddBackward0>)\n",
      "tensor([32.8986], grad_fn=<AddBackward0>)\n",
      "tensor([11.3319], grad_fn=<AddBackward0>)\n",
      "tensor([13.9831], grad_fn=<AddBackward0>)\n",
      "tensor([8.3011], grad_fn=<AddBackward0>)\n",
      "tensor([4.5708], grad_fn=<AddBackward0>)\n",
      "tensor([8.6705], grad_fn=<AddBackward0>)\n",
      "tensor([27.7289], grad_fn=<AddBackward0>)\n",
      "tensor([9.5645], grad_fn=<AddBackward0>)\n",
      "tensor([32.9032], grad_fn=<AddBackward0>)\n",
      "tensor([11.3235], grad_fn=<AddBackward0>)\n",
      "tensor([13.9757], grad_fn=<AddBackward0>)\n",
      "tensor([8.3002], grad_fn=<AddBackward0>)\n",
      "tensor([4.5566], grad_fn=<AddBackward0>)\n",
      "tensor([8.6628], grad_fn=<AddBackward0>)\n",
      "tensor([27.7453], grad_fn=<AddBackward0>)\n",
      "tensor([9.5577], grad_fn=<AddBackward0>)\n",
      "tensor([32.9077], grad_fn=<AddBackward0>)\n",
      "tensor([11.3152], grad_fn=<AddBackward0>)\n",
      "tensor([13.9685], grad_fn=<AddBackward0>)\n",
      "tensor([8.2995], grad_fn=<AddBackward0>)\n",
      "tensor([4.5425], grad_fn=<AddBackward0>)\n",
      "tensor([8.6551], grad_fn=<AddBackward0>)\n",
      "tensor([27.7615], grad_fn=<AddBackward0>)\n",
      "tensor([9.5510], grad_fn=<AddBackward0>)\n",
      "tensor([32.9123], grad_fn=<AddBackward0>)\n",
      "tensor([11.3071], grad_fn=<AddBackward0>)\n",
      "tensor([13.9613], grad_fn=<AddBackward0>)\n",
      "tensor([8.2988], grad_fn=<AddBackward0>)\n",
      "tensor([4.5286], grad_fn=<AddBackward0>)\n",
      "tensor([8.6476], grad_fn=<AddBackward0>)\n",
      "tensor([27.7777], grad_fn=<AddBackward0>)\n",
      "tensor([9.5443], grad_fn=<AddBackward0>)\n",
      "tensor([32.9168], grad_fn=<AddBackward0>)\n",
      "tensor([11.2990], grad_fn=<AddBackward0>)\n",
      "tensor([13.9543], grad_fn=<AddBackward0>)\n",
      "tensor([8.2982], grad_fn=<AddBackward0>)\n",
      "tensor([4.5148], grad_fn=<AddBackward0>)\n",
      "tensor([8.6402], grad_fn=<AddBackward0>)\n",
      "tensor([27.7937], grad_fn=<AddBackward0>)\n",
      "tensor([9.5377], grad_fn=<AddBackward0>)\n",
      "tensor([32.9213], grad_fn=<AddBackward0>)\n",
      "tensor([11.2911], grad_fn=<AddBackward0>)\n",
      "tensor([13.9472], grad_fn=<AddBackward0>)\n",
      "tensor([8.2976], grad_fn=<AddBackward0>)\n",
      "tensor([4.5012], grad_fn=<AddBackward0>)\n",
      "tensor([8.6329], grad_fn=<AddBackward0>)\n",
      "tensor([27.8096], grad_fn=<AddBackward0>)\n",
      "tensor([9.5312], grad_fn=<AddBackward0>)\n",
      "tensor([32.9258], grad_fn=<AddBackward0>)\n",
      "tensor([11.2833], grad_fn=<AddBackward0>)\n",
      "tensor([13.9403], grad_fn=<AddBackward0>)\n",
      "tensor([8.2971], grad_fn=<AddBackward0>)\n",
      "tensor([4.4877], grad_fn=<AddBackward0>)\n",
      "tensor([8.6257], grad_fn=<AddBackward0>)\n",
      "tensor([27.8254], grad_fn=<AddBackward0>)\n",
      "tensor([9.5247], grad_fn=<AddBackward0>)\n",
      "tensor([32.9303], grad_fn=<AddBackward0>)\n",
      "tensor([11.2756], grad_fn=<AddBackward0>)\n",
      "tensor([13.9334], grad_fn=<AddBackward0>)\n",
      "tensor([8.2966], grad_fn=<AddBackward0>)\n",
      "tensor([4.4744], grad_fn=<AddBackward0>)\n",
      "tensor([8.6186], grad_fn=<AddBackward0>)\n",
      "tensor([27.8411], grad_fn=<AddBackward0>)\n",
      "tensor([9.5183], grad_fn=<AddBackward0>)\n",
      "tensor([32.9348], grad_fn=<AddBackward0>)\n",
      "tensor([11.2680], grad_fn=<AddBackward0>)\n",
      "tensor([13.9265], grad_fn=<AddBackward0>)\n",
      "tensor([8.2962], grad_fn=<AddBackward0>)\n",
      "tensor([4.4612], grad_fn=<AddBackward0>)\n",
      "tensor([8.6115], grad_fn=<AddBackward0>)\n",
      "tensor([27.8567], grad_fn=<AddBackward0>)\n",
      "tensor([9.5120], grad_fn=<AddBackward0>)\n",
      "tensor([32.9392], grad_fn=<AddBackward0>)\n",
      "tensor([11.2605], grad_fn=<AddBackward0>)\n",
      "tensor([13.9198], grad_fn=<AddBackward0>)\n",
      "tensor([8.2959], grad_fn=<AddBackward0>)\n",
      "tensor([4.4481], grad_fn=<AddBackward0>)\n",
      "tensor([8.6046], grad_fn=<AddBackward0>)\n",
      "tensor([27.8722], grad_fn=<AddBackward0>)\n",
      "tensor([9.5057], grad_fn=<AddBackward0>)\n",
      "tensor([32.9437], grad_fn=<AddBackward0>)\n",
      "tensor([11.2531], grad_fn=<AddBackward0>)\n",
      "tensor([13.9131], grad_fn=<AddBackward0>)\n",
      "tensor([8.2956], grad_fn=<AddBackward0>)\n",
      "tensor([4.4352], grad_fn=<AddBackward0>)\n",
      "tensor([8.5977], grad_fn=<AddBackward0>)\n",
      "tensor([27.8875], grad_fn=<AddBackward0>)\n",
      "tensor([9.4995], grad_fn=<AddBackward0>)\n",
      "tensor([32.9481], grad_fn=<AddBackward0>)\n",
      "tensor([11.2458], grad_fn=<AddBackward0>)\n",
      "tensor([13.9065], grad_fn=<AddBackward0>)\n",
      "tensor([8.2953], grad_fn=<AddBackward0>)\n",
      "tensor([4.4224], grad_fn=<AddBackward0>)\n",
      "tensor([8.5910], grad_fn=<AddBackward0>)\n",
      "tensor([27.9028], grad_fn=<AddBackward0>)\n",
      "tensor([9.4933], grad_fn=<AddBackward0>)\n",
      "tensor([32.9526], grad_fn=<AddBackward0>)\n",
      "tensor([11.2386], grad_fn=<AddBackward0>)\n",
      "tensor([13.8999], grad_fn=<AddBackward0>)\n",
      "tensor([8.2951], grad_fn=<AddBackward0>)\n",
      "tensor([4.4098], grad_fn=<AddBackward0>)\n",
      "tensor([8.5843], grad_fn=<AddBackward0>)\n",
      "tensor([27.9179], grad_fn=<AddBackward0>)\n",
      "tensor([9.4873], grad_fn=<AddBackward0>)\n",
      "tensor([32.9570], grad_fn=<AddBackward0>)\n",
      "tensor([11.2316], grad_fn=<AddBackward0>)\n",
      "tensor([13.8934], grad_fn=<AddBackward0>)\n",
      "tensor([8.2950], grad_fn=<AddBackward0>)\n",
      "tensor([4.3973], grad_fn=<AddBackward0>)\n",
      "tensor([8.5777], grad_fn=<AddBackward0>)\n",
      "tensor([27.9330], grad_fn=<AddBackward0>)\n",
      "tensor([9.4812], grad_fn=<AddBackward0>)\n",
      "tensor([32.9614], grad_fn=<AddBackward0>)\n",
      "tensor([11.2246], grad_fn=<AddBackward0>)\n",
      "tensor([13.8869], grad_fn=<AddBackward0>)\n",
      "tensor([8.2949], grad_fn=<AddBackward0>)\n",
      "tensor([4.3849], grad_fn=<AddBackward0>)\n",
      "tensor([8.5712], grad_fn=<AddBackward0>)\n",
      "tensor([27.9479], grad_fn=<AddBackward0>)\n",
      "tensor([9.4753], grad_fn=<AddBackward0>)\n",
      "tensor([32.9658], grad_fn=<AddBackward0>)\n",
      "tensor([11.2177], grad_fn=<AddBackward0>)\n",
      "tensor([13.8806], grad_fn=<AddBackward0>)\n",
      "tensor([8.2949], grad_fn=<AddBackward0>)\n",
      "tensor([4.3727], grad_fn=<AddBackward0>)\n",
      "tensor([8.5648], grad_fn=<AddBackward0>)\n",
      "tensor([27.9627], grad_fn=<AddBackward0>)\n",
      "tensor([9.4694], grad_fn=<AddBackward0>)\n",
      "tensor([32.9701], grad_fn=<AddBackward0>)\n",
      "tensor([11.2110], grad_fn=<AddBackward0>)\n",
      "tensor([13.8743], grad_fn=<AddBackward0>)\n",
      "tensor([8.2949], grad_fn=<AddBackward0>)\n",
      "tensor([4.3606], grad_fn=<AddBackward0>)\n",
      "tensor([8.5585], grad_fn=<AddBackward0>)\n",
      "tensor([27.9774], grad_fn=<AddBackward0>)\n",
      "tensor([9.4636], grad_fn=<AddBackward0>)\n",
      "tensor([32.9745], grad_fn=<AddBackward0>)\n",
      "tensor([11.2043], grad_fn=<AddBackward0>)\n",
      "tensor([13.8680], grad_fn=<AddBackward0>)\n",
      "tensor([8.2949], grad_fn=<AddBackward0>)\n",
      "tensor([4.3486], grad_fn=<AddBackward0>)\n",
      "tensor([8.5522], grad_fn=<AddBackward0>)\n",
      "tensor([27.9920], grad_fn=<AddBackward0>)\n",
      "tensor([9.4578], grad_fn=<AddBackward0>)\n",
      "tensor([32.9789], grad_fn=<AddBackward0>)\n",
      "tensor([11.1978], grad_fn=<AddBackward0>)\n",
      "tensor([13.8618], grad_fn=<AddBackward0>)\n",
      "tensor([8.2950], grad_fn=<AddBackward0>)\n",
      "tensor([4.3368], grad_fn=<AddBackward0>)\n",
      "tensor([8.5461], grad_fn=<AddBackward0>)\n",
      "tensor([28.0065], grad_fn=<AddBackward0>)\n",
      "tensor([9.4521], grad_fn=<AddBackward0>)\n",
      "tensor([32.9832], grad_fn=<AddBackward0>)\n",
      "tensor([11.1913], grad_fn=<AddBackward0>)\n",
      "tensor([13.8557], grad_fn=<AddBackward0>)\n",
      "tensor([8.2952], grad_fn=<AddBackward0>)\n",
      "tensor([4.3251], grad_fn=<AddBackward0>)\n",
      "tensor([8.5400], grad_fn=<AddBackward0>)\n",
      "tensor([28.0209], grad_fn=<AddBackward0>)\n",
      "tensor([9.4464], grad_fn=<AddBackward0>)\n",
      "tensor([32.9875], grad_fn=<AddBackward0>)\n",
      "tensor([11.1849], grad_fn=<AddBackward0>)\n",
      "tensor([13.8496], grad_fn=<AddBackward0>)\n",
      "tensor([8.2953], grad_fn=<AddBackward0>)\n",
      "tensor([4.3135], grad_fn=<AddBackward0>)\n",
      "tensor([8.5340], grad_fn=<AddBackward0>)\n",
      "tensor([28.0352], grad_fn=<AddBackward0>)\n",
      "tensor([9.4408], grad_fn=<AddBackward0>)\n",
      "tensor([32.9918], grad_fn=<AddBackward0>)\n",
      "tensor([11.1786], grad_fn=<AddBackward0>)\n",
      "tensor([13.8436], grad_fn=<AddBackward0>)\n",
      "tensor([8.2955], grad_fn=<AddBackward0>)\n",
      "tensor([4.3020], grad_fn=<AddBackward0>)\n",
      "tensor([8.5281], grad_fn=<AddBackward0>)\n",
      "tensor([28.0493], grad_fn=<AddBackward0>)\n",
      "tensor([9.4352], grad_fn=<AddBackward0>)\n",
      "tensor([32.9961], grad_fn=<AddBackward0>)\n",
      "tensor([11.1724], grad_fn=<AddBackward0>)\n",
      "tensor([13.8377], grad_fn=<AddBackward0>)\n",
      "tensor([8.2958], grad_fn=<AddBackward0>)\n",
      "tensor([4.2907], grad_fn=<AddBackward0>)\n",
      "tensor([8.5222], grad_fn=<AddBackward0>)\n",
      "tensor([28.0634], grad_fn=<AddBackward0>)\n",
      "tensor([9.4298], grad_fn=<AddBackward0>)\n",
      "tensor([33.0003], grad_fn=<AddBackward0>)\n",
      "tensor([11.1663], grad_fn=<AddBackward0>)\n",
      "tensor([13.8318], grad_fn=<AddBackward0>)\n",
      "tensor([8.2961], grad_fn=<AddBackward0>)\n",
      "tensor([4.2795], grad_fn=<AddBackward0>)\n",
      "tensor([8.5164], grad_fn=<AddBackward0>)\n",
      "tensor([28.0773], grad_fn=<AddBackward0>)\n",
      "tensor([9.4243], grad_fn=<AddBackward0>)\n",
      "tensor([33.0046], grad_fn=<AddBackward0>)\n",
      "tensor([11.1603], grad_fn=<AddBackward0>)\n",
      "tensor([13.8259], grad_fn=<AddBackward0>)\n",
      "tensor([8.2964], grad_fn=<AddBackward0>)\n",
      "tensor([4.2684], grad_fn=<AddBackward0>)\n",
      "tensor([8.5107], grad_fn=<AddBackward0>)\n",
      "tensor([28.0912], grad_fn=<AddBackward0>)\n",
      "tensor([9.4189], grad_fn=<AddBackward0>)\n",
      "tensor([33.0088], grad_fn=<AddBackward0>)\n",
      "tensor([11.1544], grad_fn=<AddBackward0>)\n",
      "tensor([13.8201], grad_fn=<AddBackward0>)\n",
      "tensor([8.2968], grad_fn=<AddBackward0>)\n",
      "tensor([4.2575], grad_fn=<AddBackward0>)\n",
      "tensor([8.5051], grad_fn=<AddBackward0>)\n",
      "tensor([28.1049], grad_fn=<AddBackward0>)\n",
      "tensor([9.4136], grad_fn=<AddBackward0>)\n",
      "tensor([33.0130], grad_fn=<AddBackward0>)\n",
      "tensor([11.1486], grad_fn=<AddBackward0>)\n",
      "tensor([13.8144], grad_fn=<AddBackward0>)\n",
      "tensor([8.2972], grad_fn=<AddBackward0>)\n",
      "tensor([4.2466], grad_fn=<AddBackward0>)\n",
      "tensor([8.4996], grad_fn=<AddBackward0>)\n",
      "tensor([28.1186], grad_fn=<AddBackward0>)\n",
      "tensor([9.4084], grad_fn=<AddBackward0>)\n",
      "tensor([33.0172], grad_fn=<AddBackward0>)\n",
      "tensor([11.1428], grad_fn=<AddBackward0>)\n",
      "tensor([13.8088], grad_fn=<AddBackward0>)\n",
      "tensor([8.2976], grad_fn=<AddBackward0>)\n",
      "tensor([4.2359], grad_fn=<AddBackward0>)\n",
      "tensor([8.4941], grad_fn=<AddBackward0>)\n",
      "tensor([28.1321], grad_fn=<AddBackward0>)\n",
      "tensor([9.4031], grad_fn=<AddBackward0>)\n",
      "tensor([33.0213], grad_fn=<AddBackward0>)\n",
      "tensor([11.1372], grad_fn=<AddBackward0>)\n",
      "tensor([13.8032], grad_fn=<AddBackward0>)\n",
      "tensor([8.2981], grad_fn=<AddBackward0>)\n",
      "tensor([4.2253], grad_fn=<AddBackward0>)\n",
      "tensor([8.4887], grad_fn=<AddBackward0>)\n",
      "tensor([28.1455], grad_fn=<AddBackward0>)\n",
      "tensor([9.3980], grad_fn=<AddBackward0>)\n",
      "tensor([33.0255], grad_fn=<AddBackward0>)\n",
      "tensor([11.1316], grad_fn=<AddBackward0>)\n",
      "tensor([13.7976], grad_fn=<AddBackward0>)\n",
      "tensor([8.2986], grad_fn=<AddBackward0>)\n",
      "tensor([4.2148], grad_fn=<AddBackward0>)\n",
      "tensor([8.4834], grad_fn=<AddBackward0>)\n",
      "tensor([28.1588], grad_fn=<AddBackward0>)\n",
      "tensor([9.3929], grad_fn=<AddBackward0>)\n",
      "tensor([33.0296], grad_fn=<AddBackward0>)\n",
      "tensor([11.1261], grad_fn=<AddBackward0>)\n",
      "tensor([13.7921], grad_fn=<AddBackward0>)\n",
      "tensor([8.2991], grad_fn=<AddBackward0>)\n",
      "tensor([4.2044], grad_fn=<AddBackward0>)\n",
      "tensor([8.4781], grad_fn=<AddBackward0>)\n",
      "tensor([28.1720], grad_fn=<AddBackward0>)\n",
      "tensor([9.3878], grad_fn=<AddBackward0>)\n",
      "tensor([33.0337], grad_fn=<AddBackward0>)\n",
      "tensor([11.1207], grad_fn=<AddBackward0>)\n",
      "tensor([13.7867], grad_fn=<AddBackward0>)\n",
      "tensor([8.2997], grad_fn=<AddBackward0>)\n",
      "tensor([4.1942], grad_fn=<AddBackward0>)\n",
      "tensor([8.4729], grad_fn=<AddBackward0>)\n",
      "tensor([28.1851], grad_fn=<AddBackward0>)\n",
      "tensor([9.3828], grad_fn=<AddBackward0>)\n",
      "tensor([33.0378], grad_fn=<AddBackward0>)\n",
      "tensor([11.1154], grad_fn=<AddBackward0>)\n",
      "tensor([13.7813], grad_fn=<AddBackward0>)\n",
      "tensor([8.3003], grad_fn=<AddBackward0>)\n",
      "tensor([4.1840], grad_fn=<AddBackward0>)\n",
      "tensor([8.4678], grad_fn=<AddBackward0>)\n",
      "tensor([28.1981], grad_fn=<AddBackward0>)\n",
      "tensor([9.3779], grad_fn=<AddBackward0>)\n",
      "tensor([33.0419], grad_fn=<AddBackward0>)\n",
      "tensor([11.1101], grad_fn=<AddBackward0>)\n",
      "tensor([13.7759], grad_fn=<AddBackward0>)\n",
      "tensor([8.3009], grad_fn=<AddBackward0>)\n",
      "tensor([4.1740], grad_fn=<AddBackward0>)\n",
      "tensor([8.4628], grad_fn=<AddBackward0>)\n",
      "tensor([28.2110], grad_fn=<AddBackward0>)\n",
      "tensor([9.3730], grad_fn=<AddBackward0>)\n",
      "tensor([33.0459], grad_fn=<AddBackward0>)\n",
      "tensor([11.1049], grad_fn=<AddBackward0>)\n",
      "tensor([13.7707], grad_fn=<AddBackward0>)\n",
      "tensor([8.3015], grad_fn=<AddBackward0>)\n",
      "tensor([4.1641], grad_fn=<AddBackward0>)\n",
      "tensor([8.4578], grad_fn=<AddBackward0>)\n",
      "tensor([28.2237], grad_fn=<AddBackward0>)\n",
      "tensor([9.3681], grad_fn=<AddBackward0>)\n",
      "tensor([33.0499], grad_fn=<AddBackward0>)\n",
      "tensor([11.0998], grad_fn=<AddBackward0>)\n",
      "tensor([13.7654], grad_fn=<AddBackward0>)\n",
      "tensor([8.3022], grad_fn=<AddBackward0>)\n",
      "tensor([4.1543], grad_fn=<AddBackward0>)\n",
      "tensor([8.4529], grad_fn=<AddBackward0>)\n",
      "tensor([28.2364], grad_fn=<AddBackward0>)\n",
      "tensor([9.3633], grad_fn=<AddBackward0>)\n",
      "tensor([33.0539], grad_fn=<AddBackward0>)\n",
      "tensor([11.0948], grad_fn=<AddBackward0>)\n",
      "tensor([13.7603], grad_fn=<AddBackward0>)\n",
      "tensor([8.3029], grad_fn=<AddBackward0>)\n",
      "tensor([4.1446], grad_fn=<AddBackward0>)\n",
      "tensor([8.4480], grad_fn=<AddBackward0>)\n",
      "tensor([28.2490], grad_fn=<AddBackward0>)\n",
      "tensor([9.3586], grad_fn=<AddBackward0>)\n",
      "tensor([33.0579], grad_fn=<AddBackward0>)\n",
      "tensor([11.0899], grad_fn=<AddBackward0>)\n",
      "tensor([13.7551], grad_fn=<AddBackward0>)\n",
      "tensor([8.3036], grad_fn=<AddBackward0>)\n",
      "tensor([4.1350], grad_fn=<AddBackward0>)\n",
      "tensor([8.4432], grad_fn=<AddBackward0>)\n",
      "tensor([28.2615], grad_fn=<AddBackward0>)\n",
      "tensor([9.3538], grad_fn=<AddBackward0>)\n",
      "tensor([33.0619], grad_fn=<AddBackward0>)\n",
      "tensor([11.0850], grad_fn=<AddBackward0>)\n",
      "tensor([13.7501], grad_fn=<AddBackward0>)\n",
      "tensor([8.3044], grad_fn=<AddBackward0>)\n",
      "tensor([4.1255], grad_fn=<AddBackward0>)\n",
      "tensor([8.4385], grad_fn=<AddBackward0>)\n",
      "tensor([28.2738], grad_fn=<AddBackward0>)\n",
      "tensor([9.3492], grad_fn=<AddBackward0>)\n",
      "tensor([33.0658], grad_fn=<AddBackward0>)\n",
      "tensor([11.0802], grad_fn=<AddBackward0>)\n",
      "tensor([13.7450], grad_fn=<AddBackward0>)\n",
      "tensor([8.3052], grad_fn=<AddBackward0>)\n",
      "tensor([4.1161], grad_fn=<AddBackward0>)\n",
      "tensor([8.4338], grad_fn=<AddBackward0>)\n",
      "tensor([28.2861], grad_fn=<AddBackward0>)\n",
      "tensor([9.3446], grad_fn=<AddBackward0>)\n",
      "tensor([33.0697], grad_fn=<AddBackward0>)\n",
      "tensor([11.0755], grad_fn=<AddBackward0>)\n",
      "tensor([13.7401], grad_fn=<AddBackward0>)\n",
      "tensor([8.3060], grad_fn=<AddBackward0>)\n",
      "tensor([4.1068], grad_fn=<AddBackward0>)\n",
      "tensor([8.4292], grad_fn=<AddBackward0>)\n",
      "tensor([28.2982], grad_fn=<AddBackward0>)\n",
      "tensor([9.3400], grad_fn=<AddBackward0>)\n",
      "tensor([33.0736], grad_fn=<AddBackward0>)\n",
      "tensor([11.0708], grad_fn=<AddBackward0>)\n",
      "tensor([13.7351], grad_fn=<AddBackward0>)\n",
      "tensor([8.3068], grad_fn=<AddBackward0>)\n",
      "tensor([4.0977], grad_fn=<AddBackward0>)\n",
      "tensor([8.4247], grad_fn=<AddBackward0>)\n",
      "tensor([28.3103], grad_fn=<AddBackward0>)\n",
      "tensor([9.3355], grad_fn=<AddBackward0>)\n",
      "tensor([33.0775], grad_fn=<AddBackward0>)\n",
      "tensor([11.0662], grad_fn=<AddBackward0>)\n",
      "tensor([13.7303], grad_fn=<AddBackward0>)\n",
      "tensor([8.3076], grad_fn=<AddBackward0>)\n",
      "tensor([4.0886], grad_fn=<AddBackward0>)\n",
      "tensor([8.4202], grad_fn=<AddBackward0>)\n",
      "tensor([28.3222], grad_fn=<AddBackward0>)\n",
      "tensor([9.3311], grad_fn=<AddBackward0>)\n",
      "tensor([33.0813], grad_fn=<AddBackward0>)\n",
      "tensor([11.0617], grad_fn=<AddBackward0>)\n",
      "tensor([13.7255], grad_fn=<AddBackward0>)\n",
      "tensor([8.3085], grad_fn=<AddBackward0>)\n",
      "tensor([4.0796], grad_fn=<AddBackward0>)\n",
      "tensor([8.4158], grad_fn=<AddBackward0>)\n",
      "tensor([28.3341], grad_fn=<AddBackward0>)\n",
      "tensor([9.3266], grad_fn=<AddBackward0>)\n",
      "tensor([33.0852], grad_fn=<AddBackward0>)\n",
      "tensor([11.0572], grad_fn=<AddBackward0>)\n",
      "tensor([13.7207], grad_fn=<AddBackward0>)\n",
      "tensor([8.3094], grad_fn=<AddBackward0>)\n",
      "tensor([4.0708], grad_fn=<AddBackward0>)\n",
      "tensor([8.4114], grad_fn=<AddBackward0>)\n",
      "tensor([28.3458], grad_fn=<AddBackward0>)\n",
      "tensor([9.3223], grad_fn=<AddBackward0>)\n",
      "tensor([33.0890], grad_fn=<AddBackward0>)\n",
      "tensor([11.0529], grad_fn=<AddBackward0>)\n",
      "tensor([13.7160], grad_fn=<AddBackward0>)\n",
      "tensor([8.3103], grad_fn=<AddBackward0>)\n",
      "tensor([4.0620], grad_fn=<AddBackward0>)\n",
      "tensor([8.4071], grad_fn=<AddBackward0>)\n",
      "tensor([28.3575], grad_fn=<AddBackward0>)\n",
      "tensor([9.3179], grad_fn=<AddBackward0>)\n",
      "tensor([33.0928], grad_fn=<AddBackward0>)\n",
      "tensor([11.0485], grad_fn=<AddBackward0>)\n",
      "tensor([13.7113], grad_fn=<AddBackward0>)\n",
      "tensor([8.3112], grad_fn=<AddBackward0>)\n",
      "tensor([4.0534], grad_fn=<AddBackward0>)\n",
      "tensor([8.4029], grad_fn=<AddBackward0>)\n",
      "tensor([28.3690], grad_fn=<AddBackward0>)\n",
      "tensor([9.3136], grad_fn=<AddBackward0>)\n",
      "tensor([33.0965], grad_fn=<AddBackward0>)\n",
      "tensor([11.0443], grad_fn=<AddBackward0>)\n",
      "tensor([13.7066], grad_fn=<AddBackward0>)\n",
      "tensor([8.3121], grad_fn=<AddBackward0>)\n",
      "tensor([4.0448], grad_fn=<AddBackward0>)\n",
      "tensor([8.3987], grad_fn=<AddBackward0>)\n",
      "tensor([28.3805], grad_fn=<AddBackward0>)\n",
      "tensor([9.3094], grad_fn=<AddBackward0>)\n",
      "tensor([33.1003], grad_fn=<AddBackward0>)\n",
      "tensor([11.0401], grad_fn=<AddBackward0>)\n",
      "tensor([13.7021], grad_fn=<AddBackward0>)\n",
      "tensor([8.3131], grad_fn=<AddBackward0>)\n",
      "tensor([4.0363], grad_fn=<AddBackward0>)\n",
      "tensor([8.3945], grad_fn=<AddBackward0>)\n",
      "tensor([28.3918], grad_fn=<AddBackward0>)\n",
      "tensor([9.3052], grad_fn=<AddBackward0>)\n",
      "tensor([33.1040], grad_fn=<AddBackward0>)\n",
      "tensor([11.0360], grad_fn=<AddBackward0>)\n",
      "tensor([13.6975], grad_fn=<AddBackward0>)\n",
      "tensor([8.3140], grad_fn=<AddBackward0>)\n",
      "tensor([4.0279], grad_fn=<AddBackward0>)\n",
      "tensor([8.3905], grad_fn=<AddBackward0>)\n",
      "tensor([28.4030], grad_fn=<AddBackward0>)\n",
      "tensor([9.3011], grad_fn=<AddBackward0>)\n",
      "tensor([33.1077], grad_fn=<AddBackward0>)\n",
      "tensor([11.0319], grad_fn=<AddBackward0>)\n",
      "tensor([13.6930], grad_fn=<AddBackward0>)\n",
      "tensor([8.3150], grad_fn=<AddBackward0>)\n",
      "tensor([4.0197], grad_fn=<AddBackward0>)\n",
      "tensor([8.3864], grad_fn=<AddBackward0>)\n",
      "tensor([28.4142], grad_fn=<AddBackward0>)\n",
      "tensor([9.2970], grad_fn=<AddBackward0>)\n",
      "tensor([33.1113], grad_fn=<AddBackward0>)\n",
      "tensor([11.0279], grad_fn=<AddBackward0>)\n",
      "tensor([13.6886], grad_fn=<AddBackward0>)\n",
      "tensor([8.3160], grad_fn=<AddBackward0>)\n",
      "tensor([4.0115], grad_fn=<AddBackward0>)\n",
      "tensor([8.3825], grad_fn=<AddBackward0>)\n",
      "tensor([28.4252], grad_fn=<AddBackward0>)\n",
      "tensor([9.2929], grad_fn=<AddBackward0>)\n",
      "tensor([33.1150], grad_fn=<AddBackward0>)\n",
      "tensor([11.0240], grad_fn=<AddBackward0>)\n",
      "tensor([13.6842], grad_fn=<AddBackward0>)\n",
      "tensor([8.3170], grad_fn=<AddBackward0>)\n",
      "tensor([4.0034], grad_fn=<AddBackward0>)\n",
      "tensor([8.3786], grad_fn=<AddBackward0>)\n",
      "tensor([28.4362], grad_fn=<AddBackward0>)\n",
      "tensor([9.2889], grad_fn=<AddBackward0>)\n",
      "tensor([33.1186], grad_fn=<AddBackward0>)\n",
      "tensor([11.0201], grad_fn=<AddBackward0>)\n",
      "tensor([13.6798], grad_fn=<AddBackward0>)\n",
      "tensor([8.3180], grad_fn=<AddBackward0>)\n",
      "tensor([3.9954], grad_fn=<AddBackward0>)\n",
      "tensor([8.3747], grad_fn=<AddBackward0>)\n",
      "tensor([28.4470], grad_fn=<AddBackward0>)\n",
      "tensor([9.2849], grad_fn=<AddBackward0>)\n",
      "tensor([33.1222], grad_fn=<AddBackward0>)\n",
      "tensor([11.0163], grad_fn=<AddBackward0>)\n",
      "tensor([13.6755], grad_fn=<AddBackward0>)\n",
      "tensor([8.3191], grad_fn=<AddBackward0>)\n",
      "tensor([3.9875], grad_fn=<AddBackward0>)\n",
      "tensor([8.3709], grad_fn=<AddBackward0>)\n",
      "tensor([28.4578], grad_fn=<AddBackward0>)\n",
      "tensor([9.2809], grad_fn=<AddBackward0>)\n",
      "tensor([33.1257], grad_fn=<AddBackward0>)\n",
      "tensor([11.0125], grad_fn=<AddBackward0>)\n",
      "tensor([13.6713], grad_fn=<AddBackward0>)\n",
      "tensor([8.3201], grad_fn=<AddBackward0>)\n",
      "tensor([3.9797], grad_fn=<AddBackward0>)\n",
      "tensor([8.3671], grad_fn=<AddBackward0>)\n",
      "tensor([28.4684], grad_fn=<AddBackward0>)\n",
      "tensor([9.2770], grad_fn=<AddBackward0>)\n",
      "tensor([33.1293], grad_fn=<AddBackward0>)\n",
      "tensor([11.0088], grad_fn=<AddBackward0>)\n",
      "tensor([13.6670], grad_fn=<AddBackward0>)\n",
      "tensor([8.3212], grad_fn=<AddBackward0>)\n",
      "tensor([3.9719], grad_fn=<AddBackward0>)\n",
      "tensor([8.3634], grad_fn=<AddBackward0>)\n",
      "tensor([28.4790], grad_fn=<AddBackward0>)\n",
      "tensor([9.2732], grad_fn=<AddBackward0>)\n",
      "tensor([33.1328], grad_fn=<AddBackward0>)\n",
      "tensor([11.0052], grad_fn=<AddBackward0>)\n",
      "tensor([13.6629], grad_fn=<AddBackward0>)\n",
      "tensor([8.3223], grad_fn=<AddBackward0>)\n",
      "tensor([3.9643], grad_fn=<AddBackward0>)\n",
      "tensor([8.3597], grad_fn=<AddBackward0>)\n",
      "tensor([28.4895], grad_fn=<AddBackward0>)\n",
      "tensor([9.2694], grad_fn=<AddBackward0>)\n",
      "tensor([33.1363], grad_fn=<AddBackward0>)\n",
      "tensor([11.0016], grad_fn=<AddBackward0>)\n",
      "tensor([13.6587], grad_fn=<AddBackward0>)\n",
      "tensor([8.3233], grad_fn=<AddBackward0>)\n",
      "tensor([3.9567], grad_fn=<AddBackward0>)\n",
      "tensor([8.3561], grad_fn=<AddBackward0>)\n",
      "tensor([28.4998], grad_fn=<AddBackward0>)\n",
      "tensor([9.2656], grad_fn=<AddBackward0>)\n",
      "tensor([33.1398], grad_fn=<AddBackward0>)\n",
      "tensor([10.9980], grad_fn=<AddBackward0>)\n",
      "tensor([13.6546], grad_fn=<AddBackward0>)\n",
      "tensor([8.3244], grad_fn=<AddBackward0>)\n",
      "tensor([3.9493], grad_fn=<AddBackward0>)\n",
      "tensor([8.3526], grad_fn=<AddBackward0>)\n",
      "tensor([28.5101], grad_fn=<AddBackward0>)\n",
      "tensor([9.2619], grad_fn=<AddBackward0>)\n",
      "tensor([33.1432], grad_fn=<AddBackward0>)\n",
      "tensor([10.9945], grad_fn=<AddBackward0>)\n",
      "tensor([13.6506], grad_fn=<AddBackward0>)\n",
      "tensor([8.3255], grad_fn=<AddBackward0>)\n",
      "tensor([3.9419], grad_fn=<AddBackward0>)\n",
      "tensor([8.3491], grad_fn=<AddBackward0>)\n",
      "tensor([28.5203], grad_fn=<AddBackward0>)\n",
      "tensor([9.2582], grad_fn=<AddBackward0>)\n",
      "tensor([33.1466], grad_fn=<AddBackward0>)\n",
      "tensor([10.9911], grad_fn=<AddBackward0>)\n",
      "tensor([13.6466], grad_fn=<AddBackward0>)\n",
      "tensor([8.3267], grad_fn=<AddBackward0>)\n",
      "tensor([3.9346], grad_fn=<AddBackward0>)\n",
      "tensor([8.3456], grad_fn=<AddBackward0>)\n",
      "tensor([28.5304], grad_fn=<AddBackward0>)\n",
      "tensor([9.2545], grad_fn=<AddBackward0>)\n",
      "tensor([33.1500], grad_fn=<AddBackward0>)\n",
      "tensor([10.9877], grad_fn=<AddBackward0>)\n",
      "tensor([13.6426], grad_fn=<AddBackward0>)\n",
      "tensor([8.3278], grad_fn=<AddBackward0>)\n",
      "tensor([3.9274], grad_fn=<AddBackward0>)\n",
      "tensor([8.3422], grad_fn=<AddBackward0>)\n",
      "tensor([28.5404], grad_fn=<AddBackward0>)\n",
      "tensor([9.2509], grad_fn=<AddBackward0>)\n",
      "tensor([33.1534], grad_fn=<AddBackward0>)\n",
      "tensor([10.9844], grad_fn=<AddBackward0>)\n",
      "tensor([13.6387], grad_fn=<AddBackward0>)\n",
      "tensor([8.3289], grad_fn=<AddBackward0>)\n",
      "tensor([3.9202], grad_fn=<AddBackward0>)\n",
      "tensor([8.3388], grad_fn=<AddBackward0>)\n",
      "tensor([28.5503], grad_fn=<AddBackward0>)\n",
      "tensor([9.2473], grad_fn=<AddBackward0>)\n",
      "tensor([33.1568], grad_fn=<AddBackward0>)\n",
      "tensor([10.9811], grad_fn=<AddBackward0>)\n",
      "tensor([13.6348], grad_fn=<AddBackward0>)\n",
      "tensor([8.3301], grad_fn=<AddBackward0>)\n",
      "tensor([3.9132], grad_fn=<AddBackward0>)\n",
      "tensor([8.3355], grad_fn=<AddBackward0>)\n",
      "tensor([28.5601], grad_fn=<AddBackward0>)\n",
      "tensor([9.2438], grad_fn=<AddBackward0>)\n",
      "tensor([33.1601], grad_fn=<AddBackward0>)\n",
      "tensor([10.9779], grad_fn=<AddBackward0>)\n",
      "tensor([13.6310], grad_fn=<AddBackward0>)\n",
      "tensor([8.3312], grad_fn=<AddBackward0>)\n",
      "tensor([3.9062], grad_fn=<AddBackward0>)\n",
      "tensor([8.3322], grad_fn=<AddBackward0>)\n",
      "tensor([28.5698], grad_fn=<AddBackward0>)\n",
      "tensor([9.2402], grad_fn=<AddBackward0>)\n",
      "tensor([33.1634], grad_fn=<AddBackward0>)\n",
      "tensor([10.9747], grad_fn=<AddBackward0>)\n",
      "tensor([13.6271], grad_fn=<AddBackward0>)\n",
      "tensor([8.3324], grad_fn=<AddBackward0>)\n",
      "tensor([3.8993], grad_fn=<AddBackward0>)\n",
      "tensor([8.3289], grad_fn=<AddBackward0>)\n",
      "tensor([28.5794], grad_fn=<AddBackward0>)\n",
      "tensor([9.2368], grad_fn=<AddBackward0>)\n",
      "tensor([33.1667], grad_fn=<AddBackward0>)\n",
      "tensor([10.9716], grad_fn=<AddBackward0>)\n",
      "tensor([13.6234], grad_fn=<AddBackward0>)\n",
      "tensor([8.3335], grad_fn=<AddBackward0>)\n",
      "tensor([3.8925], grad_fn=<AddBackward0>)\n",
      "tensor([8.3257], grad_fn=<AddBackward0>)\n",
      "tensor([28.5889], grad_fn=<AddBackward0>)\n",
      "tensor([9.2333], grad_fn=<AddBackward0>)\n",
      "tensor([33.1699], grad_fn=<AddBackward0>)\n",
      "tensor([10.9685], grad_fn=<AddBackward0>)\n",
      "tensor([13.6197], grad_fn=<AddBackward0>)\n",
      "tensor([8.3347], grad_fn=<AddBackward0>)\n",
      "tensor([3.8858], grad_fn=<AddBackward0>)\n",
      "tensor([8.3226], grad_fn=<AddBackward0>)\n",
      "tensor([28.5984], grad_fn=<AddBackward0>)\n",
      "tensor([9.2299], grad_fn=<AddBackward0>)\n",
      "tensor([33.1731], grad_fn=<AddBackward0>)\n",
      "tensor([10.9655], grad_fn=<AddBackward0>)\n",
      "tensor([13.6160], grad_fn=<AddBackward0>)\n",
      "tensor([8.3359], grad_fn=<AddBackward0>)\n",
      "tensor([3.8792], grad_fn=<AddBackward0>)\n",
      "tensor([8.3195], grad_fn=<AddBackward0>)\n",
      "tensor([28.6077], grad_fn=<AddBackward0>)\n",
      "tensor([9.2266], grad_fn=<AddBackward0>)\n",
      "tensor([33.1763], grad_fn=<AddBackward0>)\n",
      "tensor([10.9625], grad_fn=<AddBackward0>)\n",
      "tensor([13.6123], grad_fn=<AddBackward0>)\n",
      "tensor([8.3370], grad_fn=<AddBackward0>)\n",
      "tensor([3.8726], grad_fn=<AddBackward0>)\n",
      "tensor([8.3164], grad_fn=<AddBackward0>)\n",
      "tensor([28.6170], grad_fn=<AddBackward0>)\n",
      "tensor([9.2232], grad_fn=<AddBackward0>)\n",
      "tensor([33.1795], grad_fn=<AddBackward0>)\n",
      "tensor([10.9596], grad_fn=<AddBackward0>)\n",
      "tensor([13.6087], grad_fn=<AddBackward0>)\n",
      "tensor([8.3382], grad_fn=<AddBackward0>)\n",
      "tensor([3.8661], grad_fn=<AddBackward0>)\n",
      "tensor([8.3134], grad_fn=<AddBackward0>)\n",
      "tensor([28.6262], grad_fn=<AddBackward0>)\n",
      "tensor([9.2200], grad_fn=<AddBackward0>)\n",
      "tensor([33.1827], grad_fn=<AddBackward0>)\n",
      "tensor([10.9567], grad_fn=<AddBackward0>)\n",
      "tensor([13.6051], grad_fn=<AddBackward0>)\n",
      "tensor([8.3394], grad_fn=<AddBackward0>)\n",
      "tensor([3.8597], grad_fn=<AddBackward0>)\n",
      "tensor([8.3104], grad_fn=<AddBackward0>)\n",
      "tensor([28.6353], grad_fn=<AddBackward0>)\n",
      "tensor([9.2167], grad_fn=<AddBackward0>)\n",
      "tensor([33.1858], grad_fn=<AddBackward0>)\n",
      "tensor([10.9538], grad_fn=<AddBackward0>)\n",
      "tensor([13.6016], grad_fn=<AddBackward0>)\n",
      "tensor([8.3406], grad_fn=<AddBackward0>)\n",
      "tensor([3.8533], grad_fn=<AddBackward0>)\n",
      "tensor([8.3074], grad_fn=<AddBackward0>)\n",
      "tensor([28.6443], grad_fn=<AddBackward0>)\n",
      "tensor([9.2135], grad_fn=<AddBackward0>)\n",
      "tensor([33.1889], grad_fn=<AddBackward0>)\n",
      "tensor([10.9510], grad_fn=<AddBackward0>)\n",
      "tensor([13.5981], grad_fn=<AddBackward0>)\n",
      "tensor([8.3418], grad_fn=<AddBackward0>)\n",
      "tensor([3.8471], grad_fn=<AddBackward0>)\n",
      "tensor([8.3045], grad_fn=<AddBackward0>)\n",
      "tensor([28.6532], grad_fn=<AddBackward0>)\n",
      "tensor([9.2103], grad_fn=<AddBackward0>)\n",
      "tensor([33.1920], grad_fn=<AddBackward0>)\n",
      "tensor([10.9483], grad_fn=<AddBackward0>)\n",
      "tensor([13.5946], grad_fn=<AddBackward0>)\n",
      "tensor([8.3430], grad_fn=<AddBackward0>)\n",
      "tensor([3.8409], grad_fn=<AddBackward0>)\n",
      "tensor([8.3016], grad_fn=<AddBackward0>)\n",
      "tensor([28.6620], grad_fn=<AddBackward0>)\n",
      "tensor([9.2071], grad_fn=<AddBackward0>)\n",
      "tensor([33.1951], grad_fn=<AddBackward0>)\n",
      "tensor([10.9455], grad_fn=<AddBackward0>)\n",
      "tensor([13.5912], grad_fn=<AddBackward0>)\n",
      "tensor([8.3442], grad_fn=<AddBackward0>)\n",
      "tensor([3.8347], grad_fn=<AddBackward0>)\n",
      "tensor([8.2988], grad_fn=<AddBackward0>)\n",
      "tensor([28.6707], grad_fn=<AddBackward0>)\n",
      "tensor([9.2040], grad_fn=<AddBackward0>)\n",
      "tensor([33.1981], grad_fn=<AddBackward0>)\n",
      "tensor([10.9429], grad_fn=<AddBackward0>)\n",
      "tensor([13.5878], grad_fn=<AddBackward0>)\n",
      "tensor([8.3454], grad_fn=<AddBackward0>)\n",
      "tensor([3.8287], grad_fn=<AddBackward0>)\n",
      "tensor([8.2960], grad_fn=<AddBackward0>)\n",
      "tensor([28.6794], grad_fn=<AddBackward0>)\n",
      "tensor([9.2009], grad_fn=<AddBackward0>)\n",
      "tensor([33.2011], grad_fn=<AddBackward0>)\n",
      "tensor([10.9402], grad_fn=<AddBackward0>)\n",
      "tensor([13.5845], grad_fn=<AddBackward0>)\n",
      "tensor([8.3466], grad_fn=<AddBackward0>)\n",
      "tensor([3.8227], grad_fn=<AddBackward0>)\n",
      "tensor([8.2933], grad_fn=<AddBackward0>)\n",
      "tensor([28.6880], grad_fn=<AddBackward0>)\n",
      "tensor([9.1979], grad_fn=<AddBackward0>)\n",
      "tensor([33.2041], grad_fn=<AddBackward0>)\n",
      "tensor([10.9376], grad_fn=<AddBackward0>)\n",
      "tensor([13.5811], grad_fn=<AddBackward0>)\n",
      "tensor([8.3478], grad_fn=<AddBackward0>)\n",
      "tensor([3.8168], grad_fn=<AddBackward0>)\n",
      "tensor([8.2906], grad_fn=<AddBackward0>)\n",
      "tensor([28.6964], grad_fn=<AddBackward0>)\n",
      "tensor([9.1948], grad_fn=<AddBackward0>)\n",
      "tensor([33.2070], grad_fn=<AddBackward0>)\n",
      "tensor([10.9351], grad_fn=<AddBackward0>)\n",
      "tensor([13.5779], grad_fn=<AddBackward0>)\n",
      "tensor([8.3490], grad_fn=<AddBackward0>)\n",
      "tensor([3.8110], grad_fn=<AddBackward0>)\n",
      "tensor([8.2879], grad_fn=<AddBackward0>)\n",
      "tensor([28.7048], grad_fn=<AddBackward0>)\n",
      "tensor([9.1919], grad_fn=<AddBackward0>)\n",
      "tensor([33.2100], grad_fn=<AddBackward0>)\n",
      "tensor([10.9326], grad_fn=<AddBackward0>)\n",
      "tensor([13.5746], grad_fn=<AddBackward0>)\n",
      "tensor([8.3502], grad_fn=<AddBackward0>)\n",
      "tensor([3.8052], grad_fn=<AddBackward0>)\n",
      "tensor([8.2852], grad_fn=<AddBackward0>)\n",
      "tensor([28.7131], grad_fn=<AddBackward0>)\n",
      "tensor([9.1889], grad_fn=<AddBackward0>)\n",
      "tensor([33.2129], grad_fn=<AddBackward0>)\n",
      "tensor([10.9301], grad_fn=<AddBackward0>)\n",
      "tensor([13.5714], grad_fn=<AddBackward0>)\n",
      "tensor([8.3514], grad_fn=<AddBackward0>)\n",
      "tensor([3.7995], grad_fn=<AddBackward0>)\n",
      "tensor([8.2826], grad_fn=<AddBackward0>)\n",
      "tensor([28.7214], grad_fn=<AddBackward0>)\n",
      "tensor([9.1860], grad_fn=<AddBackward0>)\n",
      "tensor([33.2158], grad_fn=<AddBackward0>)\n",
      "tensor([10.9277], grad_fn=<AddBackward0>)\n",
      "tensor([13.5682], grad_fn=<AddBackward0>)\n",
      "tensor([8.3526], grad_fn=<AddBackward0>)\n",
      "tensor([3.7938], grad_fn=<AddBackward0>)\n",
      "tensor([8.2801], grad_fn=<AddBackward0>)\n",
      "tensor([28.7295], grad_fn=<AddBackward0>)\n",
      "tensor([9.1831], grad_fn=<AddBackward0>)\n",
      "tensor([33.2186], grad_fn=<AddBackward0>)\n",
      "tensor([10.9253], grad_fn=<AddBackward0>)\n",
      "tensor([13.5651], grad_fn=<AddBackward0>)\n",
      "tensor([8.3538], grad_fn=<AddBackward0>)\n",
      "tensor([3.7883], grad_fn=<AddBackward0>)\n",
      "tensor([8.2775], grad_fn=<AddBackward0>)\n",
      "tensor([28.7376], grad_fn=<AddBackward0>)\n",
      "tensor([9.1802], grad_fn=<AddBackward0>)\n",
      "tensor([33.2215], grad_fn=<AddBackward0>)\n",
      "tensor([10.9229], grad_fn=<AddBackward0>)\n",
      "tensor([13.5619], grad_fn=<AddBackward0>)\n",
      "tensor([8.3550], grad_fn=<AddBackward0>)\n",
      "tensor([3.7828], grad_fn=<AddBackward0>)\n",
      "tensor([8.2750], grad_fn=<AddBackward0>)\n",
      "tensor([28.7456], grad_fn=<AddBackward0>)\n",
      "tensor([9.1774], grad_fn=<AddBackward0>)\n",
      "tensor([33.2243], grad_fn=<AddBackward0>)\n",
      "tensor([10.9206], grad_fn=<AddBackward0>)\n",
      "tensor([13.5589], grad_fn=<AddBackward0>)\n",
      "tensor([8.3563], grad_fn=<AddBackward0>)\n",
      "tensor([3.7773], grad_fn=<AddBackward0>)\n",
      "tensor([8.2725], grad_fn=<AddBackward0>)\n",
      "tensor([28.7535], grad_fn=<AddBackward0>)\n",
      "tensor([9.1746], grad_fn=<AddBackward0>)\n",
      "tensor([33.2271], grad_fn=<AddBackward0>)\n",
      "tensor([10.9183], grad_fn=<AddBackward0>)\n",
      "tensor([13.5558], grad_fn=<AddBackward0>)\n",
      "tensor([8.3575], grad_fn=<AddBackward0>)\n",
      "tensor([3.7720], grad_fn=<AddBackward0>)\n",
      "tensor([8.2701], grad_fn=<AddBackward0>)\n",
      "tensor([28.7613], grad_fn=<AddBackward0>)\n",
      "tensor([9.1718], grad_fn=<AddBackward0>)\n",
      "tensor([33.2298], grad_fn=<AddBackward0>)\n",
      "tensor([10.9161], grad_fn=<AddBackward0>)\n",
      "tensor([13.5528], grad_fn=<AddBackward0>)\n",
      "tensor([8.3587], grad_fn=<AddBackward0>)\n",
      "tensor([3.7666], grad_fn=<AddBackward0>)\n",
      "tensor([8.2677], grad_fn=<AddBackward0>)\n",
      "tensor([28.7691], grad_fn=<AddBackward0>)\n",
      "tensor([9.1690], grad_fn=<AddBackward0>)\n",
      "tensor([33.2326], grad_fn=<AddBackward0>)\n",
      "tensor([10.9139], grad_fn=<AddBackward0>)\n",
      "tensor([13.5498], grad_fn=<AddBackward0>)\n",
      "tensor([8.3599], grad_fn=<AddBackward0>)\n",
      "tensor([3.7614], grad_fn=<AddBackward0>)\n",
      "tensor([8.2653], grad_fn=<AddBackward0>)\n",
      "tensor([28.7767], grad_fn=<AddBackward0>)\n",
      "tensor([9.1663], grad_fn=<AddBackward0>)\n",
      "tensor([33.2353], grad_fn=<AddBackward0>)\n",
      "tensor([10.9117], grad_fn=<AddBackward0>)\n",
      "tensor([13.5469], grad_fn=<AddBackward0>)\n",
      "tensor([8.3611], grad_fn=<AddBackward0>)\n",
      "tensor([3.7562], grad_fn=<AddBackward0>)\n",
      "tensor([8.2630], grad_fn=<AddBackward0>)\n",
      "tensor([28.7843], grad_fn=<AddBackward0>)\n",
      "tensor([9.1636], grad_fn=<AddBackward0>)\n",
      "tensor([33.2380], grad_fn=<AddBackward0>)\n",
      "tensor([10.9095], grad_fn=<AddBackward0>)\n",
      "tensor([13.5439], grad_fn=<AddBackward0>)\n",
      "tensor([8.3623], grad_fn=<AddBackward0>)\n",
      "tensor([3.7511], grad_fn=<AddBackward0>)\n",
      "tensor([8.2607], grad_fn=<AddBackward0>)\n",
      "tensor([28.7918], grad_fn=<AddBackward0>)\n",
      "tensor([9.1610], grad_fn=<AddBackward0>)\n",
      "tensor([33.2407], grad_fn=<AddBackward0>)\n",
      "tensor([10.9074], grad_fn=<AddBackward0>)\n",
      "tensor([13.5410], grad_fn=<AddBackward0>)\n",
      "tensor([8.3635], grad_fn=<AddBackward0>)\n",
      "tensor([3.7460], grad_fn=<AddBackward0>)\n",
      "tensor([8.2584], grad_fn=<AddBackward0>)\n",
      "tensor([28.7993], grad_fn=<AddBackward0>)\n",
      "tensor([9.1584], grad_fn=<AddBackward0>)\n",
      "tensor([33.2433], grad_fn=<AddBackward0>)\n",
      "tensor([10.9053], grad_fn=<AddBackward0>)\n",
      "tensor([13.5382], grad_fn=<AddBackward0>)\n",
      "tensor([8.3647], grad_fn=<AddBackward0>)\n",
      "tensor([3.7410], grad_fn=<AddBackward0>)\n",
      "tensor([8.2562], grad_fn=<AddBackward0>)\n",
      "tensor([28.8066], grad_fn=<AddBackward0>)\n",
      "tensor([9.1558], grad_fn=<AddBackward0>)\n",
      "tensor([33.2459], grad_fn=<AddBackward0>)\n",
      "tensor([10.9033], grad_fn=<AddBackward0>)\n",
      "tensor([13.5353], grad_fn=<AddBackward0>)\n",
      "tensor([8.3659], grad_fn=<AddBackward0>)\n",
      "tensor([3.7361], grad_fn=<AddBackward0>)\n",
      "tensor([8.2540], grad_fn=<AddBackward0>)\n",
      "tensor([28.8139], grad_fn=<AddBackward0>)\n",
      "tensor([9.1532], grad_fn=<AddBackward0>)\n",
      "tensor([33.2485], grad_fn=<AddBackward0>)\n",
      "tensor([10.9013], grad_fn=<AddBackward0>)\n",
      "tensor([13.5326], grad_fn=<AddBackward0>)\n",
      "tensor([8.3671], grad_fn=<AddBackward0>)\n",
      "tensor([3.7312], grad_fn=<AddBackward0>)\n",
      "tensor([8.2518], grad_fn=<AddBackward0>)\n",
      "tensor([28.8211], grad_fn=<AddBackward0>)\n",
      "tensor([9.1506], grad_fn=<AddBackward0>)\n",
      "tensor([33.2511], grad_fn=<AddBackward0>)\n",
      "tensor([10.8993], grad_fn=<AddBackward0>)\n",
      "tensor([13.5298], grad_fn=<AddBackward0>)\n",
      "tensor([8.3683], grad_fn=<AddBackward0>)\n",
      "tensor([3.7264], grad_fn=<AddBackward0>)\n",
      "tensor([8.2496], grad_fn=<AddBackward0>)\n",
      "tensor([28.8283], grad_fn=<AddBackward0>)\n",
      "tensor([9.1481], grad_fn=<AddBackward0>)\n",
      "tensor([33.2537], grad_fn=<AddBackward0>)\n",
      "tensor([10.8974], grad_fn=<AddBackward0>)\n",
      "tensor([13.5270], grad_fn=<AddBackward0>)\n",
      "tensor([8.3695], grad_fn=<AddBackward0>)\n",
      "tensor([3.7216], grad_fn=<AddBackward0>)\n",
      "tensor([8.2475], grad_fn=<AddBackward0>)\n",
      "tensor([28.8353], grad_fn=<AddBackward0>)\n",
      "tensor([9.1456], grad_fn=<AddBackward0>)\n",
      "tensor([33.2562], grad_fn=<AddBackward0>)\n",
      "tensor([10.8954], grad_fn=<AddBackward0>)\n",
      "tensor([13.5243], grad_fn=<AddBackward0>)\n",
      "tensor([8.3707], grad_fn=<AddBackward0>)\n",
      "tensor([3.7169], grad_fn=<AddBackward0>)\n",
      "tensor([8.2454], grad_fn=<AddBackward0>)\n",
      "tensor([28.8423], grad_fn=<AddBackward0>)\n",
      "tensor([9.1432], grad_fn=<AddBackward0>)\n",
      "tensor([33.2587], grad_fn=<AddBackward0>)\n",
      "tensor([10.8936], grad_fn=<AddBackward0>)\n",
      "tensor([13.5216], grad_fn=<AddBackward0>)\n",
      "tensor([8.3719], grad_fn=<AddBackward0>)\n",
      "tensor([3.7122], grad_fn=<AddBackward0>)\n",
      "tensor([8.2433], grad_fn=<AddBackward0>)\n",
      "tensor([28.8492], grad_fn=<AddBackward0>)\n",
      "tensor([9.1407], grad_fn=<AddBackward0>)\n",
      "tensor([33.2612], grad_fn=<AddBackward0>)\n",
      "tensor([10.8917], grad_fn=<AddBackward0>)\n",
      "tensor([13.5190], grad_fn=<AddBackward0>)\n",
      "tensor([8.3730], grad_fn=<AddBackward0>)\n",
      "tensor([3.7076], grad_fn=<AddBackward0>)\n",
      "tensor([8.2413], grad_fn=<AddBackward0>)\n",
      "tensor([28.8561], grad_fn=<AddBackward0>)\n",
      "tensor([9.1383], grad_fn=<AddBackward0>)\n",
      "tensor([33.2637], grad_fn=<AddBackward0>)\n",
      "tensor([10.8899], grad_fn=<AddBackward0>)\n",
      "tensor([13.5164], grad_fn=<AddBackward0>)\n",
      "tensor([8.3742], grad_fn=<AddBackward0>)\n",
      "tensor([3.7031], grad_fn=<AddBackward0>)\n",
      "tensor([8.2393], grad_fn=<AddBackward0>)\n",
      "tensor([28.8628], grad_fn=<AddBackward0>)\n",
      "tensor([9.1359], grad_fn=<AddBackward0>)\n",
      "tensor([33.2661], grad_fn=<AddBackward0>)\n",
      "tensor([10.8881], grad_fn=<AddBackward0>)\n",
      "tensor([13.5138], grad_fn=<AddBackward0>)\n",
      "tensor([8.3754], grad_fn=<AddBackward0>)\n",
      "tensor([3.6986], grad_fn=<AddBackward0>)\n",
      "tensor([8.2373], grad_fn=<AddBackward0>)\n",
      "tensor([28.8695], grad_fn=<AddBackward0>)\n",
      "tensor([9.1336], grad_fn=<AddBackward0>)\n",
      "tensor([33.2685], grad_fn=<AddBackward0>)\n",
      "tensor([10.8863], grad_fn=<AddBackward0>)\n",
      "tensor([13.5112], grad_fn=<AddBackward0>)\n",
      "tensor([8.3766], grad_fn=<AddBackward0>)\n",
      "tensor([3.6942], grad_fn=<AddBackward0>)\n",
      "tensor([8.2353], grad_fn=<AddBackward0>)\n",
      "tensor([28.8761], grad_fn=<AddBackward0>)\n",
      "tensor([9.1312], grad_fn=<AddBackward0>)\n",
      "tensor([33.2709], grad_fn=<AddBackward0>)\n",
      "tensor([10.8846], grad_fn=<AddBackward0>)\n",
      "tensor([13.5087], grad_fn=<AddBackward0>)\n",
      "tensor([8.3777], grad_fn=<AddBackward0>)\n",
      "tensor([3.6898], grad_fn=<AddBackward0>)\n",
      "tensor([8.2334], grad_fn=<AddBackward0>)\n",
      "tensor([28.8827], grad_fn=<AddBackward0>)\n",
      "tensor([9.1289], grad_fn=<AddBackward0>)\n",
      "tensor([33.2733], grad_fn=<AddBackward0>)\n",
      "tensor([10.8828], grad_fn=<AddBackward0>)\n",
      "tensor([13.5062], grad_fn=<AddBackward0>)\n",
      "tensor([8.3789], grad_fn=<AddBackward0>)\n",
      "tensor([3.6855], grad_fn=<AddBackward0>)\n",
      "tensor([8.2315], grad_fn=<AddBackward0>)\n",
      "tensor([28.8892], grad_fn=<AddBackward0>)\n",
      "tensor([9.1266], grad_fn=<AddBackward0>)\n",
      "tensor([33.2757], grad_fn=<AddBackward0>)\n",
      "tensor([10.8812], grad_fn=<AddBackward0>)\n",
      "tensor([13.5037], grad_fn=<AddBackward0>)\n",
      "tensor([8.3801], grad_fn=<AddBackward0>)\n",
      "tensor([3.6812], grad_fn=<AddBackward0>)\n",
      "tensor([8.2296], grad_fn=<AddBackward0>)\n",
      "tensor([28.8956], grad_fn=<AddBackward0>)\n",
      "tensor([9.1244], grad_fn=<AddBackward0>)\n",
      "tensor([33.2780], grad_fn=<AddBackward0>)\n",
      "tensor([10.8795], grad_fn=<AddBackward0>)\n",
      "tensor([13.5012], grad_fn=<AddBackward0>)\n",
      "tensor([8.3812], grad_fn=<AddBackward0>)\n",
      "tensor([3.6770], grad_fn=<AddBackward0>)\n",
      "tensor([8.2278], grad_fn=<AddBackward0>)\n",
      "tensor([28.9019], grad_fn=<AddBackward0>)\n",
      "tensor([9.1221], grad_fn=<AddBackward0>)\n",
      "tensor([33.2803], grad_fn=<AddBackward0>)\n",
      "tensor([10.8779], grad_fn=<AddBackward0>)\n",
      "tensor([13.4988], grad_fn=<AddBackward0>)\n",
      "tensor([8.3824], grad_fn=<AddBackward0>)\n",
      "tensor([3.6728], grad_fn=<AddBackward0>)\n",
      "tensor([8.2260], grad_fn=<AddBackward0>)\n",
      "tensor([28.9082], grad_fn=<AddBackward0>)\n",
      "tensor([9.1199], grad_fn=<AddBackward0>)\n",
      "tensor([33.2826], grad_fn=<AddBackward0>)\n",
      "tensor([10.8763], grad_fn=<AddBackward0>)\n",
      "tensor([13.4964], grad_fn=<AddBackward0>)\n",
      "tensor([8.3835], grad_fn=<AddBackward0>)\n",
      "tensor([3.6687], grad_fn=<AddBackward0>)\n",
      "tensor([8.2241], grad_fn=<AddBackward0>)\n",
      "tensor([28.9144], grad_fn=<AddBackward0>)\n",
      "tensor([9.1177], grad_fn=<AddBackward0>)\n",
      "tensor([33.2849], grad_fn=<AddBackward0>)\n",
      "tensor([10.8747], grad_fn=<AddBackward0>)\n",
      "tensor([13.4940], grad_fn=<AddBackward0>)\n",
      "tensor([8.3847], grad_fn=<AddBackward0>)\n",
      "tensor([3.6646], grad_fn=<AddBackward0>)\n",
      "tensor([8.2224], grad_fn=<AddBackward0>)\n",
      "tensor([28.9206], grad_fn=<AddBackward0>)\n",
      "tensor([9.1156], grad_fn=<AddBackward0>)\n",
      "tensor([33.2871], grad_fn=<AddBackward0>)\n",
      "tensor([10.8731], grad_fn=<AddBackward0>)\n",
      "tensor([13.4916], grad_fn=<AddBackward0>)\n",
      "tensor([8.3858], grad_fn=<AddBackward0>)\n",
      "tensor([3.6606], grad_fn=<AddBackward0>)\n",
      "tensor([8.2206], grad_fn=<AddBackward0>)\n",
      "tensor([28.9266], grad_fn=<AddBackward0>)\n",
      "tensor([9.1134], grad_fn=<AddBackward0>)\n",
      "tensor([33.2893], grad_fn=<AddBackward0>)\n",
      "tensor([10.8716], grad_fn=<AddBackward0>)\n",
      "tensor([13.4893], grad_fn=<AddBackward0>)\n",
      "tensor([8.3869], grad_fn=<AddBackward0>)\n",
      "tensor([3.6566], grad_fn=<AddBackward0>)\n",
      "tensor([8.2189], grad_fn=<AddBackward0>)\n",
      "tensor([28.9327], grad_fn=<AddBackward0>)\n",
      "tensor([9.1113], grad_fn=<AddBackward0>)\n",
      "tensor([33.2915], grad_fn=<AddBackward0>)\n",
      "tensor([10.8701], grad_fn=<AddBackward0>)\n",
      "tensor([13.4870], grad_fn=<AddBackward0>)\n",
      "tensor([8.3881], grad_fn=<AddBackward0>)\n",
      "tensor([3.6527], grad_fn=<AddBackward0>)\n",
      "tensor([8.2172], grad_fn=<AddBackward0>)\n",
      "tensor([28.9386], grad_fn=<AddBackward0>)\n",
      "tensor([9.1092], grad_fn=<AddBackward0>)\n",
      "tensor([33.2937], grad_fn=<AddBackward0>)\n",
      "tensor([10.8686], grad_fn=<AddBackward0>)\n",
      "tensor([13.4847], grad_fn=<AddBackward0>)\n",
      "tensor([8.3892], grad_fn=<AddBackward0>)\n",
      "tensor([3.6488], grad_fn=<AddBackward0>)\n",
      "tensor([8.2155], grad_fn=<AddBackward0>)\n",
      "tensor([28.9445], grad_fn=<AddBackward0>)\n",
      "tensor([9.1072], grad_fn=<AddBackward0>)\n",
      "tensor([33.2959], grad_fn=<AddBackward0>)\n",
      "tensor([10.8671], grad_fn=<AddBackward0>)\n",
      "tensor([13.4825], grad_fn=<AddBackward0>)\n",
      "tensor([8.3903], grad_fn=<AddBackward0>)\n",
      "tensor([3.6450], grad_fn=<AddBackward0>)\n",
      "tensor([8.2138], grad_fn=<AddBackward0>)\n",
      "tensor([28.9503], grad_fn=<AddBackward0>)\n",
      "tensor([9.1051], grad_fn=<AddBackward0>)\n",
      "tensor([33.2980], grad_fn=<AddBackward0>)\n",
      "tensor([10.8657], grad_fn=<AddBackward0>)\n",
      "tensor([13.4803], grad_fn=<AddBackward0>)\n",
      "tensor([8.3914], grad_fn=<AddBackward0>)\n",
      "tensor([3.6412], grad_fn=<AddBackward0>)\n",
      "tensor([8.2122], grad_fn=<AddBackward0>)\n",
      "tensor([28.9561], grad_fn=<AddBackward0>)\n",
      "tensor([9.1031], grad_fn=<AddBackward0>)\n",
      "tensor([33.3001], grad_fn=<AddBackward0>)\n",
      "tensor([10.8643], grad_fn=<AddBackward0>)\n",
      "tensor([13.4781], grad_fn=<AddBackward0>)\n",
      "tensor([8.3925], grad_fn=<AddBackward0>)\n",
      "tensor([3.6375], grad_fn=<AddBackward0>)\n",
      "tensor([8.2106], grad_fn=<AddBackward0>)\n",
      "tensor([28.9617], grad_fn=<AddBackward0>)\n",
      "tensor([9.1011], grad_fn=<AddBackward0>)\n",
      "tensor([33.3022], grad_fn=<AddBackward0>)\n",
      "tensor([10.8629], grad_fn=<AddBackward0>)\n",
      "tensor([13.4759], grad_fn=<AddBackward0>)\n",
      "tensor([8.3936], grad_fn=<AddBackward0>)\n",
      "tensor([3.6338], grad_fn=<AddBackward0>)\n",
      "tensor([8.2090], grad_fn=<AddBackward0>)\n",
      "tensor([28.9674], grad_fn=<AddBackward0>)\n",
      "tensor([9.0991], grad_fn=<AddBackward0>)\n",
      "tensor([33.3043], grad_fn=<AddBackward0>)\n",
      "tensor([10.8615], grad_fn=<AddBackward0>)\n",
      "tensor([13.4737], grad_fn=<AddBackward0>)\n",
      "tensor([8.3947], grad_fn=<AddBackward0>)\n",
      "tensor([3.6301], grad_fn=<AddBackward0>)\n",
      "tensor([8.2074], grad_fn=<AddBackward0>)\n",
      "tensor([28.9729], grad_fn=<AddBackward0>)\n",
      "tensor([9.0971], grad_fn=<AddBackward0>)\n",
      "tensor([33.3063], grad_fn=<AddBackward0>)\n",
      "tensor([10.8602], grad_fn=<AddBackward0>)\n",
      "tensor([13.4716], grad_fn=<AddBackward0>)\n",
      "tensor([8.3958], grad_fn=<AddBackward0>)\n",
      "tensor([3.6265], grad_fn=<AddBackward0>)\n",
      "tensor([8.2059], grad_fn=<AddBackward0>)\n",
      "tensor([28.9784], grad_fn=<AddBackward0>)\n",
      "tensor([9.0952], grad_fn=<AddBackward0>)\n",
      "tensor([33.3084], grad_fn=<AddBackward0>)\n",
      "tensor([10.8589], grad_fn=<AddBackward0>)\n",
      "tensor([13.4695], grad_fn=<AddBackward0>)\n",
      "tensor([8.3969], grad_fn=<AddBackward0>)\n",
      "tensor([3.6230], grad_fn=<AddBackward0>)\n",
      "tensor([8.2044], grad_fn=<AddBackward0>)\n",
      "tensor([28.9839], grad_fn=<AddBackward0>)\n",
      "tensor([9.0933], grad_fn=<AddBackward0>)\n",
      "tensor([33.3104], grad_fn=<AddBackward0>)\n",
      "tensor([10.8576], grad_fn=<AddBackward0>)\n",
      "tensor([13.4674], grad_fn=<AddBackward0>)\n",
      "tensor([8.3979], grad_fn=<AddBackward0>)\n",
      "tensor([3.6194], grad_fn=<AddBackward0>)\n",
      "tensor([8.2028], grad_fn=<AddBackward0>)\n",
      "tensor([28.9893], grad_fn=<AddBackward0>)\n",
      "tensor([9.0914], grad_fn=<AddBackward0>)\n",
      "tensor([33.3124], grad_fn=<AddBackward0>)\n",
      "tensor([10.8563], grad_fn=<AddBackward0>)\n",
      "tensor([13.4653], grad_fn=<AddBackward0>)\n",
      "tensor([8.3990], grad_fn=<AddBackward0>)\n",
      "tensor([3.6160], grad_fn=<AddBackward0>)\n",
      "tensor([8.2014], grad_fn=<AddBackward0>)\n",
      "tensor([28.9946], grad_fn=<AddBackward0>)\n",
      "tensor([9.0895], grad_fn=<AddBackward0>)\n",
      "tensor([33.3143], grad_fn=<AddBackward0>)\n",
      "tensor([10.8550], grad_fn=<AddBackward0>)\n",
      "tensor([13.4633], grad_fn=<AddBackward0>)\n",
      "tensor([8.4001], grad_fn=<AddBackward0>)\n",
      "tensor([3.6125], grad_fn=<AddBackward0>)\n",
      "tensor([8.1999], grad_fn=<AddBackward0>)\n",
      "tensor([28.9999], grad_fn=<AddBackward0>)\n",
      "tensor([9.0877], grad_fn=<AddBackward0>)\n",
      "tensor([33.3163], grad_fn=<AddBackward0>)\n",
      "tensor([10.8538], grad_fn=<AddBackward0>)\n",
      "tensor([13.4613], grad_fn=<AddBackward0>)\n",
      "tensor([8.4011], grad_fn=<AddBackward0>)\n",
      "tensor([3.6092], grad_fn=<AddBackward0>)\n",
      "tensor([8.1985], grad_fn=<AddBackward0>)\n",
      "tensor([29.0051], grad_fn=<AddBackward0>)\n",
      "tensor([9.0858], grad_fn=<AddBackward0>)\n",
      "tensor([33.3182], grad_fn=<AddBackward0>)\n",
      "tensor([10.8526], grad_fn=<AddBackward0>)\n",
      "tensor([13.4593], grad_fn=<AddBackward0>)\n",
      "tensor([8.4022], grad_fn=<AddBackward0>)\n",
      "tensor([3.6058], grad_fn=<AddBackward0>)\n",
      "tensor([8.1970], grad_fn=<AddBackward0>)\n",
      "tensor([29.0102], grad_fn=<AddBackward0>)\n",
      "tensor([9.0840], grad_fn=<AddBackward0>)\n",
      "tensor([33.3202], grad_fn=<AddBackward0>)\n",
      "tensor([10.8514], grad_fn=<AddBackward0>)\n",
      "tensor([13.4573], grad_fn=<AddBackward0>)\n",
      "tensor([8.4032], grad_fn=<AddBackward0>)\n",
      "tensor([3.6025], grad_fn=<AddBackward0>)\n",
      "tensor([8.1956], grad_fn=<AddBackward0>)\n",
      "tensor([29.0153], grad_fn=<AddBackward0>)\n",
      "tensor([9.0822], grad_fn=<AddBackward0>)\n",
      "tensor([33.3221], grad_fn=<AddBackward0>)\n",
      "tensor([10.8502], grad_fn=<AddBackward0>)\n",
      "tensor([13.4553], grad_fn=<AddBackward0>)\n",
      "tensor([8.4043], grad_fn=<AddBackward0>)\n",
      "tensor([3.5993], grad_fn=<AddBackward0>)\n",
      "tensor([8.1942], grad_fn=<AddBackward0>)\n",
      "tensor([29.0203], grad_fn=<AddBackward0>)\n",
      "tensor([9.0805], grad_fn=<AddBackward0>)\n",
      "tensor([33.3239], grad_fn=<AddBackward0>)\n",
      "tensor([10.8490], grad_fn=<AddBackward0>)\n",
      "tensor([13.4534], grad_fn=<AddBackward0>)\n",
      "tensor([8.4053], grad_fn=<AddBackward0>)\n",
      "tensor([3.5960], grad_fn=<AddBackward0>)\n",
      "tensor([8.1929], grad_fn=<AddBackward0>)\n",
      "tensor([29.0253], grad_fn=<AddBackward0>)\n",
      "tensor([9.0787], grad_fn=<AddBackward0>)\n",
      "tensor([33.3258], grad_fn=<AddBackward0>)\n",
      "tensor([10.8479], grad_fn=<AddBackward0>)\n",
      "tensor([13.4515], grad_fn=<AddBackward0>)\n",
      "tensor([8.4063], grad_fn=<AddBackward0>)\n",
      "tensor([3.5929], grad_fn=<AddBackward0>)\n",
      "tensor([8.1915], grad_fn=<AddBackward0>)\n",
      "tensor([29.0302], grad_fn=<AddBackward0>)\n",
      "tensor([9.0770], grad_fn=<AddBackward0>)\n",
      "tensor([33.3276], grad_fn=<AddBackward0>)\n",
      "tensor([10.8468], grad_fn=<AddBackward0>)\n",
      "tensor([13.4496], grad_fn=<AddBackward0>)\n",
      "tensor([8.4073], grad_fn=<AddBackward0>)\n",
      "tensor([3.5897], grad_fn=<AddBackward0>)\n",
      "tensor([8.1902], grad_fn=<AddBackward0>)\n",
      "tensor([29.0351], grad_fn=<AddBackward0>)\n",
      "tensor([9.0752], grad_fn=<AddBackward0>)\n",
      "tensor([33.3294], grad_fn=<AddBackward0>)\n",
      "tensor([10.8457], grad_fn=<AddBackward0>)\n",
      "tensor([13.4477], grad_fn=<AddBackward0>)\n",
      "tensor([8.4083], grad_fn=<AddBackward0>)\n",
      "tensor([3.5866], grad_fn=<AddBackward0>)\n",
      "tensor([8.1889], grad_fn=<AddBackward0>)\n",
      "tensor([29.0399], grad_fn=<AddBackward0>)\n",
      "tensor([9.0735], grad_fn=<AddBackward0>)\n",
      "tensor([33.3312], grad_fn=<AddBackward0>)\n",
      "tensor([10.8446], grad_fn=<AddBackward0>)\n",
      "tensor([13.4459], grad_fn=<AddBackward0>)\n",
      "tensor([8.4093], grad_fn=<AddBackward0>)\n",
      "tensor([3.5835], grad_fn=<AddBackward0>)\n",
      "tensor([8.1876], grad_fn=<AddBackward0>)\n",
      "tensor([29.0446], grad_fn=<AddBackward0>)\n",
      "tensor([9.0719], grad_fn=<AddBackward0>)\n",
      "tensor([33.3330], grad_fn=<AddBackward0>)\n",
      "tensor([10.8435], grad_fn=<AddBackward0>)\n",
      "tensor([13.4441], grad_fn=<AddBackward0>)\n",
      "tensor([8.4103], grad_fn=<AddBackward0>)\n",
      "tensor([3.5805], grad_fn=<AddBackward0>)\n",
      "tensor([8.1863], grad_fn=<AddBackward0>)\n",
      "tensor([29.0493], grad_fn=<AddBackward0>)\n",
      "tensor([9.0702], grad_fn=<AddBackward0>)\n",
      "tensor([33.3348], grad_fn=<AddBackward0>)\n",
      "tensor([10.8425], grad_fn=<AddBackward0>)\n",
      "tensor([13.4423], grad_fn=<AddBackward0>)\n",
      "tensor([8.4113], grad_fn=<AddBackward0>)\n",
      "tensor([3.5775], grad_fn=<AddBackward0>)\n",
      "tensor([8.1850], grad_fn=<AddBackward0>)\n",
      "tensor([29.0540], grad_fn=<AddBackward0>)\n",
      "tensor([9.0686], grad_fn=<AddBackward0>)\n",
      "tensor([33.3365], grad_fn=<AddBackward0>)\n",
      "tensor([10.8414], grad_fn=<AddBackward0>)\n",
      "tensor([13.4405], grad_fn=<AddBackward0>)\n",
      "tensor([8.4123], grad_fn=<AddBackward0>)\n",
      "tensor([3.5746], grad_fn=<AddBackward0>)\n",
      "tensor([8.1838], grad_fn=<AddBackward0>)\n",
      "tensor([29.0586], grad_fn=<AddBackward0>)\n",
      "tensor([9.0669], grad_fn=<AddBackward0>)\n",
      "tensor([33.3383], grad_fn=<AddBackward0>)\n",
      "tensor([10.8404], grad_fn=<AddBackward0>)\n",
      "tensor([13.4387], grad_fn=<AddBackward0>)\n",
      "tensor([8.4133], grad_fn=<AddBackward0>)\n",
      "tensor([3.5717], grad_fn=<AddBackward0>)\n",
      "tensor([8.1826], grad_fn=<AddBackward0>)\n",
      "tensor([29.0631], grad_fn=<AddBackward0>)\n",
      "tensor([9.0653], grad_fn=<AddBackward0>)\n",
      "tensor([33.3400], grad_fn=<AddBackward0>)\n",
      "tensor([10.8394], grad_fn=<AddBackward0>)\n",
      "tensor([13.4370], grad_fn=<AddBackward0>)\n",
      "tensor([8.4143], grad_fn=<AddBackward0>)\n",
      "tensor([3.5688], grad_fn=<AddBackward0>)\n",
      "tensor([8.1814], grad_fn=<AddBackward0>)\n",
      "tensor([29.0676], grad_fn=<AddBackward0>)\n",
      "tensor([9.0637], grad_fn=<AddBackward0>)\n",
      "tensor([33.3417], grad_fn=<AddBackward0>)\n",
      "tensor([10.8384], grad_fn=<AddBackward0>)\n",
      "tensor([13.4352], grad_fn=<AddBackward0>)\n",
      "tensor([8.4152], grad_fn=<AddBackward0>)\n",
      "tensor([3.5659], grad_fn=<AddBackward0>)\n",
      "tensor([8.1802], grad_fn=<AddBackward0>)\n",
      "tensor([29.0720], grad_fn=<AddBackward0>)\n",
      "tensor([9.0622], grad_fn=<AddBackward0>)\n",
      "tensor([33.3433], grad_fn=<AddBackward0>)\n",
      "tensor([10.8375], grad_fn=<AddBackward0>)\n",
      "tensor([13.4335], grad_fn=<AddBackward0>)\n",
      "tensor([8.4162], grad_fn=<AddBackward0>)\n",
      "tensor([3.5631], grad_fn=<AddBackward0>)\n",
      "tensor([8.1790], grad_fn=<AddBackward0>)\n",
      "tensor([29.0764], grad_fn=<AddBackward0>)\n",
      "tensor([9.0606], grad_fn=<AddBackward0>)\n",
      "tensor([33.3450], grad_fn=<AddBackward0>)\n",
      "tensor([10.8365], grad_fn=<AddBackward0>)\n",
      "tensor([13.4318], grad_fn=<AddBackward0>)\n",
      "tensor([8.4171], grad_fn=<AddBackward0>)\n",
      "tensor([3.5604], grad_fn=<AddBackward0>)\n",
      "tensor([8.1778], grad_fn=<AddBackward0>)\n",
      "tensor([29.0808], grad_fn=<AddBackward0>)\n",
      "tensor([9.0591], grad_fn=<AddBackward0>)\n",
      "tensor([33.3466], grad_fn=<AddBackward0>)\n",
      "tensor([10.8356], grad_fn=<AddBackward0>)\n",
      "tensor([13.4302], grad_fn=<AddBackward0>)\n",
      "tensor([8.4181], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-101-0c88cfda83e4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m             \u001B[0mtotal_train_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mtrain_loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             \u001B[0mtrain_loss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0;31m# if ep % 100 == 0: # Only print every 100 epoch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    164\u001B[0m                 \u001B[0mproducts\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mDefaults\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m         \"\"\"\n\u001B[0;32m--> 166\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m     91\u001B[0m         \u001B[0mgrad_tensors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad_tensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 93\u001B[0;31m     \u001B[0mgrad_tensors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_make_grads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mretain_graph\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/spinningup/lib/python3.6/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36m_make_grads\u001B[0;34m(outputs, grads)\u001B[0m\n\u001B[1;32m     33\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m                     \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"grad can be implicitly created only for scalar outputs\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m                 \u001B[0mnew_grads\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mones_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m                 \u001B[0mnew_grads\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "h_epoch = 10000  # Hyperparameter epoch\n",
    "epoch = 1000  # Epoch for training\n",
    "\n",
    "#Create underlying linear function\n",
    "x = torch.randn((10, 2))\n",
    "true_w = torch.tensor([[3.], [1.]])\n",
    "y = torch.matmul(x, true_w) + torch.randn((10, 1))\n",
    "\n",
    "# Split train_valid\n",
    "x_train = x[:8, ]\n",
    "y_train = y[:8, ]\n",
    "\n",
    "x_valid = x[8:, ]\n",
    "y_valid = y[8:, ]\n",
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[2.5], [1.3]], requires_grad=True)\n",
    "lamb = torch.tensor([3.,3.], requires_grad=True)  #Intentionally high value\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr = 0.001)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])\n",
    "\n",
    "# Note the update is currently very noisy\n",
    "# Define the loop\n",
    "for hep in range(h_epoch):\n",
    "    # Train (SGD)\n",
    "    for ep in range(epoch):\n",
    "        total_train_loss = 0\n",
    "        for i in range(len(x_train)):\n",
    "            optimizer.zero_grad()\n",
    "            y_predicted = torch.matmul(x_train[i], w)\n",
    "            train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb * torch.sum(w ** 2)\n",
    "            print(train_loss)\n",
    "            total_train_loss += train_loss\n",
    "            train_loss.backward(create_graph=True)\n",
    "            optimizer.step()\n",
    "        # if ep % 100 == 0: # Only print every 100 epoch\n",
    "        #     print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / len(x_train)))\n",
    "\n",
    "    # Train the hyperparameter\n",
    "    total_d_val_loss_d_lamb = torch.zeros(lamb.size())\n",
    "    d_valid_loss_d_w = torch.zeros(w.size())\n",
    "    for i in range(len(x_valid)):\n",
    "        w.grad.zero_()\n",
    "        y_predicted = torch.matmul(x_valid[i], w)\n",
    "        valid_loss = torch.nn.functional.mse_loss(y_predicted, y_valid[i])\n",
    "        valid_loss_grad = grad(valid_loss, w)\n",
    "        d_valid_loss_d_w += valid_loss_grad[0]\n",
    "    d_valid_loss_d_w /= len(x_valid)\n",
    "\n",
    "    for i in range(len(x_train)):\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb * torch.sum(w ** 2)\n",
    "        w.grad.zero_(), h_optimizer.zero_grad()\n",
    "        d_train_loss_d_w = grad(train_loss, w, create_graph=True)\n",
    "\n",
    "        w.grad.zero_(), h_optimizer.zero_grad()\n",
    "        d_train_loss_d_w[0].backward(d_valid_loss_d_w)\n",
    "\n",
    "        if lamb.grad is not None:\n",
    "            total_d_val_loss_d_lamb -= lamb.grad\n",
    "    total_d_val_loss_d_lamb /= len(x_train)\n",
    "\n",
    "    lamb.grad = total_d_val_loss_d_lamb\n",
    "    h_optimizer.step()\n",
    "\n",
    "    w.grad.zero_(), h_optimizer.zero_grad()\n",
    "    print('lamb after epoch '+ str(hep) + ': ' + str(lamb))\n",
    "    print('w value: ', w)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}