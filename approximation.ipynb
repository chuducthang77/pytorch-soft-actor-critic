{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Re-program the implicit differentiation optimization to check whether the program is corrected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch\n",
    "#\n",
    "#\n",
    "# class NeuralTest(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralTest, self).__init__()\n",
    "#         self.layer = nn.Linear(3, 1)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return self.layer(x)\n",
    "#\n",
    "#\n",
    "# x = torch.rand(3)\n",
    "# net = NeuralTest()\n",
    "# opt = torch.optim.Adam(net.parameters())\n",
    "# print(list(net.parameters()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad, Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 0: tensor([12.0726], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.6381], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4880], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4804], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4956], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.9000], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4933], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4878], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4844], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4829], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4819], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.8237], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4787], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4790], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4793], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4794], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4795], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.7632], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4773], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4772], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4772], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4771], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4771], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.7110], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4752], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4753], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4752], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4752], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4752], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.6644], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4734], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4735], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4734], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4734], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4734], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.6218], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4718], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4718], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4718], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4718], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4718], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.5823], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4703], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4703], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4703], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4702], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4702], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.5453], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4688], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4688], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4688], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4688], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4688], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.5104], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4674], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4674], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4674], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4674], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4674], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.4771], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4661], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4661], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4661], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4661], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4660], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.4453], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4648], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4648], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4648], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4648], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4648], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.4148], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4636], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4636], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4636], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4635], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4635], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.3854], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4624], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4624], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4624], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4623], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4623], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.3570], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4612], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4612], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4612], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4612], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4612], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.3295], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4601], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4601], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4601], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4600], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4600], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.3027], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4589], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4590], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4589], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4589], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4589], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.2767], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4579], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4579], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4578], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4578], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4578], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.2514], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4568], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4568], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4568], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4568], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4567], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.2266], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4557], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4558], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4557], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4557], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4557], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.2025], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4547], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4547], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4547], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4547], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4547], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.1788], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4537], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4537], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4537], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4537], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4537], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.1556], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4527], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4527], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4527], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4527], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4527], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.1328], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4517], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4517], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4517], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4517], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4517], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.1105], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4507], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4508], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4507], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4507], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4507], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.0885], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4498], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4498], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4498], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4498], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4497], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.0669], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4488], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4488], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4488], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4488], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4488], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.0457], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4479], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4479], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4479], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4479], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4479], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.0247], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4470], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4470], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4470], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4469], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4469], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([9.0041], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4461], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4461], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4460], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4460], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4460], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.9838], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4451], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4452], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4451], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4451], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4451], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.9637], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4442], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4443], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4442], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4442], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4442], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.9439], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4434], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4434], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4434], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4433], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4433], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.9243], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4425], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4425], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4425], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4424], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4424], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.9050], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4416], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4416], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4416], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4416], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4416], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.8859], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4407], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4407], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4407], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4407], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4407], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.8670], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4399], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4399], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4398], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4398], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4398], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.8483], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4390], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4390], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4390], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4390], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4390], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.8298], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4381], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4382], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4381], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4381], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4381], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.8115], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4373], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4373], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4373], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4373], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4373], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.7934], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4364], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4365], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4364], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4364], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4364], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.7754], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4356], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4356], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4356], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4356], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4356], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.7576], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4348], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4348], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4348], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4348], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4347], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.7400], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4339], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4340], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4339], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4339], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4339], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.7225], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4331], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4331], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4331], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4331], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4331], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.7052], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4323], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4323], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4323], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4323], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4323], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.6880], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4315], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4315], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4315], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4315], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4314], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.6709], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4307], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4307], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4307], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4306], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4306], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.6540], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4298], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4299], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4298], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4298], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4298], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.6372], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4290], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4290], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4290], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4290], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4290], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.6205], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4282], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4282], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4282], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4282], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4282], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.6040], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4274], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4274], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4274], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4274], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4274], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.5875], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4266], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4266], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4266], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4266], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4266], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.5712], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4258], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4258], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4258], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4258], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4258], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.5550], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4250], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4250], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4250], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4250], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4250], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.5389], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4242], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4242], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4242], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4242], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4242], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.5228], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4234], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4235], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4234], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4234], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4234], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.5069], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4227], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4227], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4227], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4226], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4226], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.4911], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4219], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4219], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4219], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4218], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4218], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.4754], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4211], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4211], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4211], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4211], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4211], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.4597], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4203], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4203], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4203], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4203], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4203], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.4441], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4195], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4195], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4195], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4195], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4195], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.4287], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4187], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4187], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4187], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4187], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4187], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.4133], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4180], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4180], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4179], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4179], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4179], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.3980], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4172], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4172], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4172], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4172], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4171], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.3827], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4164], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4164], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4164], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4164], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4164], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.3676], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4156], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4156], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4156], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4156], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4156], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.3525], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4148], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4149], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4148], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4148], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4148], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.3375], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4141], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4141], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4141], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4141], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4140], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.3225], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4133], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4133], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4133], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4133], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4133], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.3077], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4125], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4125], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4125], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4125], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4125], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.2928], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4118], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4118], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4118], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4117], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4117], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.2781], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4110], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4110], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4110], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4110], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4110], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.2634], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4102], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4102], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4102], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4102], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4102], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.2488], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4094], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4095], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4094], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4094], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4094], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.2342], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4087], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4087], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4087], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4087], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4086], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.2197], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4079], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4079], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4079], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4079], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4079], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.2053], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4071], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4072], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4071], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4071], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4071], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.1909], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4064], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4064], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4064], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4064], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4063], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.1766], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4056], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4056], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4056], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4056], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4056], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.1623], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4048], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4048], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4048], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4048], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4048], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.1480], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4041], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4041], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4041], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4041], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4040], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.1339], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4033], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4033], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4033], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4033], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4033], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.1197], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4025], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4025], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4025], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4025], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4025], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.1056], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4018], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4018], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4018], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4018], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4017], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.0916], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4010], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4010], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4010], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4010], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4010], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.0776], requires_grad=True)\n",
      "Train loss at 0: tensor([11.4002], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.4003], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.4002], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.4002], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.4002], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.0637], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3995], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3995], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3995], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3995], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3994], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.0498], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3987], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3987], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3987], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3987], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3987], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.0359], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3979], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3980], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3979], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3979], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3979], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.0221], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3972], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3972], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3972], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3972], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3971], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([8.0083], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3964], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3964], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3964], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3964], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3964], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.9946], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3956], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3957], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3956], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3956], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3956], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.9809], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3949], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3949], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3949], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3949], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3949], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.9673], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3941], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3941], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3941], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3941], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3941], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.9537], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3933], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3933], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3933], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3933], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3933], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.9401], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3926], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3926], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3926], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3926], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3925], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.9265], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3918], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3918], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3918], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3918], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3918], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.9130], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3910], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3910], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3910], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3910], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3910], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.8996], requires_grad=True)\n",
      "Train loss at 0: tensor([11.3903], grad_fn=<DivBackward0>)\n",
      "Train loss at 1: tensor([11.3903], grad_fn=<DivBackward0>)\n",
      "Train loss at 2: tensor([11.3903], grad_fn=<DivBackward0>)\n",
      "Train loss at 3: tensor([11.3903], grad_fn=<DivBackward0>)\n",
      "Train loss at 4: tensor([11.3902], grad_fn=<DivBackward0>)\n",
      "lamb after epoch 100: tensor([7.8861], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "h_epoch = 100  # Hyperparameter epoch\n",
    "epoch = 5  # Epoch for training\n",
    "\n",
    "#Create underlying linear function\n",
    "x = torch.rand((10, 2))\n",
    "true_w = torch.tensor([[3.], [1.]])\n",
    "y = torch.matmul(x, true_w) + torch.randn((10, 1))\n",
    "\n",
    "# Split train_valid\n",
    "x_train = x[:8, ]\n",
    "y_train = y[:8, ]\n",
    "\n",
    "x_valid = x[8:, ]\n",
    "y_valid = y[8:, ]\n",
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[0.08], [0.26]], requires_grad=True)\n",
    "lamb = torch.tensor([10.], requires_grad=True)  #Intentionally high value\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr=0.01)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])\n",
    "\n",
    "# Note the update is currently very noisy\n",
    "# Define the loop\n",
    "for _ in range(h_epoch):\n",
    "    # Train (SGD)\n",
    "    for ep in range(epoch):\n",
    "        total_train_loss = 0\n",
    "        for i in range(len(x_train)):\n",
    "            optimizer.zero_grad()\n",
    "            y_predicted = torch.matmul(x_train[i], w)\n",
    "            train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb * torch.sum(w ** 2)\n",
    "            total_train_loss += train_loss\n",
    "            train_loss.backward(create_graph=True)\n",
    "            optimizer.step()\n",
    "        print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / 3))\n",
    "\n",
    "    # Update the optimizer\n",
    "    total_d_val_loss_d_lamb = torch.zeros(lamb.size())\n",
    "    d_valid_loss_d_w = torch.zeros(w.size())\n",
    "    for i in range(len(x_valid)):\n",
    "        w.grad.zero_()\n",
    "        y_predicted = torch.matmul(x_valid[i], w)\n",
    "        valid_loss = torch.nn.functional.mse_loss(y_predicted, y_valid[i])\n",
    "        valid_loss_grad = grad(valid_loss, w)\n",
    "        d_valid_loss_d_w += valid_loss_grad[0]\n",
    "    d_valid_loss_d_w /= 8\n",
    "\n",
    "    for i in range(len(x_train)):\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb * torch.sum(w ** 2)\n",
    "        w.grad.zero_(), h_optimizer.zero_grad()\n",
    "        d_train_loss_d_w = grad(train_loss, w, create_graph=True)\n",
    "\n",
    "        w.grad.zero_(), h_optimizer.zero_grad()\n",
    "        d_train_loss_d_w[0].backward(d_valid_loss_d_w)\n",
    "\n",
    "        if lamb.grad is not None:\n",
    "            total_d_val_loss_d_lamb -= lamb.grad\n",
    "    total_d_val_loss_d_lamb /= 2\n",
    "\n",
    "    lamb.grad = total_d_val_loss_d_lamb\n",
    "    h_optimizer.step()\n",
    "\n",
    "    w.grad.zero_(), h_optimizer.zero_grad()\n",
    "    print('lamb after epoch '+ str(h_epoch) + ': ' + str(lamb))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def gather_flat_grad(loss_grad):\n",
    "    #Helper function to flatten the grad\n",
    "    return torch.cat([p.view(-1) for p in loss_grad])  #g_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
