{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Re-program the implicit differentiation optimization to check whether the program is corrected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad, Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gather_flat_grad(loss_grad):\n",
    "    #Helper function to flatten the grad\n",
    "    return torch.cat([p.view(-1) for p in loss_grad])  #g_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "- Even I set the the weight to be very close to w, the loss still fuctuates.\n",
    "    - Thought: Maybe the update is too stochastic? Learning rate too high? or rand in x? Too small dataset?, no bias term?\n",
    "    - Answer: rand in x, Fix by randn in x, increases number of epochs\n",
    "- Predicted loss < True loss means do we find a better line than the one we are generated?\n",
    "    - Happens when x is generated with rand (uniform generation) and when regularization\n",
    "\n",
    "Conclusions:\n",
    "- The SGD solution is approximately equal to the closed-form solution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# THIS BLOCK SERVES AS AN EXACT SOLUTION\n",
    "w_hat = torch.matmul(torch.matmul(torch.inverse(torch.matmul(x_train.T, x_train)), x_train.T), y_train)\n",
    "print('w_hat: ', w_hat)\n",
    "y_train_predicted = torch.matmul(x_train, w_hat)\n",
    "print('y_predicted: ', y_train_predicted)\n",
    "loss = torch.nn.functional.mse_loss(y_train_predicted, y_train)\n",
    "print('loss: ', loss)\n",
    "true_loss = torch.nn.functional.mse_loss(torch.matmul(x_train, true_w), y_train)\n",
    "print('true_loss: ', true_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# THIS BLOCK SERVES AS THE SANITY CHECK FOR THE MAIN TRAINING PROCESS\n",
    "torch.manual_seed(1)\n",
    "h_epoch = 10000  # Hyperparameter epoch\n",
    "epoch = 1000  # Epoch for training\n",
    "\n",
    "#Create underlying linear function\n",
    "x = torch.randn((10, 2))\n",
    "true_w = torch.tensor([[3.], [1.]])\n",
    "y = torch.matmul(x, true_w) + torch.randn((10, 1))\n",
    "\n",
    "# Split train_valid\n",
    "x_train = x[:8, ]\n",
    "y_train = y[:8, ]\n",
    "\n",
    "x_valid = x[8:, ]\n",
    "y_valid = y[8:, ]\n",
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[2.5], [1.3]], requires_grad=True)\n",
    "lamb = torch.tensor([3.], requires_grad=True)  #Intentionally high value\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr = 0.001)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])\n",
    "for ep in range(epoch):\n",
    "    total_train_loss = 0\n",
    "    for i in range(len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i])\n",
    "        total_train_loss += train_loss\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / len(x_train)))\n",
    "    print('w: ', w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This Section is using L2 regularization\n",
    "Observations:\n",
    "- The higher the lamb value, the higher the training loss and the more difference between closed-form weight solution and SGD weight solution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w_hat = torch.matmul(torch.matmul(torch.inverse(torch.matmul(x_train.T, x_train) + lamb * torch.eye(2)), x_train.T), y_train)\n",
    "print('w_hat: ', w_hat)\n",
    "y_train_predicted = torch.matmul(x_train, w_hat)\n",
    "print('y_predicted: ', y_train_predicted)\n",
    "loss = torch.nn.functional.mse_loss(y_train_predicted, y_train)\n",
    "print('loss: ', loss)\n",
    "true_loss = torch.nn.functional.mse_loss(torch.matmul(x_train, true_w), y_train)\n",
    "print('true_loss: ', true_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# THIS BLOCK SERVES AS THE SANITY CHECK FOR THE MAIN TRAINING PROCESS\n",
    "torch.manual_seed(1)\n",
    "h_epoch = 10000  # Hyperparameter epoch\n",
    "epoch = 1000  # Epoch for training\n",
    "\n",
    "#Create underlying linear function\n",
    "x = torch.randn((10, 2))\n",
    "true_w = torch.tensor([[3.], [1.]])\n",
    "y = torch.matmul(x, true_w) + torch.randn((10, 1))\n",
    "\n",
    "# Split train_valid\n",
    "x_train = x[:8, ]\n",
    "y_train = y[:8, ]\n",
    "\n",
    "x_valid = x[8:, ]\n",
    "y_valid = y[8:, ]\n",
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[2.5], [1.3]], requires_grad=True)\n",
    "lamb = torch.tensor([0.01], requires_grad=True)  #Change the value form 3 to 0.1 and 0.01 and observe the behavior\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr = 0.001)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])\n",
    "for ep in range(epoch):\n",
    "    total_train_loss = 0\n",
    "    for i in range(len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb  * torch.sum(w ** 2)\n",
    "        total_train_loss += train_loss\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / len(x_train)))\n",
    "    print('w: ', w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following block combines the optimization of weight and hyperparameter together\n",
    "Observations:\n",
    "    - The training loss still decreases even when the hyperparameter is wrong, but after hyperparameter adjusts, the training loss seems to decrease compared to previous hepoch, but not during epoch.\n",
    "        - Question: Does this mean the loss already converge? Does this mean optimizing lambda is not important?\n",
    "        - Thought: Which hyperparameter should we optimize?\n",
    "    - After epoch 200, the hyperparameter becomes negative, but stop at -0.0927. Used the optimal weight, the loss is even higher compared with the lambda = 0.01\n",
    "        - Question: Overfitting?\n",
    "    - Training epoch vs Hyperparam epoch\n",
    "    - Note: We have individual lambda for each weight in implicit optimization code\n",
    "    - Note: Train loss and validation loss in implicit opt code always decrease, train acc and validation acc also increases, but the test loss INCREASES, and the test acc DOES NOT improve\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "h_epoch = 10000  # Hyperparameter epoch\n",
    "epoch = 1000  # Epoch for training\n",
    "\n",
    "#Create underlying linear function\n",
    "x = torch.randn((10, 2)).cuda()\n",
    "true_w = torch.tensor([[3.], [1.]]).cuda()\n",
    "y = torch.matmul(x, true_w) + torch.randn((10, 1))\n",
    "\n",
    "# Split train_valid\n",
    "x_train = x[:8, ]\n",
    "y_train = y[:8, ].cuda()\n",
    "\n",
    "x_valid = x[8:, ]\n",
    "y_valid = y[8:, ].cuda()\n",
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[2.5], [1.3]], requires_grad=True).cuda()\n",
    "lamb = torch.tensor([3.], requires_grad=True).cuda()  #Intentionally high value\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr = 0.001)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])\n",
    "\n",
    "# Note the update is currently very noisy\n",
    "# Define the loop\n",
    "for hep in range(h_epoch):\n",
    "    # Train (SGD)\n",
    "    for ep in range(epoch):\n",
    "        total_train_loss = 0\n",
    "        for i in range(len(x_train)):\n",
    "            optimizer.zero_grad()\n",
    "            y_predicted = torch.matmul(x_train[i], w)\n",
    "            train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb * torch.sum(w ** 2)\n",
    "            total_train_loss += train_loss\n",
    "            train_loss.backward(create_graph=True)\n",
    "            optimizer.step()\n",
    "        # if ep % 100 == 0: # Only print every 100 epoch\n",
    "        #     print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / len(x_train)))\n",
    "\n",
    "    # Train the hyperparameter\n",
    "    total_d_val_loss_d_lamb = torch.zeros(lamb.size())\n",
    "    d_valid_loss_d_w = torch.zeros(w.size())\n",
    "    for i in range(len(x_valid)):\n",
    "        w.grad.zero_()\n",
    "        y_predicted = torch.matmul(x_valid[i], w)\n",
    "        valid_loss = torch.nn.functional.mse_loss(y_predicted, y_valid[i])\n",
    "        valid_loss_grad = grad(valid_loss, w)\n",
    "        d_valid_loss_d_w += valid_loss_grad[0]\n",
    "    d_valid_loss_d_w /= len(x_valid)\n",
    "\n",
    "    for i in range(len(x_train)):\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i]) + lamb * torch.sum(w ** 2)\n",
    "        w.grad.zero_(), h_optimizer.zero_grad()\n",
    "        d_train_loss_d_w = grad(train_loss, w, create_graph=True)\n",
    "\n",
    "        w.grad.zero_(), h_optimizer.zero_grad()\n",
    "        d_train_loss_d_w[0].backward(d_valid_loss_d_w)\n",
    "\n",
    "        if lamb.grad is not None:\n",
    "            total_d_val_loss_d_lamb -= lamb.grad\n",
    "    total_d_val_loss_d_lamb /= len(x_train)\n",
    "\n",
    "    lamb.grad = total_d_val_loss_d_lamb\n",
    "    h_optimizer.step()\n",
    "\n",
    "    w.grad.zero_(), h_optimizer.zero_grad()\n",
    "    print('lamb after epoch '+ str(hep) + ': ' + str(lamb))\n",
    "    print('w value: ', w)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the output with optimal weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_lambda = -0.0927\n",
    "w_hat = torch.tensor([[4.0337], [1.2063]])\n",
    "# w_hat = torch.matmul(torch.matmul(torch.inverse(torch.matmul(x_train.T, x_train) + lamb * torch.eye(2)), x_train.T), y_train)\n",
    "print('w_hat: ', w_hat)\n",
    "y_train_predicted = torch.matmul(x_train, w_hat)\n",
    "print('y_predicted: ', y_train_predicted)\n",
    "loss = torch.nn.functional.mse_loss(y_train_predicted, y_train)\n",
    "print('loss: ', loss)\n",
    "true_loss = torch.nn.functional.mse_loss(torch.matmul(x_train, true_w), y_train)\n",
    "print('true_loss: ', true_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import grad, Variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Redesign the experiment: Creating an overfitting situation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Generate the dataset\n",
    "x = torch.randn((30,1))\n",
    "size_x = x.size()\n",
    "poly_x = torch.cat((x, x**2, x**3, x**4, x**5), 1)\n",
    "\n",
    "# Add one column to learn intercept terms\n",
    "x_ones = torch.cat((torch.ones(size_x), x),1)\n",
    "poly_x_ones = torch.cat((torch.ones(size_x),poly_x), 1)\n",
    "\n",
    "true_w = torch.tensor([[1.], [2.]])\n",
    "y = torch.matmul(x_ones, true_w) + torch.randn(size_x)\n",
    "true_y = torch.matmul(x_ones, true_w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7fe4097e5cf8>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, true_y, color='y', label='True line')\n",
    "plt.legend(loc='best')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Separate the train and the valid\n",
    "x_train = x_ones[:25,]\n",
    "y_train = y[:25,]\n",
    "\n",
    "x_valid = x_ones[25:,]\n",
    "y_valid = y[25:,]\n",
    "\n",
    "poly_x_train = poly_x_ones[:25,]\n",
    "poly_x_valid = poly_x_ones[25:,]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Parameters and hyperparameters\n",
    "w = torch.tensor([[2.5], [1.3]], requires_grad=True)\n",
    "poly_w = torch.tensor([[2.5], [1.3], [2.], [1.], [-1.], [-0.5]], requires_grad=True)\n",
    "lamb = torch.tensor([3.], requires_grad=True)  #Intentionally high value\n",
    "\n",
    "#Define optimizer (Note: The choice of optimizer is similar to the problem setting)\n",
    "optimizer = torch.optim.Adam([w], lr = 0.001)\n",
    "poly_optimizer = torch.optim.Adam([poly_w], lr=0.001)\n",
    "h_optimizer = torch.optim.RMSprop([lamb])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate w_hat using closed-form solution (linear function)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat:  tensor([[1.1714],\n",
      "        [2.0775]])\n",
      "train loss:  tensor(0.5160)\n",
      "val loss:  tensor(0.9862)\n"
     ]
    }
   ],
   "source": [
    "w_hat = torch.matmul(torch.matmul(torch.inverse(torch.matmul(x_train.T, x_train)), x_train.T), y_train)\n",
    "print('w_hat: ', w_hat)\n",
    "y_train_predicted = torch.matmul(x_train, w_hat)\n",
    "loss = torch.nn.functional.mse_loss(y_train_predicted, y_train)\n",
    "print('train loss: ', loss)\n",
    "y_valid_predicted = torch.matmul(x_valid, w_hat)\n",
    "loss = torch.nn.functional.mse_loss(y_valid_predicted, y_valid)\n",
    "print('val loss: ', loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate poly_w_hat using closed-form solution (5-th order function)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly_w_hat:  tensor([[ 1.0745],\n",
      "        [ 0.1122],\n",
      "        [-1.9558],\n",
      "        [ 1.3245],\n",
      "        [ 0.7126],\n",
      "        [-0.3005]])\n",
      "poly train loss:  tensor(0.3196)\n",
      "poly val loss:  tensor(2.5492)\n"
     ]
    }
   ],
   "source": [
    "poly_w_hat = torch.matmul(torch.matmul(torch.inverse(torch.matmul(poly_x_train.T, poly_x_train)), poly_x_train.T), y_train)\n",
    "print('poly_w_hat: ', poly_w_hat)\n",
    "poly_y_train_predicted = torch.matmul(poly_x_train, poly_w_hat)\n",
    "poly_train_loss = torch.nn.functional.mse_loss(poly_y_train_predicted, y_train)\n",
    "print('poly train loss: ', poly_train_loss)\n",
    "poly_y_valid_predicted = torch.matmul(poly_x_valid, poly_w_hat)\n",
    "poly_val_loss = torch.nn.functional.mse_loss(poly_y_valid_predicted, y_valid)\n",
    "print('poly val loss: ', poly_val_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note:\n",
    "- Train loss is smaller, but validation loss for polynomial regression is greater than linear regression.\n",
    "- The weights for first order are similar to true weights, whereas the weights for 5-th order are very off.\n",
    "Conclusion:\n",
    "- Overfitting!! (The plot below shows this issue)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7fe40967bf98>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x, y here simply for visualization\n",
    "x_plot = torch.linspace(-2, 3, steps = 30)\n",
    "x_plot = x_plot.resize_((x_plot.size(0), 1))\n",
    "plot_size = x_plot.size()\n",
    "x_plot_ones = torch.cat((torch.ones(plot_size), x_plot), 1)\n",
    "poly_x_plot_ones = torch.cat((torch.ones(plot_size), x_plot, x_plot**2, x_plot**3,x_plot**4,x_plot**5),1)\n",
    "y_plot = torch.matmul(x_plot_ones, true_w)\n",
    "\n",
    "plt.clf()\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_plot, y_plot, color='y', label='True line')\n",
    "plt.plot(x_plot, torch.matmul(x_plot_ones, w_hat), color='g', label='Predicted linear line')\n",
    "plt.plot(x_plot, torch.matmul(poly_x_plot_ones, poly_w_hat), color='r', label='Predicted 5-th order line')\n",
    "plt.legend(loc='best')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run with SGD to show how close the closed-form solution with SGD solution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SGD solution is [1.1712, 2.0772], which is very close to the closed-form solution. The training loss and validation loss are also similar"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 0: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 0: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 1: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 1: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 2: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 2: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 3: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 3: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 4: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 4: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 5: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 5: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 6: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 6: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 7: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 7: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 8: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 8: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 9: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 9: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 10: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 10: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 11: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 11: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 12: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 12: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 13: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 13: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 14: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 14: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 15: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 15: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 16: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 16: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 17: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 17: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 18: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 18: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 19: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 19: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 20: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 20: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 21: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 21: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 22: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 22: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 23: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 23: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 24: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 24: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 25: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 25: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 26: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 26: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 27: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 27: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 28: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 28: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 29: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 29: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 30: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 30: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 31: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 31: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 32: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 32: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 33: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 33: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 34: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 34: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 35: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 35: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 36: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 36: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 37: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 37: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 38: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 38: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 39: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 39: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 40: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 40: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 41: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 41: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 42: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 42: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 43: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 43: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 44: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 44: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 45: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 45: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 46: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 46: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 47: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 47: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 48: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 48: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 49: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 49: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 50: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 50: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 51: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 51: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 52: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 52: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 53: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 53: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 54: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 54: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 55: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 55: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 56: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 56: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 57: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 57: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 58: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 58: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 59: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 59: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 60: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 60: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 61: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 61: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 62: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 62: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 63: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 63: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 64: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 64: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 65: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 65: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 66: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 66: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 67: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 67: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 68: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 68: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 69: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 69: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 70: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 70: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 71: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 71: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 72: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 72: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 73: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 73: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 74: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 74: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 75: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 75: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 76: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 76: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 77: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 77: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 78: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 78: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 79: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 79: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 80: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 80: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 81: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 81: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 82: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 82: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 83: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 83: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 84: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 84: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 85: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 85: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 86: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 86: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 87: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 87: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 88: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 88: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 89: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 89: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 90: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 90: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 91: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 91: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 92: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 92: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 93: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 93: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 94: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 94: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 95: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 95: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 96: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 96: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 97: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 97: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 98: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 98: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 99: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 99: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 100: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 100: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 101: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 101: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 102: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 102: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 103: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 103: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 104: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 104: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 105: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 105: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 106: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 106: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 107: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 107: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 108: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 108: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 109: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 109: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 110: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 110: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 111: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 111: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 112: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 112: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 113: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 113: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 114: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 114: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 115: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 115: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 116: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 116: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 117: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 117: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 118: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 118: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 119: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 119: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 120: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 120: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 121: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 121: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 122: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 122: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 123: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 123: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 124: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 124: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 125: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 125: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 126: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 126: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 127: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 127: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 128: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 128: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 129: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 129: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 130: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 130: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 131: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 131: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 132: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 132: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 133: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 133: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 134: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 134: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 135: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 135: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 136: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 136: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 137: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 137: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 138: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 138: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 139: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 139: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 140: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 140: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 141: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 141: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 142: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 142: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 143: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 143: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 144: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 144: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 145: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 145: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 146: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 146: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 147: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 147: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 148: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 148: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 149: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 149: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 150: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 150: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 151: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 151: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 152: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 152: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 153: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 153: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 154: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 154: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 155: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 155: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 156: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 156: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 157: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 157: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 158: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 158: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 159: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 159: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 160: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 160: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 161: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 161: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 162: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 162: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 163: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 163: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 164: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 164: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 165: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 165: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 166: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 166: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 167: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 167: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 168: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 168: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 169: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 169: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 170: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 170: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 171: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 171: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 172: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 172: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 173: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 173: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 174: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 174: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 175: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 175: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 176: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 176: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 177: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 177: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 178: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 178: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 179: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 179: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 180: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 180: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 181: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 181: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 182: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 182: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 183: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 183: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 184: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 184: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 185: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 185: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 186: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 186: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 187: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 187: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 188: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 188: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 189: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 189: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 190: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 190: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 191: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 191: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 192: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 192: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 193: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 193: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 194: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 194: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 195: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 195: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 196: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 196: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 197: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 197: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 198: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 198: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 199: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 199: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 200: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 200: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 201: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 201: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 202: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 202: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 203: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 203: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 204: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 204: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 205: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 205: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 206: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 206: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 207: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 207: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 208: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 208: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 209: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 209: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 210: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 210: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 211: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 211: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 212: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 212: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 213: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 213: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 214: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 214: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 215: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 215: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 216: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 216: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 217: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 217: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 218: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 218: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 219: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 219: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 220: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 220: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 221: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 221: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 222: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 222: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 223: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 223: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 224: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 224: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 225: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 225: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 226: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 226: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 227: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 227: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 228: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 228: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 229: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 229: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 230: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 230: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 231: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 231: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 232: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 232: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 233: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 233: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 234: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 234: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 235: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 235: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 236: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 236: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 237: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 237: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 238: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 238: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 239: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 239: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 240: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 240: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 241: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 241: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 242: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 242: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 243: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 243: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 244: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 244: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 245: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 245: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 246: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 246: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 247: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 247: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 248: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 248: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 249: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 249: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 250: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 250: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 251: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 251: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 252: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 252: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 253: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 253: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 254: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 254: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 255: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 255: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 256: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 256: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 257: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 257: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 258: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 258: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 259: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 259: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 260: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 260: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 261: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 261: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 262: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 262: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 263: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 263: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 264: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 264: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 265: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 265: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 266: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 266: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 267: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 267: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 268: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 268: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 269: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 269: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 270: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 270: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 271: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 271: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 272: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 272: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 273: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 273: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 274: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 274: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 275: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 275: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 276: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 276: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 277: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 277: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 278: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 278: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 279: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 279: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 280: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 280: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 281: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 281: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 282: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 282: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 283: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 283: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 284: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 284: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 285: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 285: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 286: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 286: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 287: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 287: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 288: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 288: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 289: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 289: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 290: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 290: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 291: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 291: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 292: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 292: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 293: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 293: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 294: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 294: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 295: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 295: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 296: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 296: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 297: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 297: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 298: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 298: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 299: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 299: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 300: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 300: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 301: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 301: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 302: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 302: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 303: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 303: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 304: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 304: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 305: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 305: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 306: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 306: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 307: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 307: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 308: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 308: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 309: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 309: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 310: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 310: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 311: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 311: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 312: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 312: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 313: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 313: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 314: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 314: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 315: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 315: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 316: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 316: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 317: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 317: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 318: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 318: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 319: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 319: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 320: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 320: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 321: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 321: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 322: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 322: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 323: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 323: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 324: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 324: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 325: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 325: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 326: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 326: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 327: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 327: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 328: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 328: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 329: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 329: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 330: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 330: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 331: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 331: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 332: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 332: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 333: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 333: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 334: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 334: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 335: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 335: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 336: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 336: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 337: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 337: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 338: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 338: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 339: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 339: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 340: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 340: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 341: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 341: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 342: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 342: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 343: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 343: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 344: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 344: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 345: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 345: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 346: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 346: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 347: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 347: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 348: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 348: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 349: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 349: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 350: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 350: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 351: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 351: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 352: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 352: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 353: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 353: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 354: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 354: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 355: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 355: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 356: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 356: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 357: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 357: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 358: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 358: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 359: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 359: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 360: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 360: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 361: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 361: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 362: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 362: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 363: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 363: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 364: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 364: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 365: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 365: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 366: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 366: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 367: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 367: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 368: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 368: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 369: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 369: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 370: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 370: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 371: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 371: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 372: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 372: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 373: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 373: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 374: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 374: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 375: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 375: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 376: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 376: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 377: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 377: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 378: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 378: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 379: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 379: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 380: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 380: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 381: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 381: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 382: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 382: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 383: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 383: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 384: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 384: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 385: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 385: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 386: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 386: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 387: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 387: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 388: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 388: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 389: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 389: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 390: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 390: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 391: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 391: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 392: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 392: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 393: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 393: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 394: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 394: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 395: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 395: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 396: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 396: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 397: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 397: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 398: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 398: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 399: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 399: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 400: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 400: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 401: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 401: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 402: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 402: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 403: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 403: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 404: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 404: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 405: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 405: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 406: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 406: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 407: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 407: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 408: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 408: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 409: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 409: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 410: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 410: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 411: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 411: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 412: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 412: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 413: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 413: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 414: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 414: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 415: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 415: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 416: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 416: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 417: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 417: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 418: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 418: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 419: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 419: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 420: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 420: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 421: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 421: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 422: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 422: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 423: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 423: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 424: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 424: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 425: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 425: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 426: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 426: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 427: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 427: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 428: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 428: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 429: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 429: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 430: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 430: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 431: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 431: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 432: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 432: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 433: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 433: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 434: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 434: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 435: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 435: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 436: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 436: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 437: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 437: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 438: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 438: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 439: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 439: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 440: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 440: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 441: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 441: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 442: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 442: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 443: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 443: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 444: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 444: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 445: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 445: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 446: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 446: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 447: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 447: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 448: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 448: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 449: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 449: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 450: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 450: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 451: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 451: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 452: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 452: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 453: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 453: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 454: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 454: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 455: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 455: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 456: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 456: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 457: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 457: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 458: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 458: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 459: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 459: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 460: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 460: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 461: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 461: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 462: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 462: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 463: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 463: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 464: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 464: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 465: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 465: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 466: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 466: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 467: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 467: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 468: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 468: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 469: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 469: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 470: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 470: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 471: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 471: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 472: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 472: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 473: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 473: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 474: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 474: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 475: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 475: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 476: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 476: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 477: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 477: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 478: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 478: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 479: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 479: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 480: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 480: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 481: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 481: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 482: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 482: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 483: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 483: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 484: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 484: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 485: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 485: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 486: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 486: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 487: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 487: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 488: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 488: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 489: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 489: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 490: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 490: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 491: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 491: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 492: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 492: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 493: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 493: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 494: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 494: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 495: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 495: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 496: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 496: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 497: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 497: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 498: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 498: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "Train loss at 499: tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "w at 499: tensor([[1.1712],\n",
      "        [2.0772]], requires_grad=True)\n",
      "val loss:  tensor(0.9854, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "epoch = 5000\n",
    "for ep in range(epoch):\n",
    "    total_train_loss = 0\n",
    "    for i in range(len(x_train)):\n",
    "        optimizer.zero_grad()\n",
    "        y_predicted = torch.matmul(x_train[i], w)\n",
    "        train_loss = torch.nn.functional.mse_loss(y_predicted, y_train[i])\n",
    "        total_train_loss += train_loss\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train loss at ' + str(ep) + ': ' + str(total_train_loss / len(x_train)))\n",
    "    print('w at ' + str(ep) + ': ' +  str(w))\n",
    "y_valid_predicted = torch.matmul(x_valid, w)\n",
    "loss = torch.nn.functional.mse_loss(y_valid_predicted, y_valid)\n",
    "print('val loss: ', loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly train loss at 0: tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "poly w at 0: tensor([[ 1.0734],\n",
      "        [ 0.2178],\n",
      "        [-1.8425],\n",
      "        [ 1.2434],\n",
      "        [ 0.6722],\n",
      "        [-0.2802]], requires_grad=True)\n",
      "poly train loss at 1: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 1: tensor([[ 1.0731],\n",
      "        [ 0.2168],\n",
      "        [-1.8443],\n",
      "        [ 1.2412],\n",
      "        [ 0.6699],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 2: tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "poly w at 2: tensor([[ 1.0734],\n",
      "        [ 0.2178],\n",
      "        [-1.8426],\n",
      "        [ 1.2435],\n",
      "        [ 0.6723],\n",
      "        [-0.2802]], requires_grad=True)\n",
      "poly train loss at 3: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 3: tensor([[ 1.0731],\n",
      "        [ 0.2167],\n",
      "        [-1.8443],\n",
      "        [ 1.2413],\n",
      "        [ 0.6699],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 4: tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "poly w at 4: tensor([[ 1.0734],\n",
      "        [ 0.2177],\n",
      "        [-1.8426],\n",
      "        [ 1.2435],\n",
      "        [ 0.6723],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 5: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 5: tensor([[ 1.0731],\n",
      "        [ 0.2167],\n",
      "        [-1.8444],\n",
      "        [ 1.2413],\n",
      "        [ 0.6699],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 6: tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "poly w at 6: tensor([[ 1.0734],\n",
      "        [ 0.2177],\n",
      "        [-1.8427],\n",
      "        [ 1.2435],\n",
      "        [ 0.6723],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 7: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 7: tensor([[ 1.0731],\n",
      "        [ 0.2166],\n",
      "        [-1.8444],\n",
      "        [ 1.2413],\n",
      "        [ 0.6699],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 8: tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "poly w at 8: tensor([[ 1.0734],\n",
      "        [ 0.2176],\n",
      "        [-1.8427],\n",
      "        [ 1.2436],\n",
      "        [ 0.6723],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 9: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 9: tensor([[ 1.0731],\n",
      "        [ 0.2166],\n",
      "        [-1.8445],\n",
      "        [ 1.2414],\n",
      "        [ 0.6699],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 10: tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "poly w at 10: tensor([[ 1.0734],\n",
      "        [ 0.2176],\n",
      "        [-1.8428],\n",
      "        [ 1.2436],\n",
      "        [ 0.6723],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 11: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 11: tensor([[ 1.0731],\n",
      "        [ 0.2166],\n",
      "        [-1.8445],\n",
      "        [ 1.2414],\n",
      "        [ 0.6700],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 12: tensor(0.3284, grad_fn=<DivBackward0>)\n",
      "poly w at 12: tensor([[ 1.0734],\n",
      "        [ 0.2176],\n",
      "        [-1.8428],\n",
      "        [ 1.2436],\n",
      "        [ 0.6723],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 13: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 13: tensor([[ 1.0731],\n",
      "        [ 0.2165],\n",
      "        [-1.8445],\n",
      "        [ 1.2414],\n",
      "        [ 0.6700],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 14: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 14: tensor([[ 1.0734],\n",
      "        [ 0.2175],\n",
      "        [-1.8428],\n",
      "        [ 1.2437],\n",
      "        [ 0.6724],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 15: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 15: tensor([[ 1.0731],\n",
      "        [ 0.2165],\n",
      "        [-1.8446],\n",
      "        [ 1.2415],\n",
      "        [ 0.6700],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 16: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 16: tensor([[ 1.0734],\n",
      "        [ 0.2175],\n",
      "        [-1.8429],\n",
      "        [ 1.2437],\n",
      "        [ 0.6724],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 17: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 17: tensor([[ 1.0731],\n",
      "        [ 0.2164],\n",
      "        [-1.8446],\n",
      "        [ 1.2415],\n",
      "        [ 0.6700],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 18: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 18: tensor([[ 1.0734],\n",
      "        [ 0.2174],\n",
      "        [-1.8429],\n",
      "        [ 1.2437],\n",
      "        [ 0.6724],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 19: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 19: tensor([[ 1.0731],\n",
      "        [ 0.2164],\n",
      "        [-1.8447],\n",
      "        [ 1.2415],\n",
      "        [ 0.6700],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 20: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 20: tensor([[ 1.0734],\n",
      "        [ 0.2174],\n",
      "        [-1.8430],\n",
      "        [ 1.2437],\n",
      "        [ 0.6724],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 21: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 21: tensor([[ 1.0731],\n",
      "        [ 0.2164],\n",
      "        [-1.8447],\n",
      "        [ 1.2416],\n",
      "        [ 0.6700],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 22: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 22: tensor([[ 1.0734],\n",
      "        [ 0.2174],\n",
      "        [-1.8430],\n",
      "        [ 1.2438],\n",
      "        [ 0.6724],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 23: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 23: tensor([[ 1.0731],\n",
      "        [ 0.2163],\n",
      "        [-1.8448],\n",
      "        [ 1.2416],\n",
      "        [ 0.6701],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 24: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 24: tensor([[ 1.0734],\n",
      "        [ 0.2173],\n",
      "        [-1.8431],\n",
      "        [ 1.2438],\n",
      "        [ 0.6724],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 25: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 25: tensor([[ 1.0731],\n",
      "        [ 0.2163],\n",
      "        [-1.8448],\n",
      "        [ 1.2416],\n",
      "        [ 0.6701],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 26: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 26: tensor([[ 1.0734],\n",
      "        [ 0.2173],\n",
      "        [-1.8431],\n",
      "        [ 1.2438],\n",
      "        [ 0.6725],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 27: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 27: tensor([[ 1.0731],\n",
      "        [ 0.2162],\n",
      "        [-1.8449],\n",
      "        [ 1.2417],\n",
      "        [ 0.6701],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 28: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 28: tensor([[ 1.0734],\n",
      "        [ 0.2172],\n",
      "        [-1.8432],\n",
      "        [ 1.2439],\n",
      "        [ 0.6725],\n",
      "        [-0.2803]], requires_grad=True)\n",
      "poly train loss at 29: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 29: tensor([[ 1.0731],\n",
      "        [ 0.2162],\n",
      "        [-1.8449],\n",
      "        [ 1.2417],\n",
      "        [ 0.6701],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 30: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 30: tensor([[ 1.0734],\n",
      "        [ 0.2172],\n",
      "        [-1.8432],\n",
      "        [ 1.2439],\n",
      "        [ 0.6725],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 31: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 31: tensor([[ 1.0731],\n",
      "        [ 0.2162],\n",
      "        [-1.8449],\n",
      "        [ 1.2417],\n",
      "        [ 0.6701],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 32: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 32: tensor([[ 1.0734],\n",
      "        [ 0.2172],\n",
      "        [-1.8432],\n",
      "        [ 1.2439],\n",
      "        [ 0.6725],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 33: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 33: tensor([[ 1.0731],\n",
      "        [ 0.2161],\n",
      "        [-1.8450],\n",
      "        [ 1.2418],\n",
      "        [ 0.6701],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 34: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 34: tensor([[ 1.0734],\n",
      "        [ 0.2171],\n",
      "        [-1.8433],\n",
      "        [ 1.2440],\n",
      "        [ 0.6725],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 35: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 35: tensor([[ 1.0731],\n",
      "        [ 0.2161],\n",
      "        [-1.8450],\n",
      "        [ 1.2418],\n",
      "        [ 0.6702],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 36: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 36: tensor([[ 1.0734],\n",
      "        [ 0.2171],\n",
      "        [-1.8433],\n",
      "        [ 1.2440],\n",
      "        [ 0.6725],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 37: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 37: tensor([[ 1.0731],\n",
      "        [ 0.2160],\n",
      "        [-1.8451],\n",
      "        [ 1.2418],\n",
      "        [ 0.6702],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 38: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 38: tensor([[ 1.0734],\n",
      "        [ 0.2170],\n",
      "        [-1.8434],\n",
      "        [ 1.2440],\n",
      "        [ 0.6726],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 39: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 39: tensor([[ 1.0731],\n",
      "        [ 0.2160],\n",
      "        [-1.8451],\n",
      "        [ 1.2419],\n",
      "        [ 0.6702],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 40: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 40: tensor([[ 1.0734],\n",
      "        [ 0.2170],\n",
      "        [-1.8434],\n",
      "        [ 1.2441],\n",
      "        [ 0.6726],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 41: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 41: tensor([[ 1.0731],\n",
      "        [ 0.2159],\n",
      "        [-1.8452],\n",
      "        [ 1.2419],\n",
      "        [ 0.6702],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 42: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 42: tensor([[ 1.0734],\n",
      "        [ 0.2169],\n",
      "        [-1.8435],\n",
      "        [ 1.2441],\n",
      "        [ 0.6726],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 43: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 43: tensor([[ 1.0731],\n",
      "        [ 0.2159],\n",
      "        [-1.8452],\n",
      "        [ 1.2419],\n",
      "        [ 0.6702],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 44: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 44: tensor([[ 1.0734],\n",
      "        [ 0.2169],\n",
      "        [-1.8435],\n",
      "        [ 1.2441],\n",
      "        [ 0.6726],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 45: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 45: tensor([[ 1.0731],\n",
      "        [ 0.2159],\n",
      "        [-1.8453],\n",
      "        [ 1.2420],\n",
      "        [ 0.6702],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 46: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 46: tensor([[ 1.0734],\n",
      "        [ 0.2169],\n",
      "        [-1.8436],\n",
      "        [ 1.2442],\n",
      "        [ 0.6726],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 47: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 47: tensor([[ 1.0731],\n",
      "        [ 0.2158],\n",
      "        [-1.8453],\n",
      "        [ 1.2420],\n",
      "        [ 0.6702],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 48: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 48: tensor([[ 1.0734],\n",
      "        [ 0.2168],\n",
      "        [-1.8436],\n",
      "        [ 1.2442],\n",
      "        [ 0.6726],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 49: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 49: tensor([[ 1.0731],\n",
      "        [ 0.2158],\n",
      "        [-1.8453],\n",
      "        [ 1.2420],\n",
      "        [ 0.6703],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 50: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 50: tensor([[ 1.0734],\n",
      "        [ 0.2168],\n",
      "        [-1.8436],\n",
      "        [ 1.2442],\n",
      "        [ 0.6727],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 51: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 51: tensor([[ 1.0731],\n",
      "        [ 0.2157],\n",
      "        [-1.8454],\n",
      "        [ 1.2421],\n",
      "        [ 0.6703],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 52: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 52: tensor([[ 1.0734],\n",
      "        [ 0.2167],\n",
      "        [-1.8437],\n",
      "        [ 1.2443],\n",
      "        [ 0.6727],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 53: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 53: tensor([[ 1.0731],\n",
      "        [ 0.2157],\n",
      "        [-1.8454],\n",
      "        [ 1.2421],\n",
      "        [ 0.6703],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 54: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 54: tensor([[ 1.0734],\n",
      "        [ 0.2167],\n",
      "        [-1.8437],\n",
      "        [ 1.2443],\n",
      "        [ 0.6727],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 55: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 55: tensor([[ 1.0731],\n",
      "        [ 0.2157],\n",
      "        [-1.8455],\n",
      "        [ 1.2421],\n",
      "        [ 0.6703],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 56: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 56: tensor([[ 1.0734],\n",
      "        [ 0.2167],\n",
      "        [-1.8438],\n",
      "        [ 1.2443],\n",
      "        [ 0.6727],\n",
      "        [-0.2804]], requires_grad=True)\n",
      "poly train loss at 57: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 57: tensor([[ 1.0731],\n",
      "        [ 0.2156],\n",
      "        [-1.8455],\n",
      "        [ 1.2421],\n",
      "        [ 0.6703],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 58: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 58: tensor([[ 1.0734],\n",
      "        [ 0.2166],\n",
      "        [-1.8438],\n",
      "        [ 1.2444],\n",
      "        [ 0.6727],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 59: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 59: tensor([[ 1.0731],\n",
      "        [ 0.2156],\n",
      "        [-1.8456],\n",
      "        [ 1.2422],\n",
      "        [ 0.6703],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 60: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 60: tensor([[ 1.0734],\n",
      "        [ 0.2166],\n",
      "        [-1.8439],\n",
      "        [ 1.2444],\n",
      "        [ 0.6727],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 61: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 61: tensor([[ 1.0731],\n",
      "        [ 0.2155],\n",
      "        [-1.8456],\n",
      "        [ 1.2422],\n",
      "        [ 0.6704],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 62: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 62: tensor([[ 1.0734],\n",
      "        [ 0.2165],\n",
      "        [-1.8439],\n",
      "        [ 1.2444],\n",
      "        [ 0.6728],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 63: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 63: tensor([[ 1.0731],\n",
      "        [ 0.2155],\n",
      "        [-1.8457],\n",
      "        [ 1.2422],\n",
      "        [ 0.6704],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 64: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 64: tensor([[ 1.0734],\n",
      "        [ 0.2165],\n",
      "        [-1.8440],\n",
      "        [ 1.2445],\n",
      "        [ 0.6728],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 65: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 65: tensor([[ 1.0731],\n",
      "        [ 0.2155],\n",
      "        [-1.8457],\n",
      "        [ 1.2423],\n",
      "        [ 0.6704],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 66: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 66: tensor([[ 1.0734],\n",
      "        [ 0.2165],\n",
      "        [-1.8440],\n",
      "        [ 1.2445],\n",
      "        [ 0.6728],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 67: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 67: tensor([[ 1.0731],\n",
      "        [ 0.2154],\n",
      "        [-1.8457],\n",
      "        [ 1.2423],\n",
      "        [ 0.6704],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 68: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 68: tensor([[ 1.0734],\n",
      "        [ 0.2164],\n",
      "        [-1.8440],\n",
      "        [ 1.2445],\n",
      "        [ 0.6728],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 69: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 69: tensor([[ 1.0731],\n",
      "        [ 0.2154],\n",
      "        [-1.8458],\n",
      "        [ 1.2423],\n",
      "        [ 0.6704],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 70: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 70: tensor([[ 1.0734],\n",
      "        [ 0.2164],\n",
      "        [-1.8441],\n",
      "        [ 1.2446],\n",
      "        [ 0.6728],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 71: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 71: tensor([[ 1.0731],\n",
      "        [ 0.2153],\n",
      "        [-1.8458],\n",
      "        [ 1.2424],\n",
      "        [ 0.6704],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 72: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 72: tensor([[ 1.0734],\n",
      "        [ 0.2163],\n",
      "        [-1.8441],\n",
      "        [ 1.2446],\n",
      "        [ 0.6728],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 73: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 73: tensor([[ 1.0731],\n",
      "        [ 0.2153],\n",
      "        [-1.8459],\n",
      "        [ 1.2424],\n",
      "        [ 0.6705],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 74: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 74: tensor([[ 1.0734],\n",
      "        [ 0.2163],\n",
      "        [-1.8442],\n",
      "        [ 1.2446],\n",
      "        [ 0.6729],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 75: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 75: tensor([[ 1.0731],\n",
      "        [ 0.2153],\n",
      "        [-1.8459],\n",
      "        [ 1.2424],\n",
      "        [ 0.6705],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 76: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 76: tensor([[ 1.0734],\n",
      "        [ 0.2163],\n",
      "        [-1.8442],\n",
      "        [ 1.2447],\n",
      "        [ 0.6729],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 77: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 77: tensor([[ 1.0731],\n",
      "        [ 0.2152],\n",
      "        [-1.8460],\n",
      "        [ 1.2425],\n",
      "        [ 0.6705],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 78: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 78: tensor([[ 1.0734],\n",
      "        [ 0.2162],\n",
      "        [-1.8443],\n",
      "        [ 1.2447],\n",
      "        [ 0.6729],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 79: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 79: tensor([[ 1.0731],\n",
      "        [ 0.2152],\n",
      "        [-1.8460],\n",
      "        [ 1.2425],\n",
      "        [ 0.6705],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 80: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 80: tensor([[ 1.0734],\n",
      "        [ 0.2162],\n",
      "        [-1.8443],\n",
      "        [ 1.2447],\n",
      "        [ 0.6729],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 81: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 81: tensor([[ 1.0731],\n",
      "        [ 0.2151],\n",
      "        [-1.8461],\n",
      "        [ 1.2425],\n",
      "        [ 0.6705],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 82: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 82: tensor([[ 1.0734],\n",
      "        [ 0.2161],\n",
      "        [-1.8444],\n",
      "        [ 1.2447],\n",
      "        [ 0.6729],\n",
      "        [-0.2805]], requires_grad=True)\n",
      "poly train loss at 83: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 83: tensor([[ 1.0731],\n",
      "        [ 0.2151],\n",
      "        [-1.8461],\n",
      "        [ 1.2426],\n",
      "        [ 0.6705],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 84: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 84: tensor([[ 1.0734],\n",
      "        [ 0.2161],\n",
      "        [-1.8444],\n",
      "        [ 1.2448],\n",
      "        [ 0.6729],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 85: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 85: tensor([[ 1.0731],\n",
      "        [ 0.2150],\n",
      "        [-1.8461],\n",
      "        [ 1.2426],\n",
      "        [ 0.6706],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 86: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 86: tensor([[ 1.0734],\n",
      "        [ 0.2160],\n",
      "        [-1.8444],\n",
      "        [ 1.2448],\n",
      "        [ 0.6730],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 87: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 87: tensor([[ 1.0731],\n",
      "        [ 0.2150],\n",
      "        [-1.8462],\n",
      "        [ 1.2426],\n",
      "        [ 0.6706],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 88: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 88: tensor([[ 1.0734],\n",
      "        [ 0.2160],\n",
      "        [-1.8445],\n",
      "        [ 1.2448],\n",
      "        [ 0.6730],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 89: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 89: tensor([[ 1.0731],\n",
      "        [ 0.2150],\n",
      "        [-1.8462],\n",
      "        [ 1.2427],\n",
      "        [ 0.6706],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 90: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 90: tensor([[ 1.0734],\n",
      "        [ 0.2160],\n",
      "        [-1.8445],\n",
      "        [ 1.2449],\n",
      "        [ 0.6730],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 91: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 91: tensor([[ 1.0731],\n",
      "        [ 0.2149],\n",
      "        [-1.8463],\n",
      "        [ 1.2427],\n",
      "        [ 0.6706],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 92: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 92: tensor([[ 1.0734],\n",
      "        [ 0.2159],\n",
      "        [-1.8446],\n",
      "        [ 1.2449],\n",
      "        [ 0.6730],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 93: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 93: tensor([[ 1.0731],\n",
      "        [ 0.2149],\n",
      "        [-1.8463],\n",
      "        [ 1.2427],\n",
      "        [ 0.6706],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 94: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 94: tensor([[ 1.0734],\n",
      "        [ 0.2159],\n",
      "        [-1.8446],\n",
      "        [ 1.2449],\n",
      "        [ 0.6730],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 95: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 95: tensor([[ 1.0731],\n",
      "        [ 0.2148],\n",
      "        [-1.8464],\n",
      "        [ 1.2428],\n",
      "        [ 0.6706],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 96: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 96: tensor([[ 1.0734],\n",
      "        [ 0.2158],\n",
      "        [-1.8447],\n",
      "        [ 1.2450],\n",
      "        [ 0.6730],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 97: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 97: tensor([[ 1.0731],\n",
      "        [ 0.2148],\n",
      "        [-1.8464],\n",
      "        [ 1.2428],\n",
      "        [ 0.6707],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 98: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 98: tensor([[ 1.0734],\n",
      "        [ 0.2158],\n",
      "        [-1.8447],\n",
      "        [ 1.2450],\n",
      "        [ 0.6730],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 99: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 99: tensor([[ 1.0731],\n",
      "        [ 0.2148],\n",
      "        [-1.8465],\n",
      "        [ 1.2428],\n",
      "        [ 0.6707],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 100: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 100: tensor([[ 1.0734],\n",
      "        [ 0.2158],\n",
      "        [-1.8447],\n",
      "        [ 1.2450],\n",
      "        [ 0.6731],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 101: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 101: tensor([[ 1.0731],\n",
      "        [ 0.2147],\n",
      "        [-1.8465],\n",
      "        [ 1.2429],\n",
      "        [ 0.6707],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 102: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 102: tensor([[ 1.0734],\n",
      "        [ 0.2157],\n",
      "        [-1.8448],\n",
      "        [ 1.2451],\n",
      "        [ 0.6731],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 103: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 103: tensor([[ 1.0731],\n",
      "        [ 0.2147],\n",
      "        [-1.8465],\n",
      "        [ 1.2429],\n",
      "        [ 0.6707],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 104: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 104: tensor([[ 1.0734],\n",
      "        [ 0.2157],\n",
      "        [-1.8448],\n",
      "        [ 1.2451],\n",
      "        [ 0.6731],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 105: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 105: tensor([[ 1.0731],\n",
      "        [ 0.2146],\n",
      "        [-1.8466],\n",
      "        [ 1.2429],\n",
      "        [ 0.6707],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 106: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 106: tensor([[ 1.0734],\n",
      "        [ 0.2156],\n",
      "        [-1.8449],\n",
      "        [ 1.2451],\n",
      "        [ 0.6731],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 107: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 107: tensor([[ 1.0731],\n",
      "        [ 0.2146],\n",
      "        [-1.8466],\n",
      "        [ 1.2429],\n",
      "        [ 0.6707],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 108: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 108: tensor([[ 1.0734],\n",
      "        [ 0.2156],\n",
      "        [-1.8449],\n",
      "        [ 1.2452],\n",
      "        [ 0.6731],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 109: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 109: tensor([[ 1.0731],\n",
      "        [ 0.2146],\n",
      "        [-1.8467],\n",
      "        [ 1.2430],\n",
      "        [ 0.6708],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 110: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 110: tensor([[ 1.0734],\n",
      "        [ 0.2156],\n",
      "        [-1.8450],\n",
      "        [ 1.2452],\n",
      "        [ 0.6731],\n",
      "        [-0.2806]], requires_grad=True)\n",
      "poly train loss at 111: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 111: tensor([[ 1.0731],\n",
      "        [ 0.2145],\n",
      "        [-1.8467],\n",
      "        [ 1.2430],\n",
      "        [ 0.6708],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 112: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 112: tensor([[ 1.0734],\n",
      "        [ 0.2155],\n",
      "        [-1.8450],\n",
      "        [ 1.2452],\n",
      "        [ 0.6732],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 113: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 113: tensor([[ 1.0731],\n",
      "        [ 0.2145],\n",
      "        [-1.8468],\n",
      "        [ 1.2430],\n",
      "        [ 0.6708],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 114: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 114: tensor([[ 1.0734],\n",
      "        [ 0.2155],\n",
      "        [-1.8451],\n",
      "        [ 1.2453],\n",
      "        [ 0.6732],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 115: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 115: tensor([[ 1.0731],\n",
      "        [ 0.2144],\n",
      "        [-1.8468],\n",
      "        [ 1.2431],\n",
      "        [ 0.6708],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 116: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 116: tensor([[ 1.0734],\n",
      "        [ 0.2154],\n",
      "        [-1.8451],\n",
      "        [ 1.2453],\n",
      "        [ 0.6732],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 117: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 117: tensor([[ 1.0731],\n",
      "        [ 0.2144],\n",
      "        [-1.8468],\n",
      "        [ 1.2431],\n",
      "        [ 0.6708],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 118: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 118: tensor([[ 1.0734],\n",
      "        [ 0.2154],\n",
      "        [-1.8451],\n",
      "        [ 1.2453],\n",
      "        [ 0.6732],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 119: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 119: tensor([[ 1.0731],\n",
      "        [ 0.2144],\n",
      "        [-1.8469],\n",
      "        [ 1.2431],\n",
      "        [ 0.6708],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 120: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 120: tensor([[ 1.0734],\n",
      "        [ 0.2154],\n",
      "        [-1.8452],\n",
      "        [ 1.2454],\n",
      "        [ 0.6732],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 121: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 121: tensor([[ 1.0731],\n",
      "        [ 0.2143],\n",
      "        [-1.8469],\n",
      "        [ 1.2432],\n",
      "        [ 0.6709],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 122: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 122: tensor([[ 1.0734],\n",
      "        [ 0.2153],\n",
      "        [-1.8452],\n",
      "        [ 1.2454],\n",
      "        [ 0.6732],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 123: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 123: tensor([[ 1.0731],\n",
      "        [ 0.2143],\n",
      "        [-1.8470],\n",
      "        [ 1.2432],\n",
      "        [ 0.6709],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 124: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 124: tensor([[ 1.0734],\n",
      "        [ 0.2153],\n",
      "        [-1.8453],\n",
      "        [ 1.2454],\n",
      "        [ 0.6733],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 125: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 125: tensor([[ 1.0731],\n",
      "        [ 0.2142],\n",
      "        [-1.8470],\n",
      "        [ 1.2432],\n",
      "        [ 0.6709],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 126: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 126: tensor([[ 1.0734],\n",
      "        [ 0.2152],\n",
      "        [-1.8453],\n",
      "        [ 1.2455],\n",
      "        [ 0.6733],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 127: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 127: tensor([[ 1.0731],\n",
      "        [ 0.2142],\n",
      "        [-1.8471],\n",
      "        [ 1.2433],\n",
      "        [ 0.6709],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 128: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 128: tensor([[ 1.0734],\n",
      "        [ 0.2152],\n",
      "        [-1.8454],\n",
      "        [ 1.2455],\n",
      "        [ 0.6733],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 129: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 129: tensor([[ 1.0731],\n",
      "        [ 0.2142],\n",
      "        [-1.8471],\n",
      "        [ 1.2433],\n",
      "        [ 0.6709],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 130: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 130: tensor([[ 1.0734],\n",
      "        [ 0.2152],\n",
      "        [-1.8454],\n",
      "        [ 1.2455],\n",
      "        [ 0.6733],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 131: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 131: tensor([[ 1.0731],\n",
      "        [ 0.2141],\n",
      "        [-1.8472],\n",
      "        [ 1.2433],\n",
      "        [ 0.6709],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 132: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 132: tensor([[ 1.0734],\n",
      "        [ 0.2151],\n",
      "        [-1.8454],\n",
      "        [ 1.2456],\n",
      "        [ 0.6733],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 133: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 133: tensor([[ 1.0731],\n",
      "        [ 0.2141],\n",
      "        [-1.8472],\n",
      "        [ 1.2434],\n",
      "        [ 0.6709],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 134: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 134: tensor([[ 1.0734],\n",
      "        [ 0.2151],\n",
      "        [-1.8455],\n",
      "        [ 1.2456],\n",
      "        [ 0.6733],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 135: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 135: tensor([[ 1.0731],\n",
      "        [ 0.2140],\n",
      "        [-1.8472],\n",
      "        [ 1.2434],\n",
      "        [ 0.6710],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 136: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 136: tensor([[ 1.0734],\n",
      "        [ 0.2150],\n",
      "        [-1.8455],\n",
      "        [ 1.2456],\n",
      "        [ 0.6734],\n",
      "        [-0.2807]], requires_grad=True)\n",
      "poly train loss at 137: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 137: tensor([[ 1.0731],\n",
      "        [ 0.2140],\n",
      "        [-1.8473],\n",
      "        [ 1.2434],\n",
      "        [ 0.6710],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 138: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 138: tensor([[ 1.0734],\n",
      "        [ 0.2150],\n",
      "        [-1.8456],\n",
      "        [ 1.2456],\n",
      "        [ 0.6734],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 139: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 139: tensor([[ 1.0731],\n",
      "        [ 0.2140],\n",
      "        [-1.8473],\n",
      "        [ 1.2435],\n",
      "        [ 0.6710],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 140: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 140: tensor([[ 1.0734],\n",
      "        [ 0.2150],\n",
      "        [-1.8456],\n",
      "        [ 1.2457],\n",
      "        [ 0.6734],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 141: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 141: tensor([[ 1.0731],\n",
      "        [ 0.2139],\n",
      "        [-1.8474],\n",
      "        [ 1.2435],\n",
      "        [ 0.6710],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 142: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 142: tensor([[ 1.0734],\n",
      "        [ 0.2149],\n",
      "        [-1.8457],\n",
      "        [ 1.2457],\n",
      "        [ 0.6734],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 143: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 143: tensor([[ 1.0731],\n",
      "        [ 0.2139],\n",
      "        [-1.8474],\n",
      "        [ 1.2435],\n",
      "        [ 0.6710],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 144: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 144: tensor([[ 1.0734],\n",
      "        [ 0.2149],\n",
      "        [-1.8457],\n",
      "        [ 1.2457],\n",
      "        [ 0.6734],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 145: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 145: tensor([[ 1.0731],\n",
      "        [ 0.2138],\n",
      "        [-1.8475],\n",
      "        [ 1.2436],\n",
      "        [ 0.6710],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 146: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 146: tensor([[ 1.0734],\n",
      "        [ 0.2148],\n",
      "        [-1.8457],\n",
      "        [ 1.2458],\n",
      "        [ 0.6734],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 147: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 147: tensor([[ 1.0731],\n",
      "        [ 0.2138],\n",
      "        [-1.8475],\n",
      "        [ 1.2436],\n",
      "        [ 0.6711],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 148: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 148: tensor([[ 1.0734],\n",
      "        [ 0.2148],\n",
      "        [-1.8458],\n",
      "        [ 1.2458],\n",
      "        [ 0.6735],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 149: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 149: tensor([[ 1.0731],\n",
      "        [ 0.2138],\n",
      "        [-1.8475],\n",
      "        [ 1.2436],\n",
      "        [ 0.6711],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 150: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 150: tensor([[ 1.0734],\n",
      "        [ 0.2148],\n",
      "        [-1.8458],\n",
      "        [ 1.2458],\n",
      "        [ 0.6735],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 151: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 151: tensor([[ 1.0731],\n",
      "        [ 0.2137],\n",
      "        [-1.8476],\n",
      "        [ 1.2437],\n",
      "        [ 0.6711],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 152: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 152: tensor([[ 1.0734],\n",
      "        [ 0.2147],\n",
      "        [-1.8459],\n",
      "        [ 1.2459],\n",
      "        [ 0.6735],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 153: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 153: tensor([[ 1.0731],\n",
      "        [ 0.2137],\n",
      "        [-1.8476],\n",
      "        [ 1.2437],\n",
      "        [ 0.6711],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 154: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 154: tensor([[ 1.0734],\n",
      "        [ 0.2147],\n",
      "        [-1.8459],\n",
      "        [ 1.2459],\n",
      "        [ 0.6735],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 155: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 155: tensor([[ 1.0731],\n",
      "        [ 0.2136],\n",
      "        [-1.8477],\n",
      "        [ 1.2437],\n",
      "        [ 0.6711],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 156: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 156: tensor([[ 1.0735],\n",
      "        [ 0.2146],\n",
      "        [-1.8460],\n",
      "        [ 1.2459],\n",
      "        [ 0.6735],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 157: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 157: tensor([[ 1.0731],\n",
      "        [ 0.2136],\n",
      "        [-1.8477],\n",
      "        [ 1.2438],\n",
      "        [ 0.6711],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 158: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 158: tensor([[ 1.0735],\n",
      "        [ 0.2146],\n",
      "        [-1.8460],\n",
      "        [ 1.2460],\n",
      "        [ 0.6735],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 159: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 159: tensor([[ 1.0731],\n",
      "        [ 0.2135],\n",
      "        [-1.8478],\n",
      "        [ 1.2438],\n",
      "        [ 0.6712],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 160: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 160: tensor([[ 1.0735],\n",
      "        [ 0.2145],\n",
      "        [-1.8461],\n",
      "        [ 1.2460],\n",
      "        [ 0.6735],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 161: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 161: tensor([[ 1.0731],\n",
      "        [ 0.2135],\n",
      "        [-1.8478],\n",
      "        [ 1.2438],\n",
      "        [ 0.6712],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 162: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 162: tensor([[ 1.0735],\n",
      "        [ 0.2145],\n",
      "        [-1.8461],\n",
      "        [ 1.2460],\n",
      "        [ 0.6736],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 163: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 163: tensor([[ 1.0731],\n",
      "        [ 0.2135],\n",
      "        [-1.8478],\n",
      "        [ 1.2438],\n",
      "        [ 0.6712],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 164: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 164: tensor([[ 1.0735],\n",
      "        [ 0.2145],\n",
      "        [-1.8461],\n",
      "        [ 1.2461],\n",
      "        [ 0.6736],\n",
      "        [-0.2808]], requires_grad=True)\n",
      "poly train loss at 165: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 165: tensor([[ 1.0731],\n",
      "        [ 0.2134],\n",
      "        [-1.8479],\n",
      "        [ 1.2439],\n",
      "        [ 0.6712],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 166: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 166: tensor([[ 1.0735],\n",
      "        [ 0.2144],\n",
      "        [-1.8462],\n",
      "        [ 1.2461],\n",
      "        [ 0.6736],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 167: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 167: tensor([[ 1.0731],\n",
      "        [ 0.2134],\n",
      "        [-1.8479],\n",
      "        [ 1.2439],\n",
      "        [ 0.6712],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 168: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 168: tensor([[ 1.0735],\n",
      "        [ 0.2144],\n",
      "        [-1.8462],\n",
      "        [ 1.2461],\n",
      "        [ 0.6736],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 169: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 169: tensor([[ 1.0731],\n",
      "        [ 0.2133],\n",
      "        [-1.8480],\n",
      "        [ 1.2439],\n",
      "        [ 0.6712],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 170: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 170: tensor([[ 1.0735],\n",
      "        [ 0.2143],\n",
      "        [-1.8463],\n",
      "        [ 1.2462],\n",
      "        [ 0.6736],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 171: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 171: tensor([[ 1.0731],\n",
      "        [ 0.2133],\n",
      "        [-1.8480],\n",
      "        [ 1.2440],\n",
      "        [ 0.6713],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 172: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 172: tensor([[ 1.0735],\n",
      "        [ 0.2143],\n",
      "        [-1.8463],\n",
      "        [ 1.2462],\n",
      "        [ 0.6736],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 173: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 173: tensor([[ 1.0731],\n",
      "        [ 0.2133],\n",
      "        [-1.8481],\n",
      "        [ 1.2440],\n",
      "        [ 0.6713],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 174: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 174: tensor([[ 1.0735],\n",
      "        [ 0.2143],\n",
      "        [-1.8464],\n",
      "        [ 1.2462],\n",
      "        [ 0.6737],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 175: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 175: tensor([[ 1.0731],\n",
      "        [ 0.2132],\n",
      "        [-1.8481],\n",
      "        [ 1.2440],\n",
      "        [ 0.6713],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 176: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 176: tensor([[ 1.0735],\n",
      "        [ 0.2142],\n",
      "        [-1.8464],\n",
      "        [ 1.2463],\n",
      "        [ 0.6737],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 177: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 177: tensor([[ 1.0731],\n",
      "        [ 0.2132],\n",
      "        [-1.8481],\n",
      "        [ 1.2441],\n",
      "        [ 0.6713],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 178: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 178: tensor([[ 1.0735],\n",
      "        [ 0.2142],\n",
      "        [-1.8464],\n",
      "        [ 1.2463],\n",
      "        [ 0.6737],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 179: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 179: tensor([[ 1.0731],\n",
      "        [ 0.2131],\n",
      "        [-1.8482],\n",
      "        [ 1.2441],\n",
      "        [ 0.6713],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 180: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 180: tensor([[ 1.0735],\n",
      "        [ 0.2141],\n",
      "        [-1.8465],\n",
      "        [ 1.2463],\n",
      "        [ 0.6737],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 181: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 181: tensor([[ 1.0731],\n",
      "        [ 0.2131],\n",
      "        [-1.8482],\n",
      "        [ 1.2441],\n",
      "        [ 0.6713],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 182: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 182: tensor([[ 1.0735],\n",
      "        [ 0.2141],\n",
      "        [-1.8465],\n",
      "        [ 1.2463],\n",
      "        [ 0.6737],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 183: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 183: tensor([[ 1.0731],\n",
      "        [ 0.2131],\n",
      "        [-1.8483],\n",
      "        [ 1.2442],\n",
      "        [ 0.6713],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 184: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 184: tensor([[ 1.0735],\n",
      "        [ 0.2141],\n",
      "        [-1.8466],\n",
      "        [ 1.2464],\n",
      "        [ 0.6737],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 185: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 185: tensor([[ 1.0731],\n",
      "        [ 0.2130],\n",
      "        [-1.8483],\n",
      "        [ 1.2442],\n",
      "        [ 0.6714],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 186: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 186: tensor([[ 1.0735],\n",
      "        [ 0.2140],\n",
      "        [-1.8466],\n",
      "        [ 1.2464],\n",
      "        [ 0.6738],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 187: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 187: tensor([[ 1.0731],\n",
      "        [ 0.2130],\n",
      "        [-1.8484],\n",
      "        [ 1.2442],\n",
      "        [ 0.6714],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 188: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 188: tensor([[ 1.0735],\n",
      "        [ 0.2140],\n",
      "        [-1.8467],\n",
      "        [ 1.2464],\n",
      "        [ 0.6738],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 189: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 189: tensor([[ 1.0731],\n",
      "        [ 0.2129],\n",
      "        [-1.8484],\n",
      "        [ 1.2443],\n",
      "        [ 0.6714],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 190: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 190: tensor([[ 1.0735],\n",
      "        [ 0.2139],\n",
      "        [-1.8467],\n",
      "        [ 1.2465],\n",
      "        [ 0.6738],\n",
      "        [-0.2809]], requires_grad=True)\n",
      "poly train loss at 191: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 191: tensor([[ 1.0731],\n",
      "        [ 0.2129],\n",
      "        [-1.8485],\n",
      "        [ 1.2443],\n",
      "        [ 0.6714],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 192: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 192: tensor([[ 1.0735],\n",
      "        [ 0.2139],\n",
      "        [-1.8467],\n",
      "        [ 1.2465],\n",
      "        [ 0.6738],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 193: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 193: tensor([[ 1.0731],\n",
      "        [ 0.2129],\n",
      "        [-1.8485],\n",
      "        [ 1.2443],\n",
      "        [ 0.6714],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 194: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 194: tensor([[ 1.0735],\n",
      "        [ 0.2139],\n",
      "        [-1.8468],\n",
      "        [ 1.2465],\n",
      "        [ 0.6738],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 195: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 195: tensor([[ 1.0731],\n",
      "        [ 0.2128],\n",
      "        [-1.8485],\n",
      "        [ 1.2444],\n",
      "        [ 0.6714],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 196: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 196: tensor([[ 1.0735],\n",
      "        [ 0.2138],\n",
      "        [-1.8468],\n",
      "        [ 1.2466],\n",
      "        [ 0.6738],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 197: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 197: tensor([[ 1.0731],\n",
      "        [ 0.2128],\n",
      "        [-1.8486],\n",
      "        [ 1.2444],\n",
      "        [ 0.6715],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 198: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 198: tensor([[ 1.0735],\n",
      "        [ 0.2138],\n",
      "        [-1.8469],\n",
      "        [ 1.2466],\n",
      "        [ 0.6738],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 199: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 199: tensor([[ 1.0731],\n",
      "        [ 0.2127],\n",
      "        [-1.8486],\n",
      "        [ 1.2444],\n",
      "        [ 0.6715],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 200: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 200: tensor([[ 1.0735],\n",
      "        [ 0.2137],\n",
      "        [-1.8469],\n",
      "        [ 1.2466],\n",
      "        [ 0.6739],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 201: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 201: tensor([[ 1.0731],\n",
      "        [ 0.2127],\n",
      "        [-1.8487],\n",
      "        [ 1.2444],\n",
      "        [ 0.6715],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 202: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 202: tensor([[ 1.0735],\n",
      "        [ 0.2137],\n",
      "        [-1.8470],\n",
      "        [ 1.2467],\n",
      "        [ 0.6739],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 203: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 203: tensor([[ 1.0731],\n",
      "        [ 0.2127],\n",
      "        [-1.8487],\n",
      "        [ 1.2445],\n",
      "        [ 0.6715],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 204: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 204: tensor([[ 1.0735],\n",
      "        [ 0.2137],\n",
      "        [-1.8470],\n",
      "        [ 1.2467],\n",
      "        [ 0.6739],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 205: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 205: tensor([[ 1.0731],\n",
      "        [ 0.2126],\n",
      "        [-1.8488],\n",
      "        [ 1.2445],\n",
      "        [ 0.6715],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 206: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 206: tensor([[ 1.0735],\n",
      "        [ 0.2136],\n",
      "        [-1.8471],\n",
      "        [ 1.2467],\n",
      "        [ 0.6739],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 207: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 207: tensor([[ 1.0731],\n",
      "        [ 0.2126],\n",
      "        [-1.8488],\n",
      "        [ 1.2445],\n",
      "        [ 0.6715],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 208: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 208: tensor([[ 1.0735],\n",
      "        [ 0.2136],\n",
      "        [-1.8471],\n",
      "        [ 1.2468],\n",
      "        [ 0.6739],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 209: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 209: tensor([[ 1.0731],\n",
      "        [ 0.2125],\n",
      "        [-1.8488],\n",
      "        [ 1.2446],\n",
      "        [ 0.6716],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 210: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 210: tensor([[ 1.0735],\n",
      "        [ 0.2135],\n",
      "        [-1.8471],\n",
      "        [ 1.2468],\n",
      "        [ 0.6739],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 211: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 211: tensor([[ 1.0731],\n",
      "        [ 0.2125],\n",
      "        [-1.8489],\n",
      "        [ 1.2446],\n",
      "        [ 0.6716],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 212: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 212: tensor([[ 1.0735],\n",
      "        [ 0.2135],\n",
      "        [-1.8472],\n",
      "        [ 1.2468],\n",
      "        [ 0.6740],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 213: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 213: tensor([[ 1.0731],\n",
      "        [ 0.2125],\n",
      "        [-1.8489],\n",
      "        [ 1.2446],\n",
      "        [ 0.6716],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 214: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 214: tensor([[ 1.0735],\n",
      "        [ 0.2135],\n",
      "        [-1.8472],\n",
      "        [ 1.2469],\n",
      "        [ 0.6740],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 215: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 215: tensor([[ 1.0731],\n",
      "        [ 0.2124],\n",
      "        [-1.8490],\n",
      "        [ 1.2447],\n",
      "        [ 0.6716],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 216: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 216: tensor([[ 1.0735],\n",
      "        [ 0.2134],\n",
      "        [-1.8473],\n",
      "        [ 1.2469],\n",
      "        [ 0.6740],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 217: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 217: tensor([[ 1.0731],\n",
      "        [ 0.2124],\n",
      "        [-1.8490],\n",
      "        [ 1.2447],\n",
      "        [ 0.6716],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 218: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 218: tensor([[ 1.0735],\n",
      "        [ 0.2134],\n",
      "        [-1.8473],\n",
      "        [ 1.2469],\n",
      "        [ 0.6740],\n",
      "        [-0.2810]], requires_grad=True)\n",
      "poly train loss at 219: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 219: tensor([[ 1.0731],\n",
      "        [ 0.2123],\n",
      "        [-1.8491],\n",
      "        [ 1.2447],\n",
      "        [ 0.6716],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 220: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 220: tensor([[ 1.0735],\n",
      "        [ 0.2133],\n",
      "        [-1.8474],\n",
      "        [ 1.2470],\n",
      "        [ 0.6740],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 221: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 221: tensor([[ 1.0731],\n",
      "        [ 0.2123],\n",
      "        [-1.8491],\n",
      "        [ 1.2448],\n",
      "        [ 0.6716],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 222: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 222: tensor([[ 1.0735],\n",
      "        [ 0.2133],\n",
      "        [-1.8474],\n",
      "        [ 1.2470],\n",
      "        [ 0.6740],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 223: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 223: tensor([[ 1.0732],\n",
      "        [ 0.2123],\n",
      "        [-1.8491],\n",
      "        [ 1.2448],\n",
      "        [ 0.6717],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 224: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 224: tensor([[ 1.0735],\n",
      "        [ 0.2133],\n",
      "        [-1.8474],\n",
      "        [ 1.2470],\n",
      "        [ 0.6741],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 225: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 225: tensor([[ 1.0732],\n",
      "        [ 0.2122],\n",
      "        [-1.8492],\n",
      "        [ 1.2448],\n",
      "        [ 0.6717],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 226: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 226: tensor([[ 1.0735],\n",
      "        [ 0.2132],\n",
      "        [-1.8475],\n",
      "        [ 1.2470],\n",
      "        [ 0.6741],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 227: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 227: tensor([[ 1.0732],\n",
      "        [ 0.2122],\n",
      "        [-1.8492],\n",
      "        [ 1.2449],\n",
      "        [ 0.6717],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 228: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 228: tensor([[ 1.0735],\n",
      "        [ 0.2132],\n",
      "        [-1.8475],\n",
      "        [ 1.2471],\n",
      "        [ 0.6741],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 229: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 229: tensor([[ 1.0732],\n",
      "        [ 0.2121],\n",
      "        [-1.8493],\n",
      "        [ 1.2449],\n",
      "        [ 0.6717],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 230: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 230: tensor([[ 1.0735],\n",
      "        [ 0.2131],\n",
      "        [-1.8476],\n",
      "        [ 1.2471],\n",
      "        [ 0.6741],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 231: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 231: tensor([[ 1.0732],\n",
      "        [ 0.2121],\n",
      "        [-1.8493],\n",
      "        [ 1.2449],\n",
      "        [ 0.6717],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 232: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 232: tensor([[ 1.0735],\n",
      "        [ 0.2131],\n",
      "        [-1.8476],\n",
      "        [ 1.2471],\n",
      "        [ 0.6741],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 233: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 233: tensor([[ 1.0732],\n",
      "        [ 0.2121],\n",
      "        [-1.8494],\n",
      "        [ 1.2450],\n",
      "        [ 0.6717],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 234: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 234: tensor([[ 1.0735],\n",
      "        [ 0.2131],\n",
      "        [-1.8477],\n",
      "        [ 1.2472],\n",
      "        [ 0.6741],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 235: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 235: tensor([[ 1.0732],\n",
      "        [ 0.2120],\n",
      "        [-1.8494],\n",
      "        [ 1.2450],\n",
      "        [ 0.6718],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 236: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 236: tensor([[ 1.0735],\n",
      "        [ 0.2130],\n",
      "        [-1.8477],\n",
      "        [ 1.2472],\n",
      "        [ 0.6742],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 237: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 237: tensor([[ 1.0732],\n",
      "        [ 0.2120],\n",
      "        [-1.8494],\n",
      "        [ 1.2450],\n",
      "        [ 0.6718],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 238: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 238: tensor([[ 1.0735],\n",
      "        [ 0.2130],\n",
      "        [-1.8477],\n",
      "        [ 1.2472],\n",
      "        [ 0.6742],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 239: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 239: tensor([[ 1.0732],\n",
      "        [ 0.2119],\n",
      "        [-1.8495],\n",
      "        [ 1.2450],\n",
      "        [ 0.6718],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 240: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 240: tensor([[ 1.0735],\n",
      "        [ 0.2129],\n",
      "        [-1.8478],\n",
      "        [ 1.2473],\n",
      "        [ 0.6742],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 241: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 241: tensor([[ 1.0732],\n",
      "        [ 0.2119],\n",
      "        [-1.8495],\n",
      "        [ 1.2451],\n",
      "        [ 0.6718],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 242: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 242: tensor([[ 1.0735],\n",
      "        [ 0.2129],\n",
      "        [-1.8478],\n",
      "        [ 1.2473],\n",
      "        [ 0.6742],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 243: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 243: tensor([[ 1.0732],\n",
      "        [ 0.2119],\n",
      "        [-1.8496],\n",
      "        [ 1.2451],\n",
      "        [ 0.6718],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 244: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 244: tensor([[ 1.0735],\n",
      "        [ 0.2129],\n",
      "        [-1.8479],\n",
      "        [ 1.2473],\n",
      "        [ 0.6742],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 245: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 245: tensor([[ 1.0732],\n",
      "        [ 0.2118],\n",
      "        [-1.8496],\n",
      "        [ 1.2451],\n",
      "        [ 0.6718],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 246: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 246: tensor([[ 1.0735],\n",
      "        [ 0.2128],\n",
      "        [-1.8479],\n",
      "        [ 1.2474],\n",
      "        [ 0.6742],\n",
      "        [-0.2811]], requires_grad=True)\n",
      "poly train loss at 247: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 247: tensor([[ 1.0732],\n",
      "        [ 0.2118],\n",
      "        [-1.8497],\n",
      "        [ 1.2452],\n",
      "        [ 0.6719],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 248: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 248: tensor([[ 1.0735],\n",
      "        [ 0.2128],\n",
      "        [-1.8480],\n",
      "        [ 1.2474],\n",
      "        [ 0.6742],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 249: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 249: tensor([[ 1.0732],\n",
      "        [ 0.2117],\n",
      "        [-1.8497],\n",
      "        [ 1.2452],\n",
      "        [ 0.6719],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 250: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 250: tensor([[ 1.0735],\n",
      "        [ 0.2127],\n",
      "        [-1.8480],\n",
      "        [ 1.2474],\n",
      "        [ 0.6743],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 251: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 251: tensor([[ 1.0732],\n",
      "        [ 0.2117],\n",
      "        [-1.8497],\n",
      "        [ 1.2452],\n",
      "        [ 0.6719],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 252: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 252: tensor([[ 1.0735],\n",
      "        [ 0.2127],\n",
      "        [-1.8480],\n",
      "        [ 1.2475],\n",
      "        [ 0.6743],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 253: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 253: tensor([[ 1.0732],\n",
      "        [ 0.2117],\n",
      "        [-1.8498],\n",
      "        [ 1.2453],\n",
      "        [ 0.6719],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 254: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 254: tensor([[ 1.0735],\n",
      "        [ 0.2127],\n",
      "        [-1.8481],\n",
      "        [ 1.2475],\n",
      "        [ 0.6743],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 255: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 255: tensor([[ 1.0732],\n",
      "        [ 0.2116],\n",
      "        [-1.8498],\n",
      "        [ 1.2453],\n",
      "        [ 0.6719],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 256: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 256: tensor([[ 1.0735],\n",
      "        [ 0.2126],\n",
      "        [-1.8481],\n",
      "        [ 1.2475],\n",
      "        [ 0.6743],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 257: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 257: tensor([[ 1.0732],\n",
      "        [ 0.2116],\n",
      "        [-1.8499],\n",
      "        [ 1.2453],\n",
      "        [ 0.6719],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 258: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 258: tensor([[ 1.0735],\n",
      "        [ 0.2126],\n",
      "        [-1.8482],\n",
      "        [ 1.2475],\n",
      "        [ 0.6743],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 259: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 259: tensor([[ 1.0732],\n",
      "        [ 0.2115],\n",
      "        [-1.8499],\n",
      "        [ 1.2454],\n",
      "        [ 0.6719],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 260: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 260: tensor([[ 1.0735],\n",
      "        [ 0.2125],\n",
      "        [-1.8482],\n",
      "        [ 1.2476],\n",
      "        [ 0.6743],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 261: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 261: tensor([[ 1.0732],\n",
      "        [ 0.2115],\n",
      "        [-1.8500],\n",
      "        [ 1.2454],\n",
      "        [ 0.6720],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 262: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 262: tensor([[ 1.0735],\n",
      "        [ 0.2125],\n",
      "        [-1.8483],\n",
      "        [ 1.2476],\n",
      "        [ 0.6744],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 263: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 263: tensor([[ 1.0732],\n",
      "        [ 0.2115],\n",
      "        [-1.8500],\n",
      "        [ 1.2454],\n",
      "        [ 0.6720],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 264: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 264: tensor([[ 1.0735],\n",
      "        [ 0.2125],\n",
      "        [-1.8483],\n",
      "        [ 1.2476],\n",
      "        [ 0.6744],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 265: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 265: tensor([[ 1.0732],\n",
      "        [ 0.2114],\n",
      "        [-1.8500],\n",
      "        [ 1.2455],\n",
      "        [ 0.6720],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 266: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 266: tensor([[ 1.0735],\n",
      "        [ 0.2124],\n",
      "        [-1.8483],\n",
      "        [ 1.2477],\n",
      "        [ 0.6744],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 267: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 267: tensor([[ 1.0732],\n",
      "        [ 0.2114],\n",
      "        [-1.8501],\n",
      "        [ 1.2455],\n",
      "        [ 0.6720],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 268: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 268: tensor([[ 1.0735],\n",
      "        [ 0.2124],\n",
      "        [-1.8484],\n",
      "        [ 1.2477],\n",
      "        [ 0.6744],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 269: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 269: tensor([[ 1.0732],\n",
      "        [ 0.2114],\n",
      "        [-1.8501],\n",
      "        [ 1.2455],\n",
      "        [ 0.6720],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 270: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 270: tensor([[ 1.0735],\n",
      "        [ 0.2123],\n",
      "        [-1.8484],\n",
      "        [ 1.2477],\n",
      "        [ 0.6744],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 271: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 271: tensor([[ 1.0732],\n",
      "        [ 0.2113],\n",
      "        [-1.8502],\n",
      "        [ 1.2455],\n",
      "        [ 0.6720],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 272: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 272: tensor([[ 1.0735],\n",
      "        [ 0.2123],\n",
      "        [-1.8485],\n",
      "        [ 1.2478],\n",
      "        [ 0.6744],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 273: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 273: tensor([[ 1.0732],\n",
      "        [ 0.2113],\n",
      "        [-1.8502],\n",
      "        [ 1.2456],\n",
      "        [ 0.6721],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 274: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 274: tensor([[ 1.0735],\n",
      "        [ 0.2123],\n",
      "        [-1.8485],\n",
      "        [ 1.2478],\n",
      "        [ 0.6744],\n",
      "        [-0.2812]], requires_grad=True)\n",
      "poly train loss at 275: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 275: tensor([[ 1.0732],\n",
      "        [ 0.2112],\n",
      "        [-1.8503],\n",
      "        [ 1.2456],\n",
      "        [ 0.6721],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 276: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 276: tensor([[ 1.0735],\n",
      "        [ 0.2122],\n",
      "        [-1.8485],\n",
      "        [ 1.2478],\n",
      "        [ 0.6745],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 277: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 277: tensor([[ 1.0732],\n",
      "        [ 0.2112],\n",
      "        [-1.8503],\n",
      "        [ 1.2456],\n",
      "        [ 0.6721],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 278: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 278: tensor([[ 1.0735],\n",
      "        [ 0.2122],\n",
      "        [-1.8486],\n",
      "        [ 1.2479],\n",
      "        [ 0.6745],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 279: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 279: tensor([[ 1.0732],\n",
      "        [ 0.2112],\n",
      "        [-1.8503],\n",
      "        [ 1.2457],\n",
      "        [ 0.6721],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 280: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 280: tensor([[ 1.0735],\n",
      "        [ 0.2122],\n",
      "        [-1.8486],\n",
      "        [ 1.2479],\n",
      "        [ 0.6745],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 281: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 281: tensor([[ 1.0732],\n",
      "        [ 0.2111],\n",
      "        [-1.8504],\n",
      "        [ 1.2457],\n",
      "        [ 0.6721],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 282: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 282: tensor([[ 1.0735],\n",
      "        [ 0.2121],\n",
      "        [-1.8487],\n",
      "        [ 1.2479],\n",
      "        [ 0.6745],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 283: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 283: tensor([[ 1.0732],\n",
      "        [ 0.2111],\n",
      "        [-1.8504],\n",
      "        [ 1.2457],\n",
      "        [ 0.6721],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 284: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 284: tensor([[ 1.0735],\n",
      "        [ 0.2121],\n",
      "        [-1.8487],\n",
      "        [ 1.2479],\n",
      "        [ 0.6745],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 285: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 285: tensor([[ 1.0732],\n",
      "        [ 0.2110],\n",
      "        [-1.8505],\n",
      "        [ 1.2458],\n",
      "        [ 0.6722],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 286: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 286: tensor([[ 1.0735],\n",
      "        [ 0.2120],\n",
      "        [-1.8488],\n",
      "        [ 1.2480],\n",
      "        [ 0.6745],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 287: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 287: tensor([[ 1.0732],\n",
      "        [ 0.2110],\n",
      "        [-1.8505],\n",
      "        [ 1.2458],\n",
      "        [ 0.6722],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 288: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 288: tensor([[ 1.0735],\n",
      "        [ 0.2120],\n",
      "        [-1.8488],\n",
      "        [ 1.2480],\n",
      "        [ 0.6746],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 289: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 289: tensor([[ 1.0732],\n",
      "        [ 0.2110],\n",
      "        [-1.8506],\n",
      "        [ 1.2458],\n",
      "        [ 0.6722],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 290: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 290: tensor([[ 1.0735],\n",
      "        [ 0.2120],\n",
      "        [-1.8488],\n",
      "        [ 1.2480],\n",
      "        [ 0.6746],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 291: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 291: tensor([[ 1.0732],\n",
      "        [ 0.2109],\n",
      "        [-1.8506],\n",
      "        [ 1.2459],\n",
      "        [ 0.6722],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 292: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 292: tensor([[ 1.0735],\n",
      "        [ 0.2119],\n",
      "        [-1.8489],\n",
      "        [ 1.2481],\n",
      "        [ 0.6746],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 293: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 293: tensor([[ 1.0732],\n",
      "        [ 0.2109],\n",
      "        [-1.8506],\n",
      "        [ 1.2459],\n",
      "        [ 0.6722],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 294: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 294: tensor([[ 1.0735],\n",
      "        [ 0.2119],\n",
      "        [-1.8489],\n",
      "        [ 1.2481],\n",
      "        [ 0.6746],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 295: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 295: tensor([[ 1.0732],\n",
      "        [ 0.2108],\n",
      "        [-1.8507],\n",
      "        [ 1.2459],\n",
      "        [ 0.6722],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 296: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 296: tensor([[ 1.0735],\n",
      "        [ 0.2118],\n",
      "        [-1.8490],\n",
      "        [ 1.2481],\n",
      "        [ 0.6746],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 297: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 297: tensor([[ 1.0732],\n",
      "        [ 0.2108],\n",
      "        [-1.8507],\n",
      "        [ 1.2459],\n",
      "        [ 0.6722],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 298: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 298: tensor([[ 1.0735],\n",
      "        [ 0.2118],\n",
      "        [-1.8490],\n",
      "        [ 1.2482],\n",
      "        [ 0.6746],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 299: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 299: tensor([[ 1.0732],\n",
      "        [ 0.2108],\n",
      "        [-1.8508],\n",
      "        [ 1.2460],\n",
      "        [ 0.6723],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 300: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 300: tensor([[ 1.0735],\n",
      "        [ 0.2118],\n",
      "        [-1.8491],\n",
      "        [ 1.2482],\n",
      "        [ 0.6747],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 301: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 301: tensor([[ 1.0732],\n",
      "        [ 0.2107],\n",
      "        [-1.8508],\n",
      "        [ 1.2460],\n",
      "        [ 0.6723],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 302: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 302: tensor([[ 1.0735],\n",
      "        [ 0.2117],\n",
      "        [-1.8491],\n",
      "        [ 1.2482],\n",
      "        [ 0.6747],\n",
      "        [-0.2813]], requires_grad=True)\n",
      "poly train loss at 303: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 303: tensor([[ 1.0732],\n",
      "        [ 0.2107],\n",
      "        [-1.8508],\n",
      "        [ 1.2460],\n",
      "        [ 0.6723],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 304: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 304: tensor([[ 1.0735],\n",
      "        [ 0.2117],\n",
      "        [-1.8491],\n",
      "        [ 1.2483],\n",
      "        [ 0.6747],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 305: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 305: tensor([[ 1.0732],\n",
      "        [ 0.2106],\n",
      "        [-1.8509],\n",
      "        [ 1.2461],\n",
      "        [ 0.6723],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 306: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 306: tensor([[ 1.0735],\n",
      "        [ 0.2116],\n",
      "        [-1.8492],\n",
      "        [ 1.2483],\n",
      "        [ 0.6747],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 307: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 307: tensor([[ 1.0732],\n",
      "        [ 0.2106],\n",
      "        [-1.8509],\n",
      "        [ 1.2461],\n",
      "        [ 0.6723],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 308: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 308: tensor([[ 1.0735],\n",
      "        [ 0.2116],\n",
      "        [-1.8492],\n",
      "        [ 1.2483],\n",
      "        [ 0.6747],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 309: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 309: tensor([[ 1.0732],\n",
      "        [ 0.2106],\n",
      "        [-1.8510],\n",
      "        [ 1.2461],\n",
      "        [ 0.6723],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 310: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 310: tensor([[ 1.0735],\n",
      "        [ 0.2116],\n",
      "        [-1.8493],\n",
      "        [ 1.2484],\n",
      "        [ 0.6747],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 311: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 311: tensor([[ 1.0732],\n",
      "        [ 0.2105],\n",
      "        [-1.8510],\n",
      "        [ 1.2462],\n",
      "        [ 0.6724],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 312: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 312: tensor([[ 1.0735],\n",
      "        [ 0.2115],\n",
      "        [-1.8493],\n",
      "        [ 1.2484],\n",
      "        [ 0.6747],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 313: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 313: tensor([[ 1.0732],\n",
      "        [ 0.2105],\n",
      "        [-1.8511],\n",
      "        [ 1.2462],\n",
      "        [ 0.6724],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 314: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 314: tensor([[ 1.0735],\n",
      "        [ 0.2115],\n",
      "        [-1.8494],\n",
      "        [ 1.2484],\n",
      "        [ 0.6748],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 315: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 315: tensor([[ 1.0732],\n",
      "        [ 0.2104],\n",
      "        [-1.8511],\n",
      "        [ 1.2462],\n",
      "        [ 0.6724],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 316: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 316: tensor([[ 1.0735],\n",
      "        [ 0.2114],\n",
      "        [-1.8494],\n",
      "        [ 1.2484],\n",
      "        [ 0.6748],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 317: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 317: tensor([[ 1.0732],\n",
      "        [ 0.2104],\n",
      "        [-1.8511],\n",
      "        [ 1.2463],\n",
      "        [ 0.6724],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 318: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 318: tensor([[ 1.0735],\n",
      "        [ 0.2114],\n",
      "        [-1.8494],\n",
      "        [ 1.2485],\n",
      "        [ 0.6748],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 319: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 319: tensor([[ 1.0732],\n",
      "        [ 0.2104],\n",
      "        [-1.8512],\n",
      "        [ 1.2463],\n",
      "        [ 0.6724],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 320: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 320: tensor([[ 1.0735],\n",
      "        [ 0.2114],\n",
      "        [-1.8495],\n",
      "        [ 1.2485],\n",
      "        [ 0.6748],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 321: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 321: tensor([[ 1.0732],\n",
      "        [ 0.2103],\n",
      "        [-1.8512],\n",
      "        [ 1.2463],\n",
      "        [ 0.6724],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 322: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 322: tensor([[ 1.0735],\n",
      "        [ 0.2113],\n",
      "        [-1.8495],\n",
      "        [ 1.2485],\n",
      "        [ 0.6748],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 323: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 323: tensor([[ 1.0732],\n",
      "        [ 0.2103],\n",
      "        [-1.8513],\n",
      "        [ 1.2463],\n",
      "        [ 0.6724],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 324: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 324: tensor([[ 1.0735],\n",
      "        [ 0.2113],\n",
      "        [-1.8496],\n",
      "        [ 1.2486],\n",
      "        [ 0.6748],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 325: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 325: tensor([[ 1.0732],\n",
      "        [ 0.2103],\n",
      "        [-1.8513],\n",
      "        [ 1.2464],\n",
      "        [ 0.6725],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 326: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 326: tensor([[ 1.0735],\n",
      "        [ 0.2113],\n",
      "        [-1.8496],\n",
      "        [ 1.2486],\n",
      "        [ 0.6749],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 327: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 327: tensor([[ 1.0732],\n",
      "        [ 0.2102],\n",
      "        [-1.8514],\n",
      "        [ 1.2464],\n",
      "        [ 0.6725],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 328: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 328: tensor([[ 1.0735],\n",
      "        [ 0.2112],\n",
      "        [-1.8496],\n",
      "        [ 1.2486],\n",
      "        [ 0.6749],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 329: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 329: tensor([[ 1.0732],\n",
      "        [ 0.2102],\n",
      "        [-1.8514],\n",
      "        [ 1.2464],\n",
      "        [ 0.6725],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 330: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 330: tensor([[ 1.0735],\n",
      "        [ 0.2112],\n",
      "        [-1.8497],\n",
      "        [ 1.2487],\n",
      "        [ 0.6749],\n",
      "        [-0.2814]], requires_grad=True)\n",
      "poly train loss at 331: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 331: tensor([[ 1.0732],\n",
      "        [ 0.2101],\n",
      "        [-1.8514],\n",
      "        [ 1.2465],\n",
      "        [ 0.6725],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 332: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 332: tensor([[ 1.0735],\n",
      "        [ 0.2111],\n",
      "        [-1.8497],\n",
      "        [ 1.2487],\n",
      "        [ 0.6749],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 333: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 333: tensor([[ 1.0732],\n",
      "        [ 0.2101],\n",
      "        [-1.8515],\n",
      "        [ 1.2465],\n",
      "        [ 0.6725],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 334: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 334: tensor([[ 1.0735],\n",
      "        [ 0.2111],\n",
      "        [-1.8498],\n",
      "        [ 1.2487],\n",
      "        [ 0.6749],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 335: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 335: tensor([[ 1.0732],\n",
      "        [ 0.2101],\n",
      "        [-1.8515],\n",
      "        [ 1.2465],\n",
      "        [ 0.6725],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 336: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 336: tensor([[ 1.0735],\n",
      "        [ 0.2111],\n",
      "        [-1.8498],\n",
      "        [ 1.2488],\n",
      "        [ 0.6749],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 337: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 337: tensor([[ 1.0732],\n",
      "        [ 0.2100],\n",
      "        [-1.8516],\n",
      "        [ 1.2466],\n",
      "        [ 0.6726],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 338: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 338: tensor([[ 1.0735],\n",
      "        [ 0.2110],\n",
      "        [-1.8499],\n",
      "        [ 1.2488],\n",
      "        [ 0.6749],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 339: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 339: tensor([[ 1.0732],\n",
      "        [ 0.2100],\n",
      "        [-1.8516],\n",
      "        [ 1.2466],\n",
      "        [ 0.6726],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 340: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 340: tensor([[ 1.0735],\n",
      "        [ 0.2110],\n",
      "        [-1.8499],\n",
      "        [ 1.2488],\n",
      "        [ 0.6750],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 341: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 341: tensor([[ 1.0732],\n",
      "        [ 0.2099],\n",
      "        [-1.8517],\n",
      "        [ 1.2466],\n",
      "        [ 0.6726],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 342: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 342: tensor([[ 1.0735],\n",
      "        [ 0.2109],\n",
      "        [-1.8499],\n",
      "        [ 1.2488],\n",
      "        [ 0.6750],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 343: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 343: tensor([[ 1.0732],\n",
      "        [ 0.2099],\n",
      "        [-1.8517],\n",
      "        [ 1.2467],\n",
      "        [ 0.6726],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 344: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 344: tensor([[ 1.0735],\n",
      "        [ 0.2109],\n",
      "        [-1.8500],\n",
      "        [ 1.2489],\n",
      "        [ 0.6750],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 345: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 345: tensor([[ 1.0732],\n",
      "        [ 0.2099],\n",
      "        [-1.8517],\n",
      "        [ 1.2467],\n",
      "        [ 0.6726],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 346: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 346: tensor([[ 1.0735],\n",
      "        [ 0.2109],\n",
      "        [-1.8500],\n",
      "        [ 1.2489],\n",
      "        [ 0.6750],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 347: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 347: tensor([[ 1.0732],\n",
      "        [ 0.2098],\n",
      "        [-1.8518],\n",
      "        [ 1.2467],\n",
      "        [ 0.6726],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 348: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 348: tensor([[ 1.0735],\n",
      "        [ 0.2108],\n",
      "        [-1.8501],\n",
      "        [ 1.2489],\n",
      "        [ 0.6750],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 349: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 349: tensor([[ 1.0732],\n",
      "        [ 0.2098],\n",
      "        [-1.8518],\n",
      "        [ 1.2467],\n",
      "        [ 0.6726],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 350: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 350: tensor([[ 1.0735],\n",
      "        [ 0.2108],\n",
      "        [-1.8501],\n",
      "        [ 1.2490],\n",
      "        [ 0.6750],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 351: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 351: tensor([[ 1.0732],\n",
      "        [ 0.2097],\n",
      "        [-1.8519],\n",
      "        [ 1.2468],\n",
      "        [ 0.6727],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 352: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 352: tensor([[ 1.0735],\n",
      "        [ 0.2107],\n",
      "        [-1.8502],\n",
      "        [ 1.2490],\n",
      "        [ 0.6751],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 353: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 353: tensor([[ 1.0732],\n",
      "        [ 0.2097],\n",
      "        [-1.8519],\n",
      "        [ 1.2468],\n",
      "        [ 0.6727],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 354: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 354: tensor([[ 1.0735],\n",
      "        [ 0.2107],\n",
      "        [-1.8502],\n",
      "        [ 1.2490],\n",
      "        [ 0.6751],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 355: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 355: tensor([[ 1.0732],\n",
      "        [ 0.2097],\n",
      "        [-1.8519],\n",
      "        [ 1.2468],\n",
      "        [ 0.6727],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 356: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 356: tensor([[ 1.0735],\n",
      "        [ 0.2107],\n",
      "        [-1.8502],\n",
      "        [ 1.2491],\n",
      "        [ 0.6751],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 357: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 357: tensor([[ 1.0732],\n",
      "        [ 0.2096],\n",
      "        [-1.8520],\n",
      "        [ 1.2469],\n",
      "        [ 0.6727],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 358: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 358: tensor([[ 1.0735],\n",
      "        [ 0.2106],\n",
      "        [-1.8503],\n",
      "        [ 1.2491],\n",
      "        [ 0.6751],\n",
      "        [-0.2815]], requires_grad=True)\n",
      "poly train loss at 359: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 359: tensor([[ 1.0732],\n",
      "        [ 0.2096],\n",
      "        [-1.8520],\n",
      "        [ 1.2469],\n",
      "        [ 0.6727],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 360: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 360: tensor([[ 1.0735],\n",
      "        [ 0.2106],\n",
      "        [-1.8503],\n",
      "        [ 1.2491],\n",
      "        [ 0.6751],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 361: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 361: tensor([[ 1.0732],\n",
      "        [ 0.2096],\n",
      "        [-1.8521],\n",
      "        [ 1.2469],\n",
      "        [ 0.6727],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 362: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 362: tensor([[ 1.0735],\n",
      "        [ 0.2106],\n",
      "        [-1.8504],\n",
      "        [ 1.2491],\n",
      "        [ 0.6751],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 363: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 363: tensor([[ 1.0732],\n",
      "        [ 0.2095],\n",
      "        [-1.8521],\n",
      "        [ 1.2470],\n",
      "        [ 0.6728],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 364: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 364: tensor([[ 1.0735],\n",
      "        [ 0.2105],\n",
      "        [-1.8504],\n",
      "        [ 1.2492],\n",
      "        [ 0.6751],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 365: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 365: tensor([[ 1.0732],\n",
      "        [ 0.2095],\n",
      "        [-1.8522],\n",
      "        [ 1.2470],\n",
      "        [ 0.6728],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 366: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 366: tensor([[ 1.0735],\n",
      "        [ 0.2105],\n",
      "        [-1.8504],\n",
      "        [ 1.2492],\n",
      "        [ 0.6752],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 367: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 367: tensor([[ 1.0732],\n",
      "        [ 0.2094],\n",
      "        [-1.8522],\n",
      "        [ 1.2470],\n",
      "        [ 0.6728],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 368: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 368: tensor([[ 1.0735],\n",
      "        [ 0.2104],\n",
      "        [-1.8505],\n",
      "        [ 1.2492],\n",
      "        [ 0.6752],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 369: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 369: tensor([[ 1.0732],\n",
      "        [ 0.2094],\n",
      "        [-1.8522],\n",
      "        [ 1.2471],\n",
      "        [ 0.6728],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 370: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 370: tensor([[ 1.0735],\n",
      "        [ 0.2104],\n",
      "        [-1.8505],\n",
      "        [ 1.2493],\n",
      "        [ 0.6752],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 371: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 371: tensor([[ 1.0732],\n",
      "        [ 0.2094],\n",
      "        [-1.8523],\n",
      "        [ 1.2471],\n",
      "        [ 0.6728],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 372: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 372: tensor([[ 1.0735],\n",
      "        [ 0.2104],\n",
      "        [-1.8506],\n",
      "        [ 1.2493],\n",
      "        [ 0.6752],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 373: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 373: tensor([[ 1.0732],\n",
      "        [ 0.2093],\n",
      "        [-1.8523],\n",
      "        [ 1.2471],\n",
      "        [ 0.6728],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 374: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 374: tensor([[ 1.0735],\n",
      "        [ 0.2103],\n",
      "        [-1.8506],\n",
      "        [ 1.2493],\n",
      "        [ 0.6752],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 375: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 375: tensor([[ 1.0732],\n",
      "        [ 0.2093],\n",
      "        [-1.8524],\n",
      "        [ 1.2471],\n",
      "        [ 0.6728],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 376: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 376: tensor([[ 1.0735],\n",
      "        [ 0.2103],\n",
      "        [-1.8507],\n",
      "        [ 1.2494],\n",
      "        [ 0.6752],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 377: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 377: tensor([[ 1.0732],\n",
      "        [ 0.2092],\n",
      "        [-1.8524],\n",
      "        [ 1.2472],\n",
      "        [ 0.6729],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 378: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 378: tensor([[ 1.0735],\n",
      "        [ 0.2102],\n",
      "        [-1.8507],\n",
      "        [ 1.2494],\n",
      "        [ 0.6753],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 379: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 379: tensor([[ 1.0732],\n",
      "        [ 0.2092],\n",
      "        [-1.8524],\n",
      "        [ 1.2472],\n",
      "        [ 0.6729],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 380: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 380: tensor([[ 1.0735],\n",
      "        [ 0.2102],\n",
      "        [-1.8507],\n",
      "        [ 1.2494],\n",
      "        [ 0.6753],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 381: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 381: tensor([[ 1.0732],\n",
      "        [ 0.2092],\n",
      "        [-1.8525],\n",
      "        [ 1.2472],\n",
      "        [ 0.6729],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 382: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 382: tensor([[ 1.0735],\n",
      "        [ 0.2102],\n",
      "        [-1.8508],\n",
      "        [ 1.2495],\n",
      "        [ 0.6753],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 383: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 383: tensor([[ 1.0732],\n",
      "        [ 0.2091],\n",
      "        [-1.8525],\n",
      "        [ 1.2473],\n",
      "        [ 0.6729],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 384: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 384: tensor([[ 1.0735],\n",
      "        [ 0.2101],\n",
      "        [-1.8508],\n",
      "        [ 1.2495],\n",
      "        [ 0.6753],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 385: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 385: tensor([[ 1.0732],\n",
      "        [ 0.2091],\n",
      "        [-1.8526],\n",
      "        [ 1.2473],\n",
      "        [ 0.6729],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 386: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 386: tensor([[ 1.0735],\n",
      "        [ 0.2101],\n",
      "        [-1.8509],\n",
      "        [ 1.2495],\n",
      "        [ 0.6753],\n",
      "        [-0.2816]], requires_grad=True)\n",
      "poly train loss at 387: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 387: tensor([[ 1.0732],\n",
      "        [ 0.2091],\n",
      "        [-1.8526],\n",
      "        [ 1.2473],\n",
      "        [ 0.6729],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 388: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 388: tensor([[ 1.0735],\n",
      "        [ 0.2101],\n",
      "        [-1.8509],\n",
      "        [ 1.2495],\n",
      "        [ 0.6753],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 389: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 389: tensor([[ 1.0732],\n",
      "        [ 0.2090],\n",
      "        [-1.8527],\n",
      "        [ 1.2474],\n",
      "        [ 0.6730],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 390: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 390: tensor([[ 1.0735],\n",
      "        [ 0.2100],\n",
      "        [-1.8509],\n",
      "        [ 1.2496],\n",
      "        [ 0.6753],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 391: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 391: tensor([[ 1.0732],\n",
      "        [ 0.2090],\n",
      "        [-1.8527],\n",
      "        [ 1.2474],\n",
      "        [ 0.6730],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 392: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 392: tensor([[ 1.0735],\n",
      "        [ 0.2100],\n",
      "        [-1.8510],\n",
      "        [ 1.2496],\n",
      "        [ 0.6754],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 393: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 393: tensor([[ 1.0732],\n",
      "        [ 0.2089],\n",
      "        [-1.8527],\n",
      "        [ 1.2474],\n",
      "        [ 0.6730],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 394: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 394: tensor([[ 1.0735],\n",
      "        [ 0.2099],\n",
      "        [-1.8510],\n",
      "        [ 1.2496],\n",
      "        [ 0.6754],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 395: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 395: tensor([[ 1.0732],\n",
      "        [ 0.2089],\n",
      "        [-1.8528],\n",
      "        [ 1.2474],\n",
      "        [ 0.6730],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 396: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 396: tensor([[ 1.0735],\n",
      "        [ 0.2099],\n",
      "        [-1.8511],\n",
      "        [ 1.2497],\n",
      "        [ 0.6754],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 397: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 397: tensor([[ 1.0732],\n",
      "        [ 0.2089],\n",
      "        [-1.8528],\n",
      "        [ 1.2475],\n",
      "        [ 0.6730],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 398: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 398: tensor([[ 1.0735],\n",
      "        [ 0.2099],\n",
      "        [-1.8511],\n",
      "        [ 1.2497],\n",
      "        [ 0.6754],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 399: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 399: tensor([[ 1.0732],\n",
      "        [ 0.2088],\n",
      "        [-1.8529],\n",
      "        [ 1.2475],\n",
      "        [ 0.6730],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 400: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 400: tensor([[ 1.0735],\n",
      "        [ 0.2098],\n",
      "        [-1.8512],\n",
      "        [ 1.2497],\n",
      "        [ 0.6754],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 401: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 401: tensor([[ 1.0732],\n",
      "        [ 0.2088],\n",
      "        [-1.8529],\n",
      "        [ 1.2475],\n",
      "        [ 0.6730],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 402: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 402: tensor([[ 1.0735],\n",
      "        [ 0.2098],\n",
      "        [-1.8512],\n",
      "        [ 1.2498],\n",
      "        [ 0.6754],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 403: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 403: tensor([[ 1.0732],\n",
      "        [ 0.2087],\n",
      "        [-1.8529],\n",
      "        [ 1.2476],\n",
      "        [ 0.6731],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 404: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 404: tensor([[ 1.0735],\n",
      "        [ 0.2097],\n",
      "        [-1.8512],\n",
      "        [ 1.2498],\n",
      "        [ 0.6755],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 405: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 405: tensor([[ 1.0732],\n",
      "        [ 0.2087],\n",
      "        [-1.8530],\n",
      "        [ 1.2476],\n",
      "        [ 0.6731],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 406: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 406: tensor([[ 1.0735],\n",
      "        [ 0.2097],\n",
      "        [-1.8513],\n",
      "        [ 1.2498],\n",
      "        [ 0.6755],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 407: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 407: tensor([[ 1.0732],\n",
      "        [ 0.2087],\n",
      "        [-1.8530],\n",
      "        [ 1.2476],\n",
      "        [ 0.6731],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 408: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 408: tensor([[ 1.0735],\n",
      "        [ 0.2097],\n",
      "        [-1.8513],\n",
      "        [ 1.2498],\n",
      "        [ 0.6755],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 409: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 409: tensor([[ 1.0732],\n",
      "        [ 0.2086],\n",
      "        [-1.8531],\n",
      "        [ 1.2477],\n",
      "        [ 0.6731],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 410: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 410: tensor([[ 1.0735],\n",
      "        [ 0.2096],\n",
      "        [-1.8514],\n",
      "        [ 1.2499],\n",
      "        [ 0.6755],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 411: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 411: tensor([[ 1.0732],\n",
      "        [ 0.2086],\n",
      "        [-1.8531],\n",
      "        [ 1.2477],\n",
      "        [ 0.6731],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 412: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 412: tensor([[ 1.0735],\n",
      "        [ 0.2096],\n",
      "        [-1.8514],\n",
      "        [ 1.2499],\n",
      "        [ 0.6755],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 413: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 413: tensor([[ 1.0732],\n",
      "        [ 0.2086],\n",
      "        [-1.8531],\n",
      "        [ 1.2477],\n",
      "        [ 0.6731],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 414: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 414: tensor([[ 1.0735],\n",
      "        [ 0.2096],\n",
      "        [-1.8514],\n",
      "        [ 1.2499],\n",
      "        [ 0.6755],\n",
      "        [-0.2817]], requires_grad=True)\n",
      "poly train loss at 415: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 415: tensor([[ 1.0732],\n",
      "        [ 0.2085],\n",
      "        [-1.8532],\n",
      "        [ 1.2477],\n",
      "        [ 0.6732],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 416: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 416: tensor([[ 1.0735],\n",
      "        [ 0.2095],\n",
      "        [-1.8515],\n",
      "        [ 1.2500],\n",
      "        [ 0.6755],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 417: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 417: tensor([[ 1.0732],\n",
      "        [ 0.2085],\n",
      "        [-1.8532],\n",
      "        [ 1.2478],\n",
      "        [ 0.6732],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 418: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 418: tensor([[ 1.0735],\n",
      "        [ 0.2095],\n",
      "        [-1.8515],\n",
      "        [ 1.2500],\n",
      "        [ 0.6756],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 419: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 419: tensor([[ 1.0732],\n",
      "        [ 0.2084],\n",
      "        [-1.8533],\n",
      "        [ 1.2478],\n",
      "        [ 0.6732],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 420: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 420: tensor([[ 1.0735],\n",
      "        [ 0.2094],\n",
      "        [-1.8516],\n",
      "        [ 1.2500],\n",
      "        [ 0.6756],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 421: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 421: tensor([[ 1.0732],\n",
      "        [ 0.2084],\n",
      "        [-1.8533],\n",
      "        [ 1.2478],\n",
      "        [ 0.6732],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 422: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 422: tensor([[ 1.0735],\n",
      "        [ 0.2094],\n",
      "        [-1.8516],\n",
      "        [ 1.2501],\n",
      "        [ 0.6756],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 423: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 423: tensor([[ 1.0732],\n",
      "        [ 0.2084],\n",
      "        [-1.8534],\n",
      "        [ 1.2479],\n",
      "        [ 0.6732],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 424: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 424: tensor([[ 1.0735],\n",
      "        [ 0.2094],\n",
      "        [-1.8516],\n",
      "        [ 1.2501],\n",
      "        [ 0.6756],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 425: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 425: tensor([[ 1.0732],\n",
      "        [ 0.2083],\n",
      "        [-1.8534],\n",
      "        [ 1.2479],\n",
      "        [ 0.6732],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 426: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 426: tensor([[ 1.0735],\n",
      "        [ 0.2093],\n",
      "        [-1.8517],\n",
      "        [ 1.2501],\n",
      "        [ 0.6756],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 427: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 427: tensor([[ 1.0732],\n",
      "        [ 0.2083],\n",
      "        [-1.8534],\n",
      "        [ 1.2479],\n",
      "        [ 0.6732],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 428: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 428: tensor([[ 1.0735],\n",
      "        [ 0.2093],\n",
      "        [-1.8517],\n",
      "        [ 1.2501],\n",
      "        [ 0.6756],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 429: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 429: tensor([[ 1.0732],\n",
      "        [ 0.2082],\n",
      "        [-1.8535],\n",
      "        [ 1.2480],\n",
      "        [ 0.6733],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 430: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 430: tensor([[ 1.0735],\n",
      "        [ 0.2092],\n",
      "        [-1.8518],\n",
      "        [ 1.2502],\n",
      "        [ 0.6757],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 431: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 431: tensor([[ 1.0732],\n",
      "        [ 0.2082],\n",
      "        [-1.8535],\n",
      "        [ 1.2480],\n",
      "        [ 0.6733],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 432: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 432: tensor([[ 1.0735],\n",
      "        [ 0.2092],\n",
      "        [-1.8518],\n",
      "        [ 1.2502],\n",
      "        [ 0.6757],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 433: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 433: tensor([[ 1.0732],\n",
      "        [ 0.2082],\n",
      "        [-1.8536],\n",
      "        [ 1.2480],\n",
      "        [ 0.6733],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 434: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 434: tensor([[ 1.0735],\n",
      "        [ 0.2092],\n",
      "        [-1.8519],\n",
      "        [ 1.2502],\n",
      "        [ 0.6757],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 435: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 435: tensor([[ 1.0732],\n",
      "        [ 0.2081],\n",
      "        [-1.8536],\n",
      "        [ 1.2481],\n",
      "        [ 0.6733],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 436: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 436: tensor([[ 1.0735],\n",
      "        [ 0.2091],\n",
      "        [-1.8519],\n",
      "        [ 1.2503],\n",
      "        [ 0.6757],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 437: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 437: tensor([[ 1.0732],\n",
      "        [ 0.2081],\n",
      "        [-1.8536],\n",
      "        [ 1.2481],\n",
      "        [ 0.6733],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 438: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 438: tensor([[ 1.0735],\n",
      "        [ 0.2091],\n",
      "        [-1.8519],\n",
      "        [ 1.2503],\n",
      "        [ 0.6757],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 439: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 439: tensor([[ 1.0732],\n",
      "        [ 0.2081],\n",
      "        [-1.8537],\n",
      "        [ 1.2481],\n",
      "        [ 0.6733],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 440: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 440: tensor([[ 1.0735],\n",
      "        [ 0.2091],\n",
      "        [-1.8520],\n",
      "        [ 1.2503],\n",
      "        [ 0.6757],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 441: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 441: tensor([[ 1.0732],\n",
      "        [ 0.2080],\n",
      "        [-1.8537],\n",
      "        [ 1.2481],\n",
      "        [ 0.6733],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 442: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 442: tensor([[ 1.0735],\n",
      "        [ 0.2090],\n",
      "        [-1.8520],\n",
      "        [ 1.2504],\n",
      "        [ 0.6757],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 443: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 443: tensor([[ 1.0732],\n",
      "        [ 0.2080],\n",
      "        [-1.8538],\n",
      "        [ 1.2482],\n",
      "        [ 0.6734],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 444: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 444: tensor([[ 1.0735],\n",
      "        [ 0.2090],\n",
      "        [-1.8521],\n",
      "        [ 1.2504],\n",
      "        [ 0.6758],\n",
      "        [-0.2818]], requires_grad=True)\n",
      "poly train loss at 445: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 445: tensor([[ 1.0732],\n",
      "        [ 0.2079],\n",
      "        [-1.8538],\n",
      "        [ 1.2482],\n",
      "        [ 0.6734],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 446: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 446: tensor([[ 1.0735],\n",
      "        [ 0.2089],\n",
      "        [-1.8521],\n",
      "        [ 1.2504],\n",
      "        [ 0.6758],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 447: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 447: tensor([[ 1.0732],\n",
      "        [ 0.2079],\n",
      "        [-1.8538],\n",
      "        [ 1.2482],\n",
      "        [ 0.6734],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 448: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 448: tensor([[ 1.0735],\n",
      "        [ 0.2089],\n",
      "        [-1.8521],\n",
      "        [ 1.2504],\n",
      "        [ 0.6758],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 449: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 449: tensor([[ 1.0732],\n",
      "        [ 0.2079],\n",
      "        [-1.8539],\n",
      "        [ 1.2483],\n",
      "        [ 0.6734],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 450: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 450: tensor([[ 1.0735],\n",
      "        [ 0.2089],\n",
      "        [-1.8522],\n",
      "        [ 1.2505],\n",
      "        [ 0.6758],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 451: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 451: tensor([[ 1.0732],\n",
      "        [ 0.2078],\n",
      "        [-1.8539],\n",
      "        [ 1.2483],\n",
      "        [ 0.6734],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 452: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 452: tensor([[ 1.0735],\n",
      "        [ 0.2088],\n",
      "        [-1.8522],\n",
      "        [ 1.2505],\n",
      "        [ 0.6758],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 453: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 453: tensor([[ 1.0732],\n",
      "        [ 0.2078],\n",
      "        [-1.8540],\n",
      "        [ 1.2483],\n",
      "        [ 0.6734],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 454: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 454: tensor([[ 1.0735],\n",
      "        [ 0.2088],\n",
      "        [-1.8523],\n",
      "        [ 1.2505],\n",
      "        [ 0.6758],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 455: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 455: tensor([[ 1.0732],\n",
      "        [ 0.2078],\n",
      "        [-1.8540],\n",
      "        [ 1.2484],\n",
      "        [ 0.6735],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 456: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 456: tensor([[ 1.0735],\n",
      "        [ 0.2088],\n",
      "        [-1.8523],\n",
      "        [ 1.2506],\n",
      "        [ 0.6758],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 457: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 457: tensor([[ 1.0732],\n",
      "        [ 0.2077],\n",
      "        [-1.8541],\n",
      "        [ 1.2484],\n",
      "        [ 0.6735],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 458: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 458: tensor([[ 1.0735],\n",
      "        [ 0.2087],\n",
      "        [-1.8523],\n",
      "        [ 1.2506],\n",
      "        [ 0.6759],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 459: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 459: tensor([[ 1.0732],\n",
      "        [ 0.2077],\n",
      "        [-1.8541],\n",
      "        [ 1.2484],\n",
      "        [ 0.6735],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 460: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 460: tensor([[ 1.0735],\n",
      "        [ 0.2087],\n",
      "        [-1.8524],\n",
      "        [ 1.2506],\n",
      "        [ 0.6759],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 461: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 461: tensor([[ 1.0732],\n",
      "        [ 0.2076],\n",
      "        [-1.8541],\n",
      "        [ 1.2484],\n",
      "        [ 0.6735],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 462: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 462: tensor([[ 1.0735],\n",
      "        [ 0.2086],\n",
      "        [-1.8524],\n",
      "        [ 1.2507],\n",
      "        [ 0.6759],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 463: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 463: tensor([[ 1.0732],\n",
      "        [ 0.2076],\n",
      "        [-1.8542],\n",
      "        [ 1.2485],\n",
      "        [ 0.6735],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 464: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 464: tensor([[ 1.0735],\n",
      "        [ 0.2086],\n",
      "        [-1.8525],\n",
      "        [ 1.2507],\n",
      "        [ 0.6759],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 465: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 465: tensor([[ 1.0732],\n",
      "        [ 0.2076],\n",
      "        [-1.8542],\n",
      "        [ 1.2485],\n",
      "        [ 0.6735],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 466: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 466: tensor([[ 1.0735],\n",
      "        [ 0.2086],\n",
      "        [-1.8525],\n",
      "        [ 1.2507],\n",
      "        [ 0.6759],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 467: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 467: tensor([[ 1.0732],\n",
      "        [ 0.2075],\n",
      "        [-1.8543],\n",
      "        [ 1.2485],\n",
      "        [ 0.6735],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 468: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 468: tensor([[ 1.0735],\n",
      "        [ 0.2085],\n",
      "        [-1.8526],\n",
      "        [ 1.2507],\n",
      "        [ 0.6759],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 469: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 469: tensor([[ 1.0732],\n",
      "        [ 0.2075],\n",
      "        [-1.8543],\n",
      "        [ 1.2486],\n",
      "        [ 0.6736],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 470: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 470: tensor([[ 1.0735],\n",
      "        [ 0.2085],\n",
      "        [-1.8526],\n",
      "        [ 1.2508],\n",
      "        [ 0.6760],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 471: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 471: tensor([[ 1.0732],\n",
      "        [ 0.2074],\n",
      "        [-1.8543],\n",
      "        [ 1.2486],\n",
      "        [ 0.6736],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 472: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 472: tensor([[ 1.0735],\n",
      "        [ 0.2084],\n",
      "        [-1.8526],\n",
      "        [ 1.2508],\n",
      "        [ 0.6760],\n",
      "        [-0.2819]], requires_grad=True)\n",
      "poly train loss at 473: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 473: tensor([[ 1.0732],\n",
      "        [ 0.2074],\n",
      "        [-1.8544],\n",
      "        [ 1.2486],\n",
      "        [ 0.6736],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 474: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 474: tensor([[ 1.0735],\n",
      "        [ 0.2084],\n",
      "        [-1.8527],\n",
      "        [ 1.2508],\n",
      "        [ 0.6760],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 475: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 475: tensor([[ 1.0732],\n",
      "        [ 0.2074],\n",
      "        [-1.8544],\n",
      "        [ 1.2487],\n",
      "        [ 0.6736],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 476: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 476: tensor([[ 1.0735],\n",
      "        [ 0.2084],\n",
      "        [-1.8527],\n",
      "        [ 1.2509],\n",
      "        [ 0.6760],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 477: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 477: tensor([[ 1.0732],\n",
      "        [ 0.2073],\n",
      "        [-1.8545],\n",
      "        [ 1.2487],\n",
      "        [ 0.6736],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 478: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 478: tensor([[ 1.0735],\n",
      "        [ 0.2083],\n",
      "        [-1.8528],\n",
      "        [ 1.2509],\n",
      "        [ 0.6760],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 479: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 479: tensor([[ 1.0732],\n",
      "        [ 0.2073],\n",
      "        [-1.8545],\n",
      "        [ 1.2487],\n",
      "        [ 0.6736],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 480: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 480: tensor([[ 1.0735],\n",
      "        [ 0.2083],\n",
      "        [-1.8528],\n",
      "        [ 1.2509],\n",
      "        [ 0.6760],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 481: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 481: tensor([[ 1.0732],\n",
      "        [ 0.2073],\n",
      "        [-1.8545],\n",
      "        [ 1.2487],\n",
      "        [ 0.6737],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 482: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 482: tensor([[ 1.0735],\n",
      "        [ 0.2083],\n",
      "        [-1.8528],\n",
      "        [ 1.2510],\n",
      "        [ 0.6760],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 483: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 483: tensor([[ 1.0732],\n",
      "        [ 0.2072],\n",
      "        [-1.8546],\n",
      "        [ 1.2488],\n",
      "        [ 0.6737],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 484: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 484: tensor([[ 1.0735],\n",
      "        [ 0.2082],\n",
      "        [-1.8529],\n",
      "        [ 1.2510],\n",
      "        [ 0.6761],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 485: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 485: tensor([[ 1.0732],\n",
      "        [ 0.2072],\n",
      "        [-1.8546],\n",
      "        [ 1.2488],\n",
      "        [ 0.6737],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 486: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 486: tensor([[ 1.0735],\n",
      "        [ 0.2082],\n",
      "        [-1.8529],\n",
      "        [ 1.2510],\n",
      "        [ 0.6761],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 487: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 487: tensor([[ 1.0732],\n",
      "        [ 0.2071],\n",
      "        [-1.8547],\n",
      "        [ 1.2488],\n",
      "        [ 0.6737],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 488: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 488: tensor([[ 1.0735],\n",
      "        [ 0.2081],\n",
      "        [-1.8530],\n",
      "        [ 1.2510],\n",
      "        [ 0.6761],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 489: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 489: tensor([[ 1.0732],\n",
      "        [ 0.2071],\n",
      "        [-1.8547],\n",
      "        [ 1.2489],\n",
      "        [ 0.6737],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 490: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 490: tensor([[ 1.0735],\n",
      "        [ 0.2081],\n",
      "        [-1.8530],\n",
      "        [ 1.2511],\n",
      "        [ 0.6761],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 491: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 491: tensor([[ 1.0732],\n",
      "        [ 0.2071],\n",
      "        [-1.8547],\n",
      "        [ 1.2489],\n",
      "        [ 0.6737],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 492: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 492: tensor([[ 1.0735],\n",
      "        [ 0.2081],\n",
      "        [-1.8530],\n",
      "        [ 1.2511],\n",
      "        [ 0.6761],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 493: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 493: tensor([[ 1.0732],\n",
      "        [ 0.2070],\n",
      "        [-1.8548],\n",
      "        [ 1.2489],\n",
      "        [ 0.6737],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 494: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 494: tensor([[ 1.0735],\n",
      "        [ 0.2080],\n",
      "        [-1.8531],\n",
      "        [ 1.2511],\n",
      "        [ 0.6761],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 495: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 495: tensor([[ 1.0732],\n",
      "        [ 0.2070],\n",
      "        [-1.8548],\n",
      "        [ 1.2490],\n",
      "        [ 0.6738],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 496: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 496: tensor([[ 1.0735],\n",
      "        [ 0.2080],\n",
      "        [-1.8531],\n",
      "        [ 1.2512],\n",
      "        [ 0.6761],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 497: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 497: tensor([[ 1.0732],\n",
      "        [ 0.2070],\n",
      "        [-1.8549],\n",
      "        [ 1.2490],\n",
      "        [ 0.6738],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 498: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 498: tensor([[ 1.0735],\n",
      "        [ 0.2080],\n",
      "        [-1.8532],\n",
      "        [ 1.2512],\n",
      "        [ 0.6762],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 499: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 499: tensor([[ 1.0732],\n",
      "        [ 0.2069],\n",
      "        [-1.8549],\n",
      "        [ 1.2490],\n",
      "        [ 0.6738],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 500: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 500: tensor([[ 1.0735],\n",
      "        [ 0.2079],\n",
      "        [-1.8532],\n",
      "        [ 1.2512],\n",
      "        [ 0.6762],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 501: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 501: tensor([[ 1.0732],\n",
      "        [ 0.2069],\n",
      "        [-1.8549],\n",
      "        [ 1.2490],\n",
      "        [ 0.6738],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 502: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 502: tensor([[ 1.0735],\n",
      "        [ 0.2079],\n",
      "        [-1.8532],\n",
      "        [ 1.2513],\n",
      "        [ 0.6762],\n",
      "        [-0.2820]], requires_grad=True)\n",
      "poly train loss at 503: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 503: tensor([[ 1.0732],\n",
      "        [ 0.2068],\n",
      "        [-1.8550],\n",
      "        [ 1.2491],\n",
      "        [ 0.6738],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 504: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 504: tensor([[ 1.0735],\n",
      "        [ 0.2078],\n",
      "        [-1.8533],\n",
      "        [ 1.2513],\n",
      "        [ 0.6762],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 505: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 505: tensor([[ 1.0732],\n",
      "        [ 0.2068],\n",
      "        [-1.8550],\n",
      "        [ 1.2491],\n",
      "        [ 0.6738],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 506: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 506: tensor([[ 1.0735],\n",
      "        [ 0.2078],\n",
      "        [-1.8533],\n",
      "        [ 1.2513],\n",
      "        [ 0.6762],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 507: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 507: tensor([[ 1.0732],\n",
      "        [ 0.2068],\n",
      "        [-1.8551],\n",
      "        [ 1.2491],\n",
      "        [ 0.6738],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 508: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 508: tensor([[ 1.0735],\n",
      "        [ 0.2078],\n",
      "        [-1.8534],\n",
      "        [ 1.2513],\n",
      "        [ 0.6762],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 509: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 509: tensor([[ 1.0732],\n",
      "        [ 0.2067],\n",
      "        [-1.8551],\n",
      "        [ 1.2492],\n",
      "        [ 0.6739],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 510: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 510: tensor([[ 1.0735],\n",
      "        [ 0.2077],\n",
      "        [-1.8534],\n",
      "        [ 1.2514],\n",
      "        [ 0.6763],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 511: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 511: tensor([[ 1.0732],\n",
      "        [ 0.2067],\n",
      "        [-1.8552],\n",
      "        [ 1.2492],\n",
      "        [ 0.6739],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 512: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 512: tensor([[ 1.0735],\n",
      "        [ 0.2077],\n",
      "        [-1.8534],\n",
      "        [ 1.2514],\n",
      "        [ 0.6763],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 513: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 513: tensor([[ 1.0732],\n",
      "        [ 0.2067],\n",
      "        [-1.8552],\n",
      "        [ 1.2492],\n",
      "        [ 0.6739],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 514: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 514: tensor([[ 1.0735],\n",
      "        [ 0.2077],\n",
      "        [-1.8535],\n",
      "        [ 1.2514],\n",
      "        [ 0.6763],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 515: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 515: tensor([[ 1.0732],\n",
      "        [ 0.2066],\n",
      "        [-1.8552],\n",
      "        [ 1.2492],\n",
      "        [ 0.6739],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 516: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 516: tensor([[ 1.0735],\n",
      "        [ 0.2076],\n",
      "        [-1.8535],\n",
      "        [ 1.2515],\n",
      "        [ 0.6763],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 517: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 517: tensor([[ 1.0732],\n",
      "        [ 0.2066],\n",
      "        [-1.8553],\n",
      "        [ 1.2493],\n",
      "        [ 0.6739],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 518: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 518: tensor([[ 1.0735],\n",
      "        [ 0.2076],\n",
      "        [-1.8536],\n",
      "        [ 1.2515],\n",
      "        [ 0.6763],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 519: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 519: tensor([[ 1.0732],\n",
      "        [ 0.2065],\n",
      "        [-1.8553],\n",
      "        [ 1.2493],\n",
      "        [ 0.6739],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 520: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 520: tensor([[ 1.0735],\n",
      "        [ 0.2075],\n",
      "        [-1.8536],\n",
      "        [ 1.2515],\n",
      "        [ 0.6763],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 521: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 521: tensor([[ 1.0732],\n",
      "        [ 0.2065],\n",
      "        [-1.8554],\n",
      "        [ 1.2493],\n",
      "        [ 0.6739],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 522: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 522: tensor([[ 1.0735],\n",
      "        [ 0.2075],\n",
      "        [-1.8536],\n",
      "        [ 1.2516],\n",
      "        [ 0.6763],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 523: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 523: tensor([[ 1.0732],\n",
      "        [ 0.2065],\n",
      "        [-1.8554],\n",
      "        [ 1.2494],\n",
      "        [ 0.6740],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 524: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 524: tensor([[ 1.0735],\n",
      "        [ 0.2075],\n",
      "        [-1.8537],\n",
      "        [ 1.2516],\n",
      "        [ 0.6764],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 525: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 525: tensor([[ 1.0732],\n",
      "        [ 0.2064],\n",
      "        [-1.8554],\n",
      "        [ 1.2494],\n",
      "        [ 0.6740],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 526: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 526: tensor([[ 1.0735],\n",
      "        [ 0.2074],\n",
      "        [-1.8537],\n",
      "        [ 1.2516],\n",
      "        [ 0.6764],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 527: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 527: tensor([[ 1.0732],\n",
      "        [ 0.2064],\n",
      "        [-1.8555],\n",
      "        [ 1.2494],\n",
      "        [ 0.6740],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 528: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 528: tensor([[ 1.0735],\n",
      "        [ 0.2074],\n",
      "        [-1.8538],\n",
      "        [ 1.2516],\n",
      "        [ 0.6764],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 529: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 529: tensor([[ 1.0732],\n",
      "        [ 0.2064],\n",
      "        [-1.8555],\n",
      "        [ 1.2495],\n",
      "        [ 0.6740],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 530: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 530: tensor([[ 1.0735],\n",
      "        [ 0.2074],\n",
      "        [-1.8538],\n",
      "        [ 1.2517],\n",
      "        [ 0.6764],\n",
      "        [-0.2821]], requires_grad=True)\n",
      "poly train loss at 531: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 531: tensor([[ 1.0732],\n",
      "        [ 0.2063],\n",
      "        [-1.8556],\n",
      "        [ 1.2495],\n",
      "        [ 0.6740],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 532: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 532: tensor([[ 1.0735],\n",
      "        [ 0.2073],\n",
      "        [-1.8538],\n",
      "        [ 1.2517],\n",
      "        [ 0.6764],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 533: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 533: tensor([[ 1.0732],\n",
      "        [ 0.2063],\n",
      "        [-1.8556],\n",
      "        [ 1.2495],\n",
      "        [ 0.6740],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 534: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 534: tensor([[ 1.0735],\n",
      "        [ 0.2073],\n",
      "        [-1.8539],\n",
      "        [ 1.2517],\n",
      "        [ 0.6764],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 535: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 535: tensor([[ 1.0732],\n",
      "        [ 0.2062],\n",
      "        [-1.8556],\n",
      "        [ 1.2495],\n",
      "        [ 0.6741],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 536: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 536: tensor([[ 1.0735],\n",
      "        [ 0.2072],\n",
      "        [-1.8539],\n",
      "        [ 1.2518],\n",
      "        [ 0.6764],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 537: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 537: tensor([[ 1.0732],\n",
      "        [ 0.2062],\n",
      "        [-1.8557],\n",
      "        [ 1.2496],\n",
      "        [ 0.6741],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 538: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 538: tensor([[ 1.0735],\n",
      "        [ 0.2072],\n",
      "        [-1.8540],\n",
      "        [ 1.2518],\n",
      "        [ 0.6765],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 539: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 539: tensor([[ 1.0732],\n",
      "        [ 0.2062],\n",
      "        [-1.8557],\n",
      "        [ 1.2496],\n",
      "        [ 0.6741],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 540: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 540: tensor([[ 1.0735],\n",
      "        [ 0.2072],\n",
      "        [-1.8540],\n",
      "        [ 1.2518],\n",
      "        [ 0.6765],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 541: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 541: tensor([[ 1.0732],\n",
      "        [ 0.2061],\n",
      "        [-1.8558],\n",
      "        [ 1.2496],\n",
      "        [ 0.6741],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 542: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 542: tensor([[ 1.0735],\n",
      "        [ 0.2071],\n",
      "        [-1.8540],\n",
      "        [ 1.2518],\n",
      "        [ 0.6765],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 543: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 543: tensor([[ 1.0732],\n",
      "        [ 0.2061],\n",
      "        [-1.8558],\n",
      "        [ 1.2497],\n",
      "        [ 0.6741],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 544: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 544: tensor([[ 1.0735],\n",
      "        [ 0.2071],\n",
      "        [-1.8541],\n",
      "        [ 1.2519],\n",
      "        [ 0.6765],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 545: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 545: tensor([[ 1.0732],\n",
      "        [ 0.2061],\n",
      "        [-1.8558],\n",
      "        [ 1.2497],\n",
      "        [ 0.6741],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 546: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 546: tensor([[ 1.0735],\n",
      "        [ 0.2071],\n",
      "        [-1.8541],\n",
      "        [ 1.2519],\n",
      "        [ 0.6765],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 547: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 547: tensor([[ 1.0732],\n",
      "        [ 0.2060],\n",
      "        [-1.8559],\n",
      "        [ 1.2497],\n",
      "        [ 0.6741],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 548: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 548: tensor([[ 1.0735],\n",
      "        [ 0.2070],\n",
      "        [-1.8542],\n",
      "        [ 1.2519],\n",
      "        [ 0.6765],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 549: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 549: tensor([[ 1.0732],\n",
      "        [ 0.2060],\n",
      "        [-1.8559],\n",
      "        [ 1.2497],\n",
      "        [ 0.6742],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 550: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 550: tensor([[ 1.0735],\n",
      "        [ 0.2070],\n",
      "        [-1.8542],\n",
      "        [ 1.2520],\n",
      "        [ 0.6765],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 551: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 551: tensor([[ 1.0732],\n",
      "        [ 0.2059],\n",
      "        [-1.8560],\n",
      "        [ 1.2498],\n",
      "        [ 0.6742],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 552: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 552: tensor([[ 1.0735],\n",
      "        [ 0.2069],\n",
      "        [-1.8542],\n",
      "        [ 1.2520],\n",
      "        [ 0.6766],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 553: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 553: tensor([[ 1.0732],\n",
      "        [ 0.2059],\n",
      "        [-1.8560],\n",
      "        [ 1.2498],\n",
      "        [ 0.6742],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 554: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 554: tensor([[ 1.0735],\n",
      "        [ 0.2069],\n",
      "        [-1.8543],\n",
      "        [ 1.2520],\n",
      "        [ 0.6766],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 555: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 555: tensor([[ 1.0732],\n",
      "        [ 0.2059],\n",
      "        [-1.8560],\n",
      "        [ 1.2498],\n",
      "        [ 0.6742],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 556: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 556: tensor([[ 1.0735],\n",
      "        [ 0.2069],\n",
      "        [-1.8543],\n",
      "        [ 1.2521],\n",
      "        [ 0.6766],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 557: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 557: tensor([[ 1.0732],\n",
      "        [ 0.2058],\n",
      "        [-1.8561],\n",
      "        [ 1.2499],\n",
      "        [ 0.6742],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 558: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 558: tensor([[ 1.0735],\n",
      "        [ 0.2068],\n",
      "        [-1.8544],\n",
      "        [ 1.2521],\n",
      "        [ 0.6766],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 559: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 559: tensor([[ 1.0732],\n",
      "        [ 0.2058],\n",
      "        [-1.8561],\n",
      "        [ 1.2499],\n",
      "        [ 0.6742],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 560: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 560: tensor([[ 1.0735],\n",
      "        [ 0.2068],\n",
      "        [-1.8544],\n",
      "        [ 1.2521],\n",
      "        [ 0.6766],\n",
      "        [-0.2822]], requires_grad=True)\n",
      "poly train loss at 561: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 561: tensor([[ 1.0732],\n",
      "        [ 0.2058],\n",
      "        [-1.8562],\n",
      "        [ 1.2499],\n",
      "        [ 0.6742],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 562: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 562: tensor([[ 1.0735],\n",
      "        [ 0.2068],\n",
      "        [-1.8545],\n",
      "        [ 1.2521],\n",
      "        [ 0.6766],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 563: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 563: tensor([[ 1.0732],\n",
      "        [ 0.2057],\n",
      "        [-1.8562],\n",
      "        [ 1.2500],\n",
      "        [ 0.6743],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 564: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 564: tensor([[ 1.0735],\n",
      "        [ 0.2067],\n",
      "        [-1.8545],\n",
      "        [ 1.2522],\n",
      "        [ 0.6767],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 565: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 565: tensor([[ 1.0732],\n",
      "        [ 0.2057],\n",
      "        [-1.8562],\n",
      "        [ 1.2500],\n",
      "        [ 0.6743],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 566: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 566: tensor([[ 1.0735],\n",
      "        [ 0.2067],\n",
      "        [-1.8545],\n",
      "        [ 1.2522],\n",
      "        [ 0.6767],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 567: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 567: tensor([[ 1.0732],\n",
      "        [ 0.2056],\n",
      "        [-1.8563],\n",
      "        [ 1.2500],\n",
      "        [ 0.6743],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 568: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 568: tensor([[ 1.0735],\n",
      "        [ 0.2066],\n",
      "        [-1.8546],\n",
      "        [ 1.2522],\n",
      "        [ 0.6767],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 569: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 569: tensor([[ 1.0732],\n",
      "        [ 0.2056],\n",
      "        [-1.8563],\n",
      "        [ 1.2500],\n",
      "        [ 0.6743],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 570: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 570: tensor([[ 1.0736],\n",
      "        [ 0.2066],\n",
      "        [-1.8546],\n",
      "        [ 1.2523],\n",
      "        [ 0.6767],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 571: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 571: tensor([[ 1.0732],\n",
      "        [ 0.2056],\n",
      "        [-1.8564],\n",
      "        [ 1.2501],\n",
      "        [ 0.6743],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 572: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 572: tensor([[ 1.0736],\n",
      "        [ 0.2066],\n",
      "        [-1.8547],\n",
      "        [ 1.2523],\n",
      "        [ 0.6767],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 573: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 573: tensor([[ 1.0732],\n",
      "        [ 0.2055],\n",
      "        [-1.8564],\n",
      "        [ 1.2501],\n",
      "        [ 0.6743],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 574: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 574: tensor([[ 1.0736],\n",
      "        [ 0.2065],\n",
      "        [-1.8547],\n",
      "        [ 1.2523],\n",
      "        [ 0.6767],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 575: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 575: tensor([[ 1.0732],\n",
      "        [ 0.2055],\n",
      "        [-1.8564],\n",
      "        [ 1.2501],\n",
      "        [ 0.6743],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 576: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 576: tensor([[ 1.0736],\n",
      "        [ 0.2065],\n",
      "        [-1.8547],\n",
      "        [ 1.2523],\n",
      "        [ 0.6767],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 577: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 577: tensor([[ 1.0732],\n",
      "        [ 0.2055],\n",
      "        [-1.8565],\n",
      "        [ 1.2502],\n",
      "        [ 0.6744],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 578: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 578: tensor([[ 1.0736],\n",
      "        [ 0.2065],\n",
      "        [-1.8548],\n",
      "        [ 1.2524],\n",
      "        [ 0.6768],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 579: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 579: tensor([[ 1.0732],\n",
      "        [ 0.2054],\n",
      "        [-1.8565],\n",
      "        [ 1.2502],\n",
      "        [ 0.6744],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 580: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 580: tensor([[ 1.0736],\n",
      "        [ 0.2064],\n",
      "        [-1.8548],\n",
      "        [ 1.2524],\n",
      "        [ 0.6768],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 581: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 581: tensor([[ 1.0732],\n",
      "        [ 0.2054],\n",
      "        [-1.8566],\n",
      "        [ 1.2502],\n",
      "        [ 0.6744],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 582: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 582: tensor([[ 1.0736],\n",
      "        [ 0.2064],\n",
      "        [-1.8549],\n",
      "        [ 1.2524],\n",
      "        [ 0.6768],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 583: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 583: tensor([[ 1.0732],\n",
      "        [ 0.2053],\n",
      "        [-1.8566],\n",
      "        [ 1.2502],\n",
      "        [ 0.6744],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 584: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 584: tensor([[ 1.0736],\n",
      "        [ 0.2063],\n",
      "        [-1.8549],\n",
      "        [ 1.2525],\n",
      "        [ 0.6768],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 585: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 585: tensor([[ 1.0732],\n",
      "        [ 0.2053],\n",
      "        [-1.8566],\n",
      "        [ 1.2503],\n",
      "        [ 0.6744],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 586: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 586: tensor([[ 1.0736],\n",
      "        [ 0.2063],\n",
      "        [-1.8549],\n",
      "        [ 1.2525],\n",
      "        [ 0.6768],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 587: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 587: tensor([[ 1.0732],\n",
      "        [ 0.2053],\n",
      "        [-1.8567],\n",
      "        [ 1.2503],\n",
      "        [ 0.6744],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 588: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 588: tensor([[ 1.0736],\n",
      "        [ 0.2063],\n",
      "        [-1.8550],\n",
      "        [ 1.2525],\n",
      "        [ 0.6768],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 589: tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "poly w at 589: tensor([[ 1.0732],\n",
      "        [ 0.2052],\n",
      "        [-1.8567],\n",
      "        [ 1.2503],\n",
      "        [ 0.6745],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 590: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 590: tensor([[ 1.0736],\n",
      "        [ 0.2062],\n",
      "        [-1.8550],\n",
      "        [ 1.2525],\n",
      "        [ 0.6768],\n",
      "        [-0.2823]], requires_grad=True)\n",
      "poly train loss at 591: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 591: tensor([[ 1.0732],\n",
      "        [ 0.2052],\n",
      "        [-1.8568],\n",
      "        [ 1.2504],\n",
      "        [ 0.6745],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 592: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 592: tensor([[ 1.0736],\n",
      "        [ 0.2062],\n",
      "        [-1.8551],\n",
      "        [ 1.2526],\n",
      "        [ 0.6769],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 593: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 593: tensor([[ 1.0732],\n",
      "        [ 0.2052],\n",
      "        [-1.8568],\n",
      "        [ 1.2504],\n",
      "        [ 0.6745],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 594: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 594: tensor([[ 1.0736],\n",
      "        [ 0.2062],\n",
      "        [-1.8551],\n",
      "        [ 1.2526],\n",
      "        [ 0.6769],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 595: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 595: tensor([[ 1.0732],\n",
      "        [ 0.2051],\n",
      "        [-1.8568],\n",
      "        [ 1.2504],\n",
      "        [ 0.6745],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 596: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 596: tensor([[ 1.0736],\n",
      "        [ 0.2061],\n",
      "        [-1.8551],\n",
      "        [ 1.2526],\n",
      "        [ 0.6769],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 597: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 597: tensor([[ 1.0732],\n",
      "        [ 0.2051],\n",
      "        [-1.8569],\n",
      "        [ 1.2505],\n",
      "        [ 0.6745],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 598: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 598: tensor([[ 1.0736],\n",
      "        [ 0.2061],\n",
      "        [-1.8552],\n",
      "        [ 1.2527],\n",
      "        [ 0.6769],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 599: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 599: tensor([[ 1.0732],\n",
      "        [ 0.2051],\n",
      "        [-1.8569],\n",
      "        [ 1.2505],\n",
      "        [ 0.6745],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 600: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 600: tensor([[ 1.0736],\n",
      "        [ 0.2061],\n",
      "        [-1.8552],\n",
      "        [ 1.2527],\n",
      "        [ 0.6769],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 601: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 601: tensor([[ 1.0732],\n",
      "        [ 0.2050],\n",
      "        [-1.8570],\n",
      "        [ 1.2505],\n",
      "        [ 0.6745],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 602: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 602: tensor([[ 1.0736],\n",
      "        [ 0.2060],\n",
      "        [-1.8553],\n",
      "        [ 1.2527],\n",
      "        [ 0.6769],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 603: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 603: tensor([[ 1.0732],\n",
      "        [ 0.2050],\n",
      "        [-1.8570],\n",
      "        [ 1.2505],\n",
      "        [ 0.6746],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 604: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 604: tensor([[ 1.0736],\n",
      "        [ 0.2060],\n",
      "        [-1.8553],\n",
      "        [ 1.2528],\n",
      "        [ 0.6769],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 605: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 605: tensor([[ 1.0732],\n",
      "        [ 0.2049],\n",
      "        [-1.8570],\n",
      "        [ 1.2506],\n",
      "        [ 0.6746],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 606: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 606: tensor([[ 1.0736],\n",
      "        [ 0.2059],\n",
      "        [-1.8553],\n",
      "        [ 1.2528],\n",
      "        [ 0.6770],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 607: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 607: tensor([[ 1.0732],\n",
      "        [ 0.2049],\n",
      "        [-1.8571],\n",
      "        [ 1.2506],\n",
      "        [ 0.6746],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 608: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 608: tensor([[ 1.0736],\n",
      "        [ 0.2059],\n",
      "        [-1.8554],\n",
      "        [ 1.2528],\n",
      "        [ 0.6770],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 609: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 609: tensor([[ 1.0732],\n",
      "        [ 0.2049],\n",
      "        [-1.8571],\n",
      "        [ 1.2506],\n",
      "        [ 0.6746],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 610: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 610: tensor([[ 1.0736],\n",
      "        [ 0.2059],\n",
      "        [-1.8554],\n",
      "        [ 1.2528],\n",
      "        [ 0.6770],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 611: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 611: tensor([[ 1.0732],\n",
      "        [ 0.2048],\n",
      "        [-1.8572],\n",
      "        [ 1.2507],\n",
      "        [ 0.6746],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 612: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 612: tensor([[ 1.0736],\n",
      "        [ 0.2058],\n",
      "        [-1.8555],\n",
      "        [ 1.2529],\n",
      "        [ 0.6770],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 613: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 613: tensor([[ 1.0732],\n",
      "        [ 0.2048],\n",
      "        [-1.8572],\n",
      "        [ 1.2507],\n",
      "        [ 0.6746],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 614: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 614: tensor([[ 1.0736],\n",
      "        [ 0.2058],\n",
      "        [-1.8555],\n",
      "        [ 1.2529],\n",
      "        [ 0.6770],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 615: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 615: tensor([[ 1.0732],\n",
      "        [ 0.2048],\n",
      "        [-1.8572],\n",
      "        [ 1.2507],\n",
      "        [ 0.6746],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 616: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 616: tensor([[ 1.0736],\n",
      "        [ 0.2058],\n",
      "        [-1.8555],\n",
      "        [ 1.2529],\n",
      "        [ 0.6770],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 617: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 617: tensor([[ 1.0732],\n",
      "        [ 0.2047],\n",
      "        [-1.8573],\n",
      "        [ 1.2507],\n",
      "        [ 0.6747],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 618: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 618: tensor([[ 1.0736],\n",
      "        [ 0.2057],\n",
      "        [-1.8556],\n",
      "        [ 1.2530],\n",
      "        [ 0.6770],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 619: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 619: tensor([[ 1.0732],\n",
      "        [ 0.2047],\n",
      "        [-1.8573],\n",
      "        [ 1.2508],\n",
      "        [ 0.6747],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 620: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 620: tensor([[ 1.0736],\n",
      "        [ 0.2057],\n",
      "        [-1.8556],\n",
      "        [ 1.2530],\n",
      "        [ 0.6771],\n",
      "        [-0.2824]], requires_grad=True)\n",
      "poly train loss at 621: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 621: tensor([[ 1.0732],\n",
      "        [ 0.2046],\n",
      "        [-1.8574],\n",
      "        [ 1.2508],\n",
      "        [ 0.6747],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 622: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 622: tensor([[ 1.0736],\n",
      "        [ 0.2056],\n",
      "        [-1.8557],\n",
      "        [ 1.2530],\n",
      "        [ 0.6771],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 623: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 623: tensor([[ 1.0732],\n",
      "        [ 0.2046],\n",
      "        [-1.8574],\n",
      "        [ 1.2508],\n",
      "        [ 0.6747],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 624: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 624: tensor([[ 1.0736],\n",
      "        [ 0.2056],\n",
      "        [-1.8557],\n",
      "        [ 1.2530],\n",
      "        [ 0.6771],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 625: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 625: tensor([[ 1.0732],\n",
      "        [ 0.2046],\n",
      "        [-1.8574],\n",
      "        [ 1.2509],\n",
      "        [ 0.6747],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 626: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 626: tensor([[ 1.0736],\n",
      "        [ 0.2056],\n",
      "        [-1.8557],\n",
      "        [ 1.2531],\n",
      "        [ 0.6771],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 627: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 627: tensor([[ 1.0732],\n",
      "        [ 0.2045],\n",
      "        [-1.8575],\n",
      "        [ 1.2509],\n",
      "        [ 0.6747],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 628: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 628: tensor([[ 1.0736],\n",
      "        [ 0.2055],\n",
      "        [-1.8558],\n",
      "        [ 1.2531],\n",
      "        [ 0.6771],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 629: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 629: tensor([[ 1.0732],\n",
      "        [ 0.2045],\n",
      "        [-1.8575],\n",
      "        [ 1.2509],\n",
      "        [ 0.6747],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 630: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 630: tensor([[ 1.0736],\n",
      "        [ 0.2055],\n",
      "        [-1.8558],\n",
      "        [ 1.2531],\n",
      "        [ 0.6771],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 631: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 631: tensor([[ 1.0732],\n",
      "        [ 0.2045],\n",
      "        [-1.8576],\n",
      "        [ 1.2509],\n",
      "        [ 0.6748],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 632: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 632: tensor([[ 1.0736],\n",
      "        [ 0.2055],\n",
      "        [-1.8559],\n",
      "        [ 1.2532],\n",
      "        [ 0.6772],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 633: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 633: tensor([[ 1.0732],\n",
      "        [ 0.2044],\n",
      "        [-1.8576],\n",
      "        [ 1.2510],\n",
      "        [ 0.6748],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 634: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 634: tensor([[ 1.0736],\n",
      "        [ 0.2054],\n",
      "        [-1.8559],\n",
      "        [ 1.2532],\n",
      "        [ 0.6772],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 635: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 635: tensor([[ 1.0732],\n",
      "        [ 0.2044],\n",
      "        [-1.8576],\n",
      "        [ 1.2510],\n",
      "        [ 0.6748],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 636: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 636: tensor([[ 1.0736],\n",
      "        [ 0.2054],\n",
      "        [-1.8559],\n",
      "        [ 1.2532],\n",
      "        [ 0.6772],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 637: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 637: tensor([[ 1.0732],\n",
      "        [ 0.2044],\n",
      "        [-1.8577],\n",
      "        [ 1.2510],\n",
      "        [ 0.6748],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 638: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 638: tensor([[ 1.0736],\n",
      "        [ 0.2054],\n",
      "        [-1.8560],\n",
      "        [ 1.2532],\n",
      "        [ 0.6772],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 639: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 639: tensor([[ 1.0732],\n",
      "        [ 0.2043],\n",
      "        [-1.8577],\n",
      "        [ 1.2511],\n",
      "        [ 0.6748],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 640: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 640: tensor([[ 1.0736],\n",
      "        [ 0.2053],\n",
      "        [-1.8560],\n",
      "        [ 1.2533],\n",
      "        [ 0.6772],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 641: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 641: tensor([[ 1.0732],\n",
      "        [ 0.2043],\n",
      "        [-1.8578],\n",
      "        [ 1.2511],\n",
      "        [ 0.6748],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 642: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 642: tensor([[ 1.0736],\n",
      "        [ 0.2053],\n",
      "        [-1.8560],\n",
      "        [ 1.2533],\n",
      "        [ 0.6772],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 643: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 643: tensor([[ 1.0732],\n",
      "        [ 0.2042],\n",
      "        [-1.8578],\n",
      "        [ 1.2511],\n",
      "        [ 0.6748],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 644: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 644: tensor([[ 1.0736],\n",
      "        [ 0.2052],\n",
      "        [-1.8561],\n",
      "        [ 1.2533],\n",
      "        [ 0.6772],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 645: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 645: tensor([[ 1.0732],\n",
      "        [ 0.2042],\n",
      "        [-1.8578],\n",
      "        [ 1.2511],\n",
      "        [ 0.6749],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 646: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 646: tensor([[ 1.0736],\n",
      "        [ 0.2052],\n",
      "        [-1.8561],\n",
      "        [ 1.2534],\n",
      "        [ 0.6773],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 647: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 647: tensor([[ 1.0732],\n",
      "        [ 0.2042],\n",
      "        [-1.8579],\n",
      "        [ 1.2512],\n",
      "        [ 0.6749],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 648: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 648: tensor([[ 1.0736],\n",
      "        [ 0.2052],\n",
      "        [-1.8562],\n",
      "        [ 1.2534],\n",
      "        [ 0.6773],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 649: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 649: tensor([[ 1.0733],\n",
      "        [ 0.2041],\n",
      "        [-1.8579],\n",
      "        [ 1.2512],\n",
      "        [ 0.6749],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 650: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 650: tensor([[ 1.0736],\n",
      "        [ 0.2051],\n",
      "        [-1.8562],\n",
      "        [ 1.2534],\n",
      "        [ 0.6773],\n",
      "        [-0.2825]], requires_grad=True)\n",
      "poly train loss at 651: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 651: tensor([[ 1.0733],\n",
      "        [ 0.2041],\n",
      "        [-1.8580],\n",
      "        [ 1.2512],\n",
      "        [ 0.6749],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 652: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 652: tensor([[ 1.0736],\n",
      "        [ 0.2051],\n",
      "        [-1.8562],\n",
      "        [ 1.2534],\n",
      "        [ 0.6773],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 653: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 653: tensor([[ 1.0733],\n",
      "        [ 0.2041],\n",
      "        [-1.8580],\n",
      "        [ 1.2513],\n",
      "        [ 0.6749],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 654: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 654: tensor([[ 1.0736],\n",
      "        [ 0.2051],\n",
      "        [-1.8563],\n",
      "        [ 1.2535],\n",
      "        [ 0.6773],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 655: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 655: tensor([[ 1.0733],\n",
      "        [ 0.2040],\n",
      "        [-1.8580],\n",
      "        [ 1.2513],\n",
      "        [ 0.6749],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 656: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 656: tensor([[ 1.0736],\n",
      "        [ 0.2050],\n",
      "        [-1.8563],\n",
      "        [ 1.2535],\n",
      "        [ 0.6773],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 657: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 657: tensor([[ 1.0733],\n",
      "        [ 0.2040],\n",
      "        [-1.8581],\n",
      "        [ 1.2513],\n",
      "        [ 0.6749],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 658: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 658: tensor([[ 1.0736],\n",
      "        [ 0.2050],\n",
      "        [-1.8564],\n",
      "        [ 1.2535],\n",
      "        [ 0.6773],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 659: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 659: tensor([[ 1.0733],\n",
      "        [ 0.2040],\n",
      "        [-1.8581],\n",
      "        [ 1.2513],\n",
      "        [ 0.6750],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 660: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 660: tensor([[ 1.0736],\n",
      "        [ 0.2050],\n",
      "        [-1.8564],\n",
      "        [ 1.2536],\n",
      "        [ 0.6774],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 661: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 661: tensor([[ 1.0733],\n",
      "        [ 0.2039],\n",
      "        [-1.8582],\n",
      "        [ 1.2514],\n",
      "        [ 0.6750],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 662: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 662: tensor([[ 1.0736],\n",
      "        [ 0.2049],\n",
      "        [-1.8564],\n",
      "        [ 1.2536],\n",
      "        [ 0.6774],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 663: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 663: tensor([[ 1.0733],\n",
      "        [ 0.2039],\n",
      "        [-1.8582],\n",
      "        [ 1.2514],\n",
      "        [ 0.6750],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 664: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 664: tensor([[ 1.0736],\n",
      "        [ 0.2049],\n",
      "        [-1.8565],\n",
      "        [ 1.2536],\n",
      "        [ 0.6774],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 665: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 665: tensor([[ 1.0733],\n",
      "        [ 0.2038],\n",
      "        [-1.8582],\n",
      "        [ 1.2514],\n",
      "        [ 0.6750],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 666: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 666: tensor([[ 1.0736],\n",
      "        [ 0.2048],\n",
      "        [-1.8565],\n",
      "        [ 1.2536],\n",
      "        [ 0.6774],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 667: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 667: tensor([[ 1.0733],\n",
      "        [ 0.2038],\n",
      "        [-1.8583],\n",
      "        [ 1.2515],\n",
      "        [ 0.6750],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 668: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 668: tensor([[ 1.0736],\n",
      "        [ 0.2048],\n",
      "        [-1.8566],\n",
      "        [ 1.2537],\n",
      "        [ 0.6774],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 669: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 669: tensor([[ 1.0733],\n",
      "        [ 0.2038],\n",
      "        [-1.8583],\n",
      "        [ 1.2515],\n",
      "        [ 0.6750],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 670: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 670: tensor([[ 1.0736],\n",
      "        [ 0.2048],\n",
      "        [-1.8566],\n",
      "        [ 1.2537],\n",
      "        [ 0.6774],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 671: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 671: tensor([[ 1.0733],\n",
      "        [ 0.2037],\n",
      "        [-1.8583],\n",
      "        [ 1.2515],\n",
      "        [ 0.6751],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 672: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 672: tensor([[ 1.0736],\n",
      "        [ 0.2047],\n",
      "        [-1.8566],\n",
      "        [ 1.2537],\n",
      "        [ 0.6774],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 673: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 673: tensor([[ 1.0733],\n",
      "        [ 0.2037],\n",
      "        [-1.8584],\n",
      "        [ 1.2515],\n",
      "        [ 0.6751],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 674: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 674: tensor([[ 1.0736],\n",
      "        [ 0.2047],\n",
      "        [-1.8567],\n",
      "        [ 1.2538],\n",
      "        [ 0.6775],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 675: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 675: tensor([[ 1.0733],\n",
      "        [ 0.2037],\n",
      "        [-1.8584],\n",
      "        [ 1.2516],\n",
      "        [ 0.6751],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 676: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 676: tensor([[ 1.0736],\n",
      "        [ 0.2047],\n",
      "        [-1.8567],\n",
      "        [ 1.2538],\n",
      "        [ 0.6775],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 677: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 677: tensor([[ 1.0733],\n",
      "        [ 0.2036],\n",
      "        [-1.8585],\n",
      "        [ 1.2516],\n",
      "        [ 0.6751],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 678: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 678: tensor([[ 1.0736],\n",
      "        [ 0.2046],\n",
      "        [-1.8568],\n",
      "        [ 1.2538],\n",
      "        [ 0.6775],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 679: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 679: tensor([[ 1.0733],\n",
      "        [ 0.2036],\n",
      "        [-1.8585],\n",
      "        [ 1.2516],\n",
      "        [ 0.6751],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 680: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 680: tensor([[ 1.0736],\n",
      "        [ 0.2046],\n",
      "        [-1.8568],\n",
      "        [ 1.2538],\n",
      "        [ 0.6775],\n",
      "        [-0.2826]], requires_grad=True)\n",
      "poly train loss at 681: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 681: tensor([[ 1.0733],\n",
      "        [ 0.2036],\n",
      "        [-1.8585],\n",
      "        [ 1.2517],\n",
      "        [ 0.6751],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 682: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 682: tensor([[ 1.0736],\n",
      "        [ 0.2046],\n",
      "        [-1.8568],\n",
      "        [ 1.2539],\n",
      "        [ 0.6775],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 683: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 683: tensor([[ 1.0733],\n",
      "        [ 0.2035],\n",
      "        [-1.8586],\n",
      "        [ 1.2517],\n",
      "        [ 0.6751],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 684: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 684: tensor([[ 1.0736],\n",
      "        [ 0.2045],\n",
      "        [-1.8569],\n",
      "        [ 1.2539],\n",
      "        [ 0.6775],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 685: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 685: tensor([[ 1.0733],\n",
      "        [ 0.2035],\n",
      "        [-1.8586],\n",
      "        [ 1.2517],\n",
      "        [ 0.6752],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 686: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 686: tensor([[ 1.0736],\n",
      "        [ 0.2045],\n",
      "        [-1.8569],\n",
      "        [ 1.2539],\n",
      "        [ 0.6775],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 687: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 687: tensor([[ 1.0733],\n",
      "        [ 0.2034],\n",
      "        [-1.8587],\n",
      "        [ 1.2517],\n",
      "        [ 0.6752],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 688: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 688: tensor([[ 1.0736],\n",
      "        [ 0.2044],\n",
      "        [-1.8570],\n",
      "        [ 1.2540],\n",
      "        [ 0.6776],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 689: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 689: tensor([[ 1.0733],\n",
      "        [ 0.2034],\n",
      "        [-1.8587],\n",
      "        [ 1.2518],\n",
      "        [ 0.6752],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 690: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 690: tensor([[ 1.0736],\n",
      "        [ 0.2044],\n",
      "        [-1.8570],\n",
      "        [ 1.2540],\n",
      "        [ 0.6776],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 691: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 691: tensor([[ 1.0733],\n",
      "        [ 0.2034],\n",
      "        [-1.8587],\n",
      "        [ 1.2518],\n",
      "        [ 0.6752],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 692: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 692: tensor([[ 1.0736],\n",
      "        [ 0.2044],\n",
      "        [-1.8570],\n",
      "        [ 1.2540],\n",
      "        [ 0.6776],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 693: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 693: tensor([[ 1.0733],\n",
      "        [ 0.2033],\n",
      "        [-1.8588],\n",
      "        [ 1.2518],\n",
      "        [ 0.6752],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 694: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 694: tensor([[ 1.0736],\n",
      "        [ 0.2043],\n",
      "        [-1.8571],\n",
      "        [ 1.2541],\n",
      "        [ 0.6776],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 695: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 695: tensor([[ 1.0733],\n",
      "        [ 0.2033],\n",
      "        [-1.8588],\n",
      "        [ 1.2519],\n",
      "        [ 0.6752],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 696: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 696: tensor([[ 1.0736],\n",
      "        [ 0.2043],\n",
      "        [-1.8571],\n",
      "        [ 1.2541],\n",
      "        [ 0.6776],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 697: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 697: tensor([[ 1.0733],\n",
      "        [ 0.2033],\n",
      "        [-1.8589],\n",
      "        [ 1.2519],\n",
      "        [ 0.6752],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 698: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 698: tensor([[ 1.0736],\n",
      "        [ 0.2043],\n",
      "        [-1.8571],\n",
      "        [ 1.2541],\n",
      "        [ 0.6776],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 699: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 699: tensor([[ 1.0733],\n",
      "        [ 0.2032],\n",
      "        [-1.8589],\n",
      "        [ 1.2519],\n",
      "        [ 0.6753],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 700: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 700: tensor([[ 1.0736],\n",
      "        [ 0.2042],\n",
      "        [-1.8572],\n",
      "        [ 1.2541],\n",
      "        [ 0.6776],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 701: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 701: tensor([[ 1.0733],\n",
      "        [ 0.2032],\n",
      "        [-1.8589],\n",
      "        [ 1.2520],\n",
      "        [ 0.6753],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 702: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 702: tensor([[ 1.0736],\n",
      "        [ 0.2042],\n",
      "        [-1.8572],\n",
      "        [ 1.2542],\n",
      "        [ 0.6777],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 703: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 703: tensor([[ 1.0733],\n",
      "        [ 0.2032],\n",
      "        [-1.8590],\n",
      "        [ 1.2520],\n",
      "        [ 0.6753],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 704: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 704: tensor([[ 1.0736],\n",
      "        [ 0.2042],\n",
      "        [-1.8573],\n",
      "        [ 1.2542],\n",
      "        [ 0.6777],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 705: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 705: tensor([[ 1.0733],\n",
      "        [ 0.2031],\n",
      "        [-1.8590],\n",
      "        [ 1.2520],\n",
      "        [ 0.6753],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 706: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 706: tensor([[ 1.0736],\n",
      "        [ 0.2041],\n",
      "        [-1.8573],\n",
      "        [ 1.2542],\n",
      "        [ 0.6777],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 707: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 707: tensor([[ 1.0733],\n",
      "        [ 0.2031],\n",
      "        [-1.8591],\n",
      "        [ 1.2520],\n",
      "        [ 0.6753],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 708: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 708: tensor([[ 1.0736],\n",
      "        [ 0.2041],\n",
      "        [-1.8573],\n",
      "        [ 1.2543],\n",
      "        [ 0.6777],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 709: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 709: tensor([[ 1.0733],\n",
      "        [ 0.2030],\n",
      "        [-1.8591],\n",
      "        [ 1.2521],\n",
      "        [ 0.6753],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 710: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 710: tensor([[ 1.0736],\n",
      "        [ 0.2040],\n",
      "        [-1.8574],\n",
      "        [ 1.2543],\n",
      "        [ 0.6777],\n",
      "        [-0.2827]], requires_grad=True)\n",
      "poly train loss at 711: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 711: tensor([[ 1.0733],\n",
      "        [ 0.2030],\n",
      "        [-1.8591],\n",
      "        [ 1.2521],\n",
      "        [ 0.6753],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 712: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 712: tensor([[ 1.0736],\n",
      "        [ 0.2040],\n",
      "        [-1.8574],\n",
      "        [ 1.2543],\n",
      "        [ 0.6777],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 713: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 713: tensor([[ 1.0733],\n",
      "        [ 0.2030],\n",
      "        [-1.8592],\n",
      "        [ 1.2521],\n",
      "        [ 0.6754],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 714: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 714: tensor([[ 1.0736],\n",
      "        [ 0.2040],\n",
      "        [-1.8575],\n",
      "        [ 1.2543],\n",
      "        [ 0.6777],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 715: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 715: tensor([[ 1.0733],\n",
      "        [ 0.2029],\n",
      "        [-1.8592],\n",
      "        [ 1.2522],\n",
      "        [ 0.6754],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 716: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 716: tensor([[ 1.0736],\n",
      "        [ 0.2039],\n",
      "        [-1.8575],\n",
      "        [ 1.2544],\n",
      "        [ 0.6778],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 717: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 717: tensor([[ 1.0733],\n",
      "        [ 0.2029],\n",
      "        [-1.8592],\n",
      "        [ 1.2522],\n",
      "        [ 0.6754],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 718: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 718: tensor([[ 1.0736],\n",
      "        [ 0.2039],\n",
      "        [-1.8575],\n",
      "        [ 1.2544],\n",
      "        [ 0.6778],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 719: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 719: tensor([[ 1.0733],\n",
      "        [ 0.2029],\n",
      "        [-1.8593],\n",
      "        [ 1.2522],\n",
      "        [ 0.6754],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 720: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 720: tensor([[ 1.0736],\n",
      "        [ 0.2039],\n",
      "        [-1.8576],\n",
      "        [ 1.2544],\n",
      "        [ 0.6778],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 721: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 721: tensor([[ 1.0733],\n",
      "        [ 0.2028],\n",
      "        [-1.8593],\n",
      "        [ 1.2522],\n",
      "        [ 0.6754],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 722: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 722: tensor([[ 1.0736],\n",
      "        [ 0.2038],\n",
      "        [-1.8576],\n",
      "        [ 1.2545],\n",
      "        [ 0.6778],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 723: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 723: tensor([[ 1.0733],\n",
      "        [ 0.2028],\n",
      "        [-1.8594],\n",
      "        [ 1.2523],\n",
      "        [ 0.6754],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 724: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 724: tensor([[ 1.0736],\n",
      "        [ 0.2038],\n",
      "        [-1.8577],\n",
      "        [ 1.2545],\n",
      "        [ 0.6778],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 725: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 725: tensor([[ 1.0733],\n",
      "        [ 0.2028],\n",
      "        [-1.8594],\n",
      "        [ 1.2523],\n",
      "        [ 0.6754],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 726: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 726: tensor([[ 1.0736],\n",
      "        [ 0.2038],\n",
      "        [-1.8577],\n",
      "        [ 1.2545],\n",
      "        [ 0.6778],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 727: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 727: tensor([[ 1.0733],\n",
      "        [ 0.2027],\n",
      "        [-1.8594],\n",
      "        [ 1.2523],\n",
      "        [ 0.6755],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 728: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 728: tensor([[ 1.0736],\n",
      "        [ 0.2037],\n",
      "        [-1.8577],\n",
      "        [ 1.2545],\n",
      "        [ 0.6778],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 729: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 729: tensor([[ 1.0733],\n",
      "        [ 0.2027],\n",
      "        [-1.8595],\n",
      "        [ 1.2523],\n",
      "        [ 0.6755],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 730: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 730: tensor([[ 1.0736],\n",
      "        [ 0.2037],\n",
      "        [-1.8578],\n",
      "        [ 1.2546],\n",
      "        [ 0.6779],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 731: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 731: tensor([[ 1.0733],\n",
      "        [ 0.2026],\n",
      "        [-1.8595],\n",
      "        [ 1.2524],\n",
      "        [ 0.6755],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 732: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 732: tensor([[ 1.0736],\n",
      "        [ 0.2036],\n",
      "        [-1.8578],\n",
      "        [ 1.2546],\n",
      "        [ 0.6779],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 733: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 733: tensor([[ 1.0733],\n",
      "        [ 0.2026],\n",
      "        [-1.8596],\n",
      "        [ 1.2524],\n",
      "        [ 0.6755],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 734: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 734: tensor([[ 1.0736],\n",
      "        [ 0.2036],\n",
      "        [-1.8579],\n",
      "        [ 1.2546],\n",
      "        [ 0.6779],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 735: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 735: tensor([[ 1.0733],\n",
      "        [ 0.2026],\n",
      "        [-1.8596],\n",
      "        [ 1.2524],\n",
      "        [ 0.6755],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 736: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 736: tensor([[ 1.0736],\n",
      "        [ 0.2036],\n",
      "        [-1.8579],\n",
      "        [ 1.2546],\n",
      "        [ 0.6779],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 737: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 737: tensor([[ 1.0733],\n",
      "        [ 0.2025],\n",
      "        [-1.8596],\n",
      "        [ 1.2525],\n",
      "        [ 0.6755],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 738: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 738: tensor([[ 1.0736],\n",
      "        [ 0.2035],\n",
      "        [-1.8579],\n",
      "        [ 1.2547],\n",
      "        [ 0.6779],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 739: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 739: tensor([[ 1.0733],\n",
      "        [ 0.2025],\n",
      "        [-1.8597],\n",
      "        [ 1.2525],\n",
      "        [ 0.6755],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 740: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 740: tensor([[ 1.0736],\n",
      "        [ 0.2035],\n",
      "        [-1.8580],\n",
      "        [ 1.2547],\n",
      "        [ 0.6779],\n",
      "        [-0.2828]], requires_grad=True)\n",
      "poly train loss at 741: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 741: tensor([[ 1.0733],\n",
      "        [ 0.2025],\n",
      "        [-1.8597],\n",
      "        [ 1.2525],\n",
      "        [ 0.6756],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 742: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 742: tensor([[ 1.0736],\n",
      "        [ 0.2035],\n",
      "        [-1.8580],\n",
      "        [ 1.2547],\n",
      "        [ 0.6779],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 743: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 743: tensor([[ 1.0733],\n",
      "        [ 0.2024],\n",
      "        [-1.8598],\n",
      "        [ 1.2525],\n",
      "        [ 0.6756],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 744: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 744: tensor([[ 1.0736],\n",
      "        [ 0.2034],\n",
      "        [-1.8580],\n",
      "        [ 1.2548],\n",
      "        [ 0.6780],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 745: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 745: tensor([[ 1.0733],\n",
      "        [ 0.2024],\n",
      "        [-1.8598],\n",
      "        [ 1.2526],\n",
      "        [ 0.6756],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 746: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 746: tensor([[ 1.0736],\n",
      "        [ 0.2034],\n",
      "        [-1.8581],\n",
      "        [ 1.2548],\n",
      "        [ 0.6780],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 747: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 747: tensor([[ 1.0733],\n",
      "        [ 0.2024],\n",
      "        [-1.8598],\n",
      "        [ 1.2526],\n",
      "        [ 0.6756],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 748: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 748: tensor([[ 1.0736],\n",
      "        [ 0.2034],\n",
      "        [-1.8581],\n",
      "        [ 1.2548],\n",
      "        [ 0.6780],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 749: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 749: tensor([[ 1.0733],\n",
      "        [ 0.2023],\n",
      "        [-1.8599],\n",
      "        [ 1.2526],\n",
      "        [ 0.6756],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 750: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 750: tensor([[ 1.0736],\n",
      "        [ 0.2033],\n",
      "        [-1.8582],\n",
      "        [ 1.2548],\n",
      "        [ 0.6780],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 751: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 751: tensor([[ 1.0733],\n",
      "        [ 0.2023],\n",
      "        [-1.8599],\n",
      "        [ 1.2527],\n",
      "        [ 0.6756],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 752: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 752: tensor([[ 1.0736],\n",
      "        [ 0.2033],\n",
      "        [-1.8582],\n",
      "        [ 1.2549],\n",
      "        [ 0.6780],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 753: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 753: tensor([[ 1.0733],\n",
      "        [ 0.2023],\n",
      "        [-1.8599],\n",
      "        [ 1.2527],\n",
      "        [ 0.6756],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 754: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 754: tensor([[ 1.0736],\n",
      "        [ 0.2033],\n",
      "        [-1.8582],\n",
      "        [ 1.2549],\n",
      "        [ 0.6780],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 755: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 755: tensor([[ 1.0733],\n",
      "        [ 0.2022],\n",
      "        [-1.8600],\n",
      "        [ 1.2527],\n",
      "        [ 0.6757],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 756: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 756: tensor([[ 1.0736],\n",
      "        [ 0.2032],\n",
      "        [-1.8583],\n",
      "        [ 1.2549],\n",
      "        [ 0.6780],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 757: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 757: tensor([[ 1.0733],\n",
      "        [ 0.2022],\n",
      "        [-1.8600],\n",
      "        [ 1.2527],\n",
      "        [ 0.6757],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 758: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 758: tensor([[ 1.0736],\n",
      "        [ 0.2032],\n",
      "        [-1.8583],\n",
      "        [ 1.2550],\n",
      "        [ 0.6781],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 759: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 759: tensor([[ 1.0733],\n",
      "        [ 0.2021],\n",
      "        [-1.8601],\n",
      "        [ 1.2528],\n",
      "        [ 0.6757],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 760: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 760: tensor([[ 1.0736],\n",
      "        [ 0.2031],\n",
      "        [-1.8584],\n",
      "        [ 1.2550],\n",
      "        [ 0.6781],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 761: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 761: tensor([[ 1.0733],\n",
      "        [ 0.2021],\n",
      "        [-1.8601],\n",
      "        [ 1.2528],\n",
      "        [ 0.6757],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 762: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 762: tensor([[ 1.0736],\n",
      "        [ 0.2031],\n",
      "        [-1.8584],\n",
      "        [ 1.2550],\n",
      "        [ 0.6781],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 763: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 763: tensor([[ 1.0733],\n",
      "        [ 0.2021],\n",
      "        [-1.8601],\n",
      "        [ 1.2528],\n",
      "        [ 0.6757],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 764: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 764: tensor([[ 1.0736],\n",
      "        [ 0.2031],\n",
      "        [-1.8584],\n",
      "        [ 1.2550],\n",
      "        [ 0.6781],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 765: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 765: tensor([[ 1.0733],\n",
      "        [ 0.2020],\n",
      "        [-1.8602],\n",
      "        [ 1.2529],\n",
      "        [ 0.6757],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 766: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 766: tensor([[ 1.0736],\n",
      "        [ 0.2030],\n",
      "        [-1.8585],\n",
      "        [ 1.2551],\n",
      "        [ 0.6781],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 767: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 767: tensor([[ 1.0733],\n",
      "        [ 0.2020],\n",
      "        [-1.8602],\n",
      "        [ 1.2529],\n",
      "        [ 0.6757],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 768: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 768: tensor([[ 1.0736],\n",
      "        [ 0.2030],\n",
      "        [-1.8585],\n",
      "        [ 1.2551],\n",
      "        [ 0.6781],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 769: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 769: tensor([[ 1.0733],\n",
      "        [ 0.2020],\n",
      "        [-1.8603],\n",
      "        [ 1.2529],\n",
      "        [ 0.6758],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 770: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 770: tensor([[ 1.0736],\n",
      "        [ 0.2030],\n",
      "        [-1.8585],\n",
      "        [ 1.2551],\n",
      "        [ 0.6781],\n",
      "        [-0.2829]], requires_grad=True)\n",
      "poly train loss at 771: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 771: tensor([[ 1.0733],\n",
      "        [ 0.2019],\n",
      "        [-1.8603],\n",
      "        [ 1.2529],\n",
      "        [ 0.6758],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 772: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 772: tensor([[ 1.0736],\n",
      "        [ 0.2029],\n",
      "        [-1.8586],\n",
      "        [ 1.2552],\n",
      "        [ 0.6782],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 773: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 773: tensor([[ 1.0733],\n",
      "        [ 0.2019],\n",
      "        [-1.8603],\n",
      "        [ 1.2530],\n",
      "        [ 0.6758],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 774: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 774: tensor([[ 1.0736],\n",
      "        [ 0.2029],\n",
      "        [-1.8586],\n",
      "        [ 1.2552],\n",
      "        [ 0.6782],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 775: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 775: tensor([[ 1.0733],\n",
      "        [ 0.2019],\n",
      "        [-1.8604],\n",
      "        [ 1.2530],\n",
      "        [ 0.6758],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 776: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 776: tensor([[ 1.0736],\n",
      "        [ 0.2029],\n",
      "        [-1.8587],\n",
      "        [ 1.2552],\n",
      "        [ 0.6782],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 777: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 777: tensor([[ 1.0733],\n",
      "        [ 0.2018],\n",
      "        [-1.8604],\n",
      "        [ 1.2530],\n",
      "        [ 0.6758],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 778: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 778: tensor([[ 1.0736],\n",
      "        [ 0.2028],\n",
      "        [-1.8587],\n",
      "        [ 1.2552],\n",
      "        [ 0.6782],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 779: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 779: tensor([[ 1.0733],\n",
      "        [ 0.2018],\n",
      "        [-1.8605],\n",
      "        [ 1.2531],\n",
      "        [ 0.6758],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 780: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 780: tensor([[ 1.0736],\n",
      "        [ 0.2028],\n",
      "        [-1.8587],\n",
      "        [ 1.2553],\n",
      "        [ 0.6782],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 781: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 781: tensor([[ 1.0733],\n",
      "        [ 0.2018],\n",
      "        [-1.8605],\n",
      "        [ 1.2531],\n",
      "        [ 0.6758],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 782: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 782: tensor([[ 1.0736],\n",
      "        [ 0.2028],\n",
      "        [-1.8588],\n",
      "        [ 1.2553],\n",
      "        [ 0.6782],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 783: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 783: tensor([[ 1.0733],\n",
      "        [ 0.2017],\n",
      "        [-1.8605],\n",
      "        [ 1.2531],\n",
      "        [ 0.6759],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 784: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 784: tensor([[ 1.0736],\n",
      "        [ 0.2027],\n",
      "        [-1.8588],\n",
      "        [ 1.2553],\n",
      "        [ 0.6782],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 785: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 785: tensor([[ 1.0733],\n",
      "        [ 0.2017],\n",
      "        [-1.8606],\n",
      "        [ 1.2531],\n",
      "        [ 0.6759],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 786: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 786: tensor([[ 1.0736],\n",
      "        [ 0.2027],\n",
      "        [-1.8589],\n",
      "        [ 1.2554],\n",
      "        [ 0.6783],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 787: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 787: tensor([[ 1.0733],\n",
      "        [ 0.2016],\n",
      "        [-1.8606],\n",
      "        [ 1.2532],\n",
      "        [ 0.6759],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 788: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 788: tensor([[ 1.0736],\n",
      "        [ 0.2026],\n",
      "        [-1.8589],\n",
      "        [ 1.2554],\n",
      "        [ 0.6783],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 789: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 789: tensor([[ 1.0733],\n",
      "        [ 0.2016],\n",
      "        [-1.8606],\n",
      "        [ 1.2532],\n",
      "        [ 0.6759],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 790: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 790: tensor([[ 1.0736],\n",
      "        [ 0.2026],\n",
      "        [-1.8589],\n",
      "        [ 1.2554],\n",
      "        [ 0.6783],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 791: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 791: tensor([[ 1.0733],\n",
      "        [ 0.2016],\n",
      "        [-1.8607],\n",
      "        [ 1.2532],\n",
      "        [ 0.6759],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 792: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 792: tensor([[ 1.0736],\n",
      "        [ 0.2026],\n",
      "        [-1.8590],\n",
      "        [ 1.2554],\n",
      "        [ 0.6783],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 793: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 793: tensor([[ 1.0733],\n",
      "        [ 0.2015],\n",
      "        [-1.8607],\n",
      "        [ 1.2533],\n",
      "        [ 0.6759],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 794: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 794: tensor([[ 1.0736],\n",
      "        [ 0.2025],\n",
      "        [-1.8590],\n",
      "        [ 1.2555],\n",
      "        [ 0.6783],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 795: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 795: tensor([[ 1.0733],\n",
      "        [ 0.2015],\n",
      "        [-1.8608],\n",
      "        [ 1.2533],\n",
      "        [ 0.6759],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 796: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 796: tensor([[ 1.0736],\n",
      "        [ 0.2025],\n",
      "        [-1.8590],\n",
      "        [ 1.2555],\n",
      "        [ 0.6783],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 797: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 797: tensor([[ 1.0733],\n",
      "        [ 0.2015],\n",
      "        [-1.8608],\n",
      "        [ 1.2533],\n",
      "        [ 0.6760],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 798: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 798: tensor([[ 1.0736],\n",
      "        [ 0.2025],\n",
      "        [-1.8591],\n",
      "        [ 1.2555],\n",
      "        [ 0.6783],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 799: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 799: tensor([[ 1.0733],\n",
      "        [ 0.2014],\n",
      "        [-1.8608],\n",
      "        [ 1.2533],\n",
      "        [ 0.6760],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 800: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 800: tensor([[ 1.0736],\n",
      "        [ 0.2024],\n",
      "        [-1.8591],\n",
      "        [ 1.2556],\n",
      "        [ 0.6784],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 801: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 801: tensor([[ 1.0733],\n",
      "        [ 0.2014],\n",
      "        [-1.8609],\n",
      "        [ 1.2534],\n",
      "        [ 0.6760],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 802: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 802: tensor([[ 1.0736],\n",
      "        [ 0.2024],\n",
      "        [-1.8592],\n",
      "        [ 1.2556],\n",
      "        [ 0.6784],\n",
      "        [-0.2830]], requires_grad=True)\n",
      "poly train loss at 803: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 803: tensor([[ 1.0733],\n",
      "        [ 0.2014],\n",
      "        [-1.8609],\n",
      "        [ 1.2534],\n",
      "        [ 0.6760],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 804: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 804: tensor([[ 1.0736],\n",
      "        [ 0.2024],\n",
      "        [-1.8592],\n",
      "        [ 1.2556],\n",
      "        [ 0.6784],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 805: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 805: tensor([[ 1.0733],\n",
      "        [ 0.2013],\n",
      "        [-1.8609],\n",
      "        [ 1.2534],\n",
      "        [ 0.6760],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 806: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 806: tensor([[ 1.0736],\n",
      "        [ 0.2023],\n",
      "        [-1.8592],\n",
      "        [ 1.2556],\n",
      "        [ 0.6784],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 807: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 807: tensor([[ 1.0733],\n",
      "        [ 0.2013],\n",
      "        [-1.8610],\n",
      "        [ 1.2534],\n",
      "        [ 0.6760],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 808: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 808: tensor([[ 1.0736],\n",
      "        [ 0.2023],\n",
      "        [-1.8593],\n",
      "        [ 1.2557],\n",
      "        [ 0.6784],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 809: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 809: tensor([[ 1.0733],\n",
      "        [ 0.2013],\n",
      "        [-1.8610],\n",
      "        [ 1.2535],\n",
      "        [ 0.6760],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 810: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 810: tensor([[ 1.0736],\n",
      "        [ 0.2023],\n",
      "        [-1.8593],\n",
      "        [ 1.2557],\n",
      "        [ 0.6784],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 811: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 811: tensor([[ 1.0733],\n",
      "        [ 0.2012],\n",
      "        [-1.8611],\n",
      "        [ 1.2535],\n",
      "        [ 0.6761],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 812: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 812: tensor([[ 1.0736],\n",
      "        [ 0.2022],\n",
      "        [-1.8594],\n",
      "        [ 1.2557],\n",
      "        [ 0.6784],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 813: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 813: tensor([[ 1.0733],\n",
      "        [ 0.2012],\n",
      "        [-1.8611],\n",
      "        [ 1.2535],\n",
      "        [ 0.6761],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 814: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 814: tensor([[ 1.0736],\n",
      "        [ 0.2022],\n",
      "        [-1.8594],\n",
      "        [ 1.2557],\n",
      "        [ 0.6785],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 815: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 815: tensor([[ 1.0733],\n",
      "        [ 0.2012],\n",
      "        [-1.8611],\n",
      "        [ 1.2536],\n",
      "        [ 0.6761],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 816: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 816: tensor([[ 1.0736],\n",
      "        [ 0.2022],\n",
      "        [-1.8594],\n",
      "        [ 1.2558],\n",
      "        [ 0.6785],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 817: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 817: tensor([[ 1.0733],\n",
      "        [ 0.2011],\n",
      "        [-1.8612],\n",
      "        [ 1.2536],\n",
      "        [ 0.6761],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 818: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 818: tensor([[ 1.0736],\n",
      "        [ 0.2021],\n",
      "        [-1.8595],\n",
      "        [ 1.2558],\n",
      "        [ 0.6785],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 819: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 819: tensor([[ 1.0733],\n",
      "        [ 0.2011],\n",
      "        [-1.8612],\n",
      "        [ 1.2536],\n",
      "        [ 0.6761],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 820: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 820: tensor([[ 1.0736],\n",
      "        [ 0.2021],\n",
      "        [-1.8595],\n",
      "        [ 1.2558],\n",
      "        [ 0.6785],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 821: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 821: tensor([[ 1.0733],\n",
      "        [ 0.2010],\n",
      "        [-1.8613],\n",
      "        [ 1.2536],\n",
      "        [ 0.6761],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 822: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 822: tensor([[ 1.0736],\n",
      "        [ 0.2020],\n",
      "        [-1.8595],\n",
      "        [ 1.2559],\n",
      "        [ 0.6785],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 823: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 823: tensor([[ 1.0733],\n",
      "        [ 0.2010],\n",
      "        [-1.8613],\n",
      "        [ 1.2537],\n",
      "        [ 0.6761],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 824: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 824: tensor([[ 1.0736],\n",
      "        [ 0.2020],\n",
      "        [-1.8596],\n",
      "        [ 1.2559],\n",
      "        [ 0.6785],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 825: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 825: tensor([[ 1.0733],\n",
      "        [ 0.2010],\n",
      "        [-1.8613],\n",
      "        [ 1.2537],\n",
      "        [ 0.6761],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 826: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 826: tensor([[ 1.0736],\n",
      "        [ 0.2020],\n",
      "        [-1.8596],\n",
      "        [ 1.2559],\n",
      "        [ 0.6785],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 827: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 827: tensor([[ 1.0733],\n",
      "        [ 0.2009],\n",
      "        [-1.8614],\n",
      "        [ 1.2537],\n",
      "        [ 0.6762],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 828: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 828: tensor([[ 1.0736],\n",
      "        [ 0.2019],\n",
      "        [-1.8597],\n",
      "        [ 1.2559],\n",
      "        [ 0.6786],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 829: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 829: tensor([[ 1.0733],\n",
      "        [ 0.2009],\n",
      "        [-1.8614],\n",
      "        [ 1.2538],\n",
      "        [ 0.6762],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 830: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 830: tensor([[ 1.0736],\n",
      "        [ 0.2019],\n",
      "        [-1.8597],\n",
      "        [ 1.2560],\n",
      "        [ 0.6786],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 831: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 831: tensor([[ 1.0733],\n",
      "        [ 0.2009],\n",
      "        [-1.8614],\n",
      "        [ 1.2538],\n",
      "        [ 0.6762],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 832: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 832: tensor([[ 1.0736],\n",
      "        [ 0.2019],\n",
      "        [-1.8597],\n",
      "        [ 1.2560],\n",
      "        [ 0.6786],\n",
      "        [-0.2831]], requires_grad=True)\n",
      "poly train loss at 833: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 833: tensor([[ 1.0733],\n",
      "        [ 0.2008],\n",
      "        [-1.8615],\n",
      "        [ 1.2538],\n",
      "        [ 0.6762],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 834: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 834: tensor([[ 1.0736],\n",
      "        [ 0.2018],\n",
      "        [-1.8598],\n",
      "        [ 1.2560],\n",
      "        [ 0.6786],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 835: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 835: tensor([[ 1.0733],\n",
      "        [ 0.2008],\n",
      "        [-1.8615],\n",
      "        [ 1.2538],\n",
      "        [ 0.6762],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 836: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 836: tensor([[ 1.0736],\n",
      "        [ 0.2018],\n",
      "        [-1.8598],\n",
      "        [ 1.2561],\n",
      "        [ 0.6786],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 837: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 837: tensor([[ 1.0733],\n",
      "        [ 0.2008],\n",
      "        [-1.8616],\n",
      "        [ 1.2539],\n",
      "        [ 0.6762],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 838: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 838: tensor([[ 1.0736],\n",
      "        [ 0.2018],\n",
      "        [-1.8599],\n",
      "        [ 1.2561],\n",
      "        [ 0.6786],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 839: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 839: tensor([[ 1.0733],\n",
      "        [ 0.2007],\n",
      "        [-1.8616],\n",
      "        [ 1.2539],\n",
      "        [ 0.6762],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 840: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 840: tensor([[ 1.0736],\n",
      "        [ 0.2017],\n",
      "        [-1.8599],\n",
      "        [ 1.2561],\n",
      "        [ 0.6786],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 841: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 841: tensor([[ 1.0733],\n",
      "        [ 0.2007],\n",
      "        [-1.8616],\n",
      "        [ 1.2539],\n",
      "        [ 0.6763],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 842: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 842: tensor([[ 1.0736],\n",
      "        [ 0.2017],\n",
      "        [-1.8599],\n",
      "        [ 1.2561],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 843: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 843: tensor([[ 1.0733],\n",
      "        [ 0.2007],\n",
      "        [-1.8617],\n",
      "        [ 1.2539],\n",
      "        [ 0.6763],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 844: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 844: tensor([[ 1.0736],\n",
      "        [ 0.2017],\n",
      "        [-1.8600],\n",
      "        [ 1.2562],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 845: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 845: tensor([[ 1.0733],\n",
      "        [ 0.2006],\n",
      "        [-1.8617],\n",
      "        [ 1.2540],\n",
      "        [ 0.6763],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 846: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 846: tensor([[ 1.0736],\n",
      "        [ 0.2016],\n",
      "        [-1.8600],\n",
      "        [ 1.2562],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 847: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 847: tensor([[ 1.0733],\n",
      "        [ 0.2006],\n",
      "        [-1.8618],\n",
      "        [ 1.2540],\n",
      "        [ 0.6763],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 848: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 848: tensor([[ 1.0736],\n",
      "        [ 0.2016],\n",
      "        [-1.8600],\n",
      "        [ 1.2562],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 849: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 849: tensor([[ 1.0733],\n",
      "        [ 0.2006],\n",
      "        [-1.8618],\n",
      "        [ 1.2540],\n",
      "        [ 0.6763],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 850: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 850: tensor([[ 1.0736],\n",
      "        [ 0.2016],\n",
      "        [-1.8601],\n",
      "        [ 1.2562],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 851: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 851: tensor([[ 1.0733],\n",
      "        [ 0.2005],\n",
      "        [-1.8618],\n",
      "        [ 1.2541],\n",
      "        [ 0.6763],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 852: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 852: tensor([[ 1.0736],\n",
      "        [ 0.2015],\n",
      "        [-1.8601],\n",
      "        [ 1.2563],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 853: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 853: tensor([[ 1.0733],\n",
      "        [ 0.2005],\n",
      "        [-1.8619],\n",
      "        [ 1.2541],\n",
      "        [ 0.6763],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 854: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 854: tensor([[ 1.0736],\n",
      "        [ 0.2015],\n",
      "        [-1.8602],\n",
      "        [ 1.2563],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 855: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 855: tensor([[ 1.0733],\n",
      "        [ 0.2004],\n",
      "        [-1.8619],\n",
      "        [ 1.2541],\n",
      "        [ 0.6764],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 856: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 856: tensor([[ 1.0736],\n",
      "        [ 0.2014],\n",
      "        [-1.8602],\n",
      "        [ 1.2563],\n",
      "        [ 0.6787],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 857: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 857: tensor([[ 1.0733],\n",
      "        [ 0.2004],\n",
      "        [-1.8619],\n",
      "        [ 1.2541],\n",
      "        [ 0.6764],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 858: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 858: tensor([[ 1.0736],\n",
      "        [ 0.2014],\n",
      "        [-1.8602],\n",
      "        [ 1.2564],\n",
      "        [ 0.6788],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 859: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 859: tensor([[ 1.0733],\n",
      "        [ 0.2004],\n",
      "        [-1.8620],\n",
      "        [ 1.2542],\n",
      "        [ 0.6764],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 860: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 860: tensor([[ 1.0736],\n",
      "        [ 0.2014],\n",
      "        [-1.8603],\n",
      "        [ 1.2564],\n",
      "        [ 0.6788],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 861: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 861: tensor([[ 1.0733],\n",
      "        [ 0.2003],\n",
      "        [-1.8620],\n",
      "        [ 1.2542],\n",
      "        [ 0.6764],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 862: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 862: tensor([[ 1.0736],\n",
      "        [ 0.2013],\n",
      "        [-1.8603],\n",
      "        [ 1.2564],\n",
      "        [ 0.6788],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 863: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 863: tensor([[ 1.0733],\n",
      "        [ 0.2003],\n",
      "        [-1.8621],\n",
      "        [ 1.2542],\n",
      "        [ 0.6764],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 864: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 864: tensor([[ 1.0736],\n",
      "        [ 0.2013],\n",
      "        [-1.8603],\n",
      "        [ 1.2564],\n",
      "        [ 0.6788],\n",
      "        [-0.2832]], requires_grad=True)\n",
      "poly train loss at 865: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 865: tensor([[ 1.0733],\n",
      "        [ 0.2003],\n",
      "        [-1.8621],\n",
      "        [ 1.2543],\n",
      "        [ 0.6764],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 866: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 866: tensor([[ 1.0736],\n",
      "        [ 0.2013],\n",
      "        [-1.8604],\n",
      "        [ 1.2565],\n",
      "        [ 0.6788],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 867: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 867: tensor([[ 1.0733],\n",
      "        [ 0.2002],\n",
      "        [-1.8621],\n",
      "        [ 1.2543],\n",
      "        [ 0.6764],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 868: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 868: tensor([[ 1.0736],\n",
      "        [ 0.2012],\n",
      "        [-1.8604],\n",
      "        [ 1.2565],\n",
      "        [ 0.6788],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 869: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 869: tensor([[ 1.0733],\n",
      "        [ 0.2002],\n",
      "        [-1.8622],\n",
      "        [ 1.2543],\n",
      "        [ 0.6765],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 870: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 870: tensor([[ 1.0736],\n",
      "        [ 0.2012],\n",
      "        [-1.8605],\n",
      "        [ 1.2565],\n",
      "        [ 0.6788],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 871: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 871: tensor([[ 1.0733],\n",
      "        [ 0.2002],\n",
      "        [-1.8622],\n",
      "        [ 1.2543],\n",
      "        [ 0.6765],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 872: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 872: tensor([[ 1.0736],\n",
      "        [ 0.2012],\n",
      "        [-1.8605],\n",
      "        [ 1.2565],\n",
      "        [ 0.6789],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 873: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 873: tensor([[ 1.0733],\n",
      "        [ 0.2001],\n",
      "        [-1.8622],\n",
      "        [ 1.2544],\n",
      "        [ 0.6765],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 874: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 874: tensor([[ 1.0736],\n",
      "        [ 0.2011],\n",
      "        [-1.8605],\n",
      "        [ 1.2566],\n",
      "        [ 0.6789],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 875: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 875: tensor([[ 1.0733],\n",
      "        [ 0.2001],\n",
      "        [-1.8623],\n",
      "        [ 1.2544],\n",
      "        [ 0.6765],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 876: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 876: tensor([[ 1.0736],\n",
      "        [ 0.2011],\n",
      "        [-1.8606],\n",
      "        [ 1.2566],\n",
      "        [ 0.6789],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 877: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 877: tensor([[ 1.0733],\n",
      "        [ 0.2001],\n",
      "        [-1.8623],\n",
      "        [ 1.2544],\n",
      "        [ 0.6765],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 878: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 878: tensor([[ 1.0736],\n",
      "        [ 0.2011],\n",
      "        [-1.8606],\n",
      "        [ 1.2566],\n",
      "        [ 0.6789],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 879: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 879: tensor([[ 1.0733],\n",
      "        [ 0.2000],\n",
      "        [-1.8624],\n",
      "        [ 1.2544],\n",
      "        [ 0.6765],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 880: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 880: tensor([[ 1.0736],\n",
      "        [ 0.2010],\n",
      "        [-1.8606],\n",
      "        [ 1.2567],\n",
      "        [ 0.6789],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 881: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 881: tensor([[ 1.0733],\n",
      "        [ 0.2000],\n",
      "        [-1.8624],\n",
      "        [ 1.2545],\n",
      "        [ 0.6765],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 882: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 882: tensor([[ 1.0736],\n",
      "        [ 0.2010],\n",
      "        [-1.8607],\n",
      "        [ 1.2567],\n",
      "        [ 0.6789],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 883: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 883: tensor([[ 1.0733],\n",
      "        [ 0.2000],\n",
      "        [-1.8624],\n",
      "        [ 1.2545],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 884: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 884: tensor([[ 1.0736],\n",
      "        [ 0.2010],\n",
      "        [-1.8607],\n",
      "        [ 1.2567],\n",
      "        [ 0.6789],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 885: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 885: tensor([[ 1.0733],\n",
      "        [ 0.1999],\n",
      "        [-1.8625],\n",
      "        [ 1.2545],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 886: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 886: tensor([[ 1.0736],\n",
      "        [ 0.2009],\n",
      "        [-1.8608],\n",
      "        [ 1.2567],\n",
      "        [ 0.6790],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 887: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 887: tensor([[ 1.0733],\n",
      "        [ 0.1999],\n",
      "        [-1.8625],\n",
      "        [ 1.2546],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 888: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 888: tensor([[ 1.0736],\n",
      "        [ 0.2009],\n",
      "        [-1.8608],\n",
      "        [ 1.2568],\n",
      "        [ 0.6790],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 889: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 889: tensor([[ 1.0733],\n",
      "        [ 0.1999],\n",
      "        [-1.8625],\n",
      "        [ 1.2546],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 890: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 890: tensor([[ 1.0736],\n",
      "        [ 0.2009],\n",
      "        [-1.8608],\n",
      "        [ 1.2568],\n",
      "        [ 0.6790],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 891: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 891: tensor([[ 1.0733],\n",
      "        [ 0.1998],\n",
      "        [-1.8626],\n",
      "        [ 1.2546],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 892: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 892: tensor([[ 1.0736],\n",
      "        [ 0.2008],\n",
      "        [-1.8609],\n",
      "        [ 1.2568],\n",
      "        [ 0.6790],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 893: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 893: tensor([[ 1.0733],\n",
      "        [ 0.1998],\n",
      "        [-1.8626],\n",
      "        [ 1.2546],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 894: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 894: tensor([[ 1.0736],\n",
      "        [ 0.2008],\n",
      "        [-1.8609],\n",
      "        [ 1.2569],\n",
      "        [ 0.6790],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 895: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 895: tensor([[ 1.0733],\n",
      "        [ 0.1997],\n",
      "        [-1.8627],\n",
      "        [ 1.2547],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 896: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 896: tensor([[ 1.0736],\n",
      "        [ 0.2007],\n",
      "        [-1.8609],\n",
      "        [ 1.2569],\n",
      "        [ 0.6790],\n",
      "        [-0.2833]], requires_grad=True)\n",
      "poly train loss at 897: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 897: tensor([[ 1.0733],\n",
      "        [ 0.1997],\n",
      "        [-1.8627],\n",
      "        [ 1.2547],\n",
      "        [ 0.6766],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 898: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 898: tensor([[ 1.0736],\n",
      "        [ 0.2007],\n",
      "        [-1.8610],\n",
      "        [ 1.2569],\n",
      "        [ 0.6790],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 899: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 899: tensor([[ 1.0733],\n",
      "        [ 0.1997],\n",
      "        [-1.8627],\n",
      "        [ 1.2547],\n",
      "        [ 0.6767],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 900: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 900: tensor([[ 1.0736],\n",
      "        [ 0.2007],\n",
      "        [-1.8610],\n",
      "        [ 1.2569],\n",
      "        [ 0.6791],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 901: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 901: tensor([[ 1.0733],\n",
      "        [ 0.1996],\n",
      "        [-1.8628],\n",
      "        [ 1.2547],\n",
      "        [ 0.6767],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 902: tensor(0.3283, grad_fn=<DivBackward0>)\n",
      "poly w at 902: tensor([[ 1.0736],\n",
      "        [ 0.2006],\n",
      "        [-1.8611],\n",
      "        [ 1.2570],\n",
      "        [ 0.6791],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 903: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 903: tensor([[ 1.0733],\n",
      "        [ 0.1996],\n",
      "        [-1.8628],\n",
      "        [ 1.2548],\n",
      "        [ 0.6767],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 904: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 904: tensor([[ 1.0736],\n",
      "        [ 0.2006],\n",
      "        [-1.8611],\n",
      "        [ 1.2570],\n",
      "        [ 0.6791],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 905: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 905: tensor([[ 1.0733],\n",
      "        [ 0.1996],\n",
      "        [-1.8628],\n",
      "        [ 1.2548],\n",
      "        [ 0.6767],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 906: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 906: tensor([[ 1.0736],\n",
      "        [ 0.2006],\n",
      "        [-1.8611],\n",
      "        [ 1.2570],\n",
      "        [ 0.6791],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 907: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 907: tensor([[ 1.0733],\n",
      "        [ 0.1995],\n",
      "        [-1.8629],\n",
      "        [ 1.2548],\n",
      "        [ 0.6767],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 908: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 908: tensor([[ 1.0736],\n",
      "        [ 0.2005],\n",
      "        [-1.8612],\n",
      "        [ 1.2570],\n",
      "        [ 0.6791],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 909: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 909: tensor([[ 1.0733],\n",
      "        [ 0.1995],\n",
      "        [-1.8629],\n",
      "        [ 1.2549],\n",
      "        [ 0.6767],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 910: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 910: tensor([[ 1.0736],\n",
      "        [ 0.2005],\n",
      "        [-1.8612],\n",
      "        [ 1.2571],\n",
      "        [ 0.6791],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 911: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 911: tensor([[ 1.0733],\n",
      "        [ 0.1995],\n",
      "        [-1.8630],\n",
      "        [ 1.2549],\n",
      "        [ 0.6767],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 912: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 912: tensor([[ 1.0736],\n",
      "        [ 0.2005],\n",
      "        [-1.8612],\n",
      "        [ 1.2571],\n",
      "        [ 0.6791],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 913: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 913: tensor([[ 1.0733],\n",
      "        [ 0.1994],\n",
      "        [-1.8630],\n",
      "        [ 1.2549],\n",
      "        [ 0.6768],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 914: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 914: tensor([[ 1.0736],\n",
      "        [ 0.2004],\n",
      "        [-1.8613],\n",
      "        [ 1.2571],\n",
      "        [ 0.6792],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 915: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 915: tensor([[ 1.0733],\n",
      "        [ 0.1994],\n",
      "        [-1.8630],\n",
      "        [ 1.2549],\n",
      "        [ 0.6768],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 916: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 916: tensor([[ 1.0736],\n",
      "        [ 0.2004],\n",
      "        [-1.8613],\n",
      "        [ 1.2572],\n",
      "        [ 0.6792],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 917: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 917: tensor([[ 1.0733],\n",
      "        [ 0.1994],\n",
      "        [-1.8631],\n",
      "        [ 1.2550],\n",
      "        [ 0.6768],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 918: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 918: tensor([[ 1.0736],\n",
      "        [ 0.2004],\n",
      "        [-1.8614],\n",
      "        [ 1.2572],\n",
      "        [ 0.6792],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 919: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 919: tensor([[ 1.0733],\n",
      "        [ 0.1993],\n",
      "        [-1.8631],\n",
      "        [ 1.2550],\n",
      "        [ 0.6768],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 920: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 920: tensor([[ 1.0736],\n",
      "        [ 0.2003],\n",
      "        [-1.8614],\n",
      "        [ 1.2572],\n",
      "        [ 0.6792],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 921: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 921: tensor([[ 1.0733],\n",
      "        [ 0.1993],\n",
      "        [-1.8631],\n",
      "        [ 1.2550],\n",
      "        [ 0.6768],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 922: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 922: tensor([[ 1.0736],\n",
      "        [ 0.2003],\n",
      "        [-1.8614],\n",
      "        [ 1.2572],\n",
      "        [ 0.6792],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 923: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 923: tensor([[ 1.0733],\n",
      "        [ 0.1993],\n",
      "        [-1.8632],\n",
      "        [ 1.2551],\n",
      "        [ 0.6768],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 924: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 924: tensor([[ 1.0736],\n",
      "        [ 0.2003],\n",
      "        [-1.8615],\n",
      "        [ 1.2573],\n",
      "        [ 0.6792],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 925: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 925: tensor([[ 1.0733],\n",
      "        [ 0.1992],\n",
      "        [-1.8632],\n",
      "        [ 1.2551],\n",
      "        [ 0.6768],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 926: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 926: tensor([[ 1.0736],\n",
      "        [ 0.2002],\n",
      "        [-1.8615],\n",
      "        [ 1.2573],\n",
      "        [ 0.6792],\n",
      "        [-0.2834]], requires_grad=True)\n",
      "poly train loss at 927: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 927: tensor([[ 1.0733],\n",
      "        [ 0.1992],\n",
      "        [-1.8633],\n",
      "        [ 1.2551],\n",
      "        [ 0.6769],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 928: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 928: tensor([[ 1.0736],\n",
      "        [ 0.2002],\n",
      "        [-1.8615],\n",
      "        [ 1.2573],\n",
      "        [ 0.6792],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 929: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 929: tensor([[ 1.0733],\n",
      "        [ 0.1992],\n",
      "        [-1.8633],\n",
      "        [ 1.2551],\n",
      "        [ 0.6769],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 930: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 930: tensor([[ 1.0736],\n",
      "        [ 0.2002],\n",
      "        [-1.8616],\n",
      "        [ 1.2573],\n",
      "        [ 0.6793],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 931: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 931: tensor([[ 1.0733],\n",
      "        [ 0.1991],\n",
      "        [-1.8633],\n",
      "        [ 1.2552],\n",
      "        [ 0.6769],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 932: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 932: tensor([[ 1.0736],\n",
      "        [ 0.2001],\n",
      "        [-1.8616],\n",
      "        [ 1.2574],\n",
      "        [ 0.6793],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 933: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 933: tensor([[ 1.0733],\n",
      "        [ 0.1991],\n",
      "        [-1.8634],\n",
      "        [ 1.2552],\n",
      "        [ 0.6769],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 934: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 934: tensor([[ 1.0736],\n",
      "        [ 0.2001],\n",
      "        [-1.8617],\n",
      "        [ 1.2574],\n",
      "        [ 0.6793],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 935: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 935: tensor([[ 1.0733],\n",
      "        [ 0.1990],\n",
      "        [-1.8634],\n",
      "        [ 1.2552],\n",
      "        [ 0.6769],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 936: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 936: tensor([[ 1.0736],\n",
      "        [ 0.2001],\n",
      "        [-1.8617],\n",
      "        [ 1.2574],\n",
      "        [ 0.6793],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 937: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 937: tensor([[ 1.0733],\n",
      "        [ 0.1990],\n",
      "        [-1.8634],\n",
      "        [ 1.2552],\n",
      "        [ 0.6769],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 938: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 938: tensor([[ 1.0736],\n",
      "        [ 0.2000],\n",
      "        [-1.8617],\n",
      "        [ 1.2575],\n",
      "        [ 0.6793],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 939: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 939: tensor([[ 1.0733],\n",
      "        [ 0.1990],\n",
      "        [-1.8635],\n",
      "        [ 1.2553],\n",
      "        [ 0.6769],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 940: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 940: tensor([[ 1.0736],\n",
      "        [ 0.2000],\n",
      "        [-1.8618],\n",
      "        [ 1.2575],\n",
      "        [ 0.6793],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 941: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 941: tensor([[ 1.0733],\n",
      "        [ 0.1989],\n",
      "        [-1.8635],\n",
      "        [ 1.2553],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 942: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 942: tensor([[ 1.0736],\n",
      "        [ 0.1999],\n",
      "        [-1.8618],\n",
      "        [ 1.2575],\n",
      "        [ 0.6793],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 943: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 943: tensor([[ 1.0733],\n",
      "        [ 0.1989],\n",
      "        [-1.8636],\n",
      "        [ 1.2553],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 944: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 944: tensor([[ 1.0736],\n",
      "        [ 0.1999],\n",
      "        [-1.8618],\n",
      "        [ 1.2575],\n",
      "        [ 0.6794],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 945: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 945: tensor([[ 1.0733],\n",
      "        [ 0.1989],\n",
      "        [-1.8636],\n",
      "        [ 1.2554],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 946: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 946: tensor([[ 1.0736],\n",
      "        [ 0.1999],\n",
      "        [-1.8619],\n",
      "        [ 1.2576],\n",
      "        [ 0.6794],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 947: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 947: tensor([[ 1.0733],\n",
      "        [ 0.1988],\n",
      "        [-1.8636],\n",
      "        [ 1.2554],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 948: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 948: tensor([[ 1.0736],\n",
      "        [ 0.1998],\n",
      "        [-1.8619],\n",
      "        [ 1.2576],\n",
      "        [ 0.6794],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 949: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 949: tensor([[ 1.0733],\n",
      "        [ 0.1988],\n",
      "        [-1.8637],\n",
      "        [ 1.2554],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 950: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 950: tensor([[ 1.0736],\n",
      "        [ 0.1998],\n",
      "        [-1.8620],\n",
      "        [ 1.2576],\n",
      "        [ 0.6794],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 951: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 951: tensor([[ 1.0733],\n",
      "        [ 0.1988],\n",
      "        [-1.8637],\n",
      "        [ 1.2554],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 952: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 952: tensor([[ 1.0736],\n",
      "        [ 0.1998],\n",
      "        [-1.8620],\n",
      "        [ 1.2576],\n",
      "        [ 0.6794],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 953: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 953: tensor([[ 1.0733],\n",
      "        [ 0.1987],\n",
      "        [-1.8637],\n",
      "        [ 1.2555],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 954: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 954: tensor([[ 1.0736],\n",
      "        [ 0.1997],\n",
      "        [-1.8620],\n",
      "        [ 1.2577],\n",
      "        [ 0.6794],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 955: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 955: tensor([[ 1.0733],\n",
      "        [ 0.1987],\n",
      "        [-1.8638],\n",
      "        [ 1.2555],\n",
      "        [ 0.6770],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 956: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 956: tensor([[ 1.0736],\n",
      "        [ 0.1997],\n",
      "        [-1.8621],\n",
      "        [ 1.2577],\n",
      "        [ 0.6794],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 957: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 957: tensor([[ 1.0733],\n",
      "        [ 0.1987],\n",
      "        [-1.8638],\n",
      "        [ 1.2555],\n",
      "        [ 0.6771],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 958: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 958: tensor([[ 1.0736],\n",
      "        [ 0.1997],\n",
      "        [-1.8621],\n",
      "        [ 1.2577],\n",
      "        [ 0.6795],\n",
      "        [-0.2835]], requires_grad=True)\n",
      "poly train loss at 959: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 959: tensor([[ 1.0733],\n",
      "        [ 0.1986],\n",
      "        [-1.8638],\n",
      "        [ 1.2555],\n",
      "        [ 0.6771],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 960: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 960: tensor([[ 1.0736],\n",
      "        [ 0.1996],\n",
      "        [-1.8621],\n",
      "        [ 1.2578],\n",
      "        [ 0.6795],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 961: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 961: tensor([[ 1.0733],\n",
      "        [ 0.1986],\n",
      "        [-1.8639],\n",
      "        [ 1.2556],\n",
      "        [ 0.6771],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 962: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 962: tensor([[ 1.0736],\n",
      "        [ 0.1996],\n",
      "        [-1.8622],\n",
      "        [ 1.2578],\n",
      "        [ 0.6795],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 963: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 963: tensor([[ 1.0733],\n",
      "        [ 0.1986],\n",
      "        [-1.8639],\n",
      "        [ 1.2556],\n",
      "        [ 0.6771],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 964: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 964: tensor([[ 1.0736],\n",
      "        [ 0.1996],\n",
      "        [-1.8622],\n",
      "        [ 1.2578],\n",
      "        [ 0.6795],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 965: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 965: tensor([[ 1.0733],\n",
      "        [ 0.1985],\n",
      "        [-1.8640],\n",
      "        [ 1.2556],\n",
      "        [ 0.6771],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 966: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 966: tensor([[ 1.0736],\n",
      "        [ 0.1995],\n",
      "        [-1.8623],\n",
      "        [ 1.2578],\n",
      "        [ 0.6795],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 967: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 967: tensor([[ 1.0733],\n",
      "        [ 0.1985],\n",
      "        [-1.8640],\n",
      "        [ 1.2556],\n",
      "        [ 0.6771],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 968: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 968: tensor([[ 1.0736],\n",
      "        [ 0.1995],\n",
      "        [-1.8623],\n",
      "        [ 1.2579],\n",
      "        [ 0.6795],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 969: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 969: tensor([[ 1.0733],\n",
      "        [ 0.1985],\n",
      "        [-1.8640],\n",
      "        [ 1.2557],\n",
      "        [ 0.6771],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 970: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 970: tensor([[ 1.0736],\n",
      "        [ 0.1995],\n",
      "        [-1.8623],\n",
      "        [ 1.2579],\n",
      "        [ 0.6795],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 971: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 971: tensor([[ 1.0733],\n",
      "        [ 0.1984],\n",
      "        [-1.8641],\n",
      "        [ 1.2557],\n",
      "        [ 0.6772],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 972: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 972: tensor([[ 1.0736],\n",
      "        [ 0.1994],\n",
      "        [-1.8624],\n",
      "        [ 1.2579],\n",
      "        [ 0.6795],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 973: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 973: tensor([[ 1.0733],\n",
      "        [ 0.1984],\n",
      "        [-1.8641],\n",
      "        [ 1.2557],\n",
      "        [ 0.6772],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 974: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 974: tensor([[ 1.0736],\n",
      "        [ 0.1994],\n",
      "        [-1.8624],\n",
      "        [ 1.2579],\n",
      "        [ 0.6796],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 975: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 975: tensor([[ 1.0733],\n",
      "        [ 0.1984],\n",
      "        [-1.8641],\n",
      "        [ 1.2558],\n",
      "        [ 0.6772],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 976: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 976: tensor([[ 1.0736],\n",
      "        [ 0.1994],\n",
      "        [-1.8624],\n",
      "        [ 1.2580],\n",
      "        [ 0.6796],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 977: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 977: tensor([[ 1.0733],\n",
      "        [ 0.1983],\n",
      "        [-1.8642],\n",
      "        [ 1.2558],\n",
      "        [ 0.6772],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 978: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 978: tensor([[ 1.0736],\n",
      "        [ 0.1993],\n",
      "        [-1.8625],\n",
      "        [ 1.2580],\n",
      "        [ 0.6796],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 979: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 979: tensor([[ 1.0733],\n",
      "        [ 0.1983],\n",
      "        [-1.8642],\n",
      "        [ 1.2558],\n",
      "        [ 0.6772],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 980: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 980: tensor([[ 1.0736],\n",
      "        [ 0.1993],\n",
      "        [-1.8625],\n",
      "        [ 1.2580],\n",
      "        [ 0.6796],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 981: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 981: tensor([[ 1.0733],\n",
      "        [ 0.1983],\n",
      "        [-1.8643],\n",
      "        [ 1.2558],\n",
      "        [ 0.6772],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 982: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 982: tensor([[ 1.0736],\n",
      "        [ 0.1993],\n",
      "        [-1.8625],\n",
      "        [ 1.2581],\n",
      "        [ 0.6796],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 983: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 983: tensor([[ 1.0733],\n",
      "        [ 0.1982],\n",
      "        [-1.8643],\n",
      "        [ 1.2559],\n",
      "        [ 0.6772],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 984: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 984: tensor([[ 1.0736],\n",
      "        [ 0.1992],\n",
      "        [-1.8626],\n",
      "        [ 1.2581],\n",
      "        [ 0.6796],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 985: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 985: tensor([[ 1.0733],\n",
      "        [ 0.1982],\n",
      "        [-1.8643],\n",
      "        [ 1.2559],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 986: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 986: tensor([[ 1.0736],\n",
      "        [ 0.1992],\n",
      "        [-1.8626],\n",
      "        [ 1.2581],\n",
      "        [ 0.6796],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 987: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 987: tensor([[ 1.0733],\n",
      "        [ 0.1982],\n",
      "        [-1.8644],\n",
      "        [ 1.2559],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 988: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 988: tensor([[ 1.0736],\n",
      "        [ 0.1992],\n",
      "        [-1.8627],\n",
      "        [ 1.2581],\n",
      "        [ 0.6797],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 989: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 989: tensor([[ 1.0733],\n",
      "        [ 0.1981],\n",
      "        [-1.8644],\n",
      "        [ 1.2559],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 990: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 990: tensor([[ 1.0736],\n",
      "        [ 0.1991],\n",
      "        [-1.8627],\n",
      "        [ 1.2582],\n",
      "        [ 0.6797],\n",
      "        [-0.2836]], requires_grad=True)\n",
      "poly train loss at 991: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 991: tensor([[ 1.0733],\n",
      "        [ 0.1981],\n",
      "        [-1.8644],\n",
      "        [ 1.2560],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 992: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 992: tensor([[ 1.0736],\n",
      "        [ 0.1991],\n",
      "        [-1.8627],\n",
      "        [ 1.2582],\n",
      "        [ 0.6797],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 993: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 993: tensor([[ 1.0733],\n",
      "        [ 0.1981],\n",
      "        [-1.8645],\n",
      "        [ 1.2560],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 994: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 994: tensor([[ 1.0736],\n",
      "        [ 0.1991],\n",
      "        [-1.8628],\n",
      "        [ 1.2582],\n",
      "        [ 0.6797],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 995: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 995: tensor([[ 1.0733],\n",
      "        [ 0.1980],\n",
      "        [-1.8645],\n",
      "        [ 1.2560],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 996: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 996: tensor([[ 1.0736],\n",
      "        [ 0.1990],\n",
      "        [-1.8628],\n",
      "        [ 1.2582],\n",
      "        [ 0.6797],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 997: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 997: tensor([[ 1.0733],\n",
      "        [ 0.1980],\n",
      "        [-1.8646],\n",
      "        [ 1.2561],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 998: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 998: tensor([[ 1.0736],\n",
      "        [ 0.1990],\n",
      "        [-1.8628],\n",
      "        [ 1.2583],\n",
      "        [ 0.6797],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 999: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 999: tensor([[ 1.0733],\n",
      "        [ 0.1979],\n",
      "        [-1.8646],\n",
      "        [ 1.2561],\n",
      "        [ 0.6773],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1000: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1000: tensor([[ 1.0736],\n",
      "        [ 0.1990],\n",
      "        [-1.8629],\n",
      "        [ 1.2583],\n",
      "        [ 0.6797],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1001: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1001: tensor([[ 1.0733],\n",
      "        [ 0.1979],\n",
      "        [-1.8646],\n",
      "        [ 1.2561],\n",
      "        [ 0.6774],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1002: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1002: tensor([[ 1.0736],\n",
      "        [ 0.1989],\n",
      "        [-1.8629],\n",
      "        [ 1.2583],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1003: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1003: tensor([[ 1.0733],\n",
      "        [ 0.1979],\n",
      "        [-1.8647],\n",
      "        [ 1.2561],\n",
      "        [ 0.6774],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1004: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1004: tensor([[ 1.0736],\n",
      "        [ 0.1989],\n",
      "        [-1.8630],\n",
      "        [ 1.2584],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1005: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1005: tensor([[ 1.0733],\n",
      "        [ 0.1978],\n",
      "        [-1.8647],\n",
      "        [ 1.2562],\n",
      "        [ 0.6774],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1006: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1006: tensor([[ 1.0736],\n",
      "        [ 0.1988],\n",
      "        [-1.8630],\n",
      "        [ 1.2584],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1007: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1007: tensor([[ 1.0733],\n",
      "        [ 0.1978],\n",
      "        [-1.8647],\n",
      "        [ 1.2562],\n",
      "        [ 0.6774],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1008: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1008: tensor([[ 1.0736],\n",
      "        [ 0.1988],\n",
      "        [-1.8630],\n",
      "        [ 1.2584],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1009: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1009: tensor([[ 1.0733],\n",
      "        [ 0.1978],\n",
      "        [-1.8648],\n",
      "        [ 1.2562],\n",
      "        [ 0.6774],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1010: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1010: tensor([[ 1.0736],\n",
      "        [ 0.1988],\n",
      "        [-1.8631],\n",
      "        [ 1.2584],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1011: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1011: tensor([[ 1.0733],\n",
      "        [ 0.1977],\n",
      "        [-1.8648],\n",
      "        [ 1.2562],\n",
      "        [ 0.6774],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1012: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1012: tensor([[ 1.0737],\n",
      "        [ 0.1987],\n",
      "        [-1.8631],\n",
      "        [ 1.2585],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1013: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1013: tensor([[ 1.0733],\n",
      "        [ 0.1977],\n",
      "        [-1.8648],\n",
      "        [ 1.2563],\n",
      "        [ 0.6774],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1014: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1014: tensor([[ 1.0737],\n",
      "        [ 0.1987],\n",
      "        [-1.8631],\n",
      "        [ 1.2585],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1015: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1015: tensor([[ 1.0733],\n",
      "        [ 0.1977],\n",
      "        [-1.8649],\n",
      "        [ 1.2563],\n",
      "        [ 0.6775],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1016: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1016: tensor([[ 1.0737],\n",
      "        [ 0.1987],\n",
      "        [-1.8632],\n",
      "        [ 1.2585],\n",
      "        [ 0.6798],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1017: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1017: tensor([[ 1.0733],\n",
      "        [ 0.1976],\n",
      "        [-1.8649],\n",
      "        [ 1.2563],\n",
      "        [ 0.6775],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1018: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1018: tensor([[ 1.0737],\n",
      "        [ 0.1986],\n",
      "        [-1.8632],\n",
      "        [ 1.2585],\n",
      "        [ 0.6799],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1019: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1019: tensor([[ 1.0733],\n",
      "        [ 0.1976],\n",
      "        [-1.8650],\n",
      "        [ 1.2564],\n",
      "        [ 0.6775],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1020: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1020: tensor([[ 1.0737],\n",
      "        [ 0.1986],\n",
      "        [-1.8632],\n",
      "        [ 1.2586],\n",
      "        [ 0.6799],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1021: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1021: tensor([[ 1.0733],\n",
      "        [ 0.1976],\n",
      "        [-1.8650],\n",
      "        [ 1.2564],\n",
      "        [ 0.6775],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1022: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1022: tensor([[ 1.0737],\n",
      "        [ 0.1986],\n",
      "        [-1.8633],\n",
      "        [ 1.2586],\n",
      "        [ 0.6799],\n",
      "        [-0.2837]], requires_grad=True)\n",
      "poly train loss at 1023: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1023: tensor([[ 1.0733],\n",
      "        [ 0.1975],\n",
      "        [-1.8650],\n",
      "        [ 1.2564],\n",
      "        [ 0.6775],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1024: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1024: tensor([[ 1.0737],\n",
      "        [ 0.1985],\n",
      "        [-1.8633],\n",
      "        [ 1.2586],\n",
      "        [ 0.6799],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1025: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1025: tensor([[ 1.0733],\n",
      "        [ 0.1975],\n",
      "        [-1.8651],\n",
      "        [ 1.2564],\n",
      "        [ 0.6775],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1026: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1026: tensor([[ 1.0737],\n",
      "        [ 0.1985],\n",
      "        [-1.8634],\n",
      "        [ 1.2586],\n",
      "        [ 0.6799],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1027: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1027: tensor([[ 1.0733],\n",
      "        [ 0.1975],\n",
      "        [-1.8651],\n",
      "        [ 1.2565],\n",
      "        [ 0.6775],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1028: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1028: tensor([[ 1.0737],\n",
      "        [ 0.1985],\n",
      "        [-1.8634],\n",
      "        [ 1.2587],\n",
      "        [ 0.6799],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1029: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1029: tensor([[ 1.0733],\n",
      "        [ 0.1974],\n",
      "        [-1.8651],\n",
      "        [ 1.2565],\n",
      "        [ 0.6776],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1030: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1030: tensor([[ 1.0737],\n",
      "        [ 0.1984],\n",
      "        [-1.8634],\n",
      "        [ 1.2587],\n",
      "        [ 0.6799],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1031: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1031: tensor([[ 1.0733],\n",
      "        [ 0.1974],\n",
      "        [-1.8652],\n",
      "        [ 1.2565],\n",
      "        [ 0.6776],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1032: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1032: tensor([[ 1.0737],\n",
      "        [ 0.1984],\n",
      "        [-1.8635],\n",
      "        [ 1.2587],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1033: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1033: tensor([[ 1.0733],\n",
      "        [ 0.1974],\n",
      "        [-1.8652],\n",
      "        [ 1.2565],\n",
      "        [ 0.6776],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1034: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1034: tensor([[ 1.0737],\n",
      "        [ 0.1984],\n",
      "        [-1.8635],\n",
      "        [ 1.2588],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1035: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1035: tensor([[ 1.0733],\n",
      "        [ 0.1973],\n",
      "        [-1.8653],\n",
      "        [ 1.2566],\n",
      "        [ 0.6776],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1036: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1036: tensor([[ 1.0737],\n",
      "        [ 0.1983],\n",
      "        [-1.8635],\n",
      "        [ 1.2588],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1037: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1037: tensor([[ 1.0733],\n",
      "        [ 0.1973],\n",
      "        [-1.8653],\n",
      "        [ 1.2566],\n",
      "        [ 0.6776],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1038: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1038: tensor([[ 1.0737],\n",
      "        [ 0.1983],\n",
      "        [-1.8636],\n",
      "        [ 1.2588],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1039: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1039: tensor([[ 1.0733],\n",
      "        [ 0.1973],\n",
      "        [-1.8653],\n",
      "        [ 1.2566],\n",
      "        [ 0.6776],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1040: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1040: tensor([[ 1.0737],\n",
      "        [ 0.1983],\n",
      "        [-1.8636],\n",
      "        [ 1.2588],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1041: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1041: tensor([[ 1.0733],\n",
      "        [ 0.1972],\n",
      "        [-1.8654],\n",
      "        [ 1.2566],\n",
      "        [ 0.6776],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1042: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1042: tensor([[ 1.0737],\n",
      "        [ 0.1982],\n",
      "        [-1.8636],\n",
      "        [ 1.2589],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1043: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1043: tensor([[ 1.0733],\n",
      "        [ 0.1972],\n",
      "        [-1.8654],\n",
      "        [ 1.2567],\n",
      "        [ 0.6776],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1044: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1044: tensor([[ 1.0737],\n",
      "        [ 0.1982],\n",
      "        [-1.8637],\n",
      "        [ 1.2589],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1045: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1045: tensor([[ 1.0733],\n",
      "        [ 0.1972],\n",
      "        [-1.8654],\n",
      "        [ 1.2567],\n",
      "        [ 0.6777],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1046: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1046: tensor([[ 1.0737],\n",
      "        [ 0.1982],\n",
      "        [-1.8637],\n",
      "        [ 1.2589],\n",
      "        [ 0.6800],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1047: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1047: tensor([[ 1.0733],\n",
      "        [ 0.1971],\n",
      "        [-1.8655],\n",
      "        [ 1.2567],\n",
      "        [ 0.6777],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1048: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1048: tensor([[ 1.0737],\n",
      "        [ 0.1981],\n",
      "        [-1.8638],\n",
      "        [ 1.2589],\n",
      "        [ 0.6801],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1049: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1049: tensor([[ 1.0733],\n",
      "        [ 0.1971],\n",
      "        [-1.8655],\n",
      "        [ 1.2568],\n",
      "        [ 0.6777],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1050: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1050: tensor([[ 1.0737],\n",
      "        [ 0.1981],\n",
      "        [-1.8638],\n",
      "        [ 1.2590],\n",
      "        [ 0.6801],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1051: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1051: tensor([[ 1.0733],\n",
      "        [ 0.1971],\n",
      "        [-1.8655],\n",
      "        [ 1.2568],\n",
      "        [ 0.6777],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1052: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1052: tensor([[ 1.0737],\n",
      "        [ 0.1981],\n",
      "        [-1.8638],\n",
      "        [ 1.2590],\n",
      "        [ 0.6801],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1053: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1053: tensor([[ 1.0733],\n",
      "        [ 0.1970],\n",
      "        [-1.8656],\n",
      "        [ 1.2568],\n",
      "        [ 0.6777],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1054: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1054: tensor([[ 1.0737],\n",
      "        [ 0.1980],\n",
      "        [-1.8639],\n",
      "        [ 1.2590],\n",
      "        [ 0.6801],\n",
      "        [-0.2838]], requires_grad=True)\n",
      "poly train loss at 1055: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1055: tensor([[ 1.0733],\n",
      "        [ 0.1970],\n",
      "        [-1.8656],\n",
      "        [ 1.2568],\n",
      "        [ 0.6777],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1056: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1056: tensor([[ 1.0737],\n",
      "        [ 0.1980],\n",
      "        [-1.8639],\n",
      "        [ 1.2590],\n",
      "        [ 0.6801],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1057: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1057: tensor([[ 1.0733],\n",
      "        [ 0.1970],\n",
      "        [-1.8657],\n",
      "        [ 1.2569],\n",
      "        [ 0.6777],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1058: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1058: tensor([[ 1.0737],\n",
      "        [ 0.1980],\n",
      "        [-1.8639],\n",
      "        [ 1.2591],\n",
      "        [ 0.6801],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1059: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1059: tensor([[ 1.0733],\n",
      "        [ 0.1969],\n",
      "        [-1.8657],\n",
      "        [ 1.2569],\n",
      "        [ 0.6778],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1060: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1060: tensor([[ 1.0737],\n",
      "        [ 0.1979],\n",
      "        [-1.8640],\n",
      "        [ 1.2591],\n",
      "        [ 0.6801],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1061: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1061: tensor([[ 1.0733],\n",
      "        [ 0.1969],\n",
      "        [-1.8657],\n",
      "        [ 1.2569],\n",
      "        [ 0.6778],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1062: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1062: tensor([[ 1.0737],\n",
      "        [ 0.1979],\n",
      "        [-1.8640],\n",
      "        [ 1.2591],\n",
      "        [ 0.6802],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1063: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1063: tensor([[ 1.0733],\n",
      "        [ 0.1969],\n",
      "        [-1.8658],\n",
      "        [ 1.2569],\n",
      "        [ 0.6778],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1064: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1064: tensor([[ 1.0737],\n",
      "        [ 0.1979],\n",
      "        [-1.8640],\n",
      "        [ 1.2592],\n",
      "        [ 0.6802],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1065: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1065: tensor([[ 1.0733],\n",
      "        [ 0.1968],\n",
      "        [-1.8658],\n",
      "        [ 1.2570],\n",
      "        [ 0.6778],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1066: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1066: tensor([[ 1.0737],\n",
      "        [ 0.1978],\n",
      "        [-1.8641],\n",
      "        [ 1.2592],\n",
      "        [ 0.6802],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1067: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1067: tensor([[ 1.0733],\n",
      "        [ 0.1968],\n",
      "        [-1.8658],\n",
      "        [ 1.2570],\n",
      "        [ 0.6778],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1068: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1068: tensor([[ 1.0737],\n",
      "        [ 0.1978],\n",
      "        [-1.8641],\n",
      "        [ 1.2592],\n",
      "        [ 0.6802],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1069: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1069: tensor([[ 1.0733],\n",
      "        [ 0.1968],\n",
      "        [-1.8659],\n",
      "        [ 1.2570],\n",
      "        [ 0.6778],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1070: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1070: tensor([[ 1.0737],\n",
      "        [ 0.1978],\n",
      "        [-1.8642],\n",
      "        [ 1.2592],\n",
      "        [ 0.6802],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1071: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1071: tensor([[ 1.0733],\n",
      "        [ 0.1967],\n",
      "        [-1.8659],\n",
      "        [ 1.2570],\n",
      "        [ 0.6778],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1072: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1072: tensor([[ 1.0737],\n",
      "        [ 0.1977],\n",
      "        [-1.8642],\n",
      "        [ 1.2593],\n",
      "        [ 0.6802],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1073: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1073: tensor([[ 1.0733],\n",
      "        [ 0.1967],\n",
      "        [-1.8659],\n",
      "        [ 1.2571],\n",
      "        [ 0.6778],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1074: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1074: tensor([[ 1.0737],\n",
      "        [ 0.1977],\n",
      "        [-1.8642],\n",
      "        [ 1.2593],\n",
      "        [ 0.6802],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1075: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1075: tensor([[ 1.0733],\n",
      "        [ 0.1967],\n",
      "        [-1.8660],\n",
      "        [ 1.2571],\n",
      "        [ 0.6779],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1076: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1076: tensor([[ 1.0737],\n",
      "        [ 0.1977],\n",
      "        [-1.8643],\n",
      "        [ 1.2593],\n",
      "        [ 0.6803],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1077: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1077: tensor([[ 1.0733],\n",
      "        [ 0.1966],\n",
      "        [-1.8660],\n",
      "        [ 1.2571],\n",
      "        [ 0.6779],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1078: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1078: tensor([[ 1.0737],\n",
      "        [ 0.1976],\n",
      "        [-1.8643],\n",
      "        [ 1.2593],\n",
      "        [ 0.6803],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1079: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1079: tensor([[ 1.0733],\n",
      "        [ 0.1966],\n",
      "        [-1.8661],\n",
      "        [ 1.2571],\n",
      "        [ 0.6779],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1080: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1080: tensor([[ 1.0737],\n",
      "        [ 0.1976],\n",
      "        [-1.8643],\n",
      "        [ 1.2594],\n",
      "        [ 0.6803],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1081: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1081: tensor([[ 1.0733],\n",
      "        [ 0.1966],\n",
      "        [-1.8661],\n",
      "        [ 1.2572],\n",
      "        [ 0.6779],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1082: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1082: tensor([[ 1.0737],\n",
      "        [ 0.1976],\n",
      "        [-1.8644],\n",
      "        [ 1.2594],\n",
      "        [ 0.6803],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1083: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1083: tensor([[ 1.0733],\n",
      "        [ 0.1965],\n",
      "        [-1.8661],\n",
      "        [ 1.2572],\n",
      "        [ 0.6779],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1084: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1084: tensor([[ 1.0737],\n",
      "        [ 0.1975],\n",
      "        [-1.8644],\n",
      "        [ 1.2594],\n",
      "        [ 0.6803],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1085: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1085: tensor([[ 1.0733],\n",
      "        [ 0.1965],\n",
      "        [-1.8662],\n",
      "        [ 1.2572],\n",
      "        [ 0.6779],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1086: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1086: tensor([[ 1.0737],\n",
      "        [ 0.1975],\n",
      "        [-1.8644],\n",
      "        [ 1.2594],\n",
      "        [ 0.6803],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1087: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1087: tensor([[ 1.0733],\n",
      "        [ 0.1965],\n",
      "        [-1.8662],\n",
      "        [ 1.2573],\n",
      "        [ 0.6779],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1088: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1088: tensor([[ 1.0737],\n",
      "        [ 0.1975],\n",
      "        [-1.8645],\n",
      "        [ 1.2595],\n",
      "        [ 0.6803],\n",
      "        [-0.2839]], requires_grad=True)\n",
      "poly train loss at 1089: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1089: tensor([[ 1.0733],\n",
      "        [ 0.1964],\n",
      "        [-1.8662],\n",
      "        [ 1.2573],\n",
      "        [ 0.6780],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1090: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1090: tensor([[ 1.0737],\n",
      "        [ 0.1974],\n",
      "        [-1.8645],\n",
      "        [ 1.2595],\n",
      "        [ 0.6803],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1091: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1091: tensor([[ 1.0733],\n",
      "        [ 0.1964],\n",
      "        [-1.8663],\n",
      "        [ 1.2573],\n",
      "        [ 0.6780],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1092: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1092: tensor([[ 1.0737],\n",
      "        [ 0.1974],\n",
      "        [-1.8646],\n",
      "        [ 1.2595],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1093: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1093: tensor([[ 1.0733],\n",
      "        [ 0.1964],\n",
      "        [-1.8663],\n",
      "        [ 1.2573],\n",
      "        [ 0.6780],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1094: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1094: tensor([[ 1.0737],\n",
      "        [ 0.1974],\n",
      "        [-1.8646],\n",
      "        [ 1.2595],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1095: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1095: tensor([[ 1.0733],\n",
      "        [ 0.1963],\n",
      "        [-1.8663],\n",
      "        [ 1.2574],\n",
      "        [ 0.6780],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1096: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1096: tensor([[ 1.0737],\n",
      "        [ 0.1973],\n",
      "        [-1.8646],\n",
      "        [ 1.2596],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1097: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1097: tensor([[ 1.0733],\n",
      "        [ 0.1963],\n",
      "        [-1.8664],\n",
      "        [ 1.2574],\n",
      "        [ 0.6780],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1098: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1098: tensor([[ 1.0737],\n",
      "        [ 0.1973],\n",
      "        [-1.8647],\n",
      "        [ 1.2596],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1099: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1099: tensor([[ 1.0733],\n",
      "        [ 0.1963],\n",
      "        [-1.8664],\n",
      "        [ 1.2574],\n",
      "        [ 0.6780],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1100: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1100: tensor([[ 1.0737],\n",
      "        [ 0.1973],\n",
      "        [-1.8647],\n",
      "        [ 1.2596],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1101: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1101: tensor([[ 1.0734],\n",
      "        [ 0.1962],\n",
      "        [-1.8665],\n",
      "        [ 1.2574],\n",
      "        [ 0.6780],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1102: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1102: tensor([[ 1.0737],\n",
      "        [ 0.1972],\n",
      "        [-1.8647],\n",
      "        [ 1.2597],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1103: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1103: tensor([[ 1.0734],\n",
      "        [ 0.1962],\n",
      "        [-1.8665],\n",
      "        [ 1.2575],\n",
      "        [ 0.6780],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1104: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1104: tensor([[ 1.0737],\n",
      "        [ 0.1972],\n",
      "        [-1.8648],\n",
      "        [ 1.2597],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1105: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1105: tensor([[ 1.0734],\n",
      "        [ 0.1962],\n",
      "        [-1.8665],\n",
      "        [ 1.2575],\n",
      "        [ 0.6781],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1106: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1106: tensor([[ 1.0737],\n",
      "        [ 0.1972],\n",
      "        [-1.8648],\n",
      "        [ 1.2597],\n",
      "        [ 0.6804],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1107: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1107: tensor([[ 1.0734],\n",
      "        [ 0.1961],\n",
      "        [-1.8666],\n",
      "        [ 1.2575],\n",
      "        [ 0.6781],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1108: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1108: tensor([[ 1.0737],\n",
      "        [ 0.1971],\n",
      "        [-1.8648],\n",
      "        [ 1.2597],\n",
      "        [ 0.6805],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1109: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1109: tensor([[ 1.0734],\n",
      "        [ 0.1961],\n",
      "        [-1.8666],\n",
      "        [ 1.2575],\n",
      "        [ 0.6781],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1110: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1110: tensor([[ 1.0737],\n",
      "        [ 0.1971],\n",
      "        [-1.8649],\n",
      "        [ 1.2598],\n",
      "        [ 0.6805],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1111: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1111: tensor([[ 1.0734],\n",
      "        [ 0.1961],\n",
      "        [-1.8666],\n",
      "        [ 1.2576],\n",
      "        [ 0.6781],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1112: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1112: tensor([[ 1.0737],\n",
      "        [ 0.1971],\n",
      "        [-1.8649],\n",
      "        [ 1.2598],\n",
      "        [ 0.6805],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1113: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1113: tensor([[ 1.0734],\n",
      "        [ 0.1960],\n",
      "        [-1.8667],\n",
      "        [ 1.2576],\n",
      "        [ 0.6781],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1114: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1114: tensor([[ 1.0737],\n",
      "        [ 0.1970],\n",
      "        [-1.8650],\n",
      "        [ 1.2598],\n",
      "        [ 0.6805],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1115: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1115: tensor([[ 1.0734],\n",
      "        [ 0.1960],\n",
      "        [-1.8667],\n",
      "        [ 1.2576],\n",
      "        [ 0.6781],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1116: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1116: tensor([[ 1.0737],\n",
      "        [ 0.1970],\n",
      "        [-1.8650],\n",
      "        [ 1.2598],\n",
      "        [ 0.6805],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1117: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1117: tensor([[ 1.0734],\n",
      "        [ 0.1960],\n",
      "        [-1.8667],\n",
      "        [ 1.2576],\n",
      "        [ 0.6781],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1118: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1118: tensor([[ 1.0737],\n",
      "        [ 0.1970],\n",
      "        [-1.8650],\n",
      "        [ 1.2599],\n",
      "        [ 0.6805],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1119: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1119: tensor([[ 1.0734],\n",
      "        [ 0.1959],\n",
      "        [-1.8668],\n",
      "        [ 1.2577],\n",
      "        [ 0.6782],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1120: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1120: tensor([[ 1.0737],\n",
      "        [ 0.1969],\n",
      "        [-1.8651],\n",
      "        [ 1.2599],\n",
      "        [ 0.6805],\n",
      "        [-0.2840]], requires_grad=True)\n",
      "poly train loss at 1121: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1121: tensor([[ 1.0734],\n",
      "        [ 0.1959],\n",
      "        [-1.8668],\n",
      "        [ 1.2577],\n",
      "        [ 0.6782],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1122: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1122: tensor([[ 1.0737],\n",
      "        [ 0.1969],\n",
      "        [-1.8651],\n",
      "        [ 1.2599],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1123: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1123: tensor([[ 1.0734],\n",
      "        [ 0.1959],\n",
      "        [-1.8668],\n",
      "        [ 1.2577],\n",
      "        [ 0.6782],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1124: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1124: tensor([[ 1.0737],\n",
      "        [ 0.1969],\n",
      "        [-1.8651],\n",
      "        [ 1.2599],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1125: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1125: tensor([[ 1.0734],\n",
      "        [ 0.1958],\n",
      "        [-1.8669],\n",
      "        [ 1.2578],\n",
      "        [ 0.6782],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1126: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1126: tensor([[ 1.0737],\n",
      "        [ 0.1968],\n",
      "        [-1.8652],\n",
      "        [ 1.2600],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1127: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1127: tensor([[ 1.0734],\n",
      "        [ 0.1958],\n",
      "        [-1.8669],\n",
      "        [ 1.2578],\n",
      "        [ 0.6782],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1128: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1128: tensor([[ 1.0737],\n",
      "        [ 0.1968],\n",
      "        [-1.8652],\n",
      "        [ 1.2600],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1129: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1129: tensor([[ 1.0734],\n",
      "        [ 0.1958],\n",
      "        [-1.8670],\n",
      "        [ 1.2578],\n",
      "        [ 0.6782],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1130: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1130: tensor([[ 1.0737],\n",
      "        [ 0.1968],\n",
      "        [-1.8652],\n",
      "        [ 1.2600],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1131: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1131: tensor([[ 1.0734],\n",
      "        [ 0.1957],\n",
      "        [-1.8670],\n",
      "        [ 1.2578],\n",
      "        [ 0.6782],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1132: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1132: tensor([[ 1.0737],\n",
      "        [ 0.1967],\n",
      "        [-1.8653],\n",
      "        [ 1.2600],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1133: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1133: tensor([[ 1.0734],\n",
      "        [ 0.1957],\n",
      "        [-1.8670],\n",
      "        [ 1.2579],\n",
      "        [ 0.6782],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1134: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1134: tensor([[ 1.0737],\n",
      "        [ 0.1967],\n",
      "        [-1.8653],\n",
      "        [ 1.2601],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1135: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1135: tensor([[ 1.0734],\n",
      "        [ 0.1957],\n",
      "        [-1.8671],\n",
      "        [ 1.2579],\n",
      "        [ 0.6783],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1136: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1136: tensor([[ 1.0737],\n",
      "        [ 0.1967],\n",
      "        [-1.8654],\n",
      "        [ 1.2601],\n",
      "        [ 0.6806],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1137: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1137: tensor([[ 1.0734],\n",
      "        [ 0.1956],\n",
      "        [-1.8671],\n",
      "        [ 1.2579],\n",
      "        [ 0.6783],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1138: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1138: tensor([[ 1.0737],\n",
      "        [ 0.1966],\n",
      "        [-1.8654],\n",
      "        [ 1.2601],\n",
      "        [ 0.6807],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1139: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1139: tensor([[ 1.0734],\n",
      "        [ 0.1956],\n",
      "        [-1.8671],\n",
      "        [ 1.2579],\n",
      "        [ 0.6783],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1140: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1140: tensor([[ 1.0737],\n",
      "        [ 0.1966],\n",
      "        [-1.8654],\n",
      "        [ 1.2602],\n",
      "        [ 0.6807],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1141: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1141: tensor([[ 1.0734],\n",
      "        [ 0.1956],\n",
      "        [-1.8672],\n",
      "        [ 1.2580],\n",
      "        [ 0.6783],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1142: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1142: tensor([[ 1.0737],\n",
      "        [ 0.1966],\n",
      "        [-1.8655],\n",
      "        [ 1.2602],\n",
      "        [ 0.6807],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1143: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1143: tensor([[ 1.0734],\n",
      "        [ 0.1955],\n",
      "        [-1.8672],\n",
      "        [ 1.2580],\n",
      "        [ 0.6783],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1144: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1144: tensor([[ 1.0737],\n",
      "        [ 0.1965],\n",
      "        [-1.8655],\n",
      "        [ 1.2602],\n",
      "        [ 0.6807],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1145: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1145: tensor([[ 1.0734],\n",
      "        [ 0.1955],\n",
      "        [-1.8672],\n",
      "        [ 1.2580],\n",
      "        [ 0.6783],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1146: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1146: tensor([[ 1.0737],\n",
      "        [ 0.1965],\n",
      "        [-1.8655],\n",
      "        [ 1.2602],\n",
      "        [ 0.6807],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1147: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1147: tensor([[ 1.0734],\n",
      "        [ 0.1955],\n",
      "        [-1.8673],\n",
      "        [ 1.2580],\n",
      "        [ 0.6783],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1148: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1148: tensor([[ 1.0737],\n",
      "        [ 0.1965],\n",
      "        [-1.8656],\n",
      "        [ 1.2603],\n",
      "        [ 0.6807],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1149: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1149: tensor([[ 1.0734],\n",
      "        [ 0.1954],\n",
      "        [-1.8673],\n",
      "        [ 1.2581],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1150: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1150: tensor([[ 1.0737],\n",
      "        [ 0.1964],\n",
      "        [-1.8656],\n",
      "        [ 1.2603],\n",
      "        [ 0.6807],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1151: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1151: tensor([[ 1.0734],\n",
      "        [ 0.1954],\n",
      "        [-1.8674],\n",
      "        [ 1.2581],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1152: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1152: tensor([[ 1.0737],\n",
      "        [ 0.1964],\n",
      "        [-1.8656],\n",
      "        [ 1.2603],\n",
      "        [ 0.6808],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1153: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1153: tensor([[ 1.0734],\n",
      "        [ 0.1954],\n",
      "        [-1.8674],\n",
      "        [ 1.2581],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1154: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1154: tensor([[ 1.0737],\n",
      "        [ 0.1964],\n",
      "        [-1.8657],\n",
      "        [ 1.2603],\n",
      "        [ 0.6808],\n",
      "        [-0.2841]], requires_grad=True)\n",
      "poly train loss at 1155: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1155: tensor([[ 1.0734],\n",
      "        [ 0.1953],\n",
      "        [-1.8674],\n",
      "        [ 1.2581],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1156: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1156: tensor([[ 1.0737],\n",
      "        [ 0.1963],\n",
      "        [-1.8657],\n",
      "        [ 1.2604],\n",
      "        [ 0.6808],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1157: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1157: tensor([[ 1.0734],\n",
      "        [ 0.1953],\n",
      "        [-1.8675],\n",
      "        [ 1.2582],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1158: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1158: tensor([[ 1.0737],\n",
      "        [ 0.1963],\n",
      "        [-1.8657],\n",
      "        [ 1.2604],\n",
      "        [ 0.6808],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1159: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1159: tensor([[ 1.0734],\n",
      "        [ 0.1953],\n",
      "        [-1.8675],\n",
      "        [ 1.2582],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1160: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1160: tensor([[ 1.0737],\n",
      "        [ 0.1963],\n",
      "        [-1.8658],\n",
      "        [ 1.2604],\n",
      "        [ 0.6808],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1161: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1161: tensor([[ 1.0734],\n",
      "        [ 0.1952],\n",
      "        [-1.8675],\n",
      "        [ 1.2582],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1162: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1162: tensor([[ 1.0737],\n",
      "        [ 0.1962],\n",
      "        [-1.8658],\n",
      "        [ 1.2604],\n",
      "        [ 0.6808],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1163: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1163: tensor([[ 1.0734],\n",
      "        [ 0.1952],\n",
      "        [-1.8676],\n",
      "        [ 1.2583],\n",
      "        [ 0.6784],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 1164: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1164: tensor([[ 1.0737],\n",
      "        [ 0.1962],\n",
      "        [-1.8659],\n",
      "        [ 1.2605],\n",
      "        [ 0.6808],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1165: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1165: tensor([[ 1.0734],\n",
      "        [ 0.1952],\n",
      "        [-1.8676],\n",
      "        [ 1.2583],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1166: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1166: tensor([[ 1.0737],\n",
      "        [ 0.1962],\n",
      "        [-1.8659],\n",
      "        [ 1.2605],\n",
      "        [ 0.6808],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1167: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1167: tensor([[ 1.0734],\n",
      "        [ 0.1951],\n",
      "        [-1.8676],\n",
      "        [ 1.2583],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1168: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1168: tensor([[ 1.0737],\n",
      "        [ 0.1961],\n",
      "        [-1.8659],\n",
      "        [ 1.2605],\n",
      "        [ 0.6809],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1169: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1169: tensor([[ 1.0734],\n",
      "        [ 0.1951],\n",
      "        [-1.8677],\n",
      "        [ 1.2583],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1170: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1170: tensor([[ 1.0737],\n",
      "        [ 0.1961],\n",
      "        [-1.8660],\n",
      "        [ 1.2605],\n",
      "        [ 0.6809],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1171: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1171: tensor([[ 1.0734],\n",
      "        [ 0.1951],\n",
      "        [-1.8677],\n",
      "        [ 1.2584],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1172: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1172: tensor([[ 1.0737],\n",
      "        [ 0.1961],\n",
      "        [-1.8660],\n",
      "        [ 1.2606],\n",
      "        [ 0.6809],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1173: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1173: tensor([[ 1.0734],\n",
      "        [ 0.1950],\n",
      "        [-1.8677],\n",
      "        [ 1.2584],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1174: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1174: tensor([[ 1.0737],\n",
      "        [ 0.1960],\n",
      "        [-1.8660],\n",
      "        [ 1.2606],\n",
      "        [ 0.6809],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1175: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1175: tensor([[ 1.0734],\n",
      "        [ 0.1950],\n",
      "        [-1.8678],\n",
      "        [ 1.2584],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1176: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1176: tensor([[ 1.0737],\n",
      "        [ 0.1960],\n",
      "        [-1.8661],\n",
      "        [ 1.2606],\n",
      "        [ 0.6809],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1177: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1177: tensor([[ 1.0734],\n",
      "        [ 0.1950],\n",
      "        [-1.8678],\n",
      "        [ 1.2584],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1178: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1178: tensor([[ 1.0737],\n",
      "        [ 0.1960],\n",
      "        [-1.8661],\n",
      "        [ 1.2606],\n",
      "        [ 0.6809],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1179: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1179: tensor([[ 1.0734],\n",
      "        [ 0.1949],\n",
      "        [-1.8679],\n",
      "        [ 1.2585],\n",
      "        [ 0.6785],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1180: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1180: tensor([[ 1.0737],\n",
      "        [ 0.1959],\n",
      "        [-1.8661],\n",
      "        [ 1.2607],\n",
      "        [ 0.6809],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1181: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1181: tensor([[ 1.0734],\n",
      "        [ 0.1949],\n",
      "        [-1.8679],\n",
      "        [ 1.2585],\n",
      "        [ 0.6786],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1182: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1182: tensor([[ 1.0737],\n",
      "        [ 0.1959],\n",
      "        [-1.8662],\n",
      "        [ 1.2607],\n",
      "        [ 0.6810],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1183: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1183: tensor([[ 1.0734],\n",
      "        [ 0.1949],\n",
      "        [-1.8679],\n",
      "        [ 1.2585],\n",
      "        [ 0.6786],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1184: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1184: tensor([[ 1.0737],\n",
      "        [ 0.1959],\n",
      "        [-1.8662],\n",
      "        [ 1.2607],\n",
      "        [ 0.6810],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1185: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1185: tensor([[ 1.0734],\n",
      "        [ 0.1948],\n",
      "        [-1.8680],\n",
      "        [ 1.2585],\n",
      "        [ 0.6786],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1186: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1186: tensor([[ 1.0737],\n",
      "        [ 0.1958],\n",
      "        [-1.8662],\n",
      "        [ 1.2608],\n",
      "        [ 0.6810],\n",
      "        [-0.2842]], requires_grad=True)\n",
      "poly train loss at 1187: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1187: tensor([[ 1.0734],\n",
      "        [ 0.1948],\n",
      "        [-1.8680],\n",
      "        [ 1.2586],\n",
      "        [ 0.6786],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1188: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1188: tensor([[ 1.0737],\n",
      "        [ 0.1958],\n",
      "        [-1.8663],\n",
      "        [ 1.2608],\n",
      "        [ 0.6810],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1189: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1189: tensor([[ 1.0734],\n",
      "        [ 0.1948],\n",
      "        [-1.8680],\n",
      "        [ 1.2586],\n",
      "        [ 0.6786],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1190: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1190: tensor([[ 1.0737],\n",
      "        [ 0.1958],\n",
      "        [-1.8663],\n",
      "        [ 1.2608],\n",
      "        [ 0.6810],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1191: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1191: tensor([[ 1.0734],\n",
      "        [ 0.1947],\n",
      "        [-1.8681],\n",
      "        [ 1.2586],\n",
      "        [ 0.6786],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1192: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1192: tensor([[ 1.0737],\n",
      "        [ 0.1957],\n",
      "        [-1.8664],\n",
      "        [ 1.2608],\n",
      "        [ 0.6810],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1193: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1193: tensor([[ 1.0734],\n",
      "        [ 0.1947],\n",
      "        [-1.8681],\n",
      "        [ 1.2586],\n",
      "        [ 0.6786],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1194: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1194: tensor([[ 1.0737],\n",
      "        [ 0.1957],\n",
      "        [-1.8664],\n",
      "        [ 1.2609],\n",
      "        [ 0.6810],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1195: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1195: tensor([[ 1.0734],\n",
      "        [ 0.1947],\n",
      "        [-1.8681],\n",
      "        [ 1.2587],\n",
      "        [ 0.6787],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1196: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1196: tensor([[ 1.0737],\n",
      "        [ 0.1957],\n",
      "        [-1.8664],\n",
      "        [ 1.2609],\n",
      "        [ 0.6810],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1197: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1197: tensor([[ 1.0734],\n",
      "        [ 0.1946],\n",
      "        [-1.8682],\n",
      "        [ 1.2587],\n",
      "        [ 0.6787],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 1198: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1198: tensor([[ 1.0737],\n",
      "        [ 0.1956],\n",
      "        [-1.8665],\n",
      "        [ 1.2609],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1199: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1199: tensor([[ 1.0734],\n",
      "        [ 0.1946],\n",
      "        [-1.8682],\n",
      "        [ 1.2587],\n",
      "        [ 0.6787],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1200: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1200: tensor([[ 1.0737],\n",
      "        [ 0.1956],\n",
      "        [-1.8665],\n",
      "        [ 1.2609],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1201: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1201: tensor([[ 1.0734],\n",
      "        [ 0.1946],\n",
      "        [-1.8682],\n",
      "        [ 1.2587],\n",
      "        [ 0.6787],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1202: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1202: tensor([[ 1.0737],\n",
      "        [ 0.1956],\n",
      "        [-1.8665],\n",
      "        [ 1.2610],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1203: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1203: tensor([[ 1.0734],\n",
      "        [ 0.1945],\n",
      "        [-1.8683],\n",
      "        [ 1.2588],\n",
      "        [ 0.6787],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1204: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1204: tensor([[ 1.0737],\n",
      "        [ 0.1955],\n",
      "        [-1.8666],\n",
      "        [ 1.2610],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1205: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1205: tensor([[ 1.0734],\n",
      "        [ 0.1945],\n",
      "        [-1.8683],\n",
      "        [ 1.2588],\n",
      "        [ 0.6787],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1206: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1206: tensor([[ 1.0737],\n",
      "        [ 0.1955],\n",
      "        [-1.8666],\n",
      "        [ 1.2610],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1207: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1207: tensor([[ 1.0734],\n",
      "        [ 0.1945],\n",
      "        [-1.8683],\n",
      "        [ 1.2588],\n",
      "        [ 0.6787],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1208: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1208: tensor([[ 1.0737],\n",
      "        [ 0.1955],\n",
      "        [-1.8666],\n",
      "        [ 1.2610],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1209: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1209: tensor([[ 1.0734],\n",
      "        [ 0.1944],\n",
      "        [-1.8684],\n",
      "        [ 1.2588],\n",
      "        [ 0.6787],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1210: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1210: tensor([[ 1.0737],\n",
      "        [ 0.1954],\n",
      "        [-1.8667],\n",
      "        [ 1.2611],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1211: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1211: tensor([[ 1.0734],\n",
      "        [ 0.1944],\n",
      "        [-1.8684],\n",
      "        [ 1.2589],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1212: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1212: tensor([[ 1.0737],\n",
      "        [ 0.1954],\n",
      "        [-1.8667],\n",
      "        [ 1.2611],\n",
      "        [ 0.6811],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1213: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1213: tensor([[ 1.0734],\n",
      "        [ 0.1944],\n",
      "        [-1.8685],\n",
      "        [ 1.2589],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1214: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1214: tensor([[ 1.0737],\n",
      "        [ 0.1954],\n",
      "        [-1.8667],\n",
      "        [ 1.2611],\n",
      "        [ 0.6812],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1215: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1215: tensor([[ 1.0734],\n",
      "        [ 0.1943],\n",
      "        [-1.8685],\n",
      "        [ 1.2589],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1216: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1216: tensor([[ 1.0737],\n",
      "        [ 0.1953],\n",
      "        [-1.8668],\n",
      "        [ 1.2611],\n",
      "        [ 0.6812],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1217: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1217: tensor([[ 1.0734],\n",
      "        [ 0.1943],\n",
      "        [-1.8685],\n",
      "        [ 1.2590],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1218: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1218: tensor([[ 1.0737],\n",
      "        [ 0.1953],\n",
      "        [-1.8668],\n",
      "        [ 1.2612],\n",
      "        [ 0.6812],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1219: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1219: tensor([[ 1.0734],\n",
      "        [ 0.1943],\n",
      "        [-1.8686],\n",
      "        [ 1.2590],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1220: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1220: tensor([[ 1.0737],\n",
      "        [ 0.1953],\n",
      "        [-1.8668],\n",
      "        [ 1.2612],\n",
      "        [ 0.6812],\n",
      "        [-0.2843]], requires_grad=True)\n",
      "poly train loss at 1221: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1221: tensor([[ 1.0734],\n",
      "        [ 0.1942],\n",
      "        [-1.8686],\n",
      "        [ 1.2590],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1222: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1222: tensor([[ 1.0737],\n",
      "        [ 0.1952],\n",
      "        [-1.8669],\n",
      "        [ 1.2612],\n",
      "        [ 0.6812],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1223: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1223: tensor([[ 1.0734],\n",
      "        [ 0.1942],\n",
      "        [-1.8686],\n",
      "        [ 1.2590],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1224: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1224: tensor([[ 1.0737],\n",
      "        [ 0.1952],\n",
      "        [-1.8669],\n",
      "        [ 1.2612],\n",
      "        [ 0.6812],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1225: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1225: tensor([[ 1.0734],\n",
      "        [ 0.1942],\n",
      "        [-1.8687],\n",
      "        [ 1.2591],\n",
      "        [ 0.6788],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1226: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1226: tensor([[ 1.0737],\n",
      "        [ 0.1952],\n",
      "        [-1.8670],\n",
      "        [ 1.2613],\n",
      "        [ 0.6812],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1227: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1227: tensor([[ 1.0734],\n",
      "        [ 0.1941],\n",
      "        [-1.8687],\n",
      "        [ 1.2591],\n",
      "        [ 0.6789],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1228: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1228: tensor([[ 1.0737],\n",
      "        [ 0.1951],\n",
      "        [-1.8670],\n",
      "        [ 1.2613],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1229: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1229: tensor([[ 1.0734],\n",
      "        [ 0.1941],\n",
      "        [-1.8687],\n",
      "        [ 1.2591],\n",
      "        [ 0.6789],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1230: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1230: tensor([[ 1.0737],\n",
      "        [ 0.1951],\n",
      "        [-1.8670],\n",
      "        [ 1.2613],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1231: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1231: tensor([[ 1.0734],\n",
      "        [ 0.1941],\n",
      "        [-1.8688],\n",
      "        [ 1.2591],\n",
      "        [ 0.6789],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 1232: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1232: tensor([[ 1.0737],\n",
      "        [ 0.1951],\n",
      "        [-1.8671],\n",
      "        [ 1.2613],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1233: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1233: tensor([[ 1.0734],\n",
      "        [ 0.1940],\n",
      "        [-1.8688],\n",
      "        [ 1.2592],\n",
      "        [ 0.6789],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1234: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1234: tensor([[ 1.0737],\n",
      "        [ 0.1951],\n",
      "        [-1.8671],\n",
      "        [ 1.2614],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1235: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1235: tensor([[ 1.0734],\n",
      "        [ 0.1940],\n",
      "        [-1.8688],\n",
      "        [ 1.2592],\n",
      "        [ 0.6789],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1236: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1236: tensor([[ 1.0737],\n",
      "        [ 0.1950],\n",
      "        [-1.8671],\n",
      "        [ 1.2614],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1237: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1237: tensor([[ 1.0734],\n",
      "        [ 0.1940],\n",
      "        [-1.8689],\n",
      "        [ 1.2592],\n",
      "        [ 0.6789],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1238: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1238: tensor([[ 1.0737],\n",
      "        [ 0.1950],\n",
      "        [-1.8672],\n",
      "        [ 1.2614],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1239: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1239: tensor([[ 1.0734],\n",
      "        [ 0.1940],\n",
      "        [-1.8689],\n",
      "        [ 1.2592],\n",
      "        [ 0.6789],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1240: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1240: tensor([[ 1.0737],\n",
      "        [ 0.1950],\n",
      "        [-1.8672],\n",
      "        [ 1.2614],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1241: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1241: tensor([[ 1.0734],\n",
      "        [ 0.1939],\n",
      "        [-1.8689],\n",
      "        [ 1.2593],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1242: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1242: tensor([[ 1.0737],\n",
      "        [ 0.1949],\n",
      "        [-1.8672],\n",
      "        [ 1.2615],\n",
      "        [ 0.6813],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1243: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1243: tensor([[ 1.0734],\n",
      "        [ 0.1939],\n",
      "        [-1.8690],\n",
      "        [ 1.2593],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1244: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1244: tensor([[ 1.0737],\n",
      "        [ 0.1949],\n",
      "        [-1.8673],\n",
      "        [ 1.2615],\n",
      "        [ 0.6814],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1245: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1245: tensor([[ 1.0734],\n",
      "        [ 0.1939],\n",
      "        [-1.8690],\n",
      "        [ 1.2593],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1246: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1246: tensor([[ 1.0737],\n",
      "        [ 0.1949],\n",
      "        [-1.8673],\n",
      "        [ 1.2615],\n",
      "        [ 0.6814],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1247: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1247: tensor([[ 1.0734],\n",
      "        [ 0.1938],\n",
      "        [-1.8691],\n",
      "        [ 1.2593],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1248: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1248: tensor([[ 1.0737],\n",
      "        [ 0.1948],\n",
      "        [-1.8673],\n",
      "        [ 1.2616],\n",
      "        [ 0.6814],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1249: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1249: tensor([[ 1.0734],\n",
      "        [ 0.1938],\n",
      "        [-1.8691],\n",
      "        [ 1.2594],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1250: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1250: tensor([[ 1.0737],\n",
      "        [ 0.1948],\n",
      "        [-1.8674],\n",
      "        [ 1.2616],\n",
      "        [ 0.6814],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1251: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1251: tensor([[ 1.0734],\n",
      "        [ 0.1938],\n",
      "        [-1.8691],\n",
      "        [ 1.2594],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1252: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1252: tensor([[ 1.0737],\n",
      "        [ 0.1948],\n",
      "        [-1.8674],\n",
      "        [ 1.2616],\n",
      "        [ 0.6814],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1253: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1253: tensor([[ 1.0734],\n",
      "        [ 0.1937],\n",
      "        [-1.8692],\n",
      "        [ 1.2594],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1254: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1254: tensor([[ 1.0737],\n",
      "        [ 0.1947],\n",
      "        [-1.8674],\n",
      "        [ 1.2616],\n",
      "        [ 0.6814],\n",
      "        [-0.2844]], requires_grad=True)\n",
      "poly train loss at 1255: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1255: tensor([[ 1.0734],\n",
      "        [ 0.1937],\n",
      "        [-1.8692],\n",
      "        [ 1.2594],\n",
      "        [ 0.6790],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1256: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1256: tensor([[ 1.0737],\n",
      "        [ 0.1947],\n",
      "        [-1.8675],\n",
      "        [ 1.2617],\n",
      "        [ 0.6814],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1257: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1257: tensor([[ 1.0734],\n",
      "        [ 0.1937],\n",
      "        [-1.8692],\n",
      "        [ 1.2595],\n",
      "        [ 0.6791],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1258: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1258: tensor([[ 1.0737],\n",
      "        [ 0.1947],\n",
      "        [-1.8675],\n",
      "        [ 1.2617],\n",
      "        [ 0.6814],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1259: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1259: tensor([[ 1.0734],\n",
      "        [ 0.1936],\n",
      "        [-1.8693],\n",
      "        [ 1.2595],\n",
      "        [ 0.6791],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1260: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1260: tensor([[ 1.0737],\n",
      "        [ 0.1946],\n",
      "        [-1.8675],\n",
      "        [ 1.2617],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1261: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1261: tensor([[ 1.0734],\n",
      "        [ 0.1936],\n",
      "        [-1.8693],\n",
      "        [ 1.2595],\n",
      "        [ 0.6791],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1262: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1262: tensor([[ 1.0737],\n",
      "        [ 0.1946],\n",
      "        [-1.8676],\n",
      "        [ 1.2617],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1263: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1263: tensor([[ 1.0734],\n",
      "        [ 0.1936],\n",
      "        [-1.8693],\n",
      "        [ 1.2595],\n",
      "        [ 0.6791],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 1264: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1264: tensor([[ 1.0737],\n",
      "        [ 0.1946],\n",
      "        [-1.8676],\n",
      "        [ 1.2618],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1265: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1265: tensor([[ 1.0734],\n",
      "        [ 0.1935],\n",
      "        [-1.8694],\n",
      "        [ 1.2596],\n",
      "        [ 0.6791],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1266: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1266: tensor([[ 1.0737],\n",
      "        [ 0.1945],\n",
      "        [-1.8677],\n",
      "        [ 1.2618],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1267: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1267: tensor([[ 1.0734],\n",
      "        [ 0.1935],\n",
      "        [-1.8694],\n",
      "        [ 1.2596],\n",
      "        [ 0.6791],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1268: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1268: tensor([[ 1.0737],\n",
      "        [ 0.1945],\n",
      "        [-1.8677],\n",
      "        [ 1.2618],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1269: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1269: tensor([[ 1.0734],\n",
      "        [ 0.1935],\n",
      "        [-1.8694],\n",
      "        [ 1.2596],\n",
      "        [ 0.6791],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1270: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1270: tensor([[ 1.0737],\n",
      "        [ 0.1945],\n",
      "        [-1.8677],\n",
      "        [ 1.2618],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1271: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1271: tensor([[ 1.0734],\n",
      "        [ 0.1934],\n",
      "        [-1.8695],\n",
      "        [ 1.2596],\n",
      "        [ 0.6791],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1272: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1272: tensor([[ 1.0737],\n",
      "        [ 0.1944],\n",
      "        [-1.8678],\n",
      "        [ 1.2619],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1273: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1273: tensor([[ 1.0734],\n",
      "        [ 0.1934],\n",
      "        [-1.8695],\n",
      "        [ 1.2597],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1274: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1274: tensor([[ 1.0737],\n",
      "        [ 0.1944],\n",
      "        [-1.8678],\n",
      "        [ 1.2619],\n",
      "        [ 0.6815],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1275: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1275: tensor([[ 1.0734],\n",
      "        [ 0.1934],\n",
      "        [-1.8695],\n",
      "        [ 1.2597],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1276: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1276: tensor([[ 1.0737],\n",
      "        [ 0.1944],\n",
      "        [-1.8678],\n",
      "        [ 1.2619],\n",
      "        [ 0.6816],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1277: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1277: tensor([[ 1.0734],\n",
      "        [ 0.1933],\n",
      "        [-1.8696],\n",
      "        [ 1.2597],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1278: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1278: tensor([[ 1.0737],\n",
      "        [ 0.1943],\n",
      "        [-1.8679],\n",
      "        [ 1.2619],\n",
      "        [ 0.6816],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1279: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1279: tensor([[ 1.0734],\n",
      "        [ 0.1933],\n",
      "        [-1.8696],\n",
      "        [ 1.2597],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1280: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1280: tensor([[ 1.0737],\n",
      "        [ 0.1943],\n",
      "        [-1.8679],\n",
      "        [ 1.2620],\n",
      "        [ 0.6816],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1281: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1281: tensor([[ 1.0734],\n",
      "        [ 0.1933],\n",
      "        [-1.8696],\n",
      "        [ 1.2598],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1282: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1282: tensor([[ 1.0737],\n",
      "        [ 0.1943],\n",
      "        [-1.8679],\n",
      "        [ 1.2620],\n",
      "        [ 0.6816],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1283: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1283: tensor([[ 1.0734],\n",
      "        [ 0.1932],\n",
      "        [-1.8697],\n",
      "        [ 1.2598],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1284: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1284: tensor([[ 1.0737],\n",
      "        [ 0.1942],\n",
      "        [-1.8680],\n",
      "        [ 1.2620],\n",
      "        [ 0.6816],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1285: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1285: tensor([[ 1.0734],\n",
      "        [ 0.1932],\n",
      "        [-1.8697],\n",
      "        [ 1.2598],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1286: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1286: tensor([[ 1.0737],\n",
      "        [ 0.1942],\n",
      "        [-1.8680],\n",
      "        [ 1.2620],\n",
      "        [ 0.6816],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1287: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1287: tensor([[ 1.0734],\n",
      "        [ 0.1932],\n",
      "        [-1.8698],\n",
      "        [ 1.2598],\n",
      "        [ 0.6792],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1288: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1288: tensor([[ 1.0737],\n",
      "        [ 0.1942],\n",
      "        [-1.8680],\n",
      "        [ 1.2621],\n",
      "        [ 0.6816],\n",
      "        [-0.2845]], requires_grad=True)\n",
      "poly train loss at 1289: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1289: tensor([[ 1.0734],\n",
      "        [ 0.1931],\n",
      "        [-1.8698],\n",
      "        [ 1.2599],\n",
      "        [ 0.6793],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1290: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1290: tensor([[ 1.0737],\n",
      "        [ 0.1941],\n",
      "        [-1.8681],\n",
      "        [ 1.2621],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1291: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1291: tensor([[ 1.0734],\n",
      "        [ 0.1931],\n",
      "        [-1.8698],\n",
      "        [ 1.2599],\n",
      "        [ 0.6793],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1292: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1292: tensor([[ 1.0737],\n",
      "        [ 0.1941],\n",
      "        [-1.8681],\n",
      "        [ 1.2621],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1293: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1293: tensor([[ 1.0734],\n",
      "        [ 0.1931],\n",
      "        [-1.8699],\n",
      "        [ 1.2599],\n",
      "        [ 0.6793],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1294: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1294: tensor([[ 1.0737],\n",
      "        [ 0.1941],\n",
      "        [-1.8681],\n",
      "        [ 1.2621],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1295: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1295: tensor([[ 1.0734],\n",
      "        [ 0.1930],\n",
      "        [-1.8699],\n",
      "        [ 1.2599],\n",
      "        [ 0.6793],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1296: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1296: tensor([[ 1.0737],\n",
      "        [ 0.1941],\n",
      "        [-1.8682],\n",
      "        [ 1.2622],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1297: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1297: tensor([[ 1.0734],\n",
      "        [ 0.1930],\n",
      "        [-1.8699],\n",
      "        [ 1.2600],\n",
      "        [ 0.6793],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 1298: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1298: tensor([[ 1.0737],\n",
      "        [ 0.1940],\n",
      "        [-1.8682],\n",
      "        [ 1.2622],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1299: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1299: tensor([[ 1.0734],\n",
      "        [ 0.1930],\n",
      "        [-1.8700],\n",
      "        [ 1.2600],\n",
      "        [ 0.6793],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1300: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1300: tensor([[ 1.0737],\n",
      "        [ 0.1940],\n",
      "        [-1.8682],\n",
      "        [ 1.2622],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1301: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1301: tensor([[ 1.0734],\n",
      "        [ 0.1930],\n",
      "        [-1.8700],\n",
      "        [ 1.2600],\n",
      "        [ 0.6793],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1302: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1302: tensor([[ 1.0737],\n",
      "        [ 0.1940],\n",
      "        [-1.8683],\n",
      "        [ 1.2622],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1303: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1303: tensor([[ 1.0734],\n",
      "        [ 0.1929],\n",
      "        [-1.8700],\n",
      "        [ 1.2600],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1304: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1304: tensor([[ 1.0737],\n",
      "        [ 0.1939],\n",
      "        [-1.8683],\n",
      "        [ 1.2623],\n",
      "        [ 0.6817],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1305: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1305: tensor([[ 1.0734],\n",
      "        [ 0.1929],\n",
      "        [-1.8701],\n",
      "        [ 1.2601],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1306: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1306: tensor([[ 1.0737],\n",
      "        [ 0.1939],\n",
      "        [-1.8684],\n",
      "        [ 1.2623],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1307: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1307: tensor([[ 1.0734],\n",
      "        [ 0.1929],\n",
      "        [-1.8701],\n",
      "        [ 1.2601],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1308: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1308: tensor([[ 1.0737],\n",
      "        [ 0.1939],\n",
      "        [-1.8684],\n",
      "        [ 1.2623],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1309: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1309: tensor([[ 1.0734],\n",
      "        [ 0.1928],\n",
      "        [-1.8701],\n",
      "        [ 1.2601],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1310: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1310: tensor([[ 1.0737],\n",
      "        [ 0.1938],\n",
      "        [-1.8684],\n",
      "        [ 1.2623],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1311: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1311: tensor([[ 1.0734],\n",
      "        [ 0.1928],\n",
      "        [-1.8702],\n",
      "        [ 1.2602],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1312: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1312: tensor([[ 1.0737],\n",
      "        [ 0.1938],\n",
      "        [-1.8685],\n",
      "        [ 1.2624],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1313: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1313: tensor([[ 1.0734],\n",
      "        [ 0.1928],\n",
      "        [-1.8702],\n",
      "        [ 1.2602],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1314: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1314: tensor([[ 1.0737],\n",
      "        [ 0.1938],\n",
      "        [-1.8685],\n",
      "        [ 1.2624],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1315: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1315: tensor([[ 1.0734],\n",
      "        [ 0.1927],\n",
      "        [-1.8702],\n",
      "        [ 1.2602],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1316: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1316: tensor([[ 1.0737],\n",
      "        [ 0.1937],\n",
      "        [-1.8685],\n",
      "        [ 1.2624],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1317: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1317: tensor([[ 1.0734],\n",
      "        [ 0.1927],\n",
      "        [-1.8703],\n",
      "        [ 1.2602],\n",
      "        [ 0.6794],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1318: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1318: tensor([[ 1.0737],\n",
      "        [ 0.1937],\n",
      "        [-1.8686],\n",
      "        [ 1.2624],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1319: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1319: tensor([[ 1.0734],\n",
      "        [ 0.1927],\n",
      "        [-1.8703],\n",
      "        [ 1.2603],\n",
      "        [ 0.6795],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1320: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1320: tensor([[ 1.0737],\n",
      "        [ 0.1937],\n",
      "        [-1.8686],\n",
      "        [ 1.2625],\n",
      "        [ 0.6818],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1321: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1321: tensor([[ 1.0734],\n",
      "        [ 0.1926],\n",
      "        [-1.8703],\n",
      "        [ 1.2603],\n",
      "        [ 0.6795],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1322: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1322: tensor([[ 1.0737],\n",
      "        [ 0.1936],\n",
      "        [-1.8686],\n",
      "        [ 1.2625],\n",
      "        [ 0.6819],\n",
      "        [-0.2846]], requires_grad=True)\n",
      "poly train loss at 1323: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1323: tensor([[ 1.0734],\n",
      "        [ 0.1926],\n",
      "        [-1.8704],\n",
      "        [ 1.2603],\n",
      "        [ 0.6795],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1324: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1324: tensor([[ 1.0737],\n",
      "        [ 0.1936],\n",
      "        [-1.8687],\n",
      "        [ 1.2625],\n",
      "        [ 0.6819],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1325: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1325: tensor([[ 1.0734],\n",
      "        [ 0.1926],\n",
      "        [-1.8704],\n",
      "        [ 1.2603],\n",
      "        [ 0.6795],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1326: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1326: tensor([[ 1.0737],\n",
      "        [ 0.1936],\n",
      "        [-1.8687],\n",
      "        [ 1.2625],\n",
      "        [ 0.6819],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1327: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1327: tensor([[ 1.0734],\n",
      "        [ 0.1925],\n",
      "        [-1.8704],\n",
      "        [ 1.2604],\n",
      "        [ 0.6795],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1328: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1328: tensor([[ 1.0737],\n",
      "        [ 0.1935],\n",
      "        [-1.8687],\n",
      "        [ 1.2626],\n",
      "        [ 0.6819],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1329: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1329: tensor([[ 1.0734],\n",
      "        [ 0.1925],\n",
      "        [-1.8705],\n",
      "        [ 1.2604],\n",
      "        [ 0.6795],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1330: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1330: tensor([[ 1.0737],\n",
      "        [ 0.1935],\n",
      "        [-1.8688],\n",
      "        [ 1.2626],\n",
      "        [ 0.6819],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1331: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1331: tensor([[ 1.0734],\n",
      "        [ 0.1925],\n",
      "        [-1.8705],\n",
      "        [ 1.2604],\n",
      "        [ 0.6795],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 1332: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1332: tensor([[ 1.0737],\n",
      "        [ 0.1935],\n",
      "        [-1.8688],\n",
      "        [ 1.2626],\n",
      "        [ 0.6819],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1333: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1333: tensor([[ 1.0734],\n",
      "        [ 0.1924],\n",
      "        [-1.8705],\n",
      "        [ 1.2604],\n",
      "        [ 0.6795],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1334: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1334: tensor([[ 1.0737],\n",
      "        [ 0.1934],\n",
      "        [-1.8688],\n",
      "        [ 1.2626],\n",
      "        [ 0.6819],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1335: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1335: tensor([[ 1.0734],\n",
      "        [ 0.1924],\n",
      "        [-1.8706],\n",
      "        [ 1.2605],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1336: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1336: tensor([[ 1.0737],\n",
      "        [ 0.1934],\n",
      "        [-1.8689],\n",
      "        [ 1.2627],\n",
      "        [ 0.6819],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1337: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1337: tensor([[ 1.0734],\n",
      "        [ 0.1924],\n",
      "        [-1.8706],\n",
      "        [ 1.2605],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1338: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1338: tensor([[ 1.0737],\n",
      "        [ 0.1934],\n",
      "        [-1.8689],\n",
      "        [ 1.2627],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1339: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1339: tensor([[ 1.0734],\n",
      "        [ 0.1923],\n",
      "        [-1.8707],\n",
      "        [ 1.2605],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1340: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1340: tensor([[ 1.0737],\n",
      "        [ 0.1933],\n",
      "        [-1.8689],\n",
      "        [ 1.2627],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1341: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1341: tensor([[ 1.0734],\n",
      "        [ 0.1923],\n",
      "        [-1.8707],\n",
      "        [ 1.2605],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1342: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1342: tensor([[ 1.0737],\n",
      "        [ 0.1933],\n",
      "        [-1.8690],\n",
      "        [ 1.2627],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1343: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1343: tensor([[ 1.0734],\n",
      "        [ 0.1923],\n",
      "        [-1.8707],\n",
      "        [ 1.2606],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1344: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1344: tensor([[ 1.0737],\n",
      "        [ 0.1933],\n",
      "        [-1.8690],\n",
      "        [ 1.2628],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1345: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1345: tensor([[ 1.0734],\n",
      "        [ 0.1922],\n",
      "        [-1.8708],\n",
      "        [ 1.2606],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1346: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1346: tensor([[ 1.0737],\n",
      "        [ 0.1932],\n",
      "        [-1.8690],\n",
      "        [ 1.2628],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1347: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1347: tensor([[ 1.0734],\n",
      "        [ 0.1922],\n",
      "        [-1.8708],\n",
      "        [ 1.2606],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1348: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1348: tensor([[ 1.0737],\n",
      "        [ 0.1932],\n",
      "        [-1.8691],\n",
      "        [ 1.2628],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1349: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1349: tensor([[ 1.0734],\n",
      "        [ 0.1922],\n",
      "        [-1.8708],\n",
      "        [ 1.2606],\n",
      "        [ 0.6796],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1350: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1350: tensor([[ 1.0737],\n",
      "        [ 0.1932],\n",
      "        [-1.8691],\n",
      "        [ 1.2628],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1351: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1351: tensor([[ 1.0734],\n",
      "        [ 0.1922],\n",
      "        [-1.8709],\n",
      "        [ 1.2607],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1352: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1352: tensor([[ 1.0737],\n",
      "        [ 0.1932],\n",
      "        [-1.8691],\n",
      "        [ 1.2629],\n",
      "        [ 0.6820],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1353: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1353: tensor([[ 1.0734],\n",
      "        [ 0.1921],\n",
      "        [-1.8709],\n",
      "        [ 1.2607],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1354: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1354: tensor([[ 1.0737],\n",
      "        [ 0.1931],\n",
      "        [-1.8692],\n",
      "        [ 1.2629],\n",
      "        [ 0.6821],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1355: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1355: tensor([[ 1.0734],\n",
      "        [ 0.1921],\n",
      "        [-1.8709],\n",
      "        [ 1.2607],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1356: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1356: tensor([[ 1.0737],\n",
      "        [ 0.1931],\n",
      "        [-1.8692],\n",
      "        [ 1.2629],\n",
      "        [ 0.6821],\n",
      "        [-0.2847]], requires_grad=True)\n",
      "poly train loss at 1357: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1357: tensor([[ 1.0734],\n",
      "        [ 0.1921],\n",
      "        [-1.8710],\n",
      "        [ 1.2607],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1358: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1358: tensor([[ 1.0737],\n",
      "        [ 0.1931],\n",
      "        [-1.8693],\n",
      "        [ 1.2629],\n",
      "        [ 0.6821],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1359: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1359: tensor([[ 1.0734],\n",
      "        [ 0.1920],\n",
      "        [-1.8710],\n",
      "        [ 1.2608],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1360: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1360: tensor([[ 1.0737],\n",
      "        [ 0.1930],\n",
      "        [-1.8693],\n",
      "        [ 1.2630],\n",
      "        [ 0.6821],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1361: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1361: tensor([[ 1.0734],\n",
      "        [ 0.1920],\n",
      "        [-1.8710],\n",
      "        [ 1.2608],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1362: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1362: tensor([[ 1.0737],\n",
      "        [ 0.1930],\n",
      "        [-1.8693],\n",
      "        [ 1.2630],\n",
      "        [ 0.6821],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1363: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1363: tensor([[ 1.0734],\n",
      "        [ 0.1920],\n",
      "        [-1.8711],\n",
      "        [ 1.2608],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1364: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1364: tensor([[ 1.0737],\n",
      "        [ 0.1930],\n",
      "        [-1.8694],\n",
      "        [ 1.2630],\n",
      "        [ 0.6821],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1365: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1365: tensor([[ 1.0734],\n",
      "        [ 0.1919],\n",
      "        [-1.8711],\n",
      "        [ 1.2608],\n",
      "        [ 0.6797],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1366: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1366: tensor([[ 1.0737],\n",
      "        [ 0.1929],\n",
      "        [-1.8694],\n",
      "        [ 1.2630],\n",
      "        [ 0.6821],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1367: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1367: tensor([[ 1.0734],\n",
      "        [ 0.1919],\n",
      "        [-1.8711],\n",
      "        [ 1.2609],\n",
      "        [ 0.6798],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 1368: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1368: tensor([[ 1.0737],\n",
      "        [ 0.1929],\n",
      "        [-1.8694],\n",
      "        [ 1.2631],\n",
      "        [ 0.6821],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1369: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1369: tensor([[ 1.0734],\n",
      "        [ 0.1919],\n",
      "        [-1.8712],\n",
      "        [ 1.2609],\n",
      "        [ 0.6798],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1370: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1370: tensor([[ 1.0737],\n",
      "        [ 0.1929],\n",
      "        [-1.8695],\n",
      "        [ 1.2631],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1371: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1371: tensor([[ 1.0734],\n",
      "        [ 0.1918],\n",
      "        [-1.8712],\n",
      "        [ 1.2609],\n",
      "        [ 0.6798],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1372: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1372: tensor([[ 1.0737],\n",
      "        [ 0.1928],\n",
      "        [-1.8695],\n",
      "        [ 1.2631],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1373: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1373: tensor([[ 1.0734],\n",
      "        [ 0.1918],\n",
      "        [-1.8712],\n",
      "        [ 1.2609],\n",
      "        [ 0.6798],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1374: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1374: tensor([[ 1.0737],\n",
      "        [ 0.1928],\n",
      "        [-1.8695],\n",
      "        [ 1.2631],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1375: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1375: tensor([[ 1.0734],\n",
      "        [ 0.1918],\n",
      "        [-1.8713],\n",
      "        [ 1.2610],\n",
      "        [ 0.6798],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1376: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1376: tensor([[ 1.0737],\n",
      "        [ 0.1928],\n",
      "        [-1.8696],\n",
      "        [ 1.2632],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1377: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1377: tensor([[ 1.0734],\n",
      "        [ 0.1917],\n",
      "        [-1.8713],\n",
      "        [ 1.2610],\n",
      "        [ 0.6798],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1378: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1378: tensor([[ 1.0737],\n",
      "        [ 0.1927],\n",
      "        [-1.8696],\n",
      "        [ 1.2632],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1379: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1379: tensor([[ 1.0734],\n",
      "        [ 0.1917],\n",
      "        [-1.8713],\n",
      "        [ 1.2610],\n",
      "        [ 0.6798],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1380: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1380: tensor([[ 1.0737],\n",
      "        [ 0.1927],\n",
      "        [-1.8696],\n",
      "        [ 1.2632],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1381: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1381: tensor([[ 1.0734],\n",
      "        [ 0.1917],\n",
      "        [-1.8714],\n",
      "        [ 1.2610],\n",
      "        [ 0.6798],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1382: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1382: tensor([[ 1.0737],\n",
      "        [ 0.1927],\n",
      "        [-1.8697],\n",
      "        [ 1.2632],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1383: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1383: tensor([[ 1.0734],\n",
      "        [ 0.1916],\n",
      "        [-1.8714],\n",
      "        [ 1.2611],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1384: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1384: tensor([[ 1.0737],\n",
      "        [ 0.1926],\n",
      "        [-1.8697],\n",
      "        [ 1.2633],\n",
      "        [ 0.6822],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1385: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1385: tensor([[ 1.0734],\n",
      "        [ 0.1916],\n",
      "        [-1.8714],\n",
      "        [ 1.2611],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1386: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1386: tensor([[ 1.0737],\n",
      "        [ 0.1926],\n",
      "        [-1.8697],\n",
      "        [ 1.2633],\n",
      "        [ 0.6823],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1387: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1387: tensor([[ 1.0734],\n",
      "        [ 0.1916],\n",
      "        [-1.8715],\n",
      "        [ 1.2611],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1388: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1388: tensor([[ 1.0737],\n",
      "        [ 0.1926],\n",
      "        [-1.8698],\n",
      "        [ 1.2633],\n",
      "        [ 0.6823],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1389: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1389: tensor([[ 1.0734],\n",
      "        [ 0.1915],\n",
      "        [-1.8715],\n",
      "        [ 1.2611],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1390: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1390: tensor([[ 1.0737],\n",
      "        [ 0.1926],\n",
      "        [-1.8698],\n",
      "        [ 1.2633],\n",
      "        [ 0.6823],\n",
      "        [-0.2848]], requires_grad=True)\n",
      "poly train loss at 1391: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1391: tensor([[ 1.0734],\n",
      "        [ 0.1915],\n",
      "        [-1.8715],\n",
      "        [ 1.2612],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1392: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1392: tensor([[ 1.0737],\n",
      "        [ 0.1925],\n",
      "        [-1.8698],\n",
      "        [ 1.2634],\n",
      "        [ 0.6823],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1393: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1393: tensor([[ 1.0734],\n",
      "        [ 0.1915],\n",
      "        [-1.8716],\n",
      "        [ 1.2612],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1394: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1394: tensor([[ 1.0737],\n",
      "        [ 0.1925],\n",
      "        [-1.8699],\n",
      "        [ 1.2634],\n",
      "        [ 0.6823],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1395: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1395: tensor([[ 1.0734],\n",
      "        [ 0.1915],\n",
      "        [-1.8716],\n",
      "        [ 1.2612],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1396: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1396: tensor([[ 1.0737],\n",
      "        [ 0.1925],\n",
      "        [-1.8699],\n",
      "        [ 1.2634],\n",
      "        [ 0.6823],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1397: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1397: tensor([[ 1.0734],\n",
      "        [ 0.1914],\n",
      "        [-1.8716],\n",
      "        [ 1.2612],\n",
      "        [ 0.6799],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1398: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1398: tensor([[ 1.0737],\n",
      "        [ 0.1924],\n",
      "        [-1.8699],\n",
      "        [ 1.2634],\n",
      "        [ 0.6823],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1399: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1399: tensor([[ 1.0734],\n",
      "        [ 0.1914],\n",
      "        [-1.8717],\n",
      "        [ 1.2613],\n",
      "        [ 0.6800],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1400: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1400: tensor([[ 1.0737],\n",
      "        [ 0.1924],\n",
      "        [-1.8700],\n",
      "        [ 1.2635],\n",
      "        [ 0.6823],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1401: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1401: tensor([[ 1.0734],\n",
      "        [ 0.1914],\n",
      "        [-1.8717],\n",
      "        [ 1.2613],\n",
      "        [ 0.6800],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 1402: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1402: tensor([[ 1.0737],\n",
      "        [ 0.1924],\n",
      "        [-1.8700],\n",
      "        [ 1.2635],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1403: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1403: tensor([[ 1.0734],\n",
      "        [ 0.1913],\n",
      "        [-1.8717],\n",
      "        [ 1.2613],\n",
      "        [ 0.6800],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1404: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1404: tensor([[ 1.0737],\n",
      "        [ 0.1923],\n",
      "        [-1.8700],\n",
      "        [ 1.2635],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1405: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1405: tensor([[ 1.0734],\n",
      "        [ 0.1913],\n",
      "        [-1.8718],\n",
      "        [ 1.2613],\n",
      "        [ 0.6800],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1406: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1406: tensor([[ 1.0737],\n",
      "        [ 0.1923],\n",
      "        [-1.8701],\n",
      "        [ 1.2635],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1407: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1407: tensor([[ 1.0734],\n",
      "        [ 0.1913],\n",
      "        [-1.8718],\n",
      "        [ 1.2614],\n",
      "        [ 0.6800],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1408: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1408: tensor([[ 1.0737],\n",
      "        [ 0.1923],\n",
      "        [-1.8701],\n",
      "        [ 1.2636],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1409: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1409: tensor([[ 1.0734],\n",
      "        [ 0.1912],\n",
      "        [-1.8719],\n",
      "        [ 1.2614],\n",
      "        [ 0.6800],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1410: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1410: tensor([[ 1.0737],\n",
      "        [ 0.1922],\n",
      "        [-1.8701],\n",
      "        [ 1.2636],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1411: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1411: tensor([[ 1.0734],\n",
      "        [ 0.1912],\n",
      "        [-1.8719],\n",
      "        [ 1.2614],\n",
      "        [ 0.6800],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1412: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1412: tensor([[ 1.0737],\n",
      "        [ 0.1922],\n",
      "        [-1.8702],\n",
      "        [ 1.2636],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1413: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1413: tensor([[ 1.0734],\n",
      "        [ 0.1912],\n",
      "        [-1.8719],\n",
      "        [ 1.2614],\n",
      "        [ 0.6800],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1414: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1414: tensor([[ 1.0737],\n",
      "        [ 0.1922],\n",
      "        [-1.8702],\n",
      "        [ 1.2636],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1415: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1415: tensor([[ 1.0734],\n",
      "        [ 0.1911],\n",
      "        [-1.8720],\n",
      "        [ 1.2615],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1416: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1416: tensor([[ 1.0737],\n",
      "        [ 0.1921],\n",
      "        [-1.8702],\n",
      "        [ 1.2637],\n",
      "        [ 0.6824],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1417: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1417: tensor([[ 1.0734],\n",
      "        [ 0.1911],\n",
      "        [-1.8720],\n",
      "        [ 1.2615],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1418: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1418: tensor([[ 1.0737],\n",
      "        [ 0.1921],\n",
      "        [-1.8703],\n",
      "        [ 1.2637],\n",
      "        [ 0.6825],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1419: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1419: tensor([[ 1.0734],\n",
      "        [ 0.1911],\n",
      "        [-1.8720],\n",
      "        [ 1.2615],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1420: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1420: tensor([[ 1.0737],\n",
      "        [ 0.1921],\n",
      "        [-1.8703],\n",
      "        [ 1.2637],\n",
      "        [ 0.6825],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1421: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1421: tensor([[ 1.0734],\n",
      "        [ 0.1910],\n",
      "        [-1.8721],\n",
      "        [ 1.2615],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1422: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1422: tensor([[ 1.0737],\n",
      "        [ 0.1920],\n",
      "        [-1.8703],\n",
      "        [ 1.2637],\n",
      "        [ 0.6825],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1423: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1423: tensor([[ 1.0734],\n",
      "        [ 0.1910],\n",
      "        [-1.8721],\n",
      "        [ 1.2616],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1424: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1424: tensor([[ 1.0737],\n",
      "        [ 0.1920],\n",
      "        [-1.8704],\n",
      "        [ 1.2638],\n",
      "        [ 0.6825],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1425: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1425: tensor([[ 1.0734],\n",
      "        [ 0.1910],\n",
      "        [-1.8721],\n",
      "        [ 1.2616],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1426: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1426: tensor([[ 1.0737],\n",
      "        [ 0.1920],\n",
      "        [-1.8704],\n",
      "        [ 1.2638],\n",
      "        [ 0.6825],\n",
      "        [-0.2849]], requires_grad=True)\n",
      "poly train loss at 1427: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1427: tensor([[ 1.0734],\n",
      "        [ 0.1910],\n",
      "        [-1.8722],\n",
      "        [ 1.2616],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1428: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1428: tensor([[ 1.0737],\n",
      "        [ 0.1920],\n",
      "        [-1.8704],\n",
      "        [ 1.2638],\n",
      "        [ 0.6825],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1429: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1429: tensor([[ 1.0734],\n",
      "        [ 0.1909],\n",
      "        [-1.8722],\n",
      "        [ 1.2616],\n",
      "        [ 0.6801],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1430: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1430: tensor([[ 1.0737],\n",
      "        [ 0.1919],\n",
      "        [-1.8705],\n",
      "        [ 1.2638],\n",
      "        [ 0.6825],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1431: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1431: tensor([[ 1.0734],\n",
      "        [ 0.1909],\n",
      "        [-1.8722],\n",
      "        [ 1.2617],\n",
      "        [ 0.6802],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1432: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1432: tensor([[ 1.0737],\n",
      "        [ 0.1919],\n",
      "        [-1.8705],\n",
      "        [ 1.2639],\n",
      "        [ 0.6825],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1433: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1433: tensor([[ 1.0734],\n",
      "        [ 0.1909],\n",
      "        [-1.8723],\n",
      "        [ 1.2617],\n",
      "        [ 0.6802],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1434: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1434: tensor([[ 1.0737],\n",
      "        [ 0.1919],\n",
      "        [-1.8705],\n",
      "        [ 1.2639],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1435: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1435: tensor([[ 1.0734],\n",
      "        [ 0.1908],\n",
      "        [-1.8723],\n",
      "        [ 1.2617],\n",
      "        [ 0.6802],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 1436: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1436: tensor([[ 1.0737],\n",
      "        [ 0.1918],\n",
      "        [-1.8706],\n",
      "        [ 1.2639],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1437: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1437: tensor([[ 1.0734],\n",
      "        [ 0.1908],\n",
      "        [-1.8723],\n",
      "        [ 1.2617],\n",
      "        [ 0.6802],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1438: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1438: tensor([[ 1.0737],\n",
      "        [ 0.1918],\n",
      "        [-1.8706],\n",
      "        [ 1.2639],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1439: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1439: tensor([[ 1.0734],\n",
      "        [ 0.1908],\n",
      "        [-1.8724],\n",
      "        [ 1.2618],\n",
      "        [ 0.6802],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1440: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1440: tensor([[ 1.0737],\n",
      "        [ 0.1918],\n",
      "        [-1.8706],\n",
      "        [ 1.2640],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1441: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1441: tensor([[ 1.0734],\n",
      "        [ 0.1907],\n",
      "        [-1.8724],\n",
      "        [ 1.2618],\n",
      "        [ 0.6802],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1442: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1442: tensor([[ 1.0737],\n",
      "        [ 0.1917],\n",
      "        [-1.8707],\n",
      "        [ 1.2640],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1443: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1443: tensor([[ 1.0734],\n",
      "        [ 0.1907],\n",
      "        [-1.8724],\n",
      "        [ 1.2618],\n",
      "        [ 0.6802],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1444: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1444: tensor([[ 1.0737],\n",
      "        [ 0.1917],\n",
      "        [-1.8707],\n",
      "        [ 1.2640],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1445: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1445: tensor([[ 1.0734],\n",
      "        [ 0.1907],\n",
      "        [-1.8725],\n",
      "        [ 1.2618],\n",
      "        [ 0.6802],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1446: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1446: tensor([[ 1.0737],\n",
      "        [ 0.1917],\n",
      "        [-1.8707],\n",
      "        [ 1.2640],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1447: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1447: tensor([[ 1.0734],\n",
      "        [ 0.1906],\n",
      "        [-1.8725],\n",
      "        [ 1.2618],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1448: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1448: tensor([[ 1.0737],\n",
      "        [ 0.1916],\n",
      "        [-1.8708],\n",
      "        [ 1.2641],\n",
      "        [ 0.6826],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1449: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1449: tensor([[ 1.0734],\n",
      "        [ 0.1906],\n",
      "        [-1.8725],\n",
      "        [ 1.2619],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1450: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1450: tensor([[ 1.0737],\n",
      "        [ 0.1916],\n",
      "        [-1.8708],\n",
      "        [ 1.2641],\n",
      "        [ 0.6827],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1451: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1451: tensor([[ 1.0734],\n",
      "        [ 0.1906],\n",
      "        [-1.8726],\n",
      "        [ 1.2619],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1452: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1452: tensor([[ 1.0737],\n",
      "        [ 0.1916],\n",
      "        [-1.8708],\n",
      "        [ 1.2641],\n",
      "        [ 0.6827],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1453: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1453: tensor([[ 1.0734],\n",
      "        [ 0.1905],\n",
      "        [-1.8726],\n",
      "        [ 1.2619],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1454: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1454: tensor([[ 1.0737],\n",
      "        [ 0.1915],\n",
      "        [-1.8709],\n",
      "        [ 1.2641],\n",
      "        [ 0.6827],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1455: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1455: tensor([[ 1.0734],\n",
      "        [ 0.1905],\n",
      "        [-1.8726],\n",
      "        [ 1.2619],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1456: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1456: tensor([[ 1.0737],\n",
      "        [ 0.1915],\n",
      "        [-1.8709],\n",
      "        [ 1.2642],\n",
      "        [ 0.6827],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1457: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1457: tensor([[ 1.0734],\n",
      "        [ 0.1905],\n",
      "        [-1.8727],\n",
      "        [ 1.2620],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1458: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1458: tensor([[ 1.0737],\n",
      "        [ 0.1915],\n",
      "        [-1.8709],\n",
      "        [ 1.2642],\n",
      "        [ 0.6827],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1459: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1459: tensor([[ 1.0734],\n",
      "        [ 0.1905],\n",
      "        [-1.8727],\n",
      "        [ 1.2620],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1460: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1460: tensor([[ 1.0737],\n",
      "        [ 0.1915],\n",
      "        [-1.8710],\n",
      "        [ 1.2642],\n",
      "        [ 0.6827],\n",
      "        [-0.2850]], requires_grad=True)\n",
      "poly train loss at 1461: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1461: tensor([[ 1.0734],\n",
      "        [ 0.1904],\n",
      "        [-1.8727],\n",
      "        [ 1.2620],\n",
      "        [ 0.6803],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1462: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1462: tensor([[ 1.0737],\n",
      "        [ 0.1914],\n",
      "        [-1.8710],\n",
      "        [ 1.2642],\n",
      "        [ 0.6827],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1463: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1463: tensor([[ 1.0734],\n",
      "        [ 0.1904],\n",
      "        [-1.8728],\n",
      "        [ 1.2620],\n",
      "        [ 0.6804],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1464: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1464: tensor([[ 1.0737],\n",
      "        [ 0.1914],\n",
      "        [-1.8710],\n",
      "        [ 1.2643],\n",
      "        [ 0.6827],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1465: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1465: tensor([[ 1.0734],\n",
      "        [ 0.1904],\n",
      "        [-1.8728],\n",
      "        [ 1.2621],\n",
      "        [ 0.6804],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1466: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1466: tensor([[ 1.0737],\n",
      "        [ 0.1914],\n",
      "        [-1.8711],\n",
      "        [ 1.2643],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1467: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1467: tensor([[ 1.0734],\n",
      "        [ 0.1903],\n",
      "        [-1.8728],\n",
      "        [ 1.2621],\n",
      "        [ 0.6804],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1468: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1468: tensor([[ 1.0737],\n",
      "        [ 0.1913],\n",
      "        [-1.8711],\n",
      "        [ 1.2643],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1469: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1469: tensor([[ 1.0734],\n",
      "        [ 0.1903],\n",
      "        [-1.8729],\n",
      "        [ 1.2621],\n",
      "        [ 0.6804],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1470: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1470: tensor([[ 1.0737],\n",
      "        [ 0.1913],\n",
      "        [-1.8711],\n",
      "        [ 1.2643],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1471: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1471: tensor([[ 1.0734],\n",
      "        [ 0.1903],\n",
      "        [-1.8729],\n",
      "        [ 1.2621],\n",
      "        [ 0.6804],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 1472: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1472: tensor([[ 1.0737],\n",
      "        [ 0.1913],\n",
      "        [-1.8712],\n",
      "        [ 1.2644],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1473: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1473: tensor([[ 1.0734],\n",
      "        [ 0.1902],\n",
      "        [-1.8729],\n",
      "        [ 1.2622],\n",
      "        [ 0.6804],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1474: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1474: tensor([[ 1.0737],\n",
      "        [ 0.1912],\n",
      "        [-1.8712],\n",
      "        [ 1.2644],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1475: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1475: tensor([[ 1.0734],\n",
      "        [ 0.1902],\n",
      "        [-1.8730],\n",
      "        [ 1.2622],\n",
      "        [ 0.6804],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1476: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1476: tensor([[ 1.0737],\n",
      "        [ 0.1912],\n",
      "        [-1.8712],\n",
      "        [ 1.2644],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1477: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1477: tensor([[ 1.0734],\n",
      "        [ 0.1902],\n",
      "        [-1.8730],\n",
      "        [ 1.2622],\n",
      "        [ 0.6804],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1478: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1478: tensor([[ 1.0737],\n",
      "        [ 0.1912],\n",
      "        [-1.8713],\n",
      "        [ 1.2644],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1479: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1479: tensor([[ 1.0734],\n",
      "        [ 0.1901],\n",
      "        [-1.8730],\n",
      "        [ 1.2622],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1480: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1480: tensor([[ 1.0737],\n",
      "        [ 0.1911],\n",
      "        [-1.8713],\n",
      "        [ 1.2645],\n",
      "        [ 0.6828],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1481: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1481: tensor([[ 1.0734],\n",
      "        [ 0.1901],\n",
      "        [-1.8731],\n",
      "        [ 1.2623],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1482: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1482: tensor([[ 1.0738],\n",
      "        [ 0.1911],\n",
      "        [-1.8713],\n",
      "        [ 1.2645],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1483: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1483: tensor([[ 1.0734],\n",
      "        [ 0.1901],\n",
      "        [-1.8731],\n",
      "        [ 1.2623],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1484: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1484: tensor([[ 1.0738],\n",
      "        [ 0.1911],\n",
      "        [-1.8714],\n",
      "        [ 1.2645],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1485: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1485: tensor([[ 1.0734],\n",
      "        [ 0.1900],\n",
      "        [-1.8731],\n",
      "        [ 1.2623],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1486: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1486: tensor([[ 1.0738],\n",
      "        [ 0.1911],\n",
      "        [-1.8714],\n",
      "        [ 1.2645],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1487: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1487: tensor([[ 1.0734],\n",
      "        [ 0.1900],\n",
      "        [-1.8732],\n",
      "        [ 1.2623],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1488: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1488: tensor([[ 1.0738],\n",
      "        [ 0.1910],\n",
      "        [-1.8714],\n",
      "        [ 1.2646],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1489: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1489: tensor([[ 1.0734],\n",
      "        [ 0.1900],\n",
      "        [-1.8732],\n",
      "        [ 1.2624],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1490: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1490: tensor([[ 1.0738],\n",
      "        [ 0.1910],\n",
      "        [-1.8715],\n",
      "        [ 1.2646],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1491: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1491: tensor([[ 1.0734],\n",
      "        [ 0.1900],\n",
      "        [-1.8732],\n",
      "        [ 1.2624],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1492: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1492: tensor([[ 1.0738],\n",
      "        [ 0.1910],\n",
      "        [-1.8715],\n",
      "        [ 1.2646],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1493: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1493: tensor([[ 1.0734],\n",
      "        [ 0.1899],\n",
      "        [-1.8733],\n",
      "        [ 1.2624],\n",
      "        [ 0.6805],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1494: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1494: tensor([[ 1.0738],\n",
      "        [ 0.1909],\n",
      "        [-1.8715],\n",
      "        [ 1.2646],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1495: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1495: tensor([[ 1.0734],\n",
      "        [ 0.1899],\n",
      "        [-1.8733],\n",
      "        [ 1.2624],\n",
      "        [ 0.6806],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1496: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1496: tensor([[ 1.0738],\n",
      "        [ 0.1909],\n",
      "        [-1.8716],\n",
      "        [ 1.2647],\n",
      "        [ 0.6829],\n",
      "        [-0.2851]], requires_grad=True)\n",
      "poly train loss at 1497: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1497: tensor([[ 1.0734],\n",
      "        [ 0.1899],\n",
      "        [-1.8733],\n",
      "        [ 1.2625],\n",
      "        [ 0.6806],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1498: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1498: tensor([[ 1.0738],\n",
      "        [ 0.1909],\n",
      "        [-1.8716],\n",
      "        [ 1.2647],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1499: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1499: tensor([[ 1.0734],\n",
      "        [ 0.1898],\n",
      "        [-1.8734],\n",
      "        [ 1.2625],\n",
      "        [ 0.6806],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1500: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1500: tensor([[ 1.0738],\n",
      "        [ 0.1908],\n",
      "        [-1.8716],\n",
      "        [ 1.2647],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1501: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1501: tensor([[ 1.0734],\n",
      "        [ 0.1898],\n",
      "        [-1.8734],\n",
      "        [ 1.2625],\n",
      "        [ 0.6806],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1502: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1502: tensor([[ 1.0738],\n",
      "        [ 0.1908],\n",
      "        [-1.8717],\n",
      "        [ 1.2647],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1503: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1503: tensor([[ 1.0734],\n",
      "        [ 0.1898],\n",
      "        [-1.8734],\n",
      "        [ 1.2625],\n",
      "        [ 0.6806],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1504: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1504: tensor([[ 1.0738],\n",
      "        [ 0.1908],\n",
      "        [-1.8717],\n",
      "        [ 1.2647],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1505: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1505: tensor([[ 1.0734],\n",
      "        [ 0.1897],\n",
      "        [-1.8735],\n",
      "        [ 1.2626],\n",
      "        [ 0.6806],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1506: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1506: tensor([[ 1.0738],\n",
      "        [ 0.1907],\n",
      "        [-1.8717],\n",
      "        [ 1.2648],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1507: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1507: tensor([[ 1.0734],\n",
      "        [ 0.1897],\n",
      "        [-1.8735],\n",
      "        [ 1.2626],\n",
      "        [ 0.6806],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 1508: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1508: tensor([[ 1.0738],\n",
      "        [ 0.1907],\n",
      "        [-1.8718],\n",
      "        [ 1.2648],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1509: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1509: tensor([[ 1.0734],\n",
      "        [ 0.1897],\n",
      "        [-1.8735],\n",
      "        [ 1.2626],\n",
      "        [ 0.6806],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1510: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1510: tensor([[ 1.0738],\n",
      "        [ 0.1907],\n",
      "        [-1.8718],\n",
      "        [ 1.2648],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1511: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1511: tensor([[ 1.0734],\n",
      "        [ 0.1896],\n",
      "        [-1.8736],\n",
      "        [ 1.2626],\n",
      "        [ 0.6806],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1512: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1512: tensor([[ 1.0738],\n",
      "        [ 0.1906],\n",
      "        [-1.8718],\n",
      "        [ 1.2648],\n",
      "        [ 0.6830],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1513: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1513: tensor([[ 1.0734],\n",
      "        [ 0.1896],\n",
      "        [-1.8736],\n",
      "        [ 1.2627],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1514: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1514: tensor([[ 1.0738],\n",
      "        [ 0.1906],\n",
      "        [-1.8719],\n",
      "        [ 1.2649],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1515: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1515: tensor([[ 1.0734],\n",
      "        [ 0.1896],\n",
      "        [-1.8736],\n",
      "        [ 1.2627],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1516: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1516: tensor([[ 1.0738],\n",
      "        [ 0.1906],\n",
      "        [-1.8719],\n",
      "        [ 1.2649],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1517: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1517: tensor([[ 1.0734],\n",
      "        [ 0.1896],\n",
      "        [-1.8737],\n",
      "        [ 1.2627],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1518: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1518: tensor([[ 1.0738],\n",
      "        [ 0.1906],\n",
      "        [-1.8719],\n",
      "        [ 1.2649],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1519: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1519: tensor([[ 1.0734],\n",
      "        [ 0.1895],\n",
      "        [-1.8737],\n",
      "        [ 1.2627],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1520: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1520: tensor([[ 1.0738],\n",
      "        [ 0.1905],\n",
      "        [-1.8720],\n",
      "        [ 1.2649],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1521: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1521: tensor([[ 1.0734],\n",
      "        [ 0.1895],\n",
      "        [-1.8737],\n",
      "        [ 1.2628],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1522: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1522: tensor([[ 1.0738],\n",
      "        [ 0.1905],\n",
      "        [-1.8720],\n",
      "        [ 1.2650],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1523: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1523: tensor([[ 1.0734],\n",
      "        [ 0.1895],\n",
      "        [-1.8738],\n",
      "        [ 1.2628],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1524: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1524: tensor([[ 1.0738],\n",
      "        [ 0.1905],\n",
      "        [-1.8720],\n",
      "        [ 1.2650],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1525: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1525: tensor([[ 1.0734],\n",
      "        [ 0.1894],\n",
      "        [-1.8738],\n",
      "        [ 1.2628],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1526: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1526: tensor([[ 1.0738],\n",
      "        [ 0.1904],\n",
      "        [-1.8721],\n",
      "        [ 1.2650],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1527: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1527: tensor([[ 1.0734],\n",
      "        [ 0.1894],\n",
      "        [-1.8738],\n",
      "        [ 1.2628],\n",
      "        [ 0.6807],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1528: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1528: tensor([[ 1.0738],\n",
      "        [ 0.1904],\n",
      "        [-1.8721],\n",
      "        [ 1.2650],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1529: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1529: tensor([[ 1.0734],\n",
      "        [ 0.1894],\n",
      "        [-1.8739],\n",
      "        [ 1.2628],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1530: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1530: tensor([[ 1.0738],\n",
      "        [ 0.1904],\n",
      "        [-1.8721],\n",
      "        [ 1.2651],\n",
      "        [ 0.6831],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1531: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1531: tensor([[ 1.0734],\n",
      "        [ 0.1893],\n",
      "        [-1.8739],\n",
      "        [ 1.2629],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1532: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1532: tensor([[ 1.0738],\n",
      "        [ 0.1903],\n",
      "        [-1.8722],\n",
      "        [ 1.2651],\n",
      "        [ 0.6832],\n",
      "        [-0.2852]], requires_grad=True)\n",
      "poly train loss at 1533: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1533: tensor([[ 1.0734],\n",
      "        [ 0.1893],\n",
      "        [-1.8739],\n",
      "        [ 1.2629],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1534: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1534: tensor([[ 1.0738],\n",
      "        [ 0.1903],\n",
      "        [-1.8722],\n",
      "        [ 1.2651],\n",
      "        [ 0.6832],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1535: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1535: tensor([[ 1.0734],\n",
      "        [ 0.1893],\n",
      "        [-1.8740],\n",
      "        [ 1.2629],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1536: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1536: tensor([[ 1.0738],\n",
      "        [ 0.1903],\n",
      "        [-1.8722],\n",
      "        [ 1.2651],\n",
      "        [ 0.6832],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1537: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1537: tensor([[ 1.0734],\n",
      "        [ 0.1892],\n",
      "        [-1.8740],\n",
      "        [ 1.2629],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1538: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1538: tensor([[ 1.0738],\n",
      "        [ 0.1902],\n",
      "        [-1.8723],\n",
      "        [ 1.2652],\n",
      "        [ 0.6832],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1539: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1539: tensor([[ 1.0734],\n",
      "        [ 0.1892],\n",
      "        [-1.8740],\n",
      "        [ 1.2630],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1540: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1540: tensor([[ 1.0738],\n",
      "        [ 0.1902],\n",
      "        [-1.8723],\n",
      "        [ 1.2652],\n",
      "        [ 0.6832],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1541: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1541: tensor([[ 1.0734],\n",
      "        [ 0.1892],\n",
      "        [-1.8741],\n",
      "        [ 1.2630],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1542: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1542: tensor([[ 1.0738],\n",
      "        [ 0.1902],\n",
      "        [-1.8723],\n",
      "        [ 1.2652],\n",
      "        [ 0.6832],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1543: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1543: tensor([[ 1.0734],\n",
      "        [ 0.1892],\n",
      "        [-1.8741],\n",
      "        [ 1.2630],\n",
      "        [ 0.6808],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 1544: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1544: tensor([[ 1.0738],\n",
      "        [ 0.1902],\n",
      "        [-1.8724],\n",
      "        [ 1.2652],\n",
      "        [ 0.6832],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1545: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1545: tensor([[ 1.0734],\n",
      "        [ 0.1891],\n",
      "        [-1.8741],\n",
      "        [ 1.2630],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1546: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1546: tensor([[ 1.0738],\n",
      "        [ 0.1901],\n",
      "        [-1.8724],\n",
      "        [ 1.2653],\n",
      "        [ 0.6832],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1547: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1547: tensor([[ 1.0734],\n",
      "        [ 0.1891],\n",
      "        [-1.8742],\n",
      "        [ 1.2631],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1548: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1548: tensor([[ 1.0738],\n",
      "        [ 0.1901],\n",
      "        [-1.8724],\n",
      "        [ 1.2653],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1549: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1549: tensor([[ 1.0734],\n",
      "        [ 0.1891],\n",
      "        [-1.8742],\n",
      "        [ 1.2631],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1550: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1550: tensor([[ 1.0738],\n",
      "        [ 0.1901],\n",
      "        [-1.8725],\n",
      "        [ 1.2653],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1551: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1551: tensor([[ 1.0734],\n",
      "        [ 0.1890],\n",
      "        [-1.8742],\n",
      "        [ 1.2631],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1552: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1552: tensor([[ 1.0738],\n",
      "        [ 0.1900],\n",
      "        [-1.8725],\n",
      "        [ 1.2653],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1553: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1553: tensor([[ 1.0734],\n",
      "        [ 0.1890],\n",
      "        [-1.8743],\n",
      "        [ 1.2631],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1554: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1554: tensor([[ 1.0738],\n",
      "        [ 0.1900],\n",
      "        [-1.8725],\n",
      "        [ 1.2654],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1555: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1555: tensor([[ 1.0734],\n",
      "        [ 0.1890],\n",
      "        [-1.8743],\n",
      "        [ 1.2632],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1556: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1556: tensor([[ 1.0738],\n",
      "        [ 0.1900],\n",
      "        [-1.8726],\n",
      "        [ 1.2654],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1557: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1557: tensor([[ 1.0734],\n",
      "        [ 0.1889],\n",
      "        [-1.8743],\n",
      "        [ 1.2632],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1558: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1558: tensor([[ 1.0738],\n",
      "        [ 0.1899],\n",
      "        [-1.8726],\n",
      "        [ 1.2654],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1559: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1559: tensor([[ 1.0734],\n",
      "        [ 0.1889],\n",
      "        [-1.8744],\n",
      "        [ 1.2632],\n",
      "        [ 0.6809],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1560: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1560: tensor([[ 1.0738],\n",
      "        [ 0.1899],\n",
      "        [-1.8726],\n",
      "        [ 1.2654],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1561: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1561: tensor([[ 1.0734],\n",
      "        [ 0.1889],\n",
      "        [-1.8744],\n",
      "        [ 1.2632],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1562: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1562: tensor([[ 1.0738],\n",
      "        [ 0.1899],\n",
      "        [-1.8727],\n",
      "        [ 1.2654],\n",
      "        [ 0.6833],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1563: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1563: tensor([[ 1.0734],\n",
      "        [ 0.1888],\n",
      "        [-1.8744],\n",
      "        [ 1.2633],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1564: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1564: tensor([[ 1.0738],\n",
      "        [ 0.1899],\n",
      "        [-1.8727],\n",
      "        [ 1.2655],\n",
      "        [ 0.6834],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1565: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1565: tensor([[ 1.0734],\n",
      "        [ 0.1888],\n",
      "        [-1.8745],\n",
      "        [ 1.2633],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1566: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1566: tensor([[ 1.0738],\n",
      "        [ 0.1898],\n",
      "        [-1.8727],\n",
      "        [ 1.2655],\n",
      "        [ 0.6834],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1567: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1567: tensor([[ 1.0734],\n",
      "        [ 0.1888],\n",
      "        [-1.8745],\n",
      "        [ 1.2633],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1568: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1568: tensor([[ 1.0738],\n",
      "        [ 0.1898],\n",
      "        [-1.8728],\n",
      "        [ 1.2655],\n",
      "        [ 0.6834],\n",
      "        [-0.2853]], requires_grad=True)\n",
      "poly train loss at 1569: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1569: tensor([[ 1.0734],\n",
      "        [ 0.1888],\n",
      "        [-1.8745],\n",
      "        [ 1.2633],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1570: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1570: tensor([[ 1.0738],\n",
      "        [ 0.1898],\n",
      "        [-1.8728],\n",
      "        [ 1.2655],\n",
      "        [ 0.6834],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1571: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1571: tensor([[ 1.0734],\n",
      "        [ 0.1887],\n",
      "        [-1.8746],\n",
      "        [ 1.2634],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1572: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1572: tensor([[ 1.0738],\n",
      "        [ 0.1897],\n",
      "        [-1.8728],\n",
      "        [ 1.2656],\n",
      "        [ 0.6834],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1573: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1573: tensor([[ 1.0734],\n",
      "        [ 0.1887],\n",
      "        [-1.8746],\n",
      "        [ 1.2634],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1574: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1574: tensor([[ 1.0738],\n",
      "        [ 0.1897],\n",
      "        [-1.8729],\n",
      "        [ 1.2656],\n",
      "        [ 0.6834],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1575: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1575: tensor([[ 1.0734],\n",
      "        [ 0.1887],\n",
      "        [-1.8746],\n",
      "        [ 1.2634],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1576: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1576: tensor([[ 1.0738],\n",
      "        [ 0.1897],\n",
      "        [-1.8729],\n",
      "        [ 1.2656],\n",
      "        [ 0.6834],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1577: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1577: tensor([[ 1.0734],\n",
      "        [ 0.1886],\n",
      "        [-1.8746],\n",
      "        [ 1.2634],\n",
      "        [ 0.6810],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1578: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1578: tensor([[ 1.0738],\n",
      "        [ 0.1896],\n",
      "        [-1.8729],\n",
      "        [ 1.2656],\n",
      "        [ 0.6834],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1579: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1579: tensor([[ 1.0734],\n",
      "        [ 0.1886],\n",
      "        [-1.8747],\n",
      "        [ 1.2635],\n",
      "        [ 0.6811],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 1580: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1580: tensor([[ 1.0738],\n",
      "        [ 0.1896],\n",
      "        [-1.8730],\n",
      "        [ 1.2657],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1581: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1581: tensor([[ 1.0734],\n",
      "        [ 0.1886],\n",
      "        [-1.8747],\n",
      "        [ 1.2635],\n",
      "        [ 0.6811],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1582: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1582: tensor([[ 1.0738],\n",
      "        [ 0.1896],\n",
      "        [-1.8730],\n",
      "        [ 1.2657],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1583: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1583: tensor([[ 1.0734],\n",
      "        [ 0.1885],\n",
      "        [-1.8747],\n",
      "        [ 1.2635],\n",
      "        [ 0.6811],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1584: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1584: tensor([[ 1.0738],\n",
      "        [ 0.1895],\n",
      "        [-1.8730],\n",
      "        [ 1.2657],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1585: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1585: tensor([[ 1.0734],\n",
      "        [ 0.1885],\n",
      "        [-1.8748],\n",
      "        [ 1.2635],\n",
      "        [ 0.6811],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1586: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1586: tensor([[ 1.0738],\n",
      "        [ 0.1895],\n",
      "        [-1.8731],\n",
      "        [ 1.2657],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1587: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1587: tensor([[ 1.0734],\n",
      "        [ 0.1885],\n",
      "        [-1.8748],\n",
      "        [ 1.2635],\n",
      "        [ 0.6811],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1588: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1588: tensor([[ 1.0738],\n",
      "        [ 0.1895],\n",
      "        [-1.8731],\n",
      "        [ 1.2658],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1589: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1589: tensor([[ 1.0734],\n",
      "        [ 0.1885],\n",
      "        [-1.8748],\n",
      "        [ 1.2636],\n",
      "        [ 0.6811],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1590: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1590: tensor([[ 1.0738],\n",
      "        [ 0.1895],\n",
      "        [-1.8731],\n",
      "        [ 1.2658],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1591: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1591: tensor([[ 1.0734],\n",
      "        [ 0.1884],\n",
      "        [-1.8749],\n",
      "        [ 1.2636],\n",
      "        [ 0.6811],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1592: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1592: tensor([[ 1.0738],\n",
      "        [ 0.1894],\n",
      "        [-1.8732],\n",
      "        [ 1.2658],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1593: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1593: tensor([[ 1.0734],\n",
      "        [ 0.1884],\n",
      "        [-1.8749],\n",
      "        [ 1.2636],\n",
      "        [ 0.6811],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1594: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1594: tensor([[ 1.0738],\n",
      "        [ 0.1894],\n",
      "        [-1.8732],\n",
      "        [ 1.2658],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1595: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1595: tensor([[ 1.0734],\n",
      "        [ 0.1884],\n",
      "        [-1.8749],\n",
      "        [ 1.2636],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1596: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1596: tensor([[ 1.0738],\n",
      "        [ 0.1894],\n",
      "        [-1.8732],\n",
      "        [ 1.2659],\n",
      "        [ 0.6835],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1597: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1597: tensor([[ 1.0734],\n",
      "        [ 0.1883],\n",
      "        [-1.8750],\n",
      "        [ 1.2637],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1598: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1598: tensor([[ 1.0738],\n",
      "        [ 0.1893],\n",
      "        [-1.8733],\n",
      "        [ 1.2659],\n",
      "        [ 0.6836],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1599: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1599: tensor([[ 1.0734],\n",
      "        [ 0.1883],\n",
      "        [-1.8750],\n",
      "        [ 1.2637],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1600: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1600: tensor([[ 1.0738],\n",
      "        [ 0.1893],\n",
      "        [-1.8733],\n",
      "        [ 1.2659],\n",
      "        [ 0.6836],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1601: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1601: tensor([[ 1.0734],\n",
      "        [ 0.1883],\n",
      "        [-1.8750],\n",
      "        [ 1.2637],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1602: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1602: tensor([[ 1.0738],\n",
      "        [ 0.1893],\n",
      "        [-1.8733],\n",
      "        [ 1.2659],\n",
      "        [ 0.6836],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1603: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1603: tensor([[ 1.0734],\n",
      "        [ 0.1882],\n",
      "        [-1.8751],\n",
      "        [ 1.2637],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1604: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1604: tensor([[ 1.0738],\n",
      "        [ 0.1892],\n",
      "        [-1.8734],\n",
      "        [ 1.2660],\n",
      "        [ 0.6836],\n",
      "        [-0.2854]], requires_grad=True)\n",
      "poly train loss at 1605: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1605: tensor([[ 1.0734],\n",
      "        [ 0.1882],\n",
      "        [-1.8751],\n",
      "        [ 1.2638],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1606: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1606: tensor([[ 1.0738],\n",
      "        [ 0.1892],\n",
      "        [-1.8734],\n",
      "        [ 1.2660],\n",
      "        [ 0.6836],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1607: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1607: tensor([[ 1.0734],\n",
      "        [ 0.1882],\n",
      "        [-1.8751],\n",
      "        [ 1.2638],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1608: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1608: tensor([[ 1.0738],\n",
      "        [ 0.1892],\n",
      "        [-1.8734],\n",
      "        [ 1.2660],\n",
      "        [ 0.6836],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1609: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1609: tensor([[ 1.0734],\n",
      "        [ 0.1881],\n",
      "        [-1.8752],\n",
      "        [ 1.2638],\n",
      "        [ 0.6812],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1610: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1610: tensor([[ 1.0738],\n",
      "        [ 0.1892],\n",
      "        [-1.8735],\n",
      "        [ 1.2660],\n",
      "        [ 0.6836],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1611: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1611: tensor([[ 1.0734],\n",
      "        [ 0.1881],\n",
      "        [-1.8752],\n",
      "        [ 1.2638],\n",
      "        [ 0.6813],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1612: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1612: tensor([[ 1.0738],\n",
      "        [ 0.1891],\n",
      "        [-1.8735],\n",
      "        [ 1.2660],\n",
      "        [ 0.6836],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1613: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1613: tensor([[ 1.0734],\n",
      "        [ 0.1881],\n",
      "        [-1.8752],\n",
      "        [ 1.2639],\n",
      "        [ 0.6813],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1614: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1614: tensor([[ 1.0738],\n",
      "        [ 0.1891],\n",
      "        [-1.8735],\n",
      "        [ 1.2661],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1615: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1615: tensor([[ 1.0734],\n",
      "        [ 0.1881],\n",
      "        [-1.8753],\n",
      "        [ 1.2639],\n",
      "        [ 0.6813],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 1616: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1616: tensor([[ 1.0738],\n",
      "        [ 0.1891],\n",
      "        [-1.8736],\n",
      "        [ 1.2661],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1617: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1617: tensor([[ 1.0735],\n",
      "        [ 0.1880],\n",
      "        [-1.8753],\n",
      "        [ 1.2639],\n",
      "        [ 0.6813],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1618: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1618: tensor([[ 1.0738],\n",
      "        [ 0.1890],\n",
      "        [-1.8736],\n",
      "        [ 1.2661],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1619: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1619: tensor([[ 1.0735],\n",
      "        [ 0.1880],\n",
      "        [-1.8753],\n",
      "        [ 1.2639],\n",
      "        [ 0.6813],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1620: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1620: tensor([[ 1.0738],\n",
      "        [ 0.1890],\n",
      "        [-1.8736],\n",
      "        [ 1.2661],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1621: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1621: tensor([[ 1.0735],\n",
      "        [ 0.1880],\n",
      "        [-1.8754],\n",
      "        [ 1.2640],\n",
      "        [ 0.6813],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1622: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1622: tensor([[ 1.0738],\n",
      "        [ 0.1890],\n",
      "        [-1.8737],\n",
      "        [ 1.2662],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1623: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1623: tensor([[ 1.0735],\n",
      "        [ 0.1879],\n",
      "        [-1.8754],\n",
      "        [ 1.2640],\n",
      "        [ 0.6813],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1624: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1624: tensor([[ 1.0738],\n",
      "        [ 0.1889],\n",
      "        [-1.8737],\n",
      "        [ 1.2662],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1625: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1625: tensor([[ 1.0735],\n",
      "        [ 0.1879],\n",
      "        [-1.8754],\n",
      "        [ 1.2640],\n",
      "        [ 0.6813],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1626: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1626: tensor([[ 1.0738],\n",
      "        [ 0.1889],\n",
      "        [-1.8737],\n",
      "        [ 1.2662],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1627: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1627: tensor([[ 1.0735],\n",
      "        [ 0.1879],\n",
      "        [-1.8755],\n",
      "        [ 1.2640],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1628: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1628: tensor([[ 1.0738],\n",
      "        [ 0.1889],\n",
      "        [-1.8738],\n",
      "        [ 1.2662],\n",
      "        [ 0.6837],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1629: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1629: tensor([[ 1.0735],\n",
      "        [ 0.1878],\n",
      "        [-1.8755],\n",
      "        [ 1.2640],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1630: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1630: tensor([[ 1.0738],\n",
      "        [ 0.1889],\n",
      "        [-1.8738],\n",
      "        [ 1.2663],\n",
      "        [ 0.6838],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1631: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1631: tensor([[ 1.0735],\n",
      "        [ 0.1878],\n",
      "        [-1.8755],\n",
      "        [ 1.2641],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1632: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1632: tensor([[ 1.0738],\n",
      "        [ 0.1888],\n",
      "        [-1.8738],\n",
      "        [ 1.2663],\n",
      "        [ 0.6838],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1633: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1633: tensor([[ 1.0735],\n",
      "        [ 0.1878],\n",
      "        [-1.8756],\n",
      "        [ 1.2641],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1634: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1634: tensor([[ 1.0738],\n",
      "        [ 0.1888],\n",
      "        [-1.8739],\n",
      "        [ 1.2663],\n",
      "        [ 0.6838],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1635: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1635: tensor([[ 1.0735],\n",
      "        [ 0.1878],\n",
      "        [-1.8756],\n",
      "        [ 1.2641],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1636: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1636: tensor([[ 1.0738],\n",
      "        [ 0.1888],\n",
      "        [-1.8739],\n",
      "        [ 1.2663],\n",
      "        [ 0.6838],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1637: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1637: tensor([[ 1.0735],\n",
      "        [ 0.1877],\n",
      "        [-1.8756],\n",
      "        [ 1.2641],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1638: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1638: tensor([[ 1.0738],\n",
      "        [ 0.1887],\n",
      "        [-1.8739],\n",
      "        [ 1.2664],\n",
      "        [ 0.6838],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1639: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1639: tensor([[ 1.0735],\n",
      "        [ 0.1877],\n",
      "        [-1.8757],\n",
      "        [ 1.2642],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1640: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1640: tensor([[ 1.0738],\n",
      "        [ 0.1887],\n",
      "        [-1.8740],\n",
      "        [ 1.2664],\n",
      "        [ 0.6838],\n",
      "        [-0.2855]], requires_grad=True)\n",
      "poly train loss at 1641: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1641: tensor([[ 1.0735],\n",
      "        [ 0.1877],\n",
      "        [-1.8757],\n",
      "        [ 1.2642],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1642: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1642: tensor([[ 1.0738],\n",
      "        [ 0.1887],\n",
      "        [-1.8740],\n",
      "        [ 1.2664],\n",
      "        [ 0.6838],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1643: tensor(0.3301, grad_fn=<DivBackward0>)\n",
      "poly w at 1643: tensor([[ 1.0735],\n",
      "        [ 0.1876],\n",
      "        [-1.8757],\n",
      "        [ 1.2642],\n",
      "        [ 0.6814],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1644: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1644: tensor([[ 1.0738],\n",
      "        [ 0.1886],\n",
      "        [-1.8740],\n",
      "        [ 1.2664],\n",
      "        [ 0.6838],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1645: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1645: tensor([[ 1.0735],\n",
      "        [ 0.1876],\n",
      "        [-1.8758],\n",
      "        [ 1.2642],\n",
      "        [ 0.6815],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1646: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1646: tensor([[ 1.0738],\n",
      "        [ 0.1886],\n",
      "        [-1.8740],\n",
      "        [ 1.2665],\n",
      "        [ 0.6838],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1647: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1647: tensor([[ 1.0735],\n",
      "        [ 0.1876],\n",
      "        [-1.8758],\n",
      "        [ 1.2643],\n",
      "        [ 0.6815],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1648: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1648: tensor([[ 1.0738],\n",
      "        [ 0.1886],\n",
      "        [-1.8741],\n",
      "        [ 1.2665],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1649: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1649: tensor([[ 1.0735],\n",
      "        [ 0.1875],\n",
      "        [-1.8758],\n",
      "        [ 1.2643],\n",
      "        [ 0.6815],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1650: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1650: tensor([[ 1.0738],\n",
      "        [ 0.1885],\n",
      "        [-1.8741],\n",
      "        [ 1.2665],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1651: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1651: tensor([[ 1.0735],\n",
      "        [ 0.1875],\n",
      "        [-1.8759],\n",
      "        [ 1.2643],\n",
      "        [ 0.6815],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 1652: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1652: tensor([[ 1.0738],\n",
      "        [ 0.1885],\n",
      "        [-1.8741],\n",
      "        [ 1.2665],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1653: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1653: tensor([[ 1.0735],\n",
      "        [ 0.1875],\n",
      "        [-1.8759],\n",
      "        [ 1.2643],\n",
      "        [ 0.6815],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1654: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1654: tensor([[ 1.0738],\n",
      "        [ 0.1885],\n",
      "        [-1.8742],\n",
      "        [ 1.2665],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1655: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1655: tensor([[ 1.0735],\n",
      "        [ 0.1875],\n",
      "        [-1.8759],\n",
      "        [ 1.2644],\n",
      "        [ 0.6815],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1656: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1656: tensor([[ 1.0738],\n",
      "        [ 0.1885],\n",
      "        [-1.8742],\n",
      "        [ 1.2666],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1657: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1657: tensor([[ 1.0735],\n",
      "        [ 0.1874],\n",
      "        [-1.8760],\n",
      "        [ 1.2644],\n",
      "        [ 0.6815],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1658: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1658: tensor([[ 1.0738],\n",
      "        [ 0.1884],\n",
      "        [-1.8742],\n",
      "        [ 1.2666],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1659: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1659: tensor([[ 1.0735],\n",
      "        [ 0.1874],\n",
      "        [-1.8760],\n",
      "        [ 1.2644],\n",
      "        [ 0.6815],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1660: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1660: tensor([[ 1.0738],\n",
      "        [ 0.1884],\n",
      "        [-1.8743],\n",
      "        [ 1.2666],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1661: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1661: tensor([[ 1.0735],\n",
      "        [ 0.1874],\n",
      "        [-1.8760],\n",
      "        [ 1.2644],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1662: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1662: tensor([[ 1.0738],\n",
      "        [ 0.1884],\n",
      "        [-1.8743],\n",
      "        [ 1.2666],\n",
      "        [ 0.6839],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1663: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1663: tensor([[ 1.0735],\n",
      "        [ 0.1873],\n",
      "        [-1.8761],\n",
      "        [ 1.2645],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1664: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1664: tensor([[ 1.0738],\n",
      "        [ 0.1883],\n",
      "        [-1.8743],\n",
      "        [ 1.2667],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1665: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1665: tensor([[ 1.0735],\n",
      "        [ 0.1873],\n",
      "        [-1.8761],\n",
      "        [ 1.2645],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1666: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1666: tensor([[ 1.0738],\n",
      "        [ 0.1883],\n",
      "        [-1.8744],\n",
      "        [ 1.2667],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1667: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1667: tensor([[ 1.0735],\n",
      "        [ 0.1873],\n",
      "        [-1.8761],\n",
      "        [ 1.2645],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1668: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1668: tensor([[ 1.0738],\n",
      "        [ 0.1883],\n",
      "        [-1.8744],\n",
      "        [ 1.2667],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1669: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1669: tensor([[ 1.0735],\n",
      "        [ 0.1872],\n",
      "        [-1.8761],\n",
      "        [ 1.2645],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1670: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1670: tensor([[ 1.0738],\n",
      "        [ 0.1882],\n",
      "        [-1.8744],\n",
      "        [ 1.2667],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1671: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1671: tensor([[ 1.0735],\n",
      "        [ 0.1872],\n",
      "        [-1.8762],\n",
      "        [ 1.2645],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1672: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1672: tensor([[ 1.0738],\n",
      "        [ 0.1882],\n",
      "        [-1.8745],\n",
      "        [ 1.2668],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1673: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1673: tensor([[ 1.0735],\n",
      "        [ 0.1872],\n",
      "        [-1.8762],\n",
      "        [ 1.2646],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1674: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1674: tensor([[ 1.0738],\n",
      "        [ 0.1882],\n",
      "        [-1.8745],\n",
      "        [ 1.2668],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1675: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1675: tensor([[ 1.0735],\n",
      "        [ 0.1872],\n",
      "        [-1.8762],\n",
      "        [ 1.2646],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1676: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1676: tensor([[ 1.0738],\n",
      "        [ 0.1882],\n",
      "        [-1.8745],\n",
      "        [ 1.2668],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1677: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1677: tensor([[ 1.0735],\n",
      "        [ 0.1871],\n",
      "        [-1.8763],\n",
      "        [ 1.2646],\n",
      "        [ 0.6816],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1678: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1678: tensor([[ 1.0738],\n",
      "        [ 0.1881],\n",
      "        [-1.8746],\n",
      "        [ 1.2668],\n",
      "        [ 0.6840],\n",
      "        [-0.2856]], requires_grad=True)\n",
      "poly train loss at 1679: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1679: tensor([[ 1.0735],\n",
      "        [ 0.1871],\n",
      "        [-1.8763],\n",
      "        [ 1.2646],\n",
      "        [ 0.6817],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1680: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1680: tensor([[ 1.0738],\n",
      "        [ 0.1881],\n",
      "        [-1.8746],\n",
      "        [ 1.2669],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1681: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1681: tensor([[ 1.0735],\n",
      "        [ 0.1871],\n",
      "        [-1.8763],\n",
      "        [ 1.2647],\n",
      "        [ 0.6817],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1682: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1682: tensor([[ 1.0738],\n",
      "        [ 0.1881],\n",
      "        [-1.8746],\n",
      "        [ 1.2669],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1683: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1683: tensor([[ 1.0735],\n",
      "        [ 0.1870],\n",
      "        [-1.8764],\n",
      "        [ 1.2647],\n",
      "        [ 0.6817],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1684: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1684: tensor([[ 1.0738],\n",
      "        [ 0.1880],\n",
      "        [-1.8747],\n",
      "        [ 1.2669],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1685: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1685: tensor([[ 1.0735],\n",
      "        [ 0.1870],\n",
      "        [-1.8764],\n",
      "        [ 1.2647],\n",
      "        [ 0.6817],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1686: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1686: tensor([[ 1.0738],\n",
      "        [ 0.1880],\n",
      "        [-1.8747],\n",
      "        [ 1.2669],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1687: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1687: tensor([[ 1.0735],\n",
      "        [ 0.1870],\n",
      "        [-1.8764],\n",
      "        [ 1.2647],\n",
      "        [ 0.6817],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 1688: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1688: tensor([[ 1.0738],\n",
      "        [ 0.1880],\n",
      "        [-1.8747],\n",
      "        [ 1.2669],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1689: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1689: tensor([[ 1.0735],\n",
      "        [ 0.1869],\n",
      "        [-1.8765],\n",
      "        [ 1.2648],\n",
      "        [ 0.6817],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1690: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1690: tensor([[ 1.0738],\n",
      "        [ 0.1879],\n",
      "        [-1.8748],\n",
      "        [ 1.2670],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1691: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1691: tensor([[ 1.0735],\n",
      "        [ 0.1869],\n",
      "        [-1.8765],\n",
      "        [ 1.2648],\n",
      "        [ 0.6817],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1692: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1692: tensor([[ 1.0738],\n",
      "        [ 0.1879],\n",
      "        [-1.8748],\n",
      "        [ 1.2670],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1693: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1693: tensor([[ 1.0735],\n",
      "        [ 0.1869],\n",
      "        [-1.8765],\n",
      "        [ 1.2648],\n",
      "        [ 0.6817],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1694: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1694: tensor([[ 1.0738],\n",
      "        [ 0.1879],\n",
      "        [-1.8748],\n",
      "        [ 1.2670],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1695: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1695: tensor([[ 1.0735],\n",
      "        [ 0.1869],\n",
      "        [-1.8766],\n",
      "        [ 1.2648],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1696: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1696: tensor([[ 1.0738],\n",
      "        [ 0.1879],\n",
      "        [-1.8749],\n",
      "        [ 1.2670],\n",
      "        [ 0.6841],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1697: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1697: tensor([[ 1.0735],\n",
      "        [ 0.1868],\n",
      "        [-1.8766],\n",
      "        [ 1.2649],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1698: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1698: tensor([[ 1.0738],\n",
      "        [ 0.1878],\n",
      "        [-1.8749],\n",
      "        [ 1.2671],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1699: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1699: tensor([[ 1.0735],\n",
      "        [ 0.1868],\n",
      "        [-1.8766],\n",
      "        [ 1.2649],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1700: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1700: tensor([[ 1.0738],\n",
      "        [ 0.1878],\n",
      "        [-1.8749],\n",
      "        [ 1.2671],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1701: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1701: tensor([[ 1.0735],\n",
      "        [ 0.1868],\n",
      "        [-1.8767],\n",
      "        [ 1.2649],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1702: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1702: tensor([[ 1.0738],\n",
      "        [ 0.1878],\n",
      "        [-1.8749],\n",
      "        [ 1.2671],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1703: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1703: tensor([[ 1.0735],\n",
      "        [ 0.1867],\n",
      "        [-1.8767],\n",
      "        [ 1.2649],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1704: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1704: tensor([[ 1.0738],\n",
      "        [ 0.1877],\n",
      "        [-1.8750],\n",
      "        [ 1.2671],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1705: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1705: tensor([[ 1.0735],\n",
      "        [ 0.1867],\n",
      "        [-1.8767],\n",
      "        [ 1.2649],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1706: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1706: tensor([[ 1.0738],\n",
      "        [ 0.1877],\n",
      "        [-1.8750],\n",
      "        [ 1.2672],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1707: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1707: tensor([[ 1.0735],\n",
      "        [ 0.1867],\n",
      "        [-1.8768],\n",
      "        [ 1.2650],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1708: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1708: tensor([[ 1.0738],\n",
      "        [ 0.1877],\n",
      "        [-1.8750],\n",
      "        [ 1.2672],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1709: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1709: tensor([[ 1.0735],\n",
      "        [ 0.1866],\n",
      "        [-1.8768],\n",
      "        [ 1.2650],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1710: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1710: tensor([[ 1.0738],\n",
      "        [ 0.1877],\n",
      "        [-1.8751],\n",
      "        [ 1.2672],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1711: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1711: tensor([[ 1.0735],\n",
      "        [ 0.1866],\n",
      "        [-1.8768],\n",
      "        [ 1.2650],\n",
      "        [ 0.6818],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1712: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1712: tensor([[ 1.0738],\n",
      "        [ 0.1876],\n",
      "        [-1.8751],\n",
      "        [ 1.2672],\n",
      "        [ 0.6842],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1713: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1713: tensor([[ 1.0735],\n",
      "        [ 0.1866],\n",
      "        [-1.8769],\n",
      "        [ 1.2650],\n",
      "        [ 0.6819],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1714: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1714: tensor([[ 1.0738],\n",
      "        [ 0.1876],\n",
      "        [-1.8751],\n",
      "        [ 1.2673],\n",
      "        [ 0.6843],\n",
      "        [-0.2857]], requires_grad=True)\n",
      "poly train loss at 1715: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1715: tensor([[ 1.0735],\n",
      "        [ 0.1866],\n",
      "        [-1.8769],\n",
      "        [ 1.2651],\n",
      "        [ 0.6819],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1716: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1716: tensor([[ 1.0738],\n",
      "        [ 0.1876],\n",
      "        [-1.8752],\n",
      "        [ 1.2673],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1717: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1717: tensor([[ 1.0735],\n",
      "        [ 0.1865],\n",
      "        [-1.8769],\n",
      "        [ 1.2651],\n",
      "        [ 0.6819],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1718: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1718: tensor([[ 1.0738],\n",
      "        [ 0.1875],\n",
      "        [-1.8752],\n",
      "        [ 1.2673],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1719: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1719: tensor([[ 1.0735],\n",
      "        [ 0.1865],\n",
      "        [-1.8769],\n",
      "        [ 1.2651],\n",
      "        [ 0.6819],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1720: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1720: tensor([[ 1.0738],\n",
      "        [ 0.1875],\n",
      "        [-1.8752],\n",
      "        [ 1.2673],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1721: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1721: tensor([[ 1.0735],\n",
      "        [ 0.1865],\n",
      "        [-1.8770],\n",
      "        [ 1.2651],\n",
      "        [ 0.6819],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1722: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1722: tensor([[ 1.0738],\n",
      "        [ 0.1875],\n",
      "        [-1.8753],\n",
      "        [ 1.2673],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1723: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1723: tensor([[ 1.0735],\n",
      "        [ 0.1864],\n",
      "        [-1.8770],\n",
      "        [ 1.2652],\n",
      "        [ 0.6819],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1724: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1724: tensor([[ 1.0738],\n",
      "        [ 0.1874],\n",
      "        [-1.8753],\n",
      "        [ 1.2674],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1725: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1725: tensor([[ 1.0735],\n",
      "        [ 0.1864],\n",
      "        [-1.8770],\n",
      "        [ 1.2652],\n",
      "        [ 0.6819],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 1726: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1726: tensor([[ 1.0738],\n",
      "        [ 0.1874],\n",
      "        [-1.8753],\n",
      "        [ 1.2674],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1727: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1727: tensor([[ 1.0735],\n",
      "        [ 0.1864],\n",
      "        [-1.8771],\n",
      "        [ 1.2652],\n",
      "        [ 0.6819],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1728: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1728: tensor([[ 1.0738],\n",
      "        [ 0.1874],\n",
      "        [-1.8754],\n",
      "        [ 1.2674],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1729: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1729: tensor([[ 1.0735],\n",
      "        [ 0.1864],\n",
      "        [-1.8771],\n",
      "        [ 1.2652],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1730: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1730: tensor([[ 1.0738],\n",
      "        [ 0.1874],\n",
      "        [-1.8754],\n",
      "        [ 1.2674],\n",
      "        [ 0.6843],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1731: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1731: tensor([[ 1.0735],\n",
      "        [ 0.1863],\n",
      "        [-1.8771],\n",
      "        [ 1.2652],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1732: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1732: tensor([[ 1.0738],\n",
      "        [ 0.1873],\n",
      "        [-1.8754],\n",
      "        [ 1.2675],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1733: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1733: tensor([[ 1.0735],\n",
      "        [ 0.1863],\n",
      "        [-1.8772],\n",
      "        [ 1.2653],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1734: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1734: tensor([[ 1.0738],\n",
      "        [ 0.1873],\n",
      "        [-1.8755],\n",
      "        [ 1.2675],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1735: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1735: tensor([[ 1.0735],\n",
      "        [ 0.1863],\n",
      "        [-1.8772],\n",
      "        [ 1.2653],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1736: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1736: tensor([[ 1.0738],\n",
      "        [ 0.1873],\n",
      "        [-1.8755],\n",
      "        [ 1.2675],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1737: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1737: tensor([[ 1.0735],\n",
      "        [ 0.1862],\n",
      "        [-1.8772],\n",
      "        [ 1.2653],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1738: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1738: tensor([[ 1.0738],\n",
      "        [ 0.1872],\n",
      "        [-1.8755],\n",
      "        [ 1.2675],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1739: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1739: tensor([[ 1.0735],\n",
      "        [ 0.1862],\n",
      "        [-1.8773],\n",
      "        [ 1.2653],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1740: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1740: tensor([[ 1.0738],\n",
      "        [ 0.1872],\n",
      "        [-1.8756],\n",
      "        [ 1.2676],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1741: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1741: tensor([[ 1.0735],\n",
      "        [ 0.1862],\n",
      "        [-1.8773],\n",
      "        [ 1.2654],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1742: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1742: tensor([[ 1.0738],\n",
      "        [ 0.1872],\n",
      "        [-1.8756],\n",
      "        [ 1.2676],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1743: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1743: tensor([[ 1.0735],\n",
      "        [ 0.1861],\n",
      "        [-1.8773],\n",
      "        [ 1.2654],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1744: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1744: tensor([[ 1.0738],\n",
      "        [ 0.1871],\n",
      "        [-1.8756],\n",
      "        [ 1.2676],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1745: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1745: tensor([[ 1.0735],\n",
      "        [ 0.1861],\n",
      "        [-1.8774],\n",
      "        [ 1.2654],\n",
      "        [ 0.6820],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1746: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1746: tensor([[ 1.0738],\n",
      "        [ 0.1871],\n",
      "        [-1.8757],\n",
      "        [ 1.2676],\n",
      "        [ 0.6844],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1747: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1747: tensor([[ 1.0735],\n",
      "        [ 0.1861],\n",
      "        [-1.8774],\n",
      "        [ 1.2654],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1748: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1748: tensor([[ 1.0738],\n",
      "        [ 0.1871],\n",
      "        [-1.8757],\n",
      "        [ 1.2676],\n",
      "        [ 0.6845],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1749: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1749: tensor([[ 1.0735],\n",
      "        [ 0.1861],\n",
      "        [-1.8774],\n",
      "        [ 1.2655],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1750: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1750: tensor([[ 1.0738],\n",
      "        [ 0.1871],\n",
      "        [-1.8757],\n",
      "        [ 1.2677],\n",
      "        [ 0.6845],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1751: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1751: tensor([[ 1.0735],\n",
      "        [ 0.1860],\n",
      "        [-1.8775],\n",
      "        [ 1.2655],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1752: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1752: tensor([[ 1.0738],\n",
      "        [ 0.1870],\n",
      "        [-1.8757],\n",
      "        [ 1.2677],\n",
      "        [ 0.6845],\n",
      "        [-0.2858]], requires_grad=True)\n",
      "poly train loss at 1753: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1753: tensor([[ 1.0735],\n",
      "        [ 0.1860],\n",
      "        [-1.8775],\n",
      "        [ 1.2655],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1754: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1754: tensor([[ 1.0738],\n",
      "        [ 0.1870],\n",
      "        [-1.8758],\n",
      "        [ 1.2677],\n",
      "        [ 0.6845],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1755: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1755: tensor([[ 1.0735],\n",
      "        [ 0.1860],\n",
      "        [-1.8775],\n",
      "        [ 1.2655],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1756: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1756: tensor([[ 1.0738],\n",
      "        [ 0.1870],\n",
      "        [-1.8758],\n",
      "        [ 1.2677],\n",
      "        [ 0.6845],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1757: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1757: tensor([[ 1.0735],\n",
      "        [ 0.1859],\n",
      "        [-1.8776],\n",
      "        [ 1.2656],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1758: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1758: tensor([[ 1.0738],\n",
      "        [ 0.1869],\n",
      "        [-1.8758],\n",
      "        [ 1.2678],\n",
      "        [ 0.6845],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1759: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1759: tensor([[ 1.0735],\n",
      "        [ 0.1859],\n",
      "        [-1.8776],\n",
      "        [ 1.2656],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1760: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1760: tensor([[ 1.0738],\n",
      "        [ 0.1869],\n",
      "        [-1.8759],\n",
      "        [ 1.2678],\n",
      "        [ 0.6845],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1761: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1761: tensor([[ 1.0735],\n",
      "        [ 0.1859],\n",
      "        [-1.8776],\n",
      "        [ 1.2656],\n",
      "        [ 0.6821],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 1762: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1762: tensor([[ 1.0738],\n",
      "        [ 0.1869],\n",
      "        [-1.8759],\n",
      "        [ 1.2678],\n",
      "        [ 0.6845],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1763: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1763: tensor([[ 1.0735],\n",
      "        [ 0.1859],\n",
      "        [-1.8777],\n",
      "        [ 1.2656],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1764: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1764: tensor([[ 1.0738],\n",
      "        [ 0.1869],\n",
      "        [-1.8759],\n",
      "        [ 1.2678],\n",
      "        [ 0.6845],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1765: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1765: tensor([[ 1.0735],\n",
      "        [ 0.1858],\n",
      "        [-1.8777],\n",
      "        [ 1.2656],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1766: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1766: tensor([[ 1.0738],\n",
      "        [ 0.1868],\n",
      "        [-1.8760],\n",
      "        [ 1.2679],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1767: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1767: tensor([[ 1.0735],\n",
      "        [ 0.1858],\n",
      "        [-1.8777],\n",
      "        [ 1.2657],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1768: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1768: tensor([[ 1.0738],\n",
      "        [ 0.1868],\n",
      "        [-1.8760],\n",
      "        [ 1.2679],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1769: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1769: tensor([[ 1.0735],\n",
      "        [ 0.1858],\n",
      "        [-1.8777],\n",
      "        [ 1.2657],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1770: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1770: tensor([[ 1.0738],\n",
      "        [ 0.1868],\n",
      "        [-1.8760],\n",
      "        [ 1.2679],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1771: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1771: tensor([[ 1.0735],\n",
      "        [ 0.1857],\n",
      "        [-1.8778],\n",
      "        [ 1.2657],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1772: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1772: tensor([[ 1.0738],\n",
      "        [ 0.1867],\n",
      "        [-1.8761],\n",
      "        [ 1.2679],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1773: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1773: tensor([[ 1.0735],\n",
      "        [ 0.1857],\n",
      "        [-1.8778],\n",
      "        [ 1.2657],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1774: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1774: tensor([[ 1.0738],\n",
      "        [ 0.1867],\n",
      "        [-1.8761],\n",
      "        [ 1.2679],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1775: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1775: tensor([[ 1.0735],\n",
      "        [ 0.1857],\n",
      "        [-1.8778],\n",
      "        [ 1.2658],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1776: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1776: tensor([[ 1.0738],\n",
      "        [ 0.1867],\n",
      "        [-1.8761],\n",
      "        [ 1.2680],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1777: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1777: tensor([[ 1.0735],\n",
      "        [ 0.1856],\n",
      "        [-1.8779],\n",
      "        [ 1.2658],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1778: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1778: tensor([[ 1.0738],\n",
      "        [ 0.1867],\n",
      "        [-1.8762],\n",
      "        [ 1.2680],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1779: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1779: tensor([[ 1.0735],\n",
      "        [ 0.1856],\n",
      "        [-1.8779],\n",
      "        [ 1.2658],\n",
      "        [ 0.6822],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1780: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1780: tensor([[ 1.0738],\n",
      "        [ 0.1866],\n",
      "        [-1.8762],\n",
      "        [ 1.2680],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1781: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1781: tensor([[ 1.0735],\n",
      "        [ 0.1856],\n",
      "        [-1.8779],\n",
      "        [ 1.2658],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1782: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1782: tensor([[ 1.0738],\n",
      "        [ 0.1866],\n",
      "        [-1.8762],\n",
      "        [ 1.2680],\n",
      "        [ 0.6846],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1783: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1783: tensor([[ 1.0735],\n",
      "        [ 0.1856],\n",
      "        [-1.8780],\n",
      "        [ 1.2659],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1784: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1784: tensor([[ 1.0738],\n",
      "        [ 0.1866],\n",
      "        [-1.8763],\n",
      "        [ 1.2681],\n",
      "        [ 0.6847],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1785: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1785: tensor([[ 1.0735],\n",
      "        [ 0.1855],\n",
      "        [-1.8780],\n",
      "        [ 1.2659],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1786: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1786: tensor([[ 1.0738],\n",
      "        [ 0.1865],\n",
      "        [-1.8763],\n",
      "        [ 1.2681],\n",
      "        [ 0.6847],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1787: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1787: tensor([[ 1.0735],\n",
      "        [ 0.1855],\n",
      "        [-1.8780],\n",
      "        [ 1.2659],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1788: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1788: tensor([[ 1.0738],\n",
      "        [ 0.1865],\n",
      "        [-1.8763],\n",
      "        [ 1.2681],\n",
      "        [ 0.6847],\n",
      "        [-0.2859]], requires_grad=True)\n",
      "poly train loss at 1789: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1789: tensor([[ 1.0735],\n",
      "        [ 0.1855],\n",
      "        [-1.8781],\n",
      "        [ 1.2659],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1790: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1790: tensor([[ 1.0738],\n",
      "        [ 0.1865],\n",
      "        [-1.8763],\n",
      "        [ 1.2681],\n",
      "        [ 0.6847],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1791: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1791: tensor([[ 1.0735],\n",
      "        [ 0.1854],\n",
      "        [-1.8781],\n",
      "        [ 1.2659],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1792: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1792: tensor([[ 1.0738],\n",
      "        [ 0.1864],\n",
      "        [-1.8764],\n",
      "        [ 1.2682],\n",
      "        [ 0.6847],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1793: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1793: tensor([[ 1.0735],\n",
      "        [ 0.1854],\n",
      "        [-1.8781],\n",
      "        [ 1.2660],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1794: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1794: tensor([[ 1.0738],\n",
      "        [ 0.1864],\n",
      "        [-1.8764],\n",
      "        [ 1.2682],\n",
      "        [ 0.6847],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1795: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1795: tensor([[ 1.0735],\n",
      "        [ 0.1854],\n",
      "        [-1.8782],\n",
      "        [ 1.2660],\n",
      "        [ 0.6823],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1796: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1796: tensor([[ 1.0738],\n",
      "        [ 0.1864],\n",
      "        [-1.8764],\n",
      "        [ 1.2682],\n",
      "        [ 0.6847],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1797: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1797: tensor([[ 1.0735],\n",
      "        [ 0.1854],\n",
      "        [-1.8782],\n",
      "        [ 1.2660],\n",
      "        [ 0.6824],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1798: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1798: tensor([[ 1.0738],\n",
      "        [ 0.1864],\n",
      "        [-1.8765],\n",
      "        [ 1.2682],\n",
      "        [ 0.6847],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1799: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1799: tensor([[ 1.0735],\n",
      "        [ 0.1853],\n",
      "        [-1.8782],\n",
      "        [ 1.2660],\n",
      "        [ 0.6824],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 1800: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1800: tensor([[ 1.0738],\n",
      "        [ 0.1863],\n",
      "        [-1.8765],\n",
      "        [ 1.2682],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1801: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1801: tensor([[ 1.0735],\n",
      "        [ 0.1853],\n",
      "        [-1.8783],\n",
      "        [ 1.2661],\n",
      "        [ 0.6824],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1802: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1802: tensor([[ 1.0738],\n",
      "        [ 0.1863],\n",
      "        [-1.8765],\n",
      "        [ 1.2683],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1803: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1803: tensor([[ 1.0735],\n",
      "        [ 0.1853],\n",
      "        [-1.8783],\n",
      "        [ 1.2661],\n",
      "        [ 0.6824],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1804: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1804: tensor([[ 1.0738],\n",
      "        [ 0.1863],\n",
      "        [-1.8766],\n",
      "        [ 1.2683],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1805: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1805: tensor([[ 1.0735],\n",
      "        [ 0.1852],\n",
      "        [-1.8783],\n",
      "        [ 1.2661],\n",
      "        [ 0.6824],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1806: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1806: tensor([[ 1.0738],\n",
      "        [ 0.1862],\n",
      "        [-1.8766],\n",
      "        [ 1.2683],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1807: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1807: tensor([[ 1.0735],\n",
      "        [ 0.1852],\n",
      "        [-1.8783],\n",
      "        [ 1.2661],\n",
      "        [ 0.6824],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1808: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1808: tensor([[ 1.0738],\n",
      "        [ 0.1862],\n",
      "        [-1.8766],\n",
      "        [ 1.2683],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1809: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1809: tensor([[ 1.0735],\n",
      "        [ 0.1852],\n",
      "        [-1.8784],\n",
      "        [ 1.2662],\n",
      "        [ 0.6824],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1810: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1810: tensor([[ 1.0738],\n",
      "        [ 0.1862],\n",
      "        [-1.8767],\n",
      "        [ 1.2684],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1811: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1811: tensor([[ 1.0735],\n",
      "        [ 0.1852],\n",
      "        [-1.8784],\n",
      "        [ 1.2662],\n",
      "        [ 0.6824],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1812: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1812: tensor([[ 1.0738],\n",
      "        [ 0.1862],\n",
      "        [-1.8767],\n",
      "        [ 1.2684],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1813: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1813: tensor([[ 1.0735],\n",
      "        [ 0.1851],\n",
      "        [-1.8784],\n",
      "        [ 1.2662],\n",
      "        [ 0.6824],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1814: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1814: tensor([[ 1.0738],\n",
      "        [ 0.1861],\n",
      "        [-1.8767],\n",
      "        [ 1.2684],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1815: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1815: tensor([[ 1.0735],\n",
      "        [ 0.1851],\n",
      "        [-1.8785],\n",
      "        [ 1.2662],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1816: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1816: tensor([[ 1.0738],\n",
      "        [ 0.1861],\n",
      "        [-1.8768],\n",
      "        [ 1.2684],\n",
      "        [ 0.6848],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1817: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1817: tensor([[ 1.0735],\n",
      "        [ 0.1851],\n",
      "        [-1.8785],\n",
      "        [ 1.2662],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1818: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1818: tensor([[ 1.0738],\n",
      "        [ 0.1861],\n",
      "        [-1.8768],\n",
      "        [ 1.2685],\n",
      "        [ 0.6849],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1819: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1819: tensor([[ 1.0735],\n",
      "        [ 0.1850],\n",
      "        [-1.8785],\n",
      "        [ 1.2663],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1820: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1820: tensor([[ 1.0738],\n",
      "        [ 0.1860],\n",
      "        [-1.8768],\n",
      "        [ 1.2685],\n",
      "        [ 0.6849],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1821: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1821: tensor([[ 1.0735],\n",
      "        [ 0.1850],\n",
      "        [-1.8786],\n",
      "        [ 1.2663],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1822: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1822: tensor([[ 1.0738],\n",
      "        [ 0.1860],\n",
      "        [-1.8768],\n",
      "        [ 1.2685],\n",
      "        [ 0.6849],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1823: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1823: tensor([[ 1.0735],\n",
      "        [ 0.1850],\n",
      "        [-1.8786],\n",
      "        [ 1.2663],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1824: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1824: tensor([[ 1.0738],\n",
      "        [ 0.1860],\n",
      "        [-1.8769],\n",
      "        [ 1.2685],\n",
      "        [ 0.6849],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1825: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1825: tensor([[ 1.0735],\n",
      "        [ 0.1849],\n",
      "        [-1.8786],\n",
      "        [ 1.2663],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1826: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1826: tensor([[ 1.0738],\n",
      "        [ 0.1860],\n",
      "        [-1.8769],\n",
      "        [ 1.2685],\n",
      "        [ 0.6849],\n",
      "        [-0.2860]], requires_grad=True)\n",
      "poly train loss at 1827: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1827: tensor([[ 1.0735],\n",
      "        [ 0.1849],\n",
      "        [-1.8787],\n",
      "        [ 1.2664],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1828: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1828: tensor([[ 1.0738],\n",
      "        [ 0.1859],\n",
      "        [-1.8769],\n",
      "        [ 1.2686],\n",
      "        [ 0.6849],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1829: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1829: tensor([[ 1.0735],\n",
      "        [ 0.1849],\n",
      "        [-1.8787],\n",
      "        [ 1.2664],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1830: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1830: tensor([[ 1.0738],\n",
      "        [ 0.1859],\n",
      "        [-1.8770],\n",
      "        [ 1.2686],\n",
      "        [ 0.6849],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1831: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1831: tensor([[ 1.0735],\n",
      "        [ 0.1849],\n",
      "        [-1.8787],\n",
      "        [ 1.2664],\n",
      "        [ 0.6825],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1832: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1832: tensor([[ 1.0738],\n",
      "        [ 0.1859],\n",
      "        [-1.8770],\n",
      "        [ 1.2686],\n",
      "        [ 0.6849],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1833: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1833: tensor([[ 1.0735],\n",
      "        [ 0.1848],\n",
      "        [-1.8788],\n",
      "        [ 1.2664],\n",
      "        [ 0.6826],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1834: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1834: tensor([[ 1.0738],\n",
      "        [ 0.1858],\n",
      "        [-1.8770],\n",
      "        [ 1.2686],\n",
      "        [ 0.6849],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1835: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1835: tensor([[ 1.0735],\n",
      "        [ 0.1848],\n",
      "        [-1.8788],\n",
      "        [ 1.2664],\n",
      "        [ 0.6826],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1836: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1836: tensor([[ 1.0738],\n",
      "        [ 0.1858],\n",
      "        [-1.8771],\n",
      "        [ 1.2687],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1837: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1837: tensor([[ 1.0735],\n",
      "        [ 0.1848],\n",
      "        [-1.8788],\n",
      "        [ 1.2665],\n",
      "        [ 0.6826],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 1838: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1838: tensor([[ 1.0738],\n",
      "        [ 0.1858],\n",
      "        [-1.8771],\n",
      "        [ 1.2687],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1839: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1839: tensor([[ 1.0735],\n",
      "        [ 0.1847],\n",
      "        [-1.8788],\n",
      "        [ 1.2665],\n",
      "        [ 0.6826],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1840: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1840: tensor([[ 1.0738],\n",
      "        [ 0.1857],\n",
      "        [-1.8771],\n",
      "        [ 1.2687],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1841: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1841: tensor([[ 1.0735],\n",
      "        [ 0.1847],\n",
      "        [-1.8789],\n",
      "        [ 1.2665],\n",
      "        [ 0.6826],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1842: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1842: tensor([[ 1.0738],\n",
      "        [ 0.1857],\n",
      "        [-1.8772],\n",
      "        [ 1.2687],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1843: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1843: tensor([[ 1.0735],\n",
      "        [ 0.1847],\n",
      "        [-1.8789],\n",
      "        [ 1.2665],\n",
      "        [ 0.6826],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1844: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1844: tensor([[ 1.0738],\n",
      "        [ 0.1857],\n",
      "        [-1.8772],\n",
      "        [ 1.2688],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1845: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1845: tensor([[ 1.0735],\n",
      "        [ 0.1847],\n",
      "        [-1.8789],\n",
      "        [ 1.2666],\n",
      "        [ 0.6826],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1846: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1846: tensor([[ 1.0738],\n",
      "        [ 0.1857],\n",
      "        [-1.8772],\n",
      "        [ 1.2688],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1847: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1847: tensor([[ 1.0735],\n",
      "        [ 0.1846],\n",
      "        [-1.8790],\n",
      "        [ 1.2666],\n",
      "        [ 0.6826],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1848: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1848: tensor([[ 1.0738],\n",
      "        [ 0.1856],\n",
      "        [-1.8773],\n",
      "        [ 1.2688],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1849: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1849: tensor([[ 1.0735],\n",
      "        [ 0.1846],\n",
      "        [-1.8790],\n",
      "        [ 1.2666],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1850: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1850: tensor([[ 1.0738],\n",
      "        [ 0.1856],\n",
      "        [-1.8773],\n",
      "        [ 1.2688],\n",
      "        [ 0.6850],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1851: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1851: tensor([[ 1.0735],\n",
      "        [ 0.1846],\n",
      "        [-1.8790],\n",
      "        [ 1.2666],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1852: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1852: tensor([[ 1.0738],\n",
      "        [ 0.1856],\n",
      "        [-1.8773],\n",
      "        [ 1.2688],\n",
      "        [ 0.6851],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1853: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1853: tensor([[ 1.0735],\n",
      "        [ 0.1845],\n",
      "        [-1.8791],\n",
      "        [ 1.2667],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1854: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1854: tensor([[ 1.0738],\n",
      "        [ 0.1855],\n",
      "        [-1.8774],\n",
      "        [ 1.2689],\n",
      "        [ 0.6851],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1855: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1855: tensor([[ 1.0735],\n",
      "        [ 0.1845],\n",
      "        [-1.8791],\n",
      "        [ 1.2667],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1856: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1856: tensor([[ 1.0738],\n",
      "        [ 0.1855],\n",
      "        [-1.8774],\n",
      "        [ 1.2689],\n",
      "        [ 0.6851],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1857: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1857: tensor([[ 1.0735],\n",
      "        [ 0.1845],\n",
      "        [-1.8791],\n",
      "        [ 1.2667],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1858: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1858: tensor([[ 1.0738],\n",
      "        [ 0.1855],\n",
      "        [-1.8774],\n",
      "        [ 1.2689],\n",
      "        [ 0.6851],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1859: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1859: tensor([[ 1.0735],\n",
      "        [ 0.1845],\n",
      "        [-1.8792],\n",
      "        [ 1.2667],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1860: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1860: tensor([[ 1.0738],\n",
      "        [ 0.1855],\n",
      "        [-1.8774],\n",
      "        [ 1.2689],\n",
      "        [ 0.6851],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1861: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1861: tensor([[ 1.0735],\n",
      "        [ 0.1844],\n",
      "        [-1.8792],\n",
      "        [ 1.2667],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1862: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1862: tensor([[ 1.0738],\n",
      "        [ 0.1854],\n",
      "        [-1.8775],\n",
      "        [ 1.2690],\n",
      "        [ 0.6851],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1863: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1863: tensor([[ 1.0735],\n",
      "        [ 0.1844],\n",
      "        [-1.8792],\n",
      "        [ 1.2668],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1864: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1864: tensor([[ 1.0738],\n",
      "        [ 0.1854],\n",
      "        [-1.8775],\n",
      "        [ 1.2690],\n",
      "        [ 0.6851],\n",
      "        [-0.2861]], requires_grad=True)\n",
      "poly train loss at 1865: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1865: tensor([[ 1.0735],\n",
      "        [ 0.1844],\n",
      "        [-1.8793],\n",
      "        [ 1.2668],\n",
      "        [ 0.6827],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1866: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1866: tensor([[ 1.0738],\n",
      "        [ 0.1854],\n",
      "        [-1.8775],\n",
      "        [ 1.2690],\n",
      "        [ 0.6851],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1867: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1867: tensor([[ 1.0735],\n",
      "        [ 0.1843],\n",
      "        [-1.8793],\n",
      "        [ 1.2668],\n",
      "        [ 0.6828],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1868: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1868: tensor([[ 1.0738],\n",
      "        [ 0.1853],\n",
      "        [-1.8776],\n",
      "        [ 1.2690],\n",
      "        [ 0.6851],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1869: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1869: tensor([[ 1.0735],\n",
      "        [ 0.1843],\n",
      "        [-1.8793],\n",
      "        [ 1.2668],\n",
      "        [ 0.6828],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1870: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1870: tensor([[ 1.0738],\n",
      "        [ 0.1853],\n",
      "        [-1.8776],\n",
      "        [ 1.2690],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1871: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1871: tensor([[ 1.0735],\n",
      "        [ 0.1843],\n",
      "        [-1.8793],\n",
      "        [ 1.2669],\n",
      "        [ 0.6828],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1872: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1872: tensor([[ 1.0738],\n",
      "        [ 0.1853],\n",
      "        [-1.8776],\n",
      "        [ 1.2691],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1873: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1873: tensor([[ 1.0735],\n",
      "        [ 0.1843],\n",
      "        [-1.8794],\n",
      "        [ 1.2669],\n",
      "        [ 0.6828],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1874: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1874: tensor([[ 1.0738],\n",
      "        [ 0.1853],\n",
      "        [-1.8777],\n",
      "        [ 1.2691],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1875: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1875: tensor([[ 1.0735],\n",
      "        [ 0.1842],\n",
      "        [-1.8794],\n",
      "        [ 1.2669],\n",
      "        [ 0.6828],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 1876: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1876: tensor([[ 1.0738],\n",
      "        [ 0.1852],\n",
      "        [-1.8777],\n",
      "        [ 1.2691],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1877: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1877: tensor([[ 1.0735],\n",
      "        [ 0.1842],\n",
      "        [-1.8794],\n",
      "        [ 1.2669],\n",
      "        [ 0.6828],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1878: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1878: tensor([[ 1.0738],\n",
      "        [ 0.1852],\n",
      "        [-1.8777],\n",
      "        [ 1.2691],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1879: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1879: tensor([[ 1.0735],\n",
      "        [ 0.1842],\n",
      "        [-1.8795],\n",
      "        [ 1.2669],\n",
      "        [ 0.6828],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1880: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1880: tensor([[ 1.0738],\n",
      "        [ 0.1852],\n",
      "        [-1.8778],\n",
      "        [ 1.2692],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1881: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1881: tensor([[ 1.0735],\n",
      "        [ 0.1841],\n",
      "        [-1.8795],\n",
      "        [ 1.2670],\n",
      "        [ 0.6828],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1882: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1882: tensor([[ 1.0738],\n",
      "        [ 0.1851],\n",
      "        [-1.8778],\n",
      "        [ 1.2692],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1883: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1883: tensor([[ 1.0735],\n",
      "        [ 0.1841],\n",
      "        [-1.8795],\n",
      "        [ 1.2670],\n",
      "        [ 0.6828],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1884: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1884: tensor([[ 1.0738],\n",
      "        [ 0.1851],\n",
      "        [-1.8778],\n",
      "        [ 1.2692],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1885: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1885: tensor([[ 1.0735],\n",
      "        [ 0.1841],\n",
      "        [-1.8796],\n",
      "        [ 1.2670],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1886: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1886: tensor([[ 1.0738],\n",
      "        [ 0.1851],\n",
      "        [-1.8779],\n",
      "        [ 1.2692],\n",
      "        [ 0.6852],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1887: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1887: tensor([[ 1.0735],\n",
      "        [ 0.1841],\n",
      "        [-1.8796],\n",
      "        [ 1.2670],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1888: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1888: tensor([[ 1.0738],\n",
      "        [ 0.1851],\n",
      "        [-1.8779],\n",
      "        [ 1.2692],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1889: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1889: tensor([[ 1.0735],\n",
      "        [ 0.1840],\n",
      "        [-1.8796],\n",
      "        [ 1.2671],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1890: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1890: tensor([[ 1.0738],\n",
      "        [ 0.1850],\n",
      "        [-1.8779],\n",
      "        [ 1.2693],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1891: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1891: tensor([[ 1.0735],\n",
      "        [ 0.1840],\n",
      "        [-1.8797],\n",
      "        [ 1.2671],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1892: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1892: tensor([[ 1.0738],\n",
      "        [ 0.1850],\n",
      "        [-1.8779],\n",
      "        [ 1.2693],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1893: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1893: tensor([[ 1.0735],\n",
      "        [ 0.1840],\n",
      "        [-1.8797],\n",
      "        [ 1.2671],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1894: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1894: tensor([[ 1.0738],\n",
      "        [ 0.1850],\n",
      "        [-1.8780],\n",
      "        [ 1.2693],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1895: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1895: tensor([[ 1.0735],\n",
      "        [ 0.1839],\n",
      "        [-1.8797],\n",
      "        [ 1.2671],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1896: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1896: tensor([[ 1.0738],\n",
      "        [ 0.1849],\n",
      "        [-1.8780],\n",
      "        [ 1.2693],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1897: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1897: tensor([[ 1.0735],\n",
      "        [ 0.1839],\n",
      "        [-1.8798],\n",
      "        [ 1.2672],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1898: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1898: tensor([[ 1.0738],\n",
      "        [ 0.1849],\n",
      "        [-1.8780],\n",
      "        [ 1.2694],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1899: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1899: tensor([[ 1.0735],\n",
      "        [ 0.1839],\n",
      "        [-1.8798],\n",
      "        [ 1.2672],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1900: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1900: tensor([[ 1.0738],\n",
      "        [ 0.1849],\n",
      "        [-1.8781],\n",
      "        [ 1.2694],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1901: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1901: tensor([[ 1.0735],\n",
      "        [ 0.1839],\n",
      "        [-1.8798],\n",
      "        [ 1.2672],\n",
      "        [ 0.6829],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1902: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1902: tensor([[ 1.0738],\n",
      "        [ 0.1849],\n",
      "        [-1.8781],\n",
      "        [ 1.2694],\n",
      "        [ 0.6853],\n",
      "        [-0.2862]], requires_grad=True)\n",
      "poly train loss at 1903: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1903: tensor([[ 1.0735],\n",
      "        [ 0.1838],\n",
      "        [-1.8798],\n",
      "        [ 1.2672],\n",
      "        [ 0.6830],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1904: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1904: tensor([[ 1.0738],\n",
      "        [ 0.1848],\n",
      "        [-1.8781],\n",
      "        [ 1.2694],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1905: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1905: tensor([[ 1.0735],\n",
      "        [ 0.1838],\n",
      "        [-1.8799],\n",
      "        [ 1.2672],\n",
      "        [ 0.6830],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1906: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1906: tensor([[ 1.0738],\n",
      "        [ 0.1848],\n",
      "        [-1.8782],\n",
      "        [ 1.2695],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1907: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1907: tensor([[ 1.0735],\n",
      "        [ 0.1838],\n",
      "        [-1.8799],\n",
      "        [ 1.2673],\n",
      "        [ 0.6830],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1908: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1908: tensor([[ 1.0738],\n",
      "        [ 0.1848],\n",
      "        [-1.8782],\n",
      "        [ 1.2695],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1909: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1909: tensor([[ 1.0735],\n",
      "        [ 0.1837],\n",
      "        [-1.8799],\n",
      "        [ 1.2673],\n",
      "        [ 0.6830],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1910: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1910: tensor([[ 1.0738],\n",
      "        [ 0.1847],\n",
      "        [-1.8782],\n",
      "        [ 1.2695],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1911: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1911: tensor([[ 1.0735],\n",
      "        [ 0.1837],\n",
      "        [-1.8800],\n",
      "        [ 1.2673],\n",
      "        [ 0.6830],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1912: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1912: tensor([[ 1.0738],\n",
      "        [ 0.1847],\n",
      "        [-1.8783],\n",
      "        [ 1.2695],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1913: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1913: tensor([[ 1.0735],\n",
      "        [ 0.1837],\n",
      "        [-1.8800],\n",
      "        [ 1.2673],\n",
      "        [ 0.6830],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 1914: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1914: tensor([[ 1.0738],\n",
      "        [ 0.1847],\n",
      "        [-1.8783],\n",
      "        [ 1.2695],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1915: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1915: tensor([[ 1.0735],\n",
      "        [ 0.1837],\n",
      "        [-1.8800],\n",
      "        [ 1.2674],\n",
      "        [ 0.6830],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1916: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1916: tensor([[ 1.0738],\n",
      "        [ 0.1847],\n",
      "        [-1.8783],\n",
      "        [ 1.2696],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1917: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1917: tensor([[ 1.0735],\n",
      "        [ 0.1836],\n",
      "        [-1.8801],\n",
      "        [ 1.2674],\n",
      "        [ 0.6830],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1918: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1918: tensor([[ 1.0738],\n",
      "        [ 0.1846],\n",
      "        [-1.8783],\n",
      "        [ 1.2696],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1919: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1919: tensor([[ 1.0735],\n",
      "        [ 0.1836],\n",
      "        [-1.8801],\n",
      "        [ 1.2674],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1920: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1920: tensor([[ 1.0738],\n",
      "        [ 0.1846],\n",
      "        [-1.8784],\n",
      "        [ 1.2696],\n",
      "        [ 0.6854],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1921: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1921: tensor([[ 1.0735],\n",
      "        [ 0.1836],\n",
      "        [-1.8801],\n",
      "        [ 1.2674],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1922: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1922: tensor([[ 1.0738],\n",
      "        [ 0.1846],\n",
      "        [-1.8784],\n",
      "        [ 1.2696],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1923: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1923: tensor([[ 1.0735],\n",
      "        [ 0.1835],\n",
      "        [-1.8802],\n",
      "        [ 1.2674],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1924: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1924: tensor([[ 1.0738],\n",
      "        [ 0.1845],\n",
      "        [-1.8784],\n",
      "        [ 1.2697],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1925: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1925: tensor([[ 1.0735],\n",
      "        [ 0.1835],\n",
      "        [-1.8802],\n",
      "        [ 1.2675],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1926: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1926: tensor([[ 1.0738],\n",
      "        [ 0.1845],\n",
      "        [-1.8785],\n",
      "        [ 1.2697],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1927: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1927: tensor([[ 1.0735],\n",
      "        [ 0.1835],\n",
      "        [-1.8802],\n",
      "        [ 1.2675],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1928: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1928: tensor([[ 1.0738],\n",
      "        [ 0.1845],\n",
      "        [-1.8785],\n",
      "        [ 1.2697],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1929: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1929: tensor([[ 1.0735],\n",
      "        [ 0.1835],\n",
      "        [-1.8802],\n",
      "        [ 1.2675],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1930: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1930: tensor([[ 1.0738],\n",
      "        [ 0.1845],\n",
      "        [-1.8785],\n",
      "        [ 1.2697],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1931: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1931: tensor([[ 1.0735],\n",
      "        [ 0.1834],\n",
      "        [-1.8803],\n",
      "        [ 1.2675],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1932: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1932: tensor([[ 1.0738],\n",
      "        [ 0.1844],\n",
      "        [-1.8786],\n",
      "        [ 1.2697],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1933: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1933: tensor([[ 1.0735],\n",
      "        [ 0.1834],\n",
      "        [-1.8803],\n",
      "        [ 1.2676],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1934: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1934: tensor([[ 1.0738],\n",
      "        [ 0.1844],\n",
      "        [-1.8786],\n",
      "        [ 1.2698],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1935: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1935: tensor([[ 1.0735],\n",
      "        [ 0.1834],\n",
      "        [-1.8803],\n",
      "        [ 1.2676],\n",
      "        [ 0.6831],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1936: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1936: tensor([[ 1.0738],\n",
      "        [ 0.1844],\n",
      "        [-1.8786],\n",
      "        [ 1.2698],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1937: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1937: tensor([[ 1.0735],\n",
      "        [ 0.1833],\n",
      "        [-1.8804],\n",
      "        [ 1.2676],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1938: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1938: tensor([[ 1.0738],\n",
      "        [ 0.1843],\n",
      "        [-1.8787],\n",
      "        [ 1.2698],\n",
      "        [ 0.6855],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1939: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1939: tensor([[ 1.0735],\n",
      "        [ 0.1833],\n",
      "        [-1.8804],\n",
      "        [ 1.2676],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1940: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1940: tensor([[ 1.0738],\n",
      "        [ 0.1843],\n",
      "        [-1.8787],\n",
      "        [ 1.2698],\n",
      "        [ 0.6856],\n",
      "        [-0.2863]], requires_grad=True)\n",
      "poly train loss at 1941: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1941: tensor([[ 1.0735],\n",
      "        [ 0.1833],\n",
      "        [-1.8804],\n",
      "        [ 1.2676],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1942: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1942: tensor([[ 1.0738],\n",
      "        [ 0.1843],\n",
      "        [-1.8787],\n",
      "        [ 1.2699],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1943: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1943: tensor([[ 1.0735],\n",
      "        [ 0.1833],\n",
      "        [-1.8805],\n",
      "        [ 1.2677],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1944: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1944: tensor([[ 1.0738],\n",
      "        [ 0.1843],\n",
      "        [-1.8787],\n",
      "        [ 1.2699],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1945: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1945: tensor([[ 1.0735],\n",
      "        [ 0.1832],\n",
      "        [-1.8805],\n",
      "        [ 1.2677],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1946: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1946: tensor([[ 1.0738],\n",
      "        [ 0.1842],\n",
      "        [-1.8788],\n",
      "        [ 1.2699],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1947: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1947: tensor([[ 1.0735],\n",
      "        [ 0.1832],\n",
      "        [-1.8805],\n",
      "        [ 1.2677],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1948: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1948: tensor([[ 1.0738],\n",
      "        [ 0.1842],\n",
      "        [-1.8788],\n",
      "        [ 1.2699],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1949: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1949: tensor([[ 1.0735],\n",
      "        [ 0.1832],\n",
      "        [-1.8806],\n",
      "        [ 1.2677],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1950: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1950: tensor([[ 1.0738],\n",
      "        [ 0.1842],\n",
      "        [-1.8788],\n",
      "        [ 1.2699],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1951: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1951: tensor([[ 1.0735],\n",
      "        [ 0.1831],\n",
      "        [-1.8806],\n",
      "        [ 1.2678],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1952: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1952: tensor([[ 1.0738],\n",
      "        [ 0.1842],\n",
      "        [-1.8789],\n",
      "        [ 1.2700],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1953: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1953: tensor([[ 1.0735],\n",
      "        [ 0.1831],\n",
      "        [-1.8806],\n",
      "        [ 1.2678],\n",
      "        [ 0.6832],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 1954: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1954: tensor([[ 1.0738],\n",
      "        [ 0.1841],\n",
      "        [-1.8789],\n",
      "        [ 1.2700],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1955: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1955: tensor([[ 1.0735],\n",
      "        [ 0.1831],\n",
      "        [-1.8806],\n",
      "        [ 1.2678],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1956: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1956: tensor([[ 1.0738],\n",
      "        [ 0.1841],\n",
      "        [-1.8789],\n",
      "        [ 1.2700],\n",
      "        [ 0.6856],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1957: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1957: tensor([[ 1.0735],\n",
      "        [ 0.1831],\n",
      "        [-1.8807],\n",
      "        [ 1.2678],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1958: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1958: tensor([[ 1.0738],\n",
      "        [ 0.1841],\n",
      "        [-1.8790],\n",
      "        [ 1.2700],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1959: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1959: tensor([[ 1.0735],\n",
      "        [ 0.1830],\n",
      "        [-1.8807],\n",
      "        [ 1.2678],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1960: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1960: tensor([[ 1.0738],\n",
      "        [ 0.1840],\n",
      "        [-1.8790],\n",
      "        [ 1.2701],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1961: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1961: tensor([[ 1.0735],\n",
      "        [ 0.1830],\n",
      "        [-1.8807],\n",
      "        [ 1.2679],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1962: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1962: tensor([[ 1.0738],\n",
      "        [ 0.1840],\n",
      "        [-1.8790],\n",
      "        [ 1.2701],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1963: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1963: tensor([[ 1.0735],\n",
      "        [ 0.1830],\n",
      "        [-1.8808],\n",
      "        [ 1.2679],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1964: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1964: tensor([[ 1.0738],\n",
      "        [ 0.1840],\n",
      "        [-1.8790],\n",
      "        [ 1.2701],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1965: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1965: tensor([[ 1.0735],\n",
      "        [ 0.1829],\n",
      "        [-1.8808],\n",
      "        [ 1.2679],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1966: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1966: tensor([[ 1.0738],\n",
      "        [ 0.1840],\n",
      "        [-1.8791],\n",
      "        [ 1.2701],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1967: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1967: tensor([[ 1.0735],\n",
      "        [ 0.1829],\n",
      "        [-1.8808],\n",
      "        [ 1.2679],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1968: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1968: tensor([[ 1.0738],\n",
      "        [ 0.1839],\n",
      "        [-1.8791],\n",
      "        [ 1.2701],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1969: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1969: tensor([[ 1.0735],\n",
      "        [ 0.1829],\n",
      "        [-1.8809],\n",
      "        [ 1.2680],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1970: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1970: tensor([[ 1.0738],\n",
      "        [ 0.1839],\n",
      "        [-1.8791],\n",
      "        [ 1.2702],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1971: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1971: tensor([[ 1.0735],\n",
      "        [ 0.1829],\n",
      "        [-1.8809],\n",
      "        [ 1.2680],\n",
      "        [ 0.6833],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1972: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1972: tensor([[ 1.0738],\n",
      "        [ 0.1839],\n",
      "        [-1.8792],\n",
      "        [ 1.2702],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1973: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1973: tensor([[ 1.0735],\n",
      "        [ 0.1828],\n",
      "        [-1.8809],\n",
      "        [ 1.2680],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1974: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1974: tensor([[ 1.0738],\n",
      "        [ 0.1838],\n",
      "        [-1.8792],\n",
      "        [ 1.2702],\n",
      "        [ 0.6857],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1975: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1975: tensor([[ 1.0735],\n",
      "        [ 0.1828],\n",
      "        [-1.8809],\n",
      "        [ 1.2680],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1976: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1976: tensor([[ 1.0738],\n",
      "        [ 0.1838],\n",
      "        [-1.8792],\n",
      "        [ 1.2702],\n",
      "        [ 0.6858],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1977: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1977: tensor([[ 1.0735],\n",
      "        [ 0.1828],\n",
      "        [-1.8810],\n",
      "        [ 1.2680],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1978: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1978: tensor([[ 1.0738],\n",
      "        [ 0.1838],\n",
      "        [-1.8793],\n",
      "        [ 1.2703],\n",
      "        [ 0.6858],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1979: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1979: tensor([[ 1.0735],\n",
      "        [ 0.1828],\n",
      "        [-1.8810],\n",
      "        [ 1.2681],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1980: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1980: tensor([[ 1.0738],\n",
      "        [ 0.1838],\n",
      "        [-1.8793],\n",
      "        [ 1.2703],\n",
      "        [ 0.6858],\n",
      "        [-0.2864]], requires_grad=True)\n",
      "poly train loss at 1981: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1981: tensor([[ 1.0735],\n",
      "        [ 0.1827],\n",
      "        [-1.8810],\n",
      "        [ 1.2681],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1982: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1982: tensor([[ 1.0738],\n",
      "        [ 0.1837],\n",
      "        [-1.8793],\n",
      "        [ 1.2703],\n",
      "        [ 0.6858],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1983: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1983: tensor([[ 1.0735],\n",
      "        [ 0.1827],\n",
      "        [-1.8811],\n",
      "        [ 1.2681],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1984: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1984: tensor([[ 1.0738],\n",
      "        [ 0.1837],\n",
      "        [-1.8794],\n",
      "        [ 1.2703],\n",
      "        [ 0.6858],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1985: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1985: tensor([[ 1.0735],\n",
      "        [ 0.1827],\n",
      "        [-1.8811],\n",
      "        [ 1.2681],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1986: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1986: tensor([[ 1.0738],\n",
      "        [ 0.1837],\n",
      "        [-1.8794],\n",
      "        [ 1.2703],\n",
      "        [ 0.6858],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1987: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1987: tensor([[ 1.0735],\n",
      "        [ 0.1826],\n",
      "        [-1.8811],\n",
      "        [ 1.2682],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1988: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1988: tensor([[ 1.0738],\n",
      "        [ 0.1836],\n",
      "        [-1.8794],\n",
      "        [ 1.2704],\n",
      "        [ 0.6858],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1989: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1989: tensor([[ 1.0735],\n",
      "        [ 0.1826],\n",
      "        [-1.8812],\n",
      "        [ 1.2682],\n",
      "        [ 0.6834],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1990: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1990: tensor([[ 1.0738],\n",
      "        [ 0.1836],\n",
      "        [-1.8794],\n",
      "        [ 1.2704],\n",
      "        [ 0.6858],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1991: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1991: tensor([[ 1.0735],\n",
      "        [ 0.1826],\n",
      "        [-1.8812],\n",
      "        [ 1.2682],\n",
      "        [ 0.6835],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 1992: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1992: tensor([[ 1.0738],\n",
      "        [ 0.1836],\n",
      "        [-1.8795],\n",
      "        [ 1.2704],\n",
      "        [ 0.6858],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1993: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1993: tensor([[ 1.0735],\n",
      "        [ 0.1826],\n",
      "        [-1.8812],\n",
      "        [ 1.2682],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 1994: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1994: tensor([[ 1.0738],\n",
      "        [ 0.1836],\n",
      "        [-1.8795],\n",
      "        [ 1.2704],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1995: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1995: tensor([[ 1.0735],\n",
      "        [ 0.1825],\n",
      "        [-1.8813],\n",
      "        [ 1.2682],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 1996: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1996: tensor([[ 1.0738],\n",
      "        [ 0.1835],\n",
      "        [-1.8795],\n",
      "        [ 1.2705],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1997: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1997: tensor([[ 1.0735],\n",
      "        [ 0.1825],\n",
      "        [-1.8813],\n",
      "        [ 1.2683],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 1998: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 1998: tensor([[ 1.0738],\n",
      "        [ 0.1835],\n",
      "        [-1.8796],\n",
      "        [ 1.2705],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 1999: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 1999: tensor([[ 1.0735],\n",
      "        [ 0.1825],\n",
      "        [-1.8813],\n",
      "        [ 1.2683],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2000: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2000: tensor([[ 1.0738],\n",
      "        [ 0.1835],\n",
      "        [-1.8796],\n",
      "        [ 1.2705],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2001: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2001: tensor([[ 1.0735],\n",
      "        [ 0.1824],\n",
      "        [-1.8813],\n",
      "        [ 1.2683],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2002: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2002: tensor([[ 1.0738],\n",
      "        [ 0.1834],\n",
      "        [-1.8796],\n",
      "        [ 1.2705],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2003: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2003: tensor([[ 1.0735],\n",
      "        [ 0.1824],\n",
      "        [-1.8814],\n",
      "        [ 1.2683],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2004: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2004: tensor([[ 1.0738],\n",
      "        [ 0.1834],\n",
      "        [-1.8797],\n",
      "        [ 1.2705],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2005: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2005: tensor([[ 1.0735],\n",
      "        [ 0.1824],\n",
      "        [-1.8814],\n",
      "        [ 1.2684],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2006: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2006: tensor([[ 1.0738],\n",
      "        [ 0.1834],\n",
      "        [-1.8797],\n",
      "        [ 1.2706],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2007: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2007: tensor([[ 1.0735],\n",
      "        [ 0.1824],\n",
      "        [-1.8814],\n",
      "        [ 1.2684],\n",
      "        [ 0.6835],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2008: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2008: tensor([[ 1.0738],\n",
      "        [ 0.1834],\n",
      "        [-1.8797],\n",
      "        [ 1.2706],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2009: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2009: tensor([[ 1.0735],\n",
      "        [ 0.1823],\n",
      "        [-1.8815],\n",
      "        [ 1.2684],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2010: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2010: tensor([[ 1.0738],\n",
      "        [ 0.1833],\n",
      "        [-1.8797],\n",
      "        [ 1.2706],\n",
      "        [ 0.6859],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2011: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2011: tensor([[ 1.0735],\n",
      "        [ 0.1823],\n",
      "        [-1.8815],\n",
      "        [ 1.2684],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2012: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2012: tensor([[ 1.0738],\n",
      "        [ 0.1833],\n",
      "        [-1.8798],\n",
      "        [ 1.2706],\n",
      "        [ 0.6860],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2013: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2013: tensor([[ 1.0735],\n",
      "        [ 0.1823],\n",
      "        [-1.8815],\n",
      "        [ 1.2684],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2014: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2014: tensor([[ 1.0738],\n",
      "        [ 0.1833],\n",
      "        [-1.8798],\n",
      "        [ 1.2707],\n",
      "        [ 0.6860],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2015: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2015: tensor([[ 1.0735],\n",
      "        [ 0.1822],\n",
      "        [-1.8816],\n",
      "        [ 1.2685],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2016: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2016: tensor([[ 1.0738],\n",
      "        [ 0.1833],\n",
      "        [-1.8798],\n",
      "        [ 1.2707],\n",
      "        [ 0.6860],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2017: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2017: tensor([[ 1.0735],\n",
      "        [ 0.1822],\n",
      "        [-1.8816],\n",
      "        [ 1.2685],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2018: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2018: tensor([[ 1.0738],\n",
      "        [ 0.1832],\n",
      "        [-1.8799],\n",
      "        [ 1.2707],\n",
      "        [ 0.6860],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2019: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2019: tensor([[ 1.0735],\n",
      "        [ 0.1822],\n",
      "        [-1.8816],\n",
      "        [ 1.2685],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2020: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2020: tensor([[ 1.0738],\n",
      "        [ 0.1832],\n",
      "        [-1.8799],\n",
      "        [ 1.2707],\n",
      "        [ 0.6860],\n",
      "        [-0.2865]], requires_grad=True)\n",
      "poly train loss at 2021: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2021: tensor([[ 1.0735],\n",
      "        [ 0.1822],\n",
      "        [-1.8816],\n",
      "        [ 1.2685],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2022: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2022: tensor([[ 1.0738],\n",
      "        [ 0.1832],\n",
      "        [-1.8799],\n",
      "        [ 1.2707],\n",
      "        [ 0.6860],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2023: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2023: tensor([[ 1.0735],\n",
      "        [ 0.1821],\n",
      "        [-1.8817],\n",
      "        [ 1.2685],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2024: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2024: tensor([[ 1.0738],\n",
      "        [ 0.1831],\n",
      "        [-1.8800],\n",
      "        [ 1.2708],\n",
      "        [ 0.6860],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2025: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2025: tensor([[ 1.0735],\n",
      "        [ 0.1821],\n",
      "        [-1.8817],\n",
      "        [ 1.2686],\n",
      "        [ 0.6836],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2026: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2026: tensor([[ 1.0738],\n",
      "        [ 0.1831],\n",
      "        [-1.8800],\n",
      "        [ 1.2708],\n",
      "        [ 0.6860],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2027: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2027: tensor([[ 1.0735],\n",
      "        [ 0.1821],\n",
      "        [-1.8817],\n",
      "        [ 1.2686],\n",
      "        [ 0.6837],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2028: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2028: tensor([[ 1.0738],\n",
      "        [ 0.1831],\n",
      "        [-1.8800],\n",
      "        [ 1.2708],\n",
      "        [ 0.6860],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2029: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2029: tensor([[ 1.0735],\n",
      "        [ 0.1821],\n",
      "        [-1.8818],\n",
      "        [ 1.2686],\n",
      "        [ 0.6837],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2030: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2030: tensor([[ 1.0738],\n",
      "        [ 0.1831],\n",
      "        [-1.8800],\n",
      "        [ 1.2708],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2031: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2031: tensor([[ 1.0735],\n",
      "        [ 0.1820],\n",
      "        [-1.8818],\n",
      "        [ 1.2686],\n",
      "        [ 0.6837],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 2032: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2032: tensor([[ 1.0738],\n",
      "        [ 0.1830],\n",
      "        [-1.8801],\n",
      "        [ 1.2708],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2033: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2033: tensor([[ 1.0735],\n",
      "        [ 0.1820],\n",
      "        [-1.8818],\n",
      "        [ 1.2687],\n",
      "        [ 0.6837],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2034: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2034: tensor([[ 1.0738],\n",
      "        [ 0.1830],\n",
      "        [-1.8801],\n",
      "        [ 1.2709],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2035: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2035: tensor([[ 1.0735],\n",
      "        [ 0.1820],\n",
      "        [-1.8819],\n",
      "        [ 1.2687],\n",
      "        [ 0.6837],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2036: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2036: tensor([[ 1.0738],\n",
      "        [ 0.1830],\n",
      "        [-1.8801],\n",
      "        [ 1.2709],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2037: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2037: tensor([[ 1.0735],\n",
      "        [ 0.1819],\n",
      "        [-1.8819],\n",
      "        [ 1.2687],\n",
      "        [ 0.6837],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2038: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2038: tensor([[ 1.0738],\n",
      "        [ 0.1830],\n",
      "        [-1.8802],\n",
      "        [ 1.2709],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2039: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2039: tensor([[ 1.0735],\n",
      "        [ 0.1819],\n",
      "        [-1.8819],\n",
      "        [ 1.2687],\n",
      "        [ 0.6837],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2040: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2040: tensor([[ 1.0738],\n",
      "        [ 0.1829],\n",
      "        [-1.8802],\n",
      "        [ 1.2709],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2041: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2041: tensor([[ 1.0735],\n",
      "        [ 0.1819],\n",
      "        [-1.8819],\n",
      "        [ 1.2687],\n",
      "        [ 0.6837],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2042: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2042: tensor([[ 1.0739],\n",
      "        [ 0.1829],\n",
      "        [-1.8802],\n",
      "        [ 1.2710],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2043: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2043: tensor([[ 1.0735],\n",
      "        [ 0.1819],\n",
      "        [-1.8820],\n",
      "        [ 1.2688],\n",
      "        [ 0.6837],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2044: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2044: tensor([[ 1.0739],\n",
      "        [ 0.1829],\n",
      "        [-1.8803],\n",
      "        [ 1.2710],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2045: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2045: tensor([[ 1.0735],\n",
      "        [ 0.1818],\n",
      "        [-1.8820],\n",
      "        [ 1.2688],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2046: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2046: tensor([[ 1.0739],\n",
      "        [ 0.1828],\n",
      "        [-1.8803],\n",
      "        [ 1.2710],\n",
      "        [ 0.6861],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2047: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2047: tensor([[ 1.0735],\n",
      "        [ 0.1818],\n",
      "        [-1.8820],\n",
      "        [ 1.2688],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2048: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2048: tensor([[ 1.0739],\n",
      "        [ 0.1828],\n",
      "        [-1.8803],\n",
      "        [ 1.2710],\n",
      "        [ 0.6862],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2049: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2049: tensor([[ 1.0735],\n",
      "        [ 0.1818],\n",
      "        [-1.8821],\n",
      "        [ 1.2688],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2050: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2050: tensor([[ 1.0739],\n",
      "        [ 0.1828],\n",
      "        [-1.8803],\n",
      "        [ 1.2710],\n",
      "        [ 0.6862],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2051: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2051: tensor([[ 1.0735],\n",
      "        [ 0.1818],\n",
      "        [-1.8821],\n",
      "        [ 1.2689],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2052: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2052: tensor([[ 1.0739],\n",
      "        [ 0.1828],\n",
      "        [-1.8804],\n",
      "        [ 1.2711],\n",
      "        [ 0.6862],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2053: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2053: tensor([[ 1.0735],\n",
      "        [ 0.1817],\n",
      "        [-1.8821],\n",
      "        [ 1.2689],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2054: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2054: tensor([[ 1.0739],\n",
      "        [ 0.1827],\n",
      "        [-1.8804],\n",
      "        [ 1.2711],\n",
      "        [ 0.6862],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2055: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2055: tensor([[ 1.0735],\n",
      "        [ 0.1817],\n",
      "        [-1.8821],\n",
      "        [ 1.2689],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2056: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2056: tensor([[ 1.0739],\n",
      "        [ 0.1827],\n",
      "        [-1.8804],\n",
      "        [ 1.2711],\n",
      "        [ 0.6862],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2057: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2057: tensor([[ 1.0735],\n",
      "        [ 0.1817],\n",
      "        [-1.8822],\n",
      "        [ 1.2689],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2058: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2058: tensor([[ 1.0739],\n",
      "        [ 0.1827],\n",
      "        [-1.8805],\n",
      "        [ 1.2711],\n",
      "        [ 0.6862],\n",
      "        [-0.2866]], requires_grad=True)\n",
      "poly train loss at 2059: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2059: tensor([[ 1.0735],\n",
      "        [ 0.1816],\n",
      "        [-1.8822],\n",
      "        [ 1.2689],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2060: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2060: tensor([[ 1.0739],\n",
      "        [ 0.1826],\n",
      "        [-1.8805],\n",
      "        [ 1.2712],\n",
      "        [ 0.6862],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2061: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2061: tensor([[ 1.0735],\n",
      "        [ 0.1816],\n",
      "        [-1.8822],\n",
      "        [ 1.2690],\n",
      "        [ 0.6838],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2062: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2062: tensor([[ 1.0739],\n",
      "        [ 0.1826],\n",
      "        [-1.8805],\n",
      "        [ 1.2712],\n",
      "        [ 0.6862],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2063: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2063: tensor([[ 1.0735],\n",
      "        [ 0.1816],\n",
      "        [-1.8823],\n",
      "        [ 1.2690],\n",
      "        [ 0.6839],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2064: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2064: tensor([[ 1.0739],\n",
      "        [ 0.1826],\n",
      "        [-1.8806],\n",
      "        [ 1.2712],\n",
      "        [ 0.6862],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2065: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2065: tensor([[ 1.0735],\n",
      "        [ 0.1816],\n",
      "        [-1.8823],\n",
      "        [ 1.2690],\n",
      "        [ 0.6839],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2066: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2066: tensor([[ 1.0739],\n",
      "        [ 0.1826],\n",
      "        [-1.8806],\n",
      "        [ 1.2712],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2067: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2067: tensor([[ 1.0735],\n",
      "        [ 0.1815],\n",
      "        [-1.8823],\n",
      "        [ 1.2690],\n",
      "        [ 0.6839],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2068: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2068: tensor([[ 1.0739],\n",
      "        [ 0.1825],\n",
      "        [-1.8806],\n",
      "        [ 1.2712],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2069: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2069: tensor([[ 1.0735],\n",
      "        [ 0.1815],\n",
      "        [-1.8824],\n",
      "        [ 1.2691],\n",
      "        [ 0.6839],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2070: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2070: tensor([[ 1.0739],\n",
      "        [ 0.1825],\n",
      "        [-1.8806],\n",
      "        [ 1.2713],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2071: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2071: tensor([[ 1.0735],\n",
      "        [ 0.1815],\n",
      "        [-1.8824],\n",
      "        [ 1.2691],\n",
      "        [ 0.6839],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 2072: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2072: tensor([[ 1.0739],\n",
      "        [ 0.1825],\n",
      "        [-1.8807],\n",
      "        [ 1.2713],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2073: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2073: tensor([[ 1.0735],\n",
      "        [ 0.1814],\n",
      "        [-1.8824],\n",
      "        [ 1.2691],\n",
      "        [ 0.6839],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2074: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2074: tensor([[ 1.0739],\n",
      "        [ 0.1825],\n",
      "        [-1.8807],\n",
      "        [ 1.2713],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2075: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2075: tensor([[ 1.0735],\n",
      "        [ 0.1814],\n",
      "        [-1.8824],\n",
      "        [ 1.2691],\n",
      "        [ 0.6839],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2076: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2076: tensor([[ 1.0739],\n",
      "        [ 0.1824],\n",
      "        [-1.8807],\n",
      "        [ 1.2713],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2077: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2077: tensor([[ 1.0735],\n",
      "        [ 0.1814],\n",
      "        [-1.8825],\n",
      "        [ 1.2691],\n",
      "        [ 0.6839],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2078: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2078: tensor([[ 1.0739],\n",
      "        [ 0.1824],\n",
      "        [-1.8808],\n",
      "        [ 1.2714],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2079: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2079: tensor([[ 1.0735],\n",
      "        [ 0.1814],\n",
      "        [-1.8825],\n",
      "        [ 1.2692],\n",
      "        [ 0.6839],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2080: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2080: tensor([[ 1.0739],\n",
      "        [ 0.1824],\n",
      "        [-1.8808],\n",
      "        [ 1.2714],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2081: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2081: tensor([[ 1.0735],\n",
      "        [ 0.1813],\n",
      "        [-1.8825],\n",
      "        [ 1.2692],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2082: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2082: tensor([[ 1.0739],\n",
      "        [ 0.1823],\n",
      "        [-1.8808],\n",
      "        [ 1.2714],\n",
      "        [ 0.6863],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2083: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2083: tensor([[ 1.0735],\n",
      "        [ 0.1813],\n",
      "        [-1.8826],\n",
      "        [ 1.2692],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2084: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2084: tensor([[ 1.0739],\n",
      "        [ 0.1823],\n",
      "        [-1.8808],\n",
      "        [ 1.2714],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2085: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2085: tensor([[ 1.0735],\n",
      "        [ 0.1813],\n",
      "        [-1.8826],\n",
      "        [ 1.2692],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2086: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2086: tensor([[ 1.0739],\n",
      "        [ 0.1823],\n",
      "        [-1.8809],\n",
      "        [ 1.2714],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2087: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2087: tensor([[ 1.0735],\n",
      "        [ 0.1813],\n",
      "        [-1.8826],\n",
      "        [ 1.2692],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2088: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2088: tensor([[ 1.0739],\n",
      "        [ 0.1823],\n",
      "        [-1.8809],\n",
      "        [ 1.2715],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2089: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2089: tensor([[ 1.0735],\n",
      "        [ 0.1812],\n",
      "        [-1.8827],\n",
      "        [ 1.2693],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2090: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2090: tensor([[ 1.0739],\n",
      "        [ 0.1822],\n",
      "        [-1.8809],\n",
      "        [ 1.2715],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2091: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2091: tensor([[ 1.0735],\n",
      "        [ 0.1812],\n",
      "        [-1.8827],\n",
      "        [ 1.2693],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2092: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2092: tensor([[ 1.0739],\n",
      "        [ 0.1822],\n",
      "        [-1.8810],\n",
      "        [ 1.2715],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2093: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2093: tensor([[ 1.0735],\n",
      "        [ 0.1812],\n",
      "        [-1.8827],\n",
      "        [ 1.2693],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2094: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2094: tensor([[ 1.0739],\n",
      "        [ 0.1822],\n",
      "        [-1.8810],\n",
      "        [ 1.2715],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2095: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2095: tensor([[ 1.0735],\n",
      "        [ 0.1811],\n",
      "        [-1.8827],\n",
      "        [ 1.2693],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2096: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2096: tensor([[ 1.0739],\n",
      "        [ 0.1821],\n",
      "        [-1.8810],\n",
      "        [ 1.2715],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2097: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2097: tensor([[ 1.0735],\n",
      "        [ 0.1811],\n",
      "        [-1.8828],\n",
      "        [ 1.2694],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2098: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2098: tensor([[ 1.0739],\n",
      "        [ 0.1821],\n",
      "        [-1.8811],\n",
      "        [ 1.2716],\n",
      "        [ 0.6864],\n",
      "        [-0.2867]], requires_grad=True)\n",
      "poly train loss at 2099: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2099: tensor([[ 1.0735],\n",
      "        [ 0.1811],\n",
      "        [-1.8828],\n",
      "        [ 1.2694],\n",
      "        [ 0.6840],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2100: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2100: tensor([[ 1.0739],\n",
      "        [ 0.1821],\n",
      "        [-1.8811],\n",
      "        [ 1.2716],\n",
      "        [ 0.6864],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2101: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2101: tensor([[ 1.0735],\n",
      "        [ 0.1811],\n",
      "        [-1.8828],\n",
      "        [ 1.2694],\n",
      "        [ 0.6841],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2102: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2102: tensor([[ 1.0739],\n",
      "        [ 0.1821],\n",
      "        [-1.8811],\n",
      "        [ 1.2716],\n",
      "        [ 0.6864],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2103: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2103: tensor([[ 1.0735],\n",
      "        [ 0.1810],\n",
      "        [-1.8829],\n",
      "        [ 1.2694],\n",
      "        [ 0.6841],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2104: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2104: tensor([[ 1.0739],\n",
      "        [ 0.1820],\n",
      "        [-1.8811],\n",
      "        [ 1.2716],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2105: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2105: tensor([[ 1.0735],\n",
      "        [ 0.1810],\n",
      "        [-1.8829],\n",
      "        [ 1.2694],\n",
      "        [ 0.6841],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2106: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2106: tensor([[ 1.0739],\n",
      "        [ 0.1820],\n",
      "        [-1.8812],\n",
      "        [ 1.2717],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2107: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2107: tensor([[ 1.0735],\n",
      "        [ 0.1810],\n",
      "        [-1.8829],\n",
      "        [ 1.2695],\n",
      "        [ 0.6841],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2108: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2108: tensor([[ 1.0739],\n",
      "        [ 0.1820],\n",
      "        [-1.8812],\n",
      "        [ 1.2717],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2109: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2109: tensor([[ 1.0735],\n",
      "        [ 0.1810],\n",
      "        [-1.8829],\n",
      "        [ 1.2695],\n",
      "        [ 0.6841],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2110: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2110: tensor([[ 1.0739],\n",
      "        [ 0.1820],\n",
      "        [-1.8812],\n",
      "        [ 1.2717],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2111: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2111: tensor([[ 1.0735],\n",
      "        [ 0.1809],\n",
      "        [-1.8830],\n",
      "        [ 1.2695],\n",
      "        [ 0.6841],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 2112: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2112: tensor([[ 1.0739],\n",
      "        [ 0.1819],\n",
      "        [-1.8813],\n",
      "        [ 1.2717],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2113: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2113: tensor([[ 1.0735],\n",
      "        [ 0.1809],\n",
      "        [-1.8830],\n",
      "        [ 1.2695],\n",
      "        [ 0.6841],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2114: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2114: tensor([[ 1.0739],\n",
      "        [ 0.1819],\n",
      "        [-1.8813],\n",
      "        [ 1.2717],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2115: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2115: tensor([[ 1.0735],\n",
      "        [ 0.1809],\n",
      "        [-1.8830],\n",
      "        [ 1.2695],\n",
      "        [ 0.6841],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2116: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2116: tensor([[ 1.0739],\n",
      "        [ 0.1819],\n",
      "        [-1.8813],\n",
      "        [ 1.2718],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2117: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2117: tensor([[ 1.0735],\n",
      "        [ 0.1808],\n",
      "        [-1.8831],\n",
      "        [ 1.2696],\n",
      "        [ 0.6841],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2118: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2118: tensor([[ 1.0739],\n",
      "        [ 0.1818],\n",
      "        [-1.8813],\n",
      "        [ 1.2718],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2119: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2119: tensor([[ 1.0735],\n",
      "        [ 0.1808],\n",
      "        [-1.8831],\n",
      "        [ 1.2696],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2120: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2120: tensor([[ 1.0739],\n",
      "        [ 0.1818],\n",
      "        [-1.8814],\n",
      "        [ 1.2718],\n",
      "        [ 0.6865],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2121: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2121: tensor([[ 1.0735],\n",
      "        [ 0.1808],\n",
      "        [-1.8831],\n",
      "        [ 1.2696],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2122: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2122: tensor([[ 1.0739],\n",
      "        [ 0.1818],\n",
      "        [-1.8814],\n",
      "        [ 1.2718],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2123: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2123: tensor([[ 1.0735],\n",
      "        [ 0.1808],\n",
      "        [-1.8832],\n",
      "        [ 1.2696],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2124: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2124: tensor([[ 1.0739],\n",
      "        [ 0.1818],\n",
      "        [-1.8814],\n",
      "        [ 1.2718],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2125: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2125: tensor([[ 1.0735],\n",
      "        [ 0.1807],\n",
      "        [-1.8832],\n",
      "        [ 1.2697],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2126: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2126: tensor([[ 1.0739],\n",
      "        [ 0.1817],\n",
      "        [-1.8815],\n",
      "        [ 1.2719],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2127: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2127: tensor([[ 1.0735],\n",
      "        [ 0.1807],\n",
      "        [-1.8832],\n",
      "        [ 1.2697],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2128: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2128: tensor([[ 1.0739],\n",
      "        [ 0.1817],\n",
      "        [-1.8815],\n",
      "        [ 1.2719],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2129: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2129: tensor([[ 1.0735],\n",
      "        [ 0.1807],\n",
      "        [-1.8832],\n",
      "        [ 1.2697],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2130: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2130: tensor([[ 1.0739],\n",
      "        [ 0.1817],\n",
      "        [-1.8815],\n",
      "        [ 1.2719],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2131: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2131: tensor([[ 1.0735],\n",
      "        [ 0.1807],\n",
      "        [-1.8833],\n",
      "        [ 1.2697],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2132: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2132: tensor([[ 1.0739],\n",
      "        [ 0.1817],\n",
      "        [-1.8816],\n",
      "        [ 1.2719],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2133: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2133: tensor([[ 1.0735],\n",
      "        [ 0.1806],\n",
      "        [-1.8833],\n",
      "        [ 1.2697],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2134: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2134: tensor([[ 1.0739],\n",
      "        [ 0.1816],\n",
      "        [-1.8816],\n",
      "        [ 1.2720],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2135: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2135: tensor([[ 1.0735],\n",
      "        [ 0.1806],\n",
      "        [-1.8833],\n",
      "        [ 1.2698],\n",
      "        [ 0.6842],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2136: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2136: tensor([[ 1.0739],\n",
      "        [ 0.1816],\n",
      "        [-1.8816],\n",
      "        [ 1.2720],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2137: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2137: tensor([[ 1.0735],\n",
      "        [ 0.1806],\n",
      "        [-1.8834],\n",
      "        [ 1.2698],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2138: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2138: tensor([[ 1.0739],\n",
      "        [ 0.1816],\n",
      "        [-1.8816],\n",
      "        [ 1.2720],\n",
      "        [ 0.6866],\n",
      "        [-0.2868]], requires_grad=True)\n",
      "poly train loss at 2139: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2139: tensor([[ 1.0735],\n",
      "        [ 0.1805],\n",
      "        [-1.8834],\n",
      "        [ 1.2698],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2140: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2140: tensor([[ 1.0739],\n",
      "        [ 0.1815],\n",
      "        [-1.8817],\n",
      "        [ 1.2720],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2141: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2141: tensor([[ 1.0735],\n",
      "        [ 0.1805],\n",
      "        [-1.8834],\n",
      "        [ 1.2698],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2142: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2142: tensor([[ 1.0739],\n",
      "        [ 0.1815],\n",
      "        [-1.8817],\n",
      "        [ 1.2720],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2143: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2143: tensor([[ 1.0735],\n",
      "        [ 0.1805],\n",
      "        [-1.8834],\n",
      "        [ 1.2699],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2144: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2144: tensor([[ 1.0739],\n",
      "        [ 0.1815],\n",
      "        [-1.8817],\n",
      "        [ 1.2721],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2145: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2145: tensor([[ 1.0735],\n",
      "        [ 0.1805],\n",
      "        [-1.8835],\n",
      "        [ 1.2699],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2146: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2146: tensor([[ 1.0739],\n",
      "        [ 0.1815],\n",
      "        [-1.8818],\n",
      "        [ 1.2721],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2147: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2147: tensor([[ 1.0735],\n",
      "        [ 0.1804],\n",
      "        [-1.8835],\n",
      "        [ 1.2699],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2148: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2148: tensor([[ 1.0739],\n",
      "        [ 0.1814],\n",
      "        [-1.8818],\n",
      "        [ 1.2721],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2149: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2149: tensor([[ 1.0735],\n",
      "        [ 0.1804],\n",
      "        [-1.8835],\n",
      "        [ 1.2699],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2150: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2150: tensor([[ 1.0739],\n",
      "        [ 0.1814],\n",
      "        [-1.8818],\n",
      "        [ 1.2721],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2151: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2151: tensor([[ 1.0735],\n",
      "        [ 0.1804],\n",
      "        [-1.8836],\n",
      "        [ 1.2699],\n",
      "        [ 0.6843],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 2152: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2152: tensor([[ 1.0739],\n",
      "        [ 0.1814],\n",
      "        [-1.8818],\n",
      "        [ 1.2721],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2153: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2153: tensor([[ 1.0736],\n",
      "        [ 0.1804],\n",
      "        [-1.8836],\n",
      "        [ 1.2700],\n",
      "        [ 0.6843],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2154: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2154: tensor([[ 1.0739],\n",
      "        [ 0.1814],\n",
      "        [-1.8819],\n",
      "        [ 1.2722],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2155: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2155: tensor([[ 1.0736],\n",
      "        [ 0.1803],\n",
      "        [-1.8836],\n",
      "        [ 1.2700],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2156: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2156: tensor([[ 1.0739],\n",
      "        [ 0.1813],\n",
      "        [-1.8819],\n",
      "        [ 1.2722],\n",
      "        [ 0.6867],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2157: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2157: tensor([[ 1.0736],\n",
      "        [ 0.1803],\n",
      "        [-1.8837],\n",
      "        [ 1.2700],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2158: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2158: tensor([[ 1.0739],\n",
      "        [ 0.1813],\n",
      "        [-1.8819],\n",
      "        [ 1.2722],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2159: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2159: tensor([[ 1.0736],\n",
      "        [ 0.1803],\n",
      "        [-1.8837],\n",
      "        [ 1.2700],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2160: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2160: tensor([[ 1.0739],\n",
      "        [ 0.1813],\n",
      "        [-1.8820],\n",
      "        [ 1.2722],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2161: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2161: tensor([[ 1.0736],\n",
      "        [ 0.1802],\n",
      "        [-1.8837],\n",
      "        [ 1.2700],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2162: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2162: tensor([[ 1.0739],\n",
      "        [ 0.1813],\n",
      "        [-1.8820],\n",
      "        [ 1.2723],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2163: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2163: tensor([[ 1.0736],\n",
      "        [ 0.1802],\n",
      "        [-1.8837],\n",
      "        [ 1.2701],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2164: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2164: tensor([[ 1.0739],\n",
      "        [ 0.1812],\n",
      "        [-1.8820],\n",
      "        [ 1.2723],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2165: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2165: tensor([[ 1.0736],\n",
      "        [ 0.1802],\n",
      "        [-1.8838],\n",
      "        [ 1.2701],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2166: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2166: tensor([[ 1.0739],\n",
      "        [ 0.1812],\n",
      "        [-1.8821],\n",
      "        [ 1.2723],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2167: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2167: tensor([[ 1.0736],\n",
      "        [ 0.1802],\n",
      "        [-1.8838],\n",
      "        [ 1.2701],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2168: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2168: tensor([[ 1.0739],\n",
      "        [ 0.1812],\n",
      "        [-1.8821],\n",
      "        [ 1.2723],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2169: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2169: tensor([[ 1.0736],\n",
      "        [ 0.1801],\n",
      "        [-1.8838],\n",
      "        [ 1.2701],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2170: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2170: tensor([[ 1.0739],\n",
      "        [ 0.1811],\n",
      "        [-1.8821],\n",
      "        [ 1.2723],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2171: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2171: tensor([[ 1.0736],\n",
      "        [ 0.1801],\n",
      "        [-1.8839],\n",
      "        [ 1.2701],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2172: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2172: tensor([[ 1.0739],\n",
      "        [ 0.1811],\n",
      "        [-1.8821],\n",
      "        [ 1.2724],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2173: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2173: tensor([[ 1.0736],\n",
      "        [ 0.1801],\n",
      "        [-1.8839],\n",
      "        [ 1.2702],\n",
      "        [ 0.6844],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2174: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2174: tensor([[ 1.0739],\n",
      "        [ 0.1811],\n",
      "        [-1.8822],\n",
      "        [ 1.2724],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2175: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2175: tensor([[ 1.0736],\n",
      "        [ 0.1801],\n",
      "        [-1.8839],\n",
      "        [ 1.2702],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2176: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2176: tensor([[ 1.0739],\n",
      "        [ 0.1811],\n",
      "        [-1.8822],\n",
      "        [ 1.2724],\n",
      "        [ 0.6868],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2177: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2177: tensor([[ 1.0736],\n",
      "        [ 0.1800],\n",
      "        [-1.8839],\n",
      "        [ 1.2702],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2178: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2178: tensor([[ 1.0739],\n",
      "        [ 0.1810],\n",
      "        [-1.8822],\n",
      "        [ 1.2724],\n",
      "        [ 0.6869],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2179: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2179: tensor([[ 1.0736],\n",
      "        [ 0.1800],\n",
      "        [-1.8840],\n",
      "        [ 1.2702],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2180: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2180: tensor([[ 1.0739],\n",
      "        [ 0.1810],\n",
      "        [-1.8823],\n",
      "        [ 1.2724],\n",
      "        [ 0.6869],\n",
      "        [-0.2869]], requires_grad=True)\n",
      "poly train loss at 2181: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2181: tensor([[ 1.0736],\n",
      "        [ 0.1800],\n",
      "        [-1.8840],\n",
      "        [ 1.2703],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2182: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2182: tensor([[ 1.0739],\n",
      "        [ 0.1810],\n",
      "        [-1.8823],\n",
      "        [ 1.2725],\n",
      "        [ 0.6869],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2183: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2183: tensor([[ 1.0736],\n",
      "        [ 0.1799],\n",
      "        [-1.8840],\n",
      "        [ 1.2703],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2184: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2184: tensor([[ 1.0739],\n",
      "        [ 0.1810],\n",
      "        [-1.8823],\n",
      "        [ 1.2725],\n",
      "        [ 0.6869],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2185: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2185: tensor([[ 1.0736],\n",
      "        [ 0.1799],\n",
      "        [-1.8841],\n",
      "        [ 1.2703],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2186: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2186: tensor([[ 1.0739],\n",
      "        [ 0.1809],\n",
      "        [-1.8823],\n",
      "        [ 1.2725],\n",
      "        [ 0.6869],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2187: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2187: tensor([[ 1.0736],\n",
      "        [ 0.1799],\n",
      "        [-1.8841],\n",
      "        [ 1.2703],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2188: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2188: tensor([[ 1.0739],\n",
      "        [ 0.1809],\n",
      "        [-1.8824],\n",
      "        [ 1.2725],\n",
      "        [ 0.6869],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2189: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2189: tensor([[ 1.0736],\n",
      "        [ 0.1799],\n",
      "        [-1.8841],\n",
      "        [ 1.2703],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2190: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2190: tensor([[ 1.0739],\n",
      "        [ 0.1809],\n",
      "        [-1.8824],\n",
      "        [ 1.2726],\n",
      "        [ 0.6869],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2191: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2191: tensor([[ 1.0736],\n",
      "        [ 0.1798],\n",
      "        [-1.8842],\n",
      "        [ 1.2704],\n",
      "        [ 0.6845],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 2192: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2192: tensor([[ 1.0739],\n",
      "        [ 0.1808],\n",
      "        [-1.8824],\n",
      "        [ 1.2726],\n",
      "        [ 0.6869],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2193: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2193: tensor([[ 1.0736],\n",
      "        [ 0.1798],\n",
      "        [-1.8842],\n",
      "        [ 1.2704],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2194: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2194: tensor([[ 1.0739],\n",
      "        [ 0.1808],\n",
      "        [-1.8825],\n",
      "        [ 1.2726],\n",
      "        [ 0.6869],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2195: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2195: tensor([[ 1.0736],\n",
      "        [ 0.1798],\n",
      "        [-1.8842],\n",
      "        [ 1.2704],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2196: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2196: tensor([[ 1.0739],\n",
      "        [ 0.1808],\n",
      "        [-1.8825],\n",
      "        [ 1.2726],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2197: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2197: tensor([[ 1.0736],\n",
      "        [ 0.1798],\n",
      "        [-1.8842],\n",
      "        [ 1.2704],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2198: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2198: tensor([[ 1.0739],\n",
      "        [ 0.1808],\n",
      "        [-1.8825],\n",
      "        [ 1.2726],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2199: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2199: tensor([[ 1.0736],\n",
      "        [ 0.1797],\n",
      "        [-1.8843],\n",
      "        [ 1.2704],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2200: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2200: tensor([[ 1.0739],\n",
      "        [ 0.1807],\n",
      "        [-1.8826],\n",
      "        [ 1.2727],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2201: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2201: tensor([[ 1.0736],\n",
      "        [ 0.1797],\n",
      "        [-1.8843],\n",
      "        [ 1.2705],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2202: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2202: tensor([[ 1.0739],\n",
      "        [ 0.1807],\n",
      "        [-1.8826],\n",
      "        [ 1.2727],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2203: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2203: tensor([[ 1.0736],\n",
      "        [ 0.1797],\n",
      "        [-1.8843],\n",
      "        [ 1.2705],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2204: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2204: tensor([[ 1.0739],\n",
      "        [ 0.1807],\n",
      "        [-1.8826],\n",
      "        [ 1.2727],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2205: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2205: tensor([[ 1.0736],\n",
      "        [ 0.1797],\n",
      "        [-1.8844],\n",
      "        [ 1.2705],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2206: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2206: tensor([[ 1.0739],\n",
      "        [ 0.1807],\n",
      "        [-1.8826],\n",
      "        [ 1.2727],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2207: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2207: tensor([[ 1.0736],\n",
      "        [ 0.1796],\n",
      "        [-1.8844],\n",
      "        [ 1.2705],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2208: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2208: tensor([[ 1.0739],\n",
      "        [ 0.1806],\n",
      "        [-1.8827],\n",
      "        [ 1.2727],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2209: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2209: tensor([[ 1.0736],\n",
      "        [ 0.1796],\n",
      "        [-1.8844],\n",
      "        [ 1.2706],\n",
      "        [ 0.6846],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2210: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2210: tensor([[ 1.0739],\n",
      "        [ 0.1806],\n",
      "        [-1.8827],\n",
      "        [ 1.2728],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2211: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2211: tensor([[ 1.0736],\n",
      "        [ 0.1796],\n",
      "        [-1.8844],\n",
      "        [ 1.2706],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2212: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2212: tensor([[ 1.0739],\n",
      "        [ 0.1806],\n",
      "        [-1.8827],\n",
      "        [ 1.2728],\n",
      "        [ 0.6870],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2213: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2213: tensor([[ 1.0736],\n",
      "        [ 0.1795],\n",
      "        [-1.8845],\n",
      "        [ 1.2706],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2214: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2214: tensor([[ 1.0739],\n",
      "        [ 0.1805],\n",
      "        [-1.8828],\n",
      "        [ 1.2728],\n",
      "        [ 0.6871],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2215: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2215: tensor([[ 1.0736],\n",
      "        [ 0.1795],\n",
      "        [-1.8845],\n",
      "        [ 1.2706],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2216: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2216: tensor([[ 1.0739],\n",
      "        [ 0.1805],\n",
      "        [-1.8828],\n",
      "        [ 1.2728],\n",
      "        [ 0.6871],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2217: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2217: tensor([[ 1.0736],\n",
      "        [ 0.1795],\n",
      "        [-1.8845],\n",
      "        [ 1.2706],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2218: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2218: tensor([[ 1.0739],\n",
      "        [ 0.1805],\n",
      "        [-1.8828],\n",
      "        [ 1.2729],\n",
      "        [ 0.6871],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2219: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2219: tensor([[ 1.0736],\n",
      "        [ 0.1795],\n",
      "        [-1.8846],\n",
      "        [ 1.2707],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2220: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2220: tensor([[ 1.0739],\n",
      "        [ 0.1805],\n",
      "        [-1.8828],\n",
      "        [ 1.2729],\n",
      "        [ 0.6871],\n",
      "        [-0.2870]], requires_grad=True)\n",
      "poly train loss at 2221: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2221: tensor([[ 1.0736],\n",
      "        [ 0.1794],\n",
      "        [-1.8846],\n",
      "        [ 1.2707],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2222: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2222: tensor([[ 1.0739],\n",
      "        [ 0.1804],\n",
      "        [-1.8829],\n",
      "        [ 1.2729],\n",
      "        [ 0.6871],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2223: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2223: tensor([[ 1.0736],\n",
      "        [ 0.1794],\n",
      "        [-1.8846],\n",
      "        [ 1.2707],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2224: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2224: tensor([[ 1.0739],\n",
      "        [ 0.1804],\n",
      "        [-1.8829],\n",
      "        [ 1.2729],\n",
      "        [ 0.6871],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2225: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2225: tensor([[ 1.0736],\n",
      "        [ 0.1794],\n",
      "        [-1.8846],\n",
      "        [ 1.2707],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2226: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2226: tensor([[ 1.0739],\n",
      "        [ 0.1804],\n",
      "        [-1.8829],\n",
      "        [ 1.2729],\n",
      "        [ 0.6871],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2227: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2227: tensor([[ 1.0736],\n",
      "        [ 0.1794],\n",
      "        [-1.8847],\n",
      "        [ 1.2707],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2228: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2228: tensor([[ 1.0739],\n",
      "        [ 0.1804],\n",
      "        [-1.8830],\n",
      "        [ 1.2730],\n",
      "        [ 0.6871],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2229: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2229: tensor([[ 1.0736],\n",
      "        [ 0.1793],\n",
      "        [-1.8847],\n",
      "        [ 1.2708],\n",
      "        [ 0.6847],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2230: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2230: tensor([[ 1.0739],\n",
      "        [ 0.1803],\n",
      "        [-1.8830],\n",
      "        [ 1.2730],\n",
      "        [ 0.6871],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2231: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2231: tensor([[ 1.0736],\n",
      "        [ 0.1793],\n",
      "        [-1.8847],\n",
      "        [ 1.2708],\n",
      "        [ 0.6848],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 2232: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2232: tensor([[ 1.0739],\n",
      "        [ 0.1803],\n",
      "        [-1.8830],\n",
      "        [ 1.2730],\n",
      "        [ 0.6871],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2233: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2233: tensor([[ 1.0736],\n",
      "        [ 0.1793],\n",
      "        [-1.8848],\n",
      "        [ 1.2708],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2234: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2234: tensor([[ 1.0739],\n",
      "        [ 0.1803],\n",
      "        [-1.8830],\n",
      "        [ 1.2730],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2235: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2235: tensor([[ 1.0736],\n",
      "        [ 0.1792],\n",
      "        [-1.8848],\n",
      "        [ 1.2708],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2236: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2236: tensor([[ 1.0739],\n",
      "        [ 0.1803],\n",
      "        [-1.8831],\n",
      "        [ 1.2730],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2237: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2237: tensor([[ 1.0736],\n",
      "        [ 0.1792],\n",
      "        [-1.8848],\n",
      "        [ 1.2709],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2238: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2238: tensor([[ 1.0739],\n",
      "        [ 0.1802],\n",
      "        [-1.8831],\n",
      "        [ 1.2731],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2239: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2239: tensor([[ 1.0736],\n",
      "        [ 0.1792],\n",
      "        [-1.8848],\n",
      "        [ 1.2709],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2240: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2240: tensor([[ 1.0739],\n",
      "        [ 0.1802],\n",
      "        [-1.8831],\n",
      "        [ 1.2731],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2241: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2241: tensor([[ 1.0736],\n",
      "        [ 0.1792],\n",
      "        [-1.8849],\n",
      "        [ 1.2709],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2242: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2242: tensor([[ 1.0739],\n",
      "        [ 0.1802],\n",
      "        [-1.8832],\n",
      "        [ 1.2731],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2243: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2243: tensor([[ 1.0736],\n",
      "        [ 0.1791],\n",
      "        [-1.8849],\n",
      "        [ 1.2709],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2244: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2244: tensor([[ 1.0739],\n",
      "        [ 0.1801],\n",
      "        [-1.8832],\n",
      "        [ 1.2731],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2245: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2245: tensor([[ 1.0736],\n",
      "        [ 0.1791],\n",
      "        [-1.8849],\n",
      "        [ 1.2709],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2246: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2246: tensor([[ 1.0739],\n",
      "        [ 0.1801],\n",
      "        [-1.8832],\n",
      "        [ 1.2731],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2247: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2247: tensor([[ 1.0736],\n",
      "        [ 0.1791],\n",
      "        [-1.8850],\n",
      "        [ 1.2710],\n",
      "        [ 0.6848],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2248: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2248: tensor([[ 1.0739],\n",
      "        [ 0.1801],\n",
      "        [-1.8832],\n",
      "        [ 1.2732],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2249: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2249: tensor([[ 1.0736],\n",
      "        [ 0.1791],\n",
      "        [-1.8850],\n",
      "        [ 1.2710],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2250: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2250: tensor([[ 1.0739],\n",
      "        [ 0.1801],\n",
      "        [-1.8833],\n",
      "        [ 1.2732],\n",
      "        [ 0.6872],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2251: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2251: tensor([[ 1.0736],\n",
      "        [ 0.1790],\n",
      "        [-1.8850],\n",
      "        [ 1.2710],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2252: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2252: tensor([[ 1.0739],\n",
      "        [ 0.1800],\n",
      "        [-1.8833],\n",
      "        [ 1.2732],\n",
      "        [ 0.6873],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2253: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2253: tensor([[ 1.0736],\n",
      "        [ 0.1790],\n",
      "        [-1.8850],\n",
      "        [ 1.2710],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2254: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2254: tensor([[ 1.0739],\n",
      "        [ 0.1800],\n",
      "        [-1.8833],\n",
      "        [ 1.2732],\n",
      "        [ 0.6873],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2255: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2255: tensor([[ 1.0736],\n",
      "        [ 0.1790],\n",
      "        [-1.8851],\n",
      "        [ 1.2710],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2256: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2256: tensor([[ 1.0739],\n",
      "        [ 0.1800],\n",
      "        [-1.8834],\n",
      "        [ 1.2733],\n",
      "        [ 0.6873],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2257: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2257: tensor([[ 1.0736],\n",
      "        [ 0.1790],\n",
      "        [-1.8851],\n",
      "        [ 1.2711],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2258: tensor(0.3282, grad_fn=<DivBackward0>)\n",
      "poly w at 2258: tensor([[ 1.0739],\n",
      "        [ 0.1800],\n",
      "        [-1.8834],\n",
      "        [ 1.2733],\n",
      "        [ 0.6873],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2259: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2259: tensor([[ 1.0736],\n",
      "        [ 0.1789],\n",
      "        [-1.8851],\n",
      "        [ 1.2711],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2260: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2260: tensor([[ 1.0739],\n",
      "        [ 0.1799],\n",
      "        [-1.8834],\n",
      "        [ 1.2733],\n",
      "        [ 0.6873],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2261: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2261: tensor([[ 1.0736],\n",
      "        [ 0.1789],\n",
      "        [-1.8852],\n",
      "        [ 1.2711],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2262: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2262: tensor([[ 1.0739],\n",
      "        [ 0.1799],\n",
      "        [-1.8834],\n",
      "        [ 1.2733],\n",
      "        [ 0.6873],\n",
      "        [-0.2871]], requires_grad=True)\n",
      "poly train loss at 2263: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2263: tensor([[ 1.0736],\n",
      "        [ 0.1789],\n",
      "        [-1.8852],\n",
      "        [ 1.2711],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2264: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2264: tensor([[ 1.0739],\n",
      "        [ 0.1799],\n",
      "        [-1.8835],\n",
      "        [ 1.2733],\n",
      "        [ 0.6873],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2265: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2265: tensor([[ 1.0736],\n",
      "        [ 0.1788],\n",
      "        [-1.8852],\n",
      "        [ 1.2711],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2266: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2266: tensor([[ 1.0739],\n",
      "        [ 0.1799],\n",
      "        [-1.8835],\n",
      "        [ 1.2734],\n",
      "        [ 0.6873],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2267: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2267: tensor([[ 1.0736],\n",
      "        [ 0.1788],\n",
      "        [-1.8852],\n",
      "        [ 1.2712],\n",
      "        [ 0.6849],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2268: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2268: tensor([[ 1.0739],\n",
      "        [ 0.1798],\n",
      "        [-1.8835],\n",
      "        [ 1.2734],\n",
      "        [ 0.6873],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2269: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2269: tensor([[ 1.0736],\n",
      "        [ 0.1788],\n",
      "        [-1.8853],\n",
      "        [ 1.2712],\n",
      "        [ 0.6850],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2270: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2270: tensor([[ 1.0739],\n",
      "        [ 0.1798],\n",
      "        [-1.8836],\n",
      "        [ 1.2734],\n",
      "        [ 0.6873],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2271: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2271: tensor([[ 1.0736],\n",
      "        [ 0.1788],\n",
      "        [-1.8853],\n",
      "        [ 1.2712],\n",
      "        [ 0.6850],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2272: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2272: tensor([[ 1.0739],\n",
      "        [ 0.1798],\n",
      "        [-1.8836],\n",
      "        [ 1.2734],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2273: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2273: tensor([[ 1.0736],\n",
      "        [ 0.1787],\n",
      "        [-1.8853],\n",
      "        [ 1.2712],\n",
      "        [ 0.6850],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 2274: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2274: tensor([[ 1.0739],\n",
      "        [ 0.1797],\n",
      "        [-1.8836],\n",
      "        [ 1.2734],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2275: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2275: tensor([[ 1.0736],\n",
      "        [ 0.1787],\n",
      "        [-1.8854],\n",
      "        [ 1.2712],\n",
      "        [ 0.6850],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2276: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2276: tensor([[ 1.0739],\n",
      "        [ 0.1797],\n",
      "        [-1.8836],\n",
      "        [ 1.2735],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2277: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2277: tensor([[ 1.0736],\n",
      "        [ 0.1787],\n",
      "        [-1.8854],\n",
      "        [ 1.2713],\n",
      "        [ 0.6850],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2278: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2278: tensor([[ 1.0739],\n",
      "        [ 0.1797],\n",
      "        [-1.8837],\n",
      "        [ 1.2735],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2279: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2279: tensor([[ 1.0736],\n",
      "        [ 0.1787],\n",
      "        [-1.8854],\n",
      "        [ 1.2713],\n",
      "        [ 0.6850],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2280: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2280: tensor([[ 1.0739],\n",
      "        [ 0.1797],\n",
      "        [-1.8837],\n",
      "        [ 1.2735],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2281: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2281: tensor([[ 1.0736],\n",
      "        [ 0.1786],\n",
      "        [-1.8854],\n",
      "        [ 1.2713],\n",
      "        [ 0.6850],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2282: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2282: tensor([[ 1.0739],\n",
      "        [ 0.1796],\n",
      "        [-1.8837],\n",
      "        [ 1.2735],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2283: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2283: tensor([[ 1.0736],\n",
      "        [ 0.1786],\n",
      "        [-1.8855],\n",
      "        [ 1.2713],\n",
      "        [ 0.6850],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2284: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2284: tensor([[ 1.0739],\n",
      "        [ 0.1796],\n",
      "        [-1.8838],\n",
      "        [ 1.2735],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2285: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2285: tensor([[ 1.0736],\n",
      "        [ 0.1786],\n",
      "        [-1.8855],\n",
      "        [ 1.2713],\n",
      "        [ 0.6850],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2286: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2286: tensor([[ 1.0739],\n",
      "        [ 0.1796],\n",
      "        [-1.8838],\n",
      "        [ 1.2736],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2287: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2287: tensor([[ 1.0736],\n",
      "        [ 0.1786],\n",
      "        [-1.8855],\n",
      "        [ 1.2714],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2288: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2288: tensor([[ 1.0739],\n",
      "        [ 0.1796],\n",
      "        [-1.8838],\n",
      "        [ 1.2736],\n",
      "        [ 0.6874],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2289: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2289: tensor([[ 1.0736],\n",
      "        [ 0.1785],\n",
      "        [-1.8856],\n",
      "        [ 1.2714],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2290: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2290: tensor([[ 1.0739],\n",
      "        [ 0.1795],\n",
      "        [-1.8838],\n",
      "        [ 1.2736],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2291: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2291: tensor([[ 1.0736],\n",
      "        [ 0.1785],\n",
      "        [-1.8856],\n",
      "        [ 1.2714],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2292: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2292: tensor([[ 1.0739],\n",
      "        [ 0.1795],\n",
      "        [-1.8839],\n",
      "        [ 1.2736],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2293: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2293: tensor([[ 1.0736],\n",
      "        [ 0.1785],\n",
      "        [-1.8856],\n",
      "        [ 1.2714],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2294: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2294: tensor([[ 1.0739],\n",
      "        [ 0.1795],\n",
      "        [-1.8839],\n",
      "        [ 1.2736],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2295: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2295: tensor([[ 1.0736],\n",
      "        [ 0.1785],\n",
      "        [-1.8856],\n",
      "        [ 1.2715],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2296: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2296: tensor([[ 1.0739],\n",
      "        [ 0.1795],\n",
      "        [-1.8839],\n",
      "        [ 1.2737],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2297: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2297: tensor([[ 1.0736],\n",
      "        [ 0.1784],\n",
      "        [-1.8857],\n",
      "        [ 1.2715],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2298: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2298: tensor([[ 1.0739],\n",
      "        [ 0.1794],\n",
      "        [-1.8840],\n",
      "        [ 1.2737],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2299: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2299: tensor([[ 1.0736],\n",
      "        [ 0.1784],\n",
      "        [-1.8857],\n",
      "        [ 1.2715],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2300: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2300: tensor([[ 1.0739],\n",
      "        [ 0.1794],\n",
      "        [-1.8840],\n",
      "        [ 1.2737],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2301: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2301: tensor([[ 1.0736],\n",
      "        [ 0.1784],\n",
      "        [-1.8857],\n",
      "        [ 1.2715],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2302: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2302: tensor([[ 1.0739],\n",
      "        [ 0.1794],\n",
      "        [-1.8840],\n",
      "        [ 1.2737],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2303: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2303: tensor([[ 1.0736],\n",
      "        [ 0.1783],\n",
      "        [-1.8858],\n",
      "        [ 1.2715],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2304: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2304: tensor([[ 1.0739],\n",
      "        [ 0.1794],\n",
      "        [-1.8840],\n",
      "        [ 1.2737],\n",
      "        [ 0.6875],\n",
      "        [-0.2872]], requires_grad=True)\n",
      "poly train loss at 2305: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2305: tensor([[ 1.0736],\n",
      "        [ 0.1783],\n",
      "        [-1.8858],\n",
      "        [ 1.2716],\n",
      "        [ 0.6851],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2306: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2306: tensor([[ 1.0739],\n",
      "        [ 0.1793],\n",
      "        [-1.8841],\n",
      "        [ 1.2738],\n",
      "        [ 0.6875],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2307: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2307: tensor([[ 1.0736],\n",
      "        [ 0.1783],\n",
      "        [-1.8858],\n",
      "        [ 1.2716],\n",
      "        [ 0.6852],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2308: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2308: tensor([[ 1.0739],\n",
      "        [ 0.1793],\n",
      "        [-1.8841],\n",
      "        [ 1.2738],\n",
      "        [ 0.6875],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2309: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2309: tensor([[ 1.0736],\n",
      "        [ 0.1783],\n",
      "        [-1.8858],\n",
      "        [ 1.2716],\n",
      "        [ 0.6852],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2310: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2310: tensor([[ 1.0739],\n",
      "        [ 0.1793],\n",
      "        [-1.8841],\n",
      "        [ 1.2738],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2311: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2311: tensor([[ 1.0736],\n",
      "        [ 0.1782],\n",
      "        [-1.8859],\n",
      "        [ 1.2716],\n",
      "        [ 0.6852],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2312: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2312: tensor([[ 1.0739],\n",
      "        [ 0.1793],\n",
      "        [-1.8842],\n",
      "        [ 1.2738],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2313: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2313: tensor([[ 1.0736],\n",
      "        [ 0.1782],\n",
      "        [-1.8859],\n",
      "        [ 1.2716],\n",
      "        [ 0.6852],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2314: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2314: tensor([[ 1.0739],\n",
      "        [ 0.1792],\n",
      "        [-1.8842],\n",
      "        [ 1.2739],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2315: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2315: tensor([[ 1.0736],\n",
      "        [ 0.1782],\n",
      "        [-1.8859],\n",
      "        [ 1.2717],\n",
      "        [ 0.6852],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 2316: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2316: tensor([[ 1.0739],\n",
      "        [ 0.1792],\n",
      "        [-1.8842],\n",
      "        [ 1.2739],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2317: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2317: tensor([[ 1.0736],\n",
      "        [ 0.1782],\n",
      "        [-1.8860],\n",
      "        [ 1.2717],\n",
      "        [ 0.6852],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2318: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2318: tensor([[ 1.0739],\n",
      "        [ 0.1792],\n",
      "        [-1.8842],\n",
      "        [ 1.2739],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2319: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2319: tensor([[ 1.0736],\n",
      "        [ 0.1781],\n",
      "        [-1.8860],\n",
      "        [ 1.2717],\n",
      "        [ 0.6852],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2320: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2320: tensor([[ 1.0739],\n",
      "        [ 0.1791],\n",
      "        [-1.8843],\n",
      "        [ 1.2739],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2321: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2321: tensor([[ 1.0736],\n",
      "        [ 0.1781],\n",
      "        [-1.8860],\n",
      "        [ 1.2717],\n",
      "        [ 0.6852],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2322: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2322: tensor([[ 1.0739],\n",
      "        [ 0.1791],\n",
      "        [-1.8843],\n",
      "        [ 1.2739],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2323: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2323: tensor([[ 1.0736],\n",
      "        [ 0.1781],\n",
      "        [-1.8860],\n",
      "        [ 1.2717],\n",
      "        [ 0.6852],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2324: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2324: tensor([[ 1.0739],\n",
      "        [ 0.1791],\n",
      "        [-1.8843],\n",
      "        [ 1.2740],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2325: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2325: tensor([[ 1.0736],\n",
      "        [ 0.1781],\n",
      "        [-1.8861],\n",
      "        [ 1.2718],\n",
      "        [ 0.6852],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2326: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2326: tensor([[ 1.0739],\n",
      "        [ 0.1791],\n",
      "        [-1.8843],\n",
      "        [ 1.2740],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2327: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2327: tensor([[ 1.0736],\n",
      "        [ 0.1780],\n",
      "        [-1.8861],\n",
      "        [ 1.2718],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2328: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2328: tensor([[ 1.0739],\n",
      "        [ 0.1790],\n",
      "        [-1.8844],\n",
      "        [ 1.2740],\n",
      "        [ 0.6876],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2329: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2329: tensor([[ 1.0736],\n",
      "        [ 0.1780],\n",
      "        [-1.8861],\n",
      "        [ 1.2718],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2330: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2330: tensor([[ 1.0739],\n",
      "        [ 0.1790],\n",
      "        [-1.8844],\n",
      "        [ 1.2740],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2331: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2331: tensor([[ 1.0736],\n",
      "        [ 0.1780],\n",
      "        [-1.8861],\n",
      "        [ 1.2718],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2332: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2332: tensor([[ 1.0739],\n",
      "        [ 0.1790],\n",
      "        [-1.8844],\n",
      "        [ 1.2740],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2333: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2333: tensor([[ 1.0736],\n",
      "        [ 0.1780],\n",
      "        [-1.8862],\n",
      "        [ 1.2718],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2334: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2334: tensor([[ 1.0739],\n",
      "        [ 0.1790],\n",
      "        [-1.8845],\n",
      "        [ 1.2741],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2335: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2335: tensor([[ 1.0736],\n",
      "        [ 0.1779],\n",
      "        [-1.8862],\n",
      "        [ 1.2719],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2336: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2336: tensor([[ 1.0739],\n",
      "        [ 0.1789],\n",
      "        [-1.8845],\n",
      "        [ 1.2741],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2337: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2337: tensor([[ 1.0736],\n",
      "        [ 0.1779],\n",
      "        [-1.8862],\n",
      "        [ 1.2719],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2338: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2338: tensor([[ 1.0739],\n",
      "        [ 0.1789],\n",
      "        [-1.8845],\n",
      "        [ 1.2741],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2339: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2339: tensor([[ 1.0736],\n",
      "        [ 0.1779],\n",
      "        [-1.8863],\n",
      "        [ 1.2719],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2340: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2340: tensor([[ 1.0739],\n",
      "        [ 0.1789],\n",
      "        [-1.8845],\n",
      "        [ 1.2741],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2341: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2341: tensor([[ 1.0736],\n",
      "        [ 0.1779],\n",
      "        [-1.8863],\n",
      "        [ 1.2719],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2342: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2342: tensor([[ 1.0739],\n",
      "        [ 0.1789],\n",
      "        [-1.8846],\n",
      "        [ 1.2741],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2343: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2343: tensor([[ 1.0736],\n",
      "        [ 0.1778],\n",
      "        [-1.8863],\n",
      "        [ 1.2719],\n",
      "        [ 0.6853],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2344: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2344: tensor([[ 1.0739],\n",
      "        [ 0.1788],\n",
      "        [-1.8846],\n",
      "        [ 1.2742],\n",
      "        [ 0.6877],\n",
      "        [-0.2873]], requires_grad=True)\n",
      "poly train loss at 2345: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2345: tensor([[ 1.0736],\n",
      "        [ 0.1778],\n",
      "        [-1.8863],\n",
      "        [ 1.2720],\n",
      "        [ 0.6854],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2346: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2346: tensor([[ 1.0739],\n",
      "        [ 0.1788],\n",
      "        [-1.8846],\n",
      "        [ 1.2742],\n",
      "        [ 0.6877],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2347: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2347: tensor([[ 1.0736],\n",
      "        [ 0.1778],\n",
      "        [-1.8864],\n",
      "        [ 1.2720],\n",
      "        [ 0.6854],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2348: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2348: tensor([[ 1.0739],\n",
      "        [ 0.1788],\n",
      "        [-1.8847],\n",
      "        [ 1.2742],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2349: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2349: tensor([[ 1.0736],\n",
      "        [ 0.1778],\n",
      "        [-1.8864],\n",
      "        [ 1.2720],\n",
      "        [ 0.6854],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2350: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2350: tensor([[ 1.0739],\n",
      "        [ 0.1788],\n",
      "        [-1.8847],\n",
      "        [ 1.2742],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2351: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2351: tensor([[ 1.0736],\n",
      "        [ 0.1777],\n",
      "        [-1.8864],\n",
      "        [ 1.2720],\n",
      "        [ 0.6854],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2352: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2352: tensor([[ 1.0739],\n",
      "        [ 0.1787],\n",
      "        [-1.8847],\n",
      "        [ 1.2742],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2353: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2353: tensor([[ 1.0736],\n",
      "        [ 0.1777],\n",
      "        [-1.8865],\n",
      "        [ 1.2721],\n",
      "        [ 0.6854],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2354: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2354: tensor([[ 1.0739],\n",
      "        [ 0.1787],\n",
      "        [-1.8847],\n",
      "        [ 1.2743],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2355: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2355: tensor([[ 1.0736],\n",
      "        [ 0.1777],\n",
      "        [-1.8865],\n",
      "        [ 1.2721],\n",
      "        [ 0.6854],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2356: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2356: tensor([[ 1.0739],\n",
      "        [ 0.1787],\n",
      "        [-1.8848],\n",
      "        [ 1.2743],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2357: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2357: tensor([[ 1.0736],\n",
      "        [ 0.1776],\n",
      "        [-1.8865],\n",
      "        [ 1.2721],\n",
      "        [ 0.6854],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 2358: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2358: tensor([[ 1.0739],\n",
      "        [ 0.1787],\n",
      "        [-1.8848],\n",
      "        [ 1.2743],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2359: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2359: tensor([[ 1.0736],\n",
      "        [ 0.1776],\n",
      "        [-1.8865],\n",
      "        [ 1.2721],\n",
      "        [ 0.6854],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2360: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2360: tensor([[ 1.0739],\n",
      "        [ 0.1786],\n",
      "        [-1.8848],\n",
      "        [ 1.2743],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2361: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2361: tensor([[ 1.0736],\n",
      "        [ 0.1776],\n",
      "        [-1.8866],\n",
      "        [ 1.2721],\n",
      "        [ 0.6854],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2362: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2362: tensor([[ 1.0739],\n",
      "        [ 0.1786],\n",
      "        [-1.8849],\n",
      "        [ 1.2743],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2363: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2363: tensor([[ 1.0736],\n",
      "        [ 0.1776],\n",
      "        [-1.8866],\n",
      "        [ 1.2722],\n",
      "        [ 0.6854],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2364: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2364: tensor([[ 1.0739],\n",
      "        [ 0.1786],\n",
      "        [-1.8849],\n",
      "        [ 1.2744],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2365: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2365: tensor([[ 1.0736],\n",
      "        [ 0.1775],\n",
      "        [-1.8866],\n",
      "        [ 1.2722],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2366: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2366: tensor([[ 1.0739],\n",
      "        [ 0.1785],\n",
      "        [-1.8849],\n",
      "        [ 1.2744],\n",
      "        [ 0.6878],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2367: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2367: tensor([[ 1.0736],\n",
      "        [ 0.1775],\n",
      "        [-1.8867],\n",
      "        [ 1.2722],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2368: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2368: tensor([[ 1.0739],\n",
      "        [ 0.1785],\n",
      "        [-1.8849],\n",
      "        [ 1.2744],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2369: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2369: tensor([[ 1.0736],\n",
      "        [ 0.1775],\n",
      "        [-1.8867],\n",
      "        [ 1.2722],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2370: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2370: tensor([[ 1.0739],\n",
      "        [ 0.1785],\n",
      "        [-1.8850],\n",
      "        [ 1.2744],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2371: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2371: tensor([[ 1.0736],\n",
      "        [ 0.1775],\n",
      "        [-1.8867],\n",
      "        [ 1.2722],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2372: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2372: tensor([[ 1.0739],\n",
      "        [ 0.1785],\n",
      "        [-1.8850],\n",
      "        [ 1.2744],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2373: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2373: tensor([[ 1.0736],\n",
      "        [ 0.1774],\n",
      "        [-1.8867],\n",
      "        [ 1.2723],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2374: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2374: tensor([[ 1.0739],\n",
      "        [ 0.1784],\n",
      "        [-1.8850],\n",
      "        [ 1.2745],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2375: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2375: tensor([[ 1.0736],\n",
      "        [ 0.1774],\n",
      "        [-1.8868],\n",
      "        [ 1.2723],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2376: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2376: tensor([[ 1.0739],\n",
      "        [ 0.1784],\n",
      "        [-1.8851],\n",
      "        [ 1.2745],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2377: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2377: tensor([[ 1.0736],\n",
      "        [ 0.1774],\n",
      "        [-1.8868],\n",
      "        [ 1.2723],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2378: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2378: tensor([[ 1.0739],\n",
      "        [ 0.1784],\n",
      "        [-1.8851],\n",
      "        [ 1.2745],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2379: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2379: tensor([[ 1.0736],\n",
      "        [ 0.1774],\n",
      "        [-1.8868],\n",
      "        [ 1.2723],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2380: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2380: tensor([[ 1.0739],\n",
      "        [ 0.1784],\n",
      "        [-1.8851],\n",
      "        [ 1.2745],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2381: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2381: tensor([[ 1.0736],\n",
      "        [ 0.1773],\n",
      "        [-1.8869],\n",
      "        [ 1.2723],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2382: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2382: tensor([[ 1.0739],\n",
      "        [ 0.1783],\n",
      "        [-1.8851],\n",
      "        [ 1.2745],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2383: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2383: tensor([[ 1.0736],\n",
      "        [ 0.1773],\n",
      "        [-1.8869],\n",
      "        [ 1.2724],\n",
      "        [ 0.6855],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2384: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2384: tensor([[ 1.0739],\n",
      "        [ 0.1783],\n",
      "        [-1.8852],\n",
      "        [ 1.2746],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2385: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2385: tensor([[ 1.0736],\n",
      "        [ 0.1773],\n",
      "        [-1.8869],\n",
      "        [ 1.2724],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2386: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2386: tensor([[ 1.0739],\n",
      "        [ 0.1783],\n",
      "        [-1.8852],\n",
      "        [ 1.2746],\n",
      "        [ 0.6879],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2387: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2387: tensor([[ 1.0736],\n",
      "        [ 0.1773],\n",
      "        [-1.8869],\n",
      "        [ 1.2724],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2388: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2388: tensor([[ 1.0739],\n",
      "        [ 0.1783],\n",
      "        [-1.8852],\n",
      "        [ 1.2746],\n",
      "        [ 0.6880],\n",
      "        [-0.2874]], requires_grad=True)\n",
      "poly train loss at 2389: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2389: tensor([[ 1.0736],\n",
      "        [ 0.1772],\n",
      "        [-1.8870],\n",
      "        [ 1.2724],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2390: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2390: tensor([[ 1.0739],\n",
      "        [ 0.1782],\n",
      "        [-1.8852],\n",
      "        [ 1.2746],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2391: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2391: tensor([[ 1.0736],\n",
      "        [ 0.1772],\n",
      "        [-1.8870],\n",
      "        [ 1.2724],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2392: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2392: tensor([[ 1.0739],\n",
      "        [ 0.1782],\n",
      "        [-1.8853],\n",
      "        [ 1.2747],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2393: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2393: tensor([[ 1.0736],\n",
      "        [ 0.1772],\n",
      "        [-1.8870],\n",
      "        [ 1.2725],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2394: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2394: tensor([[ 1.0739],\n",
      "        [ 0.1782],\n",
      "        [-1.8853],\n",
      "        [ 1.2747],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2395: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2395: tensor([[ 1.0736],\n",
      "        [ 0.1772],\n",
      "        [-1.8870],\n",
      "        [ 1.2725],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2396: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2396: tensor([[ 1.0739],\n",
      "        [ 0.1782],\n",
      "        [-1.8853],\n",
      "        [ 1.2747],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2397: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2397: tensor([[ 1.0736],\n",
      "        [ 0.1771],\n",
      "        [-1.8871],\n",
      "        [ 1.2725],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2398: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2398: tensor([[ 1.0739],\n",
      "        [ 0.1781],\n",
      "        [-1.8854],\n",
      "        [ 1.2747],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2399: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2399: tensor([[ 1.0736],\n",
      "        [ 0.1771],\n",
      "        [-1.8871],\n",
      "        [ 1.2725],\n",
      "        [ 0.6856],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 2400: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2400: tensor([[ 1.0739],\n",
      "        [ 0.1781],\n",
      "        [-1.8854],\n",
      "        [ 1.2747],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2401: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2401: tensor([[ 1.0736],\n",
      "        [ 0.1771],\n",
      "        [-1.8871],\n",
      "        [ 1.2725],\n",
      "        [ 0.6856],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2402: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2402: tensor([[ 1.0739],\n",
      "        [ 0.1781],\n",
      "        [-1.8854],\n",
      "        [ 1.2748],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2403: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2403: tensor([[ 1.0736],\n",
      "        [ 0.1771],\n",
      "        [-1.8872],\n",
      "        [ 1.2726],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2404: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2404: tensor([[ 1.0739],\n",
      "        [ 0.1781],\n",
      "        [-1.8854],\n",
      "        [ 1.2748],\n",
      "        [ 0.6880],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2405: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2405: tensor([[ 1.0736],\n",
      "        [ 0.1770],\n",
      "        [-1.8872],\n",
      "        [ 1.2726],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2406: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2406: tensor([[ 1.0739],\n",
      "        [ 0.1780],\n",
      "        [-1.8855],\n",
      "        [ 1.2748],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2407: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2407: tensor([[ 1.0736],\n",
      "        [ 0.1770],\n",
      "        [-1.8872],\n",
      "        [ 1.2726],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2408: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2408: tensor([[ 1.0739],\n",
      "        [ 0.1780],\n",
      "        [-1.8855],\n",
      "        [ 1.2748],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2409: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2409: tensor([[ 1.0736],\n",
      "        [ 0.1770],\n",
      "        [-1.8872],\n",
      "        [ 1.2726],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2410: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2410: tensor([[ 1.0739],\n",
      "        [ 0.1780],\n",
      "        [-1.8855],\n",
      "        [ 1.2748],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2411: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2411: tensor([[ 1.0736],\n",
      "        [ 0.1769],\n",
      "        [-1.8873],\n",
      "        [ 1.2726],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2412: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2412: tensor([[ 1.0739],\n",
      "        [ 0.1780],\n",
      "        [-1.8856],\n",
      "        [ 1.2749],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2413: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2413: tensor([[ 1.0736],\n",
      "        [ 0.1769],\n",
      "        [-1.8873],\n",
      "        [ 1.2727],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2414: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2414: tensor([[ 1.0739],\n",
      "        [ 0.1779],\n",
      "        [-1.8856],\n",
      "        [ 1.2749],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2415: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2415: tensor([[ 1.0736],\n",
      "        [ 0.1769],\n",
      "        [-1.8873],\n",
      "        [ 1.2727],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2416: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2416: tensor([[ 1.0739],\n",
      "        [ 0.1779],\n",
      "        [-1.8856],\n",
      "        [ 1.2749],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2417: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2417: tensor([[ 1.0736],\n",
      "        [ 0.1769],\n",
      "        [-1.8874],\n",
      "        [ 1.2727],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2418: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2418: tensor([[ 1.0739],\n",
      "        [ 0.1779],\n",
      "        [-1.8856],\n",
      "        [ 1.2749],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2419: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2419: tensor([[ 1.0736],\n",
      "        [ 0.1768],\n",
      "        [-1.8874],\n",
      "        [ 1.2727],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2420: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2420: tensor([[ 1.0739],\n",
      "        [ 0.1779],\n",
      "        [-1.8857],\n",
      "        [ 1.2749],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2421: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2421: tensor([[ 1.0736],\n",
      "        [ 0.1768],\n",
      "        [-1.8874],\n",
      "        [ 1.2727],\n",
      "        [ 0.6857],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2422: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2422: tensor([[ 1.0739],\n",
      "        [ 0.1778],\n",
      "        [-1.8857],\n",
      "        [ 1.2750],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2423: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2423: tensor([[ 1.0736],\n",
      "        [ 0.1768],\n",
      "        [-1.8874],\n",
      "        [ 1.2728],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2424: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2424: tensor([[ 1.0739],\n",
      "        [ 0.1778],\n",
      "        [-1.8857],\n",
      "        [ 1.2750],\n",
      "        [ 0.6881],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2425: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2425: tensor([[ 1.0736],\n",
      "        [ 0.1768],\n",
      "        [-1.8875],\n",
      "        [ 1.2728],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2426: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2426: tensor([[ 1.0739],\n",
      "        [ 0.1778],\n",
      "        [-1.8857],\n",
      "        [ 1.2750],\n",
      "        [ 0.6882],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2427: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2427: tensor([[ 1.0736],\n",
      "        [ 0.1767],\n",
      "        [-1.8875],\n",
      "        [ 1.2728],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2428: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2428: tensor([[ 1.0739],\n",
      "        [ 0.1778],\n",
      "        [-1.8858],\n",
      "        [ 1.2750],\n",
      "        [ 0.6882],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2429: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2429: tensor([[ 1.0736],\n",
      "        [ 0.1767],\n",
      "        [-1.8875],\n",
      "        [ 1.2728],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2430: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2430: tensor([[ 1.0739],\n",
      "        [ 0.1777],\n",
      "        [-1.8858],\n",
      "        [ 1.2750],\n",
      "        [ 0.6882],\n",
      "        [-0.2875]], requires_grad=True)\n",
      "poly train loss at 2431: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2431: tensor([[ 1.0736],\n",
      "        [ 0.1767],\n",
      "        [-1.8875],\n",
      "        [ 1.2728],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2432: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2432: tensor([[ 1.0739],\n",
      "        [ 0.1777],\n",
      "        [-1.8858],\n",
      "        [ 1.2751],\n",
      "        [ 0.6882],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2433: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2433: tensor([[ 1.0736],\n",
      "        [ 0.1767],\n",
      "        [-1.8876],\n",
      "        [ 1.2729],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2434: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2434: tensor([[ 1.0739],\n",
      "        [ 0.1777],\n",
      "        [-1.8859],\n",
      "        [ 1.2751],\n",
      "        [ 0.6882],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2435: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2435: tensor([[ 1.0736],\n",
      "        [ 0.1766],\n",
      "        [-1.8876],\n",
      "        [ 1.2729],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2436: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2436: tensor([[ 1.0739],\n",
      "        [ 0.1776],\n",
      "        [-1.8859],\n",
      "        [ 1.2751],\n",
      "        [ 0.6882],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2437: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2437: tensor([[ 1.0736],\n",
      "        [ 0.1766],\n",
      "        [-1.8876],\n",
      "        [ 1.2729],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2438: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2438: tensor([[ 1.0739],\n",
      "        [ 0.1776],\n",
      "        [-1.8859],\n",
      "        [ 1.2751],\n",
      "        [ 0.6882],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2439: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2439: tensor([[ 1.0736],\n",
      "        [ 0.1766],\n",
      "        [-1.8877],\n",
      "        [ 1.2729],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2440: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2440: tensor([[ 1.0739],\n",
      "        [ 0.1776],\n",
      "        [-1.8859],\n",
      "        [ 1.2751],\n",
      "        [ 0.6882],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2441: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2441: tensor([[ 1.0736],\n",
      "        [ 0.1766],\n",
      "        [-1.8877],\n",
      "        [ 1.2729],\n",
      "        [ 0.6858],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2442: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2442: tensor([[ 1.0739],\n",
      "        [ 0.1776],\n",
      "        [-1.8860],\n",
      "        [ 1.2752],\n",
      "        [ 0.6882],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2443: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2443: tensor([[ 1.0736],\n",
      "        [ 0.1765],\n",
      "        [-1.8877],\n",
      "        [ 1.2730],\n",
      "        [ 0.6859],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 2444: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2444: tensor([[ 1.0739],\n",
      "        [ 0.1775],\n",
      "        [-1.8860],\n",
      "        [ 1.2752],\n",
      "        [ 0.6882],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2445: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2445: tensor([[ 1.0736],\n",
      "        [ 0.1765],\n",
      "        [-1.8877],\n",
      "        [ 1.2730],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2446: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2446: tensor([[ 1.0739],\n",
      "        [ 0.1775],\n",
      "        [-1.8860],\n",
      "        [ 1.2752],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2447: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2447: tensor([[ 1.0736],\n",
      "        [ 0.1765],\n",
      "        [-1.8878],\n",
      "        [ 1.2730],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2448: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2448: tensor([[ 1.0739],\n",
      "        [ 0.1775],\n",
      "        [-1.8861],\n",
      "        [ 1.2752],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2449: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2449: tensor([[ 1.0736],\n",
      "        [ 0.1765],\n",
      "        [-1.8878],\n",
      "        [ 1.2730],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2450: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2450: tensor([[ 1.0739],\n",
      "        [ 0.1775],\n",
      "        [-1.8861],\n",
      "        [ 1.2752],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2451: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2451: tensor([[ 1.0736],\n",
      "        [ 0.1764],\n",
      "        [-1.8878],\n",
      "        [ 1.2730],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2452: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2452: tensor([[ 1.0739],\n",
      "        [ 0.1774],\n",
      "        [-1.8861],\n",
      "        [ 1.2753],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2453: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2453: tensor([[ 1.0736],\n",
      "        [ 0.1764],\n",
      "        [-1.8879],\n",
      "        [ 1.2731],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2454: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2454: tensor([[ 1.0739],\n",
      "        [ 0.1774],\n",
      "        [-1.8861],\n",
      "        [ 1.2753],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2455: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2455: tensor([[ 1.0736],\n",
      "        [ 0.1764],\n",
      "        [-1.8879],\n",
      "        [ 1.2731],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2456: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2456: tensor([[ 1.0739],\n",
      "        [ 0.1774],\n",
      "        [-1.8862],\n",
      "        [ 1.2753],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2457: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2457: tensor([[ 1.0736],\n",
      "        [ 0.1764],\n",
      "        [-1.8879],\n",
      "        [ 1.2731],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2458: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2458: tensor([[ 1.0739],\n",
      "        [ 0.1774],\n",
      "        [-1.8862],\n",
      "        [ 1.2753],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2459: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2459: tensor([[ 1.0736],\n",
      "        [ 0.1763],\n",
      "        [-1.8879],\n",
      "        [ 1.2731],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2460: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2460: tensor([[ 1.0739],\n",
      "        [ 0.1773],\n",
      "        [-1.8862],\n",
      "        [ 1.2753],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2461: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2461: tensor([[ 1.0736],\n",
      "        [ 0.1763],\n",
      "        [-1.8880],\n",
      "        [ 1.2731],\n",
      "        [ 0.6859],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2462: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2462: tensor([[ 1.0739],\n",
      "        [ 0.1773],\n",
      "        [-1.8862],\n",
      "        [ 1.2754],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2463: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2463: tensor([[ 1.0736],\n",
      "        [ 0.1763],\n",
      "        [-1.8880],\n",
      "        [ 1.2732],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2464: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2464: tensor([[ 1.0739],\n",
      "        [ 0.1773],\n",
      "        [-1.8863],\n",
      "        [ 1.2754],\n",
      "        [ 0.6883],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2465: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2465: tensor([[ 1.0736],\n",
      "        [ 0.1763],\n",
      "        [-1.8880],\n",
      "        [ 1.2732],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2466: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2466: tensor([[ 1.0739],\n",
      "        [ 0.1773],\n",
      "        [-1.8863],\n",
      "        [ 1.2754],\n",
      "        [ 0.6884],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2467: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2467: tensor([[ 1.0736],\n",
      "        [ 0.1762],\n",
      "        [-1.8880],\n",
      "        [ 1.2732],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2468: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2468: tensor([[ 1.0739],\n",
      "        [ 0.1772],\n",
      "        [-1.8863],\n",
      "        [ 1.2754],\n",
      "        [ 0.6884],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2469: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2469: tensor([[ 1.0736],\n",
      "        [ 0.1762],\n",
      "        [-1.8881],\n",
      "        [ 1.2732],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2470: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2470: tensor([[ 1.0739],\n",
      "        [ 0.1772],\n",
      "        [-1.8864],\n",
      "        [ 1.2754],\n",
      "        [ 0.6884],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2471: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2471: tensor([[ 1.0736],\n",
      "        [ 0.1762],\n",
      "        [-1.8881],\n",
      "        [ 1.2732],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2472: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2472: tensor([[ 1.0739],\n",
      "        [ 0.1772],\n",
      "        [-1.8864],\n",
      "        [ 1.2755],\n",
      "        [ 0.6884],\n",
      "        [-0.2876]], requires_grad=True)\n",
      "poly train loss at 2473: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2473: tensor([[ 1.0736],\n",
      "        [ 0.1762],\n",
      "        [-1.8881],\n",
      "        [ 1.2733],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2474: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2474: tensor([[ 1.0739],\n",
      "        [ 0.1772],\n",
      "        [-1.8864],\n",
      "        [ 1.2755],\n",
      "        [ 0.6884],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2475: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2475: tensor([[ 1.0736],\n",
      "        [ 0.1761],\n",
      "        [-1.8882],\n",
      "        [ 1.2733],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2476: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2476: tensor([[ 1.0739],\n",
      "        [ 0.1771],\n",
      "        [-1.8864],\n",
      "        [ 1.2755],\n",
      "        [ 0.6884],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2477: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2477: tensor([[ 1.0736],\n",
      "        [ 0.1761],\n",
      "        [-1.8882],\n",
      "        [ 1.2733],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2478: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2478: tensor([[ 1.0739],\n",
      "        [ 0.1771],\n",
      "        [-1.8865],\n",
      "        [ 1.2755],\n",
      "        [ 0.6884],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2479: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2479: tensor([[ 1.0736],\n",
      "        [ 0.1761],\n",
      "        [-1.8882],\n",
      "        [ 1.2733],\n",
      "        [ 0.6860],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2480: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2480: tensor([[ 1.0739],\n",
      "        [ 0.1771],\n",
      "        [-1.8865],\n",
      "        [ 1.2755],\n",
      "        [ 0.6884],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2481: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2481: tensor([[ 1.0736],\n",
      "        [ 0.1761],\n",
      "        [-1.8882],\n",
      "        [ 1.2733],\n",
      "        [ 0.6861],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2482: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2482: tensor([[ 1.0739],\n",
      "        [ 0.1771],\n",
      "        [-1.8865],\n",
      "        [ 1.2756],\n",
      "        [ 0.6884],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2483: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2483: tensor([[ 1.0736],\n",
      "        [ 0.1760],\n",
      "        [-1.8883],\n",
      "        [ 1.2734],\n",
      "        [ 0.6861],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2484: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2484: tensor([[ 1.0739],\n",
      "        [ 0.1770],\n",
      "        [-1.8866],\n",
      "        [ 1.2756],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2485: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2485: tensor([[ 1.0736],\n",
      "        [ 0.1760],\n",
      "        [-1.8883],\n",
      "        [ 1.2734],\n",
      "        [ 0.6861],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 2486: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2486: tensor([[ 1.0739],\n",
      "        [ 0.1770],\n",
      "        [-1.8866],\n",
      "        [ 1.2756],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2487: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2487: tensor([[ 1.0736],\n",
      "        [ 0.1760],\n",
      "        [-1.8883],\n",
      "        [ 1.2734],\n",
      "        [ 0.6861],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2488: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2488: tensor([[ 1.0739],\n",
      "        [ 0.1770],\n",
      "        [-1.8866],\n",
      "        [ 1.2756],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2489: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2489: tensor([[ 1.0736],\n",
      "        [ 0.1760],\n",
      "        [-1.8884],\n",
      "        [ 1.2734],\n",
      "        [ 0.6861],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2490: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2490: tensor([[ 1.0739],\n",
      "        [ 0.1770],\n",
      "        [-1.8866],\n",
      "        [ 1.2756],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2491: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2491: tensor([[ 1.0736],\n",
      "        [ 0.1759],\n",
      "        [-1.8884],\n",
      "        [ 1.2734],\n",
      "        [ 0.6861],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2492: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2492: tensor([[ 1.0739],\n",
      "        [ 0.1769],\n",
      "        [-1.8867],\n",
      "        [ 1.2757],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2493: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2493: tensor([[ 1.0736],\n",
      "        [ 0.1759],\n",
      "        [-1.8884],\n",
      "        [ 1.2735],\n",
      "        [ 0.6861],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2494: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2494: tensor([[ 1.0739],\n",
      "        [ 0.1769],\n",
      "        [-1.8867],\n",
      "        [ 1.2757],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2495: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2495: tensor([[ 1.0736],\n",
      "        [ 0.1759],\n",
      "        [-1.8884],\n",
      "        [ 1.2735],\n",
      "        [ 0.6861],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2496: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2496: tensor([[ 1.0739],\n",
      "        [ 0.1769],\n",
      "        [-1.8867],\n",
      "        [ 1.2757],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2497: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2497: tensor([[ 1.0736],\n",
      "        [ 0.1759],\n",
      "        [-1.8885],\n",
      "        [ 1.2735],\n",
      "        [ 0.6861],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2498: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2498: tensor([[ 1.0739],\n",
      "        [ 0.1769],\n",
      "        [-1.8867],\n",
      "        [ 1.2757],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2499: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2499: tensor([[ 1.0736],\n",
      "        [ 0.1758],\n",
      "        [-1.8885],\n",
      "        [ 1.2735],\n",
      "        [ 0.6861],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2500: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2500: tensor([[ 1.0739],\n",
      "        [ 0.1768],\n",
      "        [-1.8868],\n",
      "        [ 1.2757],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2501: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2501: tensor([[ 1.0736],\n",
      "        [ 0.1758],\n",
      "        [-1.8885],\n",
      "        [ 1.2735],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2502: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2502: tensor([[ 1.0739],\n",
      "        [ 0.1768],\n",
      "        [-1.8868],\n",
      "        [ 1.2758],\n",
      "        [ 0.6885],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2503: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2503: tensor([[ 1.0736],\n",
      "        [ 0.1758],\n",
      "        [-1.8885],\n",
      "        [ 1.2736],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2504: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2504: tensor([[ 1.0739],\n",
      "        [ 0.1768],\n",
      "        [-1.8868],\n",
      "        [ 1.2758],\n",
      "        [ 0.6886],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2505: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2505: tensor([[ 1.0736],\n",
      "        [ 0.1758],\n",
      "        [-1.8886],\n",
      "        [ 1.2736],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2506: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2506: tensor([[ 1.0739],\n",
      "        [ 0.1768],\n",
      "        [-1.8869],\n",
      "        [ 1.2758],\n",
      "        [ 0.6886],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2507: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2507: tensor([[ 1.0736],\n",
      "        [ 0.1757],\n",
      "        [-1.8886],\n",
      "        [ 1.2736],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2508: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2508: tensor([[ 1.0739],\n",
      "        [ 0.1767],\n",
      "        [-1.8869],\n",
      "        [ 1.2758],\n",
      "        [ 0.6886],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2509: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2509: tensor([[ 1.0736],\n",
      "        [ 0.1757],\n",
      "        [-1.8886],\n",
      "        [ 1.2736],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2510: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2510: tensor([[ 1.0739],\n",
      "        [ 0.1767],\n",
      "        [-1.8869],\n",
      "        [ 1.2758],\n",
      "        [ 0.6886],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2511: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2511: tensor([[ 1.0736],\n",
      "        [ 0.1757],\n",
      "        [-1.8887],\n",
      "        [ 1.2736],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2512: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2512: tensor([[ 1.0739],\n",
      "        [ 0.1767],\n",
      "        [-1.8869],\n",
      "        [ 1.2759],\n",
      "        [ 0.6886],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2513: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2513: tensor([[ 1.0736],\n",
      "        [ 0.1757],\n",
      "        [-1.8887],\n",
      "        [ 1.2737],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2514: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2514: tensor([[ 1.0739],\n",
      "        [ 0.1767],\n",
      "        [-1.8870],\n",
      "        [ 1.2759],\n",
      "        [ 0.6886],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2515: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2515: tensor([[ 1.0736],\n",
      "        [ 0.1756],\n",
      "        [-1.8887],\n",
      "        [ 1.2737],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2516: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2516: tensor([[ 1.0739],\n",
      "        [ 0.1766],\n",
      "        [-1.8870],\n",
      "        [ 1.2759],\n",
      "        [ 0.6886],\n",
      "        [-0.2877]], requires_grad=True)\n",
      "poly train loss at 2517: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2517: tensor([[ 1.0736],\n",
      "        [ 0.1756],\n",
      "        [-1.8887],\n",
      "        [ 1.2737],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2518: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2518: tensor([[ 1.0739],\n",
      "        [ 0.1766],\n",
      "        [-1.8870],\n",
      "        [ 1.2759],\n",
      "        [ 0.6886],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2519: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2519: tensor([[ 1.0736],\n",
      "        [ 0.1756],\n",
      "        [-1.8888],\n",
      "        [ 1.2737],\n",
      "        [ 0.6862],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2520: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2520: tensor([[ 1.0739],\n",
      "        [ 0.1766],\n",
      "        [-1.8870],\n",
      "        [ 1.2759],\n",
      "        [ 0.6886],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2521: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2521: tensor([[ 1.0736],\n",
      "        [ 0.1756],\n",
      "        [-1.8888],\n",
      "        [ 1.2737],\n",
      "        [ 0.6863],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2522: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2522: tensor([[ 1.0739],\n",
      "        [ 0.1766],\n",
      "        [-1.8871],\n",
      "        [ 1.2760],\n",
      "        [ 0.6886],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2523: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2523: tensor([[ 1.0736],\n",
      "        [ 0.1755],\n",
      "        [-1.8888],\n",
      "        [ 1.2738],\n",
      "        [ 0.6863],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2524: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2524: tensor([[ 1.0739],\n",
      "        [ 0.1765],\n",
      "        [-1.8871],\n",
      "        [ 1.2760],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2525: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2525: tensor([[ 1.0736],\n",
      "        [ 0.1755],\n",
      "        [-1.8888],\n",
      "        [ 1.2738],\n",
      "        [ 0.6863],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2526: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2526: tensor([[ 1.0739],\n",
      "        [ 0.1765],\n",
      "        [-1.8871],\n",
      "        [ 1.2760],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2527: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2527: tensor([[ 1.0736],\n",
      "        [ 0.1755],\n",
      "        [-1.8889],\n",
      "        [ 1.2738],\n",
      "        [ 0.6863],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2528: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2528: tensor([[ 1.0739],\n",
      "        [ 0.1765],\n",
      "        [-1.8872],\n",
      "        [ 1.2760],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2529: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2529: tensor([[ 1.0736],\n",
      "        [ 0.1755],\n",
      "        [-1.8889],\n",
      "        [ 1.2738],\n",
      "        [ 0.6863],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 2530: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2530: tensor([[ 1.0739],\n",
      "        [ 0.1765],\n",
      "        [-1.8872],\n",
      "        [ 1.2760],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2531: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2531: tensor([[ 1.0736],\n",
      "        [ 0.1754],\n",
      "        [-1.8889],\n",
      "        [ 1.2738],\n",
      "        [ 0.6863],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2532: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2532: tensor([[ 1.0739],\n",
      "        [ 0.1764],\n",
      "        [-1.8872],\n",
      "        [ 1.2761],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2533: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2533: tensor([[ 1.0736],\n",
      "        [ 0.1754],\n",
      "        [-1.8890],\n",
      "        [ 1.2739],\n",
      "        [ 0.6863],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2534: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2534: tensor([[ 1.0739],\n",
      "        [ 0.1764],\n",
      "        [-1.8872],\n",
      "        [ 1.2761],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2535: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2535: tensor([[ 1.0736],\n",
      "        [ 0.1754],\n",
      "        [-1.8890],\n",
      "        [ 1.2739],\n",
      "        [ 0.6863],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2536: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2536: tensor([[ 1.0739],\n",
      "        [ 0.1764],\n",
      "        [-1.8873],\n",
      "        [ 1.2761],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2537: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2537: tensor([[ 1.0736],\n",
      "        [ 0.1754],\n",
      "        [-1.8890],\n",
      "        [ 1.2739],\n",
      "        [ 0.6863],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2538: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2538: tensor([[ 1.0739],\n",
      "        [ 0.1764],\n",
      "        [-1.8873],\n",
      "        [ 1.2761],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2539: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2539: tensor([[ 1.0736],\n",
      "        [ 0.1753],\n",
      "        [-1.8890],\n",
      "        [ 1.2739],\n",
      "        [ 0.6863],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2540: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2540: tensor([[ 1.0739],\n",
      "        [ 0.1763],\n",
      "        [-1.8873],\n",
      "        [ 1.2761],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2541: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2541: tensor([[ 1.0736],\n",
      "        [ 0.1753],\n",
      "        [-1.8891],\n",
      "        [ 1.2739],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2542: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2542: tensor([[ 1.0739],\n",
      "        [ 0.1763],\n",
      "        [-1.8873],\n",
      "        [ 1.2762],\n",
      "        [ 0.6887],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2543: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2543: tensor([[ 1.0736],\n",
      "        [ 0.1753],\n",
      "        [-1.8891],\n",
      "        [ 1.2740],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2544: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2544: tensor([[ 1.0739],\n",
      "        [ 0.1763],\n",
      "        [-1.8874],\n",
      "        [ 1.2762],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2545: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2545: tensor([[ 1.0736],\n",
      "        [ 0.1753],\n",
      "        [-1.8891],\n",
      "        [ 1.2740],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2546: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2546: tensor([[ 1.0739],\n",
      "        [ 0.1763],\n",
      "        [-1.8874],\n",
      "        [ 1.2762],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2547: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2547: tensor([[ 1.0736],\n",
      "        [ 0.1752],\n",
      "        [-1.8891],\n",
      "        [ 1.2740],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2548: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2548: tensor([[ 1.0739],\n",
      "        [ 0.1762],\n",
      "        [-1.8874],\n",
      "        [ 1.2762],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2549: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2549: tensor([[ 1.0736],\n",
      "        [ 0.1752],\n",
      "        [-1.8892],\n",
      "        [ 1.2740],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2550: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2550: tensor([[ 1.0739],\n",
      "        [ 0.1762],\n",
      "        [-1.8875],\n",
      "        [ 1.2762],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2551: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2551: tensor([[ 1.0736],\n",
      "        [ 0.1752],\n",
      "        [-1.8892],\n",
      "        [ 1.2740],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2552: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2552: tensor([[ 1.0739],\n",
      "        [ 0.1762],\n",
      "        [-1.8875],\n",
      "        [ 1.2763],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2553: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2553: tensor([[ 1.0736],\n",
      "        [ 0.1752],\n",
      "        [-1.8892],\n",
      "        [ 1.2741],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2554: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2554: tensor([[ 1.0739],\n",
      "        [ 0.1762],\n",
      "        [-1.8875],\n",
      "        [ 1.2763],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2555: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2555: tensor([[ 1.0736],\n",
      "        [ 0.1751],\n",
      "        [-1.8893],\n",
      "        [ 1.2741],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2556: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2556: tensor([[ 1.0739],\n",
      "        [ 0.1761],\n",
      "        [-1.8875],\n",
      "        [ 1.2763],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2557: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2557: tensor([[ 1.0736],\n",
      "        [ 0.1751],\n",
      "        [-1.8893],\n",
      "        [ 1.2741],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2558: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2558: tensor([[ 1.0739],\n",
      "        [ 0.1761],\n",
      "        [-1.8876],\n",
      "        [ 1.2763],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2559: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2559: tensor([[ 1.0736],\n",
      "        [ 0.1751],\n",
      "        [-1.8893],\n",
      "        [ 1.2741],\n",
      "        [ 0.6864],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2560: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2560: tensor([[ 1.0739],\n",
      "        [ 0.1761],\n",
      "        [-1.8876],\n",
      "        [ 1.2763],\n",
      "        [ 0.6888],\n",
      "        [-0.2878]], requires_grad=True)\n",
      "poly train loss at 2561: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2561: tensor([[ 1.0736],\n",
      "        [ 0.1751],\n",
      "        [-1.8893],\n",
      "        [ 1.2741],\n",
      "        [ 0.6865],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2562: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2562: tensor([[ 1.0739],\n",
      "        [ 0.1761],\n",
      "        [-1.8876],\n",
      "        [ 1.2764],\n",
      "        [ 0.6888],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2563: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2563: tensor([[ 1.0736],\n",
      "        [ 0.1750],\n",
      "        [-1.8894],\n",
      "        [ 1.2742],\n",
      "        [ 0.6865],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2564: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2564: tensor([[ 1.0739],\n",
      "        [ 0.1760],\n",
      "        [-1.8876],\n",
      "        [ 1.2764],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2565: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2565: tensor([[ 1.0736],\n",
      "        [ 0.1750],\n",
      "        [-1.8894],\n",
      "        [ 1.2742],\n",
      "        [ 0.6865],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2566: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2566: tensor([[ 1.0739],\n",
      "        [ 0.1760],\n",
      "        [-1.8877],\n",
      "        [ 1.2764],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2567: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2567: tensor([[ 1.0736],\n",
      "        [ 0.1750],\n",
      "        [-1.8894],\n",
      "        [ 1.2742],\n",
      "        [ 0.6865],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2568: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2568: tensor([[ 1.0739],\n",
      "        [ 0.1760],\n",
      "        [-1.8877],\n",
      "        [ 1.2764],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2569: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2569: tensor([[ 1.0736],\n",
      "        [ 0.1750],\n",
      "        [-1.8894],\n",
      "        [ 1.2742],\n",
      "        [ 0.6865],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2570: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2570: tensor([[ 1.0739],\n",
      "        [ 0.1760],\n",
      "        [-1.8877],\n",
      "        [ 1.2764],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2571: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2571: tensor([[ 1.0736],\n",
      "        [ 0.1749],\n",
      "        [-1.8895],\n",
      "        [ 1.2742],\n",
      "        [ 0.6865],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2572: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2572: tensor([[ 1.0739],\n",
      "        [ 0.1759],\n",
      "        [-1.8878],\n",
      "        [ 1.2765],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2573: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2573: tensor([[ 1.0736],\n",
      "        [ 0.1749],\n",
      "        [-1.8895],\n",
      "        [ 1.2743],\n",
      "        [ 0.6865],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 2574: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2574: tensor([[ 1.0739],\n",
      "        [ 0.1759],\n",
      "        [-1.8878],\n",
      "        [ 1.2765],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2575: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2575: tensor([[ 1.0736],\n",
      "        [ 0.1749],\n",
      "        [-1.8895],\n",
      "        [ 1.2743],\n",
      "        [ 0.6865],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2576: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2576: tensor([[ 1.0739],\n",
      "        [ 0.1759],\n",
      "        [-1.8878],\n",
      "        [ 1.2765],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2577: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2577: tensor([[ 1.0736],\n",
      "        [ 0.1749],\n",
      "        [-1.8896],\n",
      "        [ 1.2743],\n",
      "        [ 0.6865],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2578: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2578: tensor([[ 1.0739],\n",
      "        [ 0.1759],\n",
      "        [-1.8878],\n",
      "        [ 1.2765],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2579: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2579: tensor([[ 1.0736],\n",
      "        [ 0.1748],\n",
      "        [-1.8896],\n",
      "        [ 1.2743],\n",
      "        [ 0.6865],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2580: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2580: tensor([[ 1.0739],\n",
      "        [ 0.1758],\n",
      "        [-1.8879],\n",
      "        [ 1.2765],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2581: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2581: tensor([[ 1.0736],\n",
      "        [ 0.1748],\n",
      "        [-1.8896],\n",
      "        [ 1.2743],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2582: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2582: tensor([[ 1.0739],\n",
      "        [ 0.1758],\n",
      "        [-1.8879],\n",
      "        [ 1.2765],\n",
      "        [ 0.6889],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2583: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2583: tensor([[ 1.0736],\n",
      "        [ 0.1748],\n",
      "        [-1.8896],\n",
      "        [ 1.2744],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2584: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2584: tensor([[ 1.0739],\n",
      "        [ 0.1758],\n",
      "        [-1.8879],\n",
      "        [ 1.2766],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2585: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2585: tensor([[ 1.0736],\n",
      "        [ 0.1748],\n",
      "        [-1.8897],\n",
      "        [ 1.2744],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2586: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2586: tensor([[ 1.0739],\n",
      "        [ 0.1758],\n",
      "        [-1.8879],\n",
      "        [ 1.2766],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2587: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2587: tensor([[ 1.0736],\n",
      "        [ 0.1747],\n",
      "        [-1.8897],\n",
      "        [ 1.2744],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2588: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2588: tensor([[ 1.0739],\n",
      "        [ 0.1757],\n",
      "        [-1.8880],\n",
      "        [ 1.2766],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2589: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2589: tensor([[ 1.0736],\n",
      "        [ 0.1747],\n",
      "        [-1.8897],\n",
      "        [ 1.2744],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2590: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2590: tensor([[ 1.0739],\n",
      "        [ 0.1757],\n",
      "        [-1.8880],\n",
      "        [ 1.2766],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2591: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2591: tensor([[ 1.0736],\n",
      "        [ 0.1747],\n",
      "        [-1.8897],\n",
      "        [ 1.2744],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2592: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2592: tensor([[ 1.0739],\n",
      "        [ 0.1757],\n",
      "        [-1.8880],\n",
      "        [ 1.2766],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2593: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2593: tensor([[ 1.0736],\n",
      "        [ 0.1747],\n",
      "        [-1.8898],\n",
      "        [ 1.2745],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2594: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2594: tensor([[ 1.0739],\n",
      "        [ 0.1757],\n",
      "        [-1.8880],\n",
      "        [ 1.2767],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2595: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2595: tensor([[ 1.0736],\n",
      "        [ 0.1746],\n",
      "        [-1.8898],\n",
      "        [ 1.2745],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2596: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2596: tensor([[ 1.0739],\n",
      "        [ 0.1756],\n",
      "        [-1.8881],\n",
      "        [ 1.2767],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2597: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2597: tensor([[ 1.0736],\n",
      "        [ 0.1746],\n",
      "        [-1.8898],\n",
      "        [ 1.2745],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2598: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2598: tensor([[ 1.0739],\n",
      "        [ 0.1756],\n",
      "        [-1.8881],\n",
      "        [ 1.2767],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2599: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2599: tensor([[ 1.0736],\n",
      "        [ 0.1746],\n",
      "        [-1.8898],\n",
      "        [ 1.2745],\n",
      "        [ 0.6866],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2600: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2600: tensor([[ 1.0739],\n",
      "        [ 0.1756],\n",
      "        [-1.8881],\n",
      "        [ 1.2767],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2601: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2601: tensor([[ 1.0736],\n",
      "        [ 0.1746],\n",
      "        [-1.8899],\n",
      "        [ 1.2745],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2602: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2602: tensor([[ 1.0739],\n",
      "        [ 0.1756],\n",
      "        [-1.8882],\n",
      "        [ 1.2767],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2603: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2603: tensor([[ 1.0736],\n",
      "        [ 0.1745],\n",
      "        [-1.8899],\n",
      "        [ 1.2746],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2604: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2604: tensor([[ 1.0739],\n",
      "        [ 0.1755],\n",
      "        [-1.8882],\n",
      "        [ 1.2768],\n",
      "        [ 0.6890],\n",
      "        [-0.2879]], requires_grad=True)\n",
      "poly train loss at 2605: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2605: tensor([[ 1.0736],\n",
      "        [ 0.1745],\n",
      "        [-1.8899],\n",
      "        [ 1.2746],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2606: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2606: tensor([[ 1.0739],\n",
      "        [ 0.1755],\n",
      "        [-1.8882],\n",
      "        [ 1.2768],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2607: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2607: tensor([[ 1.0736],\n",
      "        [ 0.1745],\n",
      "        [-1.8900],\n",
      "        [ 1.2746],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2608: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2608: tensor([[ 1.0739],\n",
      "        [ 0.1755],\n",
      "        [-1.8882],\n",
      "        [ 1.2768],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2609: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2609: tensor([[ 1.0736],\n",
      "        [ 0.1745],\n",
      "        [-1.8900],\n",
      "        [ 1.2746],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2610: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2610: tensor([[ 1.0739],\n",
      "        [ 0.1755],\n",
      "        [-1.8883],\n",
      "        [ 1.2768],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2611: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2611: tensor([[ 1.0736],\n",
      "        [ 0.1744],\n",
      "        [-1.8900],\n",
      "        [ 1.2746],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2612: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2612: tensor([[ 1.0739],\n",
      "        [ 0.1754],\n",
      "        [-1.8883],\n",
      "        [ 1.2768],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2613: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2613: tensor([[ 1.0736],\n",
      "        [ 0.1744],\n",
      "        [-1.8900],\n",
      "        [ 1.2747],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2614: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2614: tensor([[ 1.0739],\n",
      "        [ 0.1754],\n",
      "        [-1.8883],\n",
      "        [ 1.2769],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2615: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2615: tensor([[ 1.0736],\n",
      "        [ 0.1744],\n",
      "        [-1.8901],\n",
      "        [ 1.2747],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2616: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2616: tensor([[ 1.0739],\n",
      "        [ 0.1754],\n",
      "        [-1.8883],\n",
      "        [ 1.2769],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2617: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2617: tensor([[ 1.0736],\n",
      "        [ 0.1744],\n",
      "        [-1.8901],\n",
      "        [ 1.2747],\n",
      "        [ 0.6867],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 2618: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2618: tensor([[ 1.0739],\n",
      "        [ 0.1754],\n",
      "        [-1.8884],\n",
      "        [ 1.2769],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2619: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2619: tensor([[ 1.0736],\n",
      "        [ 0.1743],\n",
      "        [-1.8901],\n",
      "        [ 1.2747],\n",
      "        [ 0.6867],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2620: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2620: tensor([[ 1.0739],\n",
      "        [ 0.1753],\n",
      "        [-1.8884],\n",
      "        [ 1.2769],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2621: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2621: tensor([[ 1.0736],\n",
      "        [ 0.1743],\n",
      "        [-1.8901],\n",
      "        [ 1.2747],\n",
      "        [ 0.6867],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2622: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2622: tensor([[ 1.0739],\n",
      "        [ 0.1753],\n",
      "        [-1.8884],\n",
      "        [ 1.2769],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2623: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2623: tensor([[ 1.0736],\n",
      "        [ 0.1743],\n",
      "        [-1.8902],\n",
      "        [ 1.2747],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2624: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2624: tensor([[ 1.0739],\n",
      "        [ 0.1753],\n",
      "        [-1.8884],\n",
      "        [ 1.2770],\n",
      "        [ 0.6891],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2625: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2625: tensor([[ 1.0736],\n",
      "        [ 0.1743],\n",
      "        [-1.8902],\n",
      "        [ 1.2748],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2626: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2626: tensor([[ 1.0739],\n",
      "        [ 0.1753],\n",
      "        [-1.8885],\n",
      "        [ 1.2770],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2627: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2627: tensor([[ 1.0736],\n",
      "        [ 0.1742],\n",
      "        [-1.8902],\n",
      "        [ 1.2748],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2628: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2628: tensor([[ 1.0739],\n",
      "        [ 0.1752],\n",
      "        [-1.8885],\n",
      "        [ 1.2770],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2629: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2629: tensor([[ 1.0736],\n",
      "        [ 0.1742],\n",
      "        [-1.8902],\n",
      "        [ 1.2748],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2630: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2630: tensor([[ 1.0739],\n",
      "        [ 0.1752],\n",
      "        [-1.8885],\n",
      "        [ 1.2770],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2631: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2631: tensor([[ 1.0736],\n",
      "        [ 0.1742],\n",
      "        [-1.8903],\n",
      "        [ 1.2748],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2632: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2632: tensor([[ 1.0739],\n",
      "        [ 0.1752],\n",
      "        [-1.8886],\n",
      "        [ 1.2770],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2633: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2633: tensor([[ 1.0736],\n",
      "        [ 0.1742],\n",
      "        [-1.8903],\n",
      "        [ 1.2748],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2634: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2634: tensor([[ 1.0739],\n",
      "        [ 0.1752],\n",
      "        [-1.8886],\n",
      "        [ 1.2771],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2635: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2635: tensor([[ 1.0736],\n",
      "        [ 0.1741],\n",
      "        [-1.8903],\n",
      "        [ 1.2749],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2636: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2636: tensor([[ 1.0739],\n",
      "        [ 0.1751],\n",
      "        [-1.8886],\n",
      "        [ 1.2771],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2637: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2637: tensor([[ 1.0736],\n",
      "        [ 0.1741],\n",
      "        [-1.8904],\n",
      "        [ 1.2749],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2638: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2638: tensor([[ 1.0739],\n",
      "        [ 0.1751],\n",
      "        [-1.8886],\n",
      "        [ 1.2771],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2639: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2639: tensor([[ 1.0736],\n",
      "        [ 0.1741],\n",
      "        [-1.8904],\n",
      "        [ 1.2749],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2640: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2640: tensor([[ 1.0739],\n",
      "        [ 0.1751],\n",
      "        [-1.8887],\n",
      "        [ 1.2771],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2641: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2641: tensor([[ 1.0736],\n",
      "        [ 0.1741],\n",
      "        [-1.8904],\n",
      "        [ 1.2749],\n",
      "        [ 0.6868],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2642: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2642: tensor([[ 1.0739],\n",
      "        [ 0.1751],\n",
      "        [-1.8887],\n",
      "        [ 1.2771],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2643: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2643: tensor([[ 1.0736],\n",
      "        [ 0.1740],\n",
      "        [-1.8904],\n",
      "        [ 1.2749],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2644: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2644: tensor([[ 1.0739],\n",
      "        [ 0.1750],\n",
      "        [-1.8887],\n",
      "        [ 1.2772],\n",
      "        [ 0.6892],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2645: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2645: tensor([[ 1.0736],\n",
      "        [ 0.1740],\n",
      "        [-1.8905],\n",
      "        [ 1.2750],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2646: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2646: tensor([[ 1.0739],\n",
      "        [ 0.1750],\n",
      "        [-1.8887],\n",
      "        [ 1.2772],\n",
      "        [ 0.6893],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2647: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2647: tensor([[ 1.0736],\n",
      "        [ 0.1740],\n",
      "        [-1.8905],\n",
      "        [ 1.2750],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2648: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2648: tensor([[ 1.0739],\n",
      "        [ 0.1750],\n",
      "        [-1.8888],\n",
      "        [ 1.2772],\n",
      "        [ 0.6893],\n",
      "        [-0.2880]], requires_grad=True)\n",
      "poly train loss at 2649: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2649: tensor([[ 1.0736],\n",
      "        [ 0.1740],\n",
      "        [-1.8905],\n",
      "        [ 1.2750],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2650: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2650: tensor([[ 1.0739],\n",
      "        [ 0.1750],\n",
      "        [-1.8888],\n",
      "        [ 1.2772],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2651: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2651: tensor([[ 1.0736],\n",
      "        [ 0.1739],\n",
      "        [-1.8905],\n",
      "        [ 1.2750],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2652: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2652: tensor([[ 1.0739],\n",
      "        [ 0.1749],\n",
      "        [-1.8888],\n",
      "        [ 1.2772],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2653: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2653: tensor([[ 1.0736],\n",
      "        [ 0.1739],\n",
      "        [-1.8906],\n",
      "        [ 1.2750],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2654: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2654: tensor([[ 1.0739],\n",
      "        [ 0.1749],\n",
      "        [-1.8888],\n",
      "        [ 1.2773],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2655: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2655: tensor([[ 1.0736],\n",
      "        [ 0.1739],\n",
      "        [-1.8906],\n",
      "        [ 1.2751],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2656: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2656: tensor([[ 1.0740],\n",
      "        [ 0.1749],\n",
      "        [-1.8889],\n",
      "        [ 1.2773],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2657: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2657: tensor([[ 1.0736],\n",
      "        [ 0.1739],\n",
      "        [-1.8906],\n",
      "        [ 1.2751],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2658: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2658: tensor([[ 1.0740],\n",
      "        [ 0.1749],\n",
      "        [-1.8889],\n",
      "        [ 1.2773],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2659: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2659: tensor([[ 1.0736],\n",
      "        [ 0.1738],\n",
      "        [-1.8906],\n",
      "        [ 1.2751],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2660: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2660: tensor([[ 1.0740],\n",
      "        [ 0.1748],\n",
      "        [-1.8889],\n",
      "        [ 1.2773],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2661: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2661: tensor([[ 1.0736],\n",
      "        [ 0.1738],\n",
      "        [-1.8907],\n",
      "        [ 1.2751],\n",
      "        [ 0.6869],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 2662: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2662: tensor([[ 1.0740],\n",
      "        [ 0.1748],\n",
      "        [-1.8890],\n",
      "        [ 1.2773],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2663: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2663: tensor([[ 1.0736],\n",
      "        [ 0.1738],\n",
      "        [-1.8907],\n",
      "        [ 1.2751],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2664: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2664: tensor([[ 1.0740],\n",
      "        [ 0.1748],\n",
      "        [-1.8890],\n",
      "        [ 1.2774],\n",
      "        [ 0.6893],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2665: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2665: tensor([[ 1.0736],\n",
      "        [ 0.1738],\n",
      "        [-1.8907],\n",
      "        [ 1.2752],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2666: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2666: tensor([[ 1.0740],\n",
      "        [ 0.1748],\n",
      "        [-1.8890],\n",
      "        [ 1.2774],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2667: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2667: tensor([[ 1.0736],\n",
      "        [ 0.1737],\n",
      "        [-1.8907],\n",
      "        [ 1.2752],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2668: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2668: tensor([[ 1.0740],\n",
      "        [ 0.1747],\n",
      "        [-1.8890],\n",
      "        [ 1.2774],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2669: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2669: tensor([[ 1.0736],\n",
      "        [ 0.1737],\n",
      "        [-1.8908],\n",
      "        [ 1.2752],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2670: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2670: tensor([[ 1.0740],\n",
      "        [ 0.1747],\n",
      "        [-1.8891],\n",
      "        [ 1.2774],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2671: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2671: tensor([[ 1.0736],\n",
      "        [ 0.1737],\n",
      "        [-1.8908],\n",
      "        [ 1.2752],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2672: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2672: tensor([[ 1.0740],\n",
      "        [ 0.1747],\n",
      "        [-1.8891],\n",
      "        [ 1.2774],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2673: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2673: tensor([[ 1.0736],\n",
      "        [ 0.1737],\n",
      "        [-1.8908],\n",
      "        [ 1.2752],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2674: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2674: tensor([[ 1.0740],\n",
      "        [ 0.1747],\n",
      "        [-1.8891],\n",
      "        [ 1.2774],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2675: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2675: tensor([[ 1.0736],\n",
      "        [ 0.1736],\n",
      "        [-1.8909],\n",
      "        [ 1.2753],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2676: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2676: tensor([[ 1.0740],\n",
      "        [ 0.1746],\n",
      "        [-1.8891],\n",
      "        [ 1.2775],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2677: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2677: tensor([[ 1.0736],\n",
      "        [ 0.1736],\n",
      "        [-1.8909],\n",
      "        [ 1.2753],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2678: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2678: tensor([[ 1.0740],\n",
      "        [ 0.1746],\n",
      "        [-1.8892],\n",
      "        [ 1.2775],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2679: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2679: tensor([[ 1.0736],\n",
      "        [ 0.1736],\n",
      "        [-1.8909],\n",
      "        [ 1.2753],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2680: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2680: tensor([[ 1.0740],\n",
      "        [ 0.1746],\n",
      "        [-1.8892],\n",
      "        [ 1.2775],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2681: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2681: tensor([[ 1.0736],\n",
      "        [ 0.1736],\n",
      "        [-1.8909],\n",
      "        [ 1.2753],\n",
      "        [ 0.6870],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2682: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2682: tensor([[ 1.0740],\n",
      "        [ 0.1746],\n",
      "        [-1.8892],\n",
      "        [ 1.2775],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2683: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2683: tensor([[ 1.0736],\n",
      "        [ 0.1735],\n",
      "        [-1.8910],\n",
      "        [ 1.2753],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2684: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2684: tensor([[ 1.0740],\n",
      "        [ 0.1746],\n",
      "        [-1.8892],\n",
      "        [ 1.2775],\n",
      "        [ 0.6894],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2685: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2685: tensor([[ 1.0736],\n",
      "        [ 0.1735],\n",
      "        [-1.8910],\n",
      "        [ 1.2754],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2686: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2686: tensor([[ 1.0740],\n",
      "        [ 0.1745],\n",
      "        [-1.8893],\n",
      "        [ 1.2776],\n",
      "        [ 0.6895],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2687: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2687: tensor([[ 1.0736],\n",
      "        [ 0.1735],\n",
      "        [-1.8910],\n",
      "        [ 1.2754],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2688: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2688: tensor([[ 1.0740],\n",
      "        [ 0.1745],\n",
      "        [-1.8893],\n",
      "        [ 1.2776],\n",
      "        [ 0.6895],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2689: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2689: tensor([[ 1.0736],\n",
      "        [ 0.1735],\n",
      "        [-1.8910],\n",
      "        [ 1.2754],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2690: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2690: tensor([[ 1.0740],\n",
      "        [ 0.1745],\n",
      "        [-1.8893],\n",
      "        [ 1.2776],\n",
      "        [ 0.6895],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2691: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2691: tensor([[ 1.0736],\n",
      "        [ 0.1734],\n",
      "        [-1.8911],\n",
      "        [ 1.2754],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2692: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2692: tensor([[ 1.0740],\n",
      "        [ 0.1745],\n",
      "        [-1.8893],\n",
      "        [ 1.2776],\n",
      "        [ 0.6895],\n",
      "        [-0.2881]], requires_grad=True)\n",
      "poly train loss at 2693: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2693: tensor([[ 1.0736],\n",
      "        [ 0.1734],\n",
      "        [-1.8911],\n",
      "        [ 1.2754],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2694: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2694: tensor([[ 1.0740],\n",
      "        [ 0.1744],\n",
      "        [-1.8894],\n",
      "        [ 1.2776],\n",
      "        [ 0.6895],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2695: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2695: tensor([[ 1.0736],\n",
      "        [ 0.1734],\n",
      "        [-1.8911],\n",
      "        [ 1.2754],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2696: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2696: tensor([[ 1.0740],\n",
      "        [ 0.1744],\n",
      "        [-1.8894],\n",
      "        [ 1.2777],\n",
      "        [ 0.6895],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2697: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2697: tensor([[ 1.0736],\n",
      "        [ 0.1734],\n",
      "        [-1.8911],\n",
      "        [ 1.2755],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2698: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2698: tensor([[ 1.0740],\n",
      "        [ 0.1744],\n",
      "        [-1.8894],\n",
      "        [ 1.2777],\n",
      "        [ 0.6895],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2699: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2699: tensor([[ 1.0736],\n",
      "        [ 0.1733],\n",
      "        [-1.8912],\n",
      "        [ 1.2755],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2700: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2700: tensor([[ 1.0740],\n",
      "        [ 0.1744],\n",
      "        [-1.8895],\n",
      "        [ 1.2777],\n",
      "        [ 0.6895],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2701: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2701: tensor([[ 1.0736],\n",
      "        [ 0.1733],\n",
      "        [-1.8912],\n",
      "        [ 1.2755],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2702: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2702: tensor([[ 1.0740],\n",
      "        [ 0.1743],\n",
      "        [-1.8895],\n",
      "        [ 1.2777],\n",
      "        [ 0.6895],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2703: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2703: tensor([[ 1.0736],\n",
      "        [ 0.1733],\n",
      "        [-1.8912],\n",
      "        [ 1.2755],\n",
      "        [ 0.6871],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2704: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2704: tensor([[ 1.0740],\n",
      "        [ 0.1743],\n",
      "        [-1.8895],\n",
      "        [ 1.2777],\n",
      "        [ 0.6895],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2705: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2705: tensor([[ 1.0736],\n",
      "        [ 0.1733],\n",
      "        [-1.8913],\n",
      "        [ 1.2755],\n",
      "        [ 0.6872],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 2706: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2706: tensor([[ 1.0740],\n",
      "        [ 0.1743],\n",
      "        [-1.8895],\n",
      "        [ 1.2778],\n",
      "        [ 0.6895],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2707: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2707: tensor([[ 1.0736],\n",
      "        [ 0.1732],\n",
      "        [-1.8913],\n",
      "        [ 1.2756],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2708: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2708: tensor([[ 1.0740],\n",
      "        [ 0.1743],\n",
      "        [-1.8896],\n",
      "        [ 1.2778],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2709: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2709: tensor([[ 1.0736],\n",
      "        [ 0.1732],\n",
      "        [-1.8913],\n",
      "        [ 1.2756],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2710: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2710: tensor([[ 1.0740],\n",
      "        [ 0.1742],\n",
      "        [-1.8896],\n",
      "        [ 1.2778],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2711: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2711: tensor([[ 1.0736],\n",
      "        [ 0.1732],\n",
      "        [-1.8913],\n",
      "        [ 1.2756],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2712: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2712: tensor([[ 1.0740],\n",
      "        [ 0.1742],\n",
      "        [-1.8896],\n",
      "        [ 1.2778],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2713: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2713: tensor([[ 1.0736],\n",
      "        [ 0.1732],\n",
      "        [-1.8914],\n",
      "        [ 1.2756],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2714: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2714: tensor([[ 1.0740],\n",
      "        [ 0.1742],\n",
      "        [-1.8896],\n",
      "        [ 1.2778],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2715: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2715: tensor([[ 1.0736],\n",
      "        [ 0.1732],\n",
      "        [-1.8914],\n",
      "        [ 1.2756],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2716: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2716: tensor([[ 1.0740],\n",
      "        [ 0.1742],\n",
      "        [-1.8897],\n",
      "        [ 1.2778],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2717: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2717: tensor([[ 1.0736],\n",
      "        [ 0.1731],\n",
      "        [-1.8914],\n",
      "        [ 1.2757],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2718: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2718: tensor([[ 1.0740],\n",
      "        [ 0.1741],\n",
      "        [-1.8897],\n",
      "        [ 1.2779],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2719: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2719: tensor([[ 1.0736],\n",
      "        [ 0.1731],\n",
      "        [-1.8914],\n",
      "        [ 1.2757],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2720: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2720: tensor([[ 1.0740],\n",
      "        [ 0.1741],\n",
      "        [-1.8897],\n",
      "        [ 1.2779],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2721: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2721: tensor([[ 1.0736],\n",
      "        [ 0.1731],\n",
      "        [-1.8915],\n",
      "        [ 1.2757],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2722: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2722: tensor([[ 1.0740],\n",
      "        [ 0.1741],\n",
      "        [-1.8897],\n",
      "        [ 1.2779],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2723: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2723: tensor([[ 1.0736],\n",
      "        [ 0.1731],\n",
      "        [-1.8915],\n",
      "        [ 1.2757],\n",
      "        [ 0.6872],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2724: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2724: tensor([[ 1.0740],\n",
      "        [ 0.1741],\n",
      "        [-1.8898],\n",
      "        [ 1.2779],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2725: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2725: tensor([[ 1.0736],\n",
      "        [ 0.1730],\n",
      "        [-1.8915],\n",
      "        [ 1.2757],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2726: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2726: tensor([[ 1.0740],\n",
      "        [ 0.1740],\n",
      "        [-1.8898],\n",
      "        [ 1.2779],\n",
      "        [ 0.6896],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2727: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2727: tensor([[ 1.0736],\n",
      "        [ 0.1730],\n",
      "        [-1.8915],\n",
      "        [ 1.2758],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2728: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2728: tensor([[ 1.0740],\n",
      "        [ 0.1740],\n",
      "        [-1.8898],\n",
      "        [ 1.2780],\n",
      "        [ 0.6897],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2729: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2729: tensor([[ 1.0736],\n",
      "        [ 0.1730],\n",
      "        [-1.8916],\n",
      "        [ 1.2758],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2730: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2730: tensor([[ 1.0740],\n",
      "        [ 0.1740],\n",
      "        [-1.8898],\n",
      "        [ 1.2780],\n",
      "        [ 0.6897],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2731: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2731: tensor([[ 1.0736],\n",
      "        [ 0.1730],\n",
      "        [-1.8916],\n",
      "        [ 1.2758],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2732: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2732: tensor([[ 1.0740],\n",
      "        [ 0.1740],\n",
      "        [-1.8899],\n",
      "        [ 1.2780],\n",
      "        [ 0.6897],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2733: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2733: tensor([[ 1.0736],\n",
      "        [ 0.1729],\n",
      "        [-1.8916],\n",
      "        [ 1.2758],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2734: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2734: tensor([[ 1.0740],\n",
      "        [ 0.1739],\n",
      "        [-1.8899],\n",
      "        [ 1.2780],\n",
      "        [ 0.6897],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2735: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2735: tensor([[ 1.0736],\n",
      "        [ 0.1729],\n",
      "        [-1.8916],\n",
      "        [ 1.2758],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2736: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2736: tensor([[ 1.0740],\n",
      "        [ 0.1739],\n",
      "        [-1.8899],\n",
      "        [ 1.2780],\n",
      "        [ 0.6897],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2737: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2737: tensor([[ 1.0736],\n",
      "        [ 0.1729],\n",
      "        [-1.8917],\n",
      "        [ 1.2758],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2738: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2738: tensor([[ 1.0740],\n",
      "        [ 0.1739],\n",
      "        [-1.8900],\n",
      "        [ 1.2781],\n",
      "        [ 0.6897],\n",
      "        [-0.2882]], requires_grad=True)\n",
      "poly train loss at 2739: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2739: tensor([[ 1.0736],\n",
      "        [ 0.1729],\n",
      "        [-1.8917],\n",
      "        [ 1.2759],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2740: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2740: tensor([[ 1.0740],\n",
      "        [ 0.1739],\n",
      "        [-1.8900],\n",
      "        [ 1.2781],\n",
      "        [ 0.6897],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2741: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2741: tensor([[ 1.0736],\n",
      "        [ 0.1728],\n",
      "        [-1.8917],\n",
      "        [ 1.2759],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2742: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2742: tensor([[ 1.0740],\n",
      "        [ 0.1738],\n",
      "        [-1.8900],\n",
      "        [ 1.2781],\n",
      "        [ 0.6897],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2743: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2743: tensor([[ 1.0736],\n",
      "        [ 0.1728],\n",
      "        [-1.8917],\n",
      "        [ 1.2759],\n",
      "        [ 0.6873],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2744: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2744: tensor([[ 1.0740],\n",
      "        [ 0.1738],\n",
      "        [-1.8900],\n",
      "        [ 1.2781],\n",
      "        [ 0.6897],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2745: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2745: tensor([[ 1.0736],\n",
      "        [ 0.1728],\n",
      "        [-1.8918],\n",
      "        [ 1.2759],\n",
      "        [ 0.6874],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2746: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2746: tensor([[ 1.0740],\n",
      "        [ 0.1738],\n",
      "        [-1.8901],\n",
      "        [ 1.2781],\n",
      "        [ 0.6897],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2747: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2747: tensor([[ 1.0736],\n",
      "        [ 0.1728],\n",
      "        [-1.8918],\n",
      "        [ 1.2759],\n",
      "        [ 0.6874],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2748: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2748: tensor([[ 1.0740],\n",
      "        [ 0.1738],\n",
      "        [-1.8901],\n",
      "        [ 1.2782],\n",
      "        [ 0.6897],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2749: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2749: tensor([[ 1.0736],\n",
      "        [ 0.1727],\n",
      "        [-1.8918],\n",
      "        [ 1.2760],\n",
      "        [ 0.6874],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2750: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2750: tensor([[ 1.0740],\n",
      "        [ 0.1738],\n",
      "        [-1.8901],\n",
      "        [ 1.2782],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2751: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2751: tensor([[ 1.0736],\n",
      "        [ 0.1727],\n",
      "        [-1.8919],\n",
      "        [ 1.2760],\n",
      "        [ 0.6874],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 2752: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2752: tensor([[ 1.0740],\n",
      "        [ 0.1737],\n",
      "        [-1.8901],\n",
      "        [ 1.2782],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2753: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2753: tensor([[ 1.0736],\n",
      "        [ 0.1727],\n",
      "        [-1.8919],\n",
      "        [ 1.2760],\n",
      "        [ 0.6874],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2754: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2754: tensor([[ 1.0740],\n",
      "        [ 0.1737],\n",
      "        [-1.8902],\n",
      "        [ 1.2782],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2755: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2755: tensor([[ 1.0736],\n",
      "        [ 0.1727],\n",
      "        [-1.8919],\n",
      "        [ 1.2760],\n",
      "        [ 0.6874],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2756: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2756: tensor([[ 1.0740],\n",
      "        [ 0.1737],\n",
      "        [-1.8902],\n",
      "        [ 1.2782],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2757: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2757: tensor([[ 1.0736],\n",
      "        [ 0.1726],\n",
      "        [-1.8919],\n",
      "        [ 1.2760],\n",
      "        [ 0.6874],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2758: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2758: tensor([[ 1.0740],\n",
      "        [ 0.1737],\n",
      "        [-1.8902],\n",
      "        [ 1.2782],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2759: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2759: tensor([[ 1.0736],\n",
      "        [ 0.1726],\n",
      "        [-1.8920],\n",
      "        [ 1.2761],\n",
      "        [ 0.6874],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2760: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2760: tensor([[ 1.0740],\n",
      "        [ 0.1736],\n",
      "        [-1.8902],\n",
      "        [ 1.2783],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2761: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2761: tensor([[ 1.0736],\n",
      "        [ 0.1726],\n",
      "        [-1.8920],\n",
      "        [ 1.2761],\n",
      "        [ 0.6874],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2762: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2762: tensor([[ 1.0740],\n",
      "        [ 0.1736],\n",
      "        [-1.8903],\n",
      "        [ 1.2783],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2763: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2763: tensor([[ 1.0736],\n",
      "        [ 0.1726],\n",
      "        [-1.8920],\n",
      "        [ 1.2761],\n",
      "        [ 0.6874],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2764: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2764: tensor([[ 1.0740],\n",
      "        [ 0.1736],\n",
      "        [-1.8903],\n",
      "        [ 1.2783],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2765: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2765: tensor([[ 1.0736],\n",
      "        [ 0.1725],\n",
      "        [-1.8920],\n",
      "        [ 1.2761],\n",
      "        [ 0.6874],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2766: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2766: tensor([[ 1.0740],\n",
      "        [ 0.1736],\n",
      "        [-1.8903],\n",
      "        [ 1.2783],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2767: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2767: tensor([[ 1.0736],\n",
      "        [ 0.1725],\n",
      "        [-1.8921],\n",
      "        [ 1.2761],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2768: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2768: tensor([[ 1.0740],\n",
      "        [ 0.1735],\n",
      "        [-1.8903],\n",
      "        [ 1.2783],\n",
      "        [ 0.6898],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2769: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2769: tensor([[ 1.0736],\n",
      "        [ 0.1725],\n",
      "        [-1.8921],\n",
      "        [ 1.2762],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2770: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2770: tensor([[ 1.0740],\n",
      "        [ 0.1735],\n",
      "        [-1.8904],\n",
      "        [ 1.2784],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2771: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2771: tensor([[ 1.0736],\n",
      "        [ 0.1725],\n",
      "        [-1.8921],\n",
      "        [ 1.2762],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2772: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2772: tensor([[ 1.0740],\n",
      "        [ 0.1735],\n",
      "        [-1.8904],\n",
      "        [ 1.2784],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2773: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2773: tensor([[ 1.0737],\n",
      "        [ 0.1725],\n",
      "        [-1.8921],\n",
      "        [ 1.2762],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2774: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2774: tensor([[ 1.0740],\n",
      "        [ 0.1735],\n",
      "        [-1.8904],\n",
      "        [ 1.2784],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2775: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2775: tensor([[ 1.0737],\n",
      "        [ 0.1724],\n",
      "        [-1.8922],\n",
      "        [ 1.2762],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2776: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2776: tensor([[ 1.0740],\n",
      "        [ 0.1734],\n",
      "        [-1.8904],\n",
      "        [ 1.2784],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2777: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2777: tensor([[ 1.0737],\n",
      "        [ 0.1724],\n",
      "        [-1.8922],\n",
      "        [ 1.2762],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2778: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2778: tensor([[ 1.0740],\n",
      "        [ 0.1734],\n",
      "        [-1.8905],\n",
      "        [ 1.2784],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2779: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2779: tensor([[ 1.0737],\n",
      "        [ 0.1724],\n",
      "        [-1.8922],\n",
      "        [ 1.2762],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2780: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2780: tensor([[ 1.0740],\n",
      "        [ 0.1734],\n",
      "        [-1.8905],\n",
      "        [ 1.2785],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2781: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2781: tensor([[ 1.0737],\n",
      "        [ 0.1724],\n",
      "        [-1.8922],\n",
      "        [ 1.2763],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2782: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2782: tensor([[ 1.0740],\n",
      "        [ 0.1734],\n",
      "        [-1.8905],\n",
      "        [ 1.2785],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2783: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2783: tensor([[ 1.0737],\n",
      "        [ 0.1723],\n",
      "        [-1.8923],\n",
      "        [ 1.2763],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2784: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2784: tensor([[ 1.0740],\n",
      "        [ 0.1733],\n",
      "        [-1.8905],\n",
      "        [ 1.2785],\n",
      "        [ 0.6899],\n",
      "        [-0.2883]], requires_grad=True)\n",
      "poly train loss at 2785: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2785: tensor([[ 1.0737],\n",
      "        [ 0.1723],\n",
      "        [-1.8923],\n",
      "        [ 1.2763],\n",
      "        [ 0.6875],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2786: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2786: tensor([[ 1.0740],\n",
      "        [ 0.1733],\n",
      "        [-1.8906],\n",
      "        [ 1.2785],\n",
      "        [ 0.6899],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2787: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2787: tensor([[ 1.0737],\n",
      "        [ 0.1723],\n",
      "        [-1.8923],\n",
      "        [ 1.2763],\n",
      "        [ 0.6876],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2788: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2788: tensor([[ 1.0740],\n",
      "        [ 0.1733],\n",
      "        [-1.8906],\n",
      "        [ 1.2785],\n",
      "        [ 0.6899],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2789: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2789: tensor([[ 1.0737],\n",
      "        [ 0.1723],\n",
      "        [-1.8923],\n",
      "        [ 1.2763],\n",
      "        [ 0.6876],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2790: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2790: tensor([[ 1.0740],\n",
      "        [ 0.1733],\n",
      "        [-1.8906],\n",
      "        [ 1.2786],\n",
      "        [ 0.6899],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2791: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2791: tensor([[ 1.0737],\n",
      "        [ 0.1722],\n",
      "        [-1.8924],\n",
      "        [ 1.2764],\n",
      "        [ 0.6876],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2792: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2792: tensor([[ 1.0740],\n",
      "        [ 0.1732],\n",
      "        [-1.8906],\n",
      "        [ 1.2786],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2793: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2793: tensor([[ 1.0737],\n",
      "        [ 0.1722],\n",
      "        [-1.8924],\n",
      "        [ 1.2764],\n",
      "        [ 0.6876],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2794: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2794: tensor([[ 1.0740],\n",
      "        [ 0.1732],\n",
      "        [-1.8907],\n",
      "        [ 1.2786],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2795: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2795: tensor([[ 1.0737],\n",
      "        [ 0.1722],\n",
      "        [-1.8924],\n",
      "        [ 1.2764],\n",
      "        [ 0.6876],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2796: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2796: tensor([[ 1.0740],\n",
      "        [ 0.1732],\n",
      "        [-1.8907],\n",
      "        [ 1.2786],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2797: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2797: tensor([[ 1.0737],\n",
      "        [ 0.1722],\n",
      "        [-1.8924],\n",
      "        [ 1.2764],\n",
      "        [ 0.6876],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 2798: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2798: tensor([[ 1.0740],\n",
      "        [ 0.1732],\n",
      "        [-1.8907],\n",
      "        [ 1.2786],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2799: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2799: tensor([[ 1.0737],\n",
      "        [ 0.1721],\n",
      "        [-1.8925],\n",
      "        [ 1.2764],\n",
      "        [ 0.6876],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2800: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2800: tensor([[ 1.0740],\n",
      "        [ 0.1731],\n",
      "        [-1.8908],\n",
      "        [ 1.2786],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2801: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2801: tensor([[ 1.0737],\n",
      "        [ 0.1721],\n",
      "        [-1.8925],\n",
      "        [ 1.2765],\n",
      "        [ 0.6876],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2802: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2802: tensor([[ 1.0740],\n",
      "        [ 0.1731],\n",
      "        [-1.8908],\n",
      "        [ 1.2787],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2803: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2803: tensor([[ 1.0737],\n",
      "        [ 0.1721],\n",
      "        [-1.8925],\n",
      "        [ 1.2765],\n",
      "        [ 0.6876],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2804: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2804: tensor([[ 1.0740],\n",
      "        [ 0.1731],\n",
      "        [-1.8908],\n",
      "        [ 1.2787],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2805: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2805: tensor([[ 1.0737],\n",
      "        [ 0.1721],\n",
      "        [-1.8925],\n",
      "        [ 1.2765],\n",
      "        [ 0.6876],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2806: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2806: tensor([[ 1.0740],\n",
      "        [ 0.1731],\n",
      "        [-1.8908],\n",
      "        [ 1.2787],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2807: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2807: tensor([[ 1.0737],\n",
      "        [ 0.1720],\n",
      "        [-1.8926],\n",
      "        [ 1.2765],\n",
      "        [ 0.6876],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2808: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2808: tensor([[ 1.0740],\n",
      "        [ 0.1731],\n",
      "        [-1.8909],\n",
      "        [ 1.2787],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2809: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2809: tensor([[ 1.0737],\n",
      "        [ 0.1720],\n",
      "        [-1.8926],\n",
      "        [ 1.2765],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2810: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2810: tensor([[ 1.0740],\n",
      "        [ 0.1730],\n",
      "        [-1.8909],\n",
      "        [ 1.2787],\n",
      "        [ 0.6900],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2811: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2811: tensor([[ 1.0737],\n",
      "        [ 0.1720],\n",
      "        [-1.8926],\n",
      "        [ 1.2765],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2812: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2812: tensor([[ 1.0740],\n",
      "        [ 0.1730],\n",
      "        [-1.8909],\n",
      "        [ 1.2788],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2813: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2813: tensor([[ 1.0737],\n",
      "        [ 0.1720],\n",
      "        [-1.8927],\n",
      "        [ 1.2766],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2814: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2814: tensor([[ 1.0740],\n",
      "        [ 0.1730],\n",
      "        [-1.8909],\n",
      "        [ 1.2788],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2815: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2815: tensor([[ 1.0737],\n",
      "        [ 0.1720],\n",
      "        [-1.8927],\n",
      "        [ 1.2766],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2816: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2816: tensor([[ 1.0740],\n",
      "        [ 0.1730],\n",
      "        [-1.8910],\n",
      "        [ 1.2788],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2817: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2817: tensor([[ 1.0737],\n",
      "        [ 0.1719],\n",
      "        [-1.8927],\n",
      "        [ 1.2766],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2818: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2818: tensor([[ 1.0740],\n",
      "        [ 0.1729],\n",
      "        [-1.8910],\n",
      "        [ 1.2788],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2819: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2819: tensor([[ 1.0737],\n",
      "        [ 0.1719],\n",
      "        [-1.8927],\n",
      "        [ 1.2766],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2820: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2820: tensor([[ 1.0740],\n",
      "        [ 0.1729],\n",
      "        [-1.8910],\n",
      "        [ 1.2788],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2821: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2821: tensor([[ 1.0737],\n",
      "        [ 0.1719],\n",
      "        [-1.8928],\n",
      "        [ 1.2766],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2822: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2822: tensor([[ 1.0740],\n",
      "        [ 0.1729],\n",
      "        [-1.8910],\n",
      "        [ 1.2789],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2823: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2823: tensor([[ 1.0737],\n",
      "        [ 0.1719],\n",
      "        [-1.8928],\n",
      "        [ 1.2767],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2824: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2824: tensor([[ 1.0740],\n",
      "        [ 0.1729],\n",
      "        [-1.8911],\n",
      "        [ 1.2789],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2825: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2825: tensor([[ 1.0737],\n",
      "        [ 0.1718],\n",
      "        [-1.8928],\n",
      "        [ 1.2767],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2826: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2826: tensor([[ 1.0740],\n",
      "        [ 0.1728],\n",
      "        [-1.8911],\n",
      "        [ 1.2789],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2827: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2827: tensor([[ 1.0737],\n",
      "        [ 0.1718],\n",
      "        [-1.8928],\n",
      "        [ 1.2767],\n",
      "        [ 0.6877],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2828: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2828: tensor([[ 1.0740],\n",
      "        [ 0.1728],\n",
      "        [-1.8911],\n",
      "        [ 1.2789],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2829: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2829: tensor([[ 1.0737],\n",
      "        [ 0.1718],\n",
      "        [-1.8929],\n",
      "        [ 1.2767],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2830: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2830: tensor([[ 1.0740],\n",
      "        [ 0.1728],\n",
      "        [-1.8911],\n",
      "        [ 1.2789],\n",
      "        [ 0.6901],\n",
      "        [-0.2884]], requires_grad=True)\n",
      "poly train loss at 2831: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2831: tensor([[ 1.0737],\n",
      "        [ 0.1718],\n",
      "        [-1.8929],\n",
      "        [ 1.2767],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2832: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2832: tensor([[ 1.0740],\n",
      "        [ 0.1728],\n",
      "        [-1.8912],\n",
      "        [ 1.2789],\n",
      "        [ 0.6901],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2833: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2833: tensor([[ 1.0737],\n",
      "        [ 0.1717],\n",
      "        [-1.8929],\n",
      "        [ 1.2768],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2834: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2834: tensor([[ 1.0740],\n",
      "        [ 0.1727],\n",
      "        [-1.8912],\n",
      "        [ 1.2790],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2835: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2835: tensor([[ 1.0737],\n",
      "        [ 0.1717],\n",
      "        [-1.8929],\n",
      "        [ 1.2768],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2836: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2836: tensor([[ 1.0740],\n",
      "        [ 0.1727],\n",
      "        [-1.8912],\n",
      "        [ 1.2790],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2837: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2837: tensor([[ 1.0737],\n",
      "        [ 0.1717],\n",
      "        [-1.8930],\n",
      "        [ 1.2768],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2838: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2838: tensor([[ 1.0740],\n",
      "        [ 0.1727],\n",
      "        [-1.8912],\n",
      "        [ 1.2790],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2839: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2839: tensor([[ 1.0737],\n",
      "        [ 0.1717],\n",
      "        [-1.8930],\n",
      "        [ 1.2768],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2840: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2840: tensor([[ 1.0740],\n",
      "        [ 0.1727],\n",
      "        [-1.8913],\n",
      "        [ 1.2790],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2841: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2841: tensor([[ 1.0737],\n",
      "        [ 0.1716],\n",
      "        [-1.8930],\n",
      "        [ 1.2768],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2842: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2842: tensor([[ 1.0740],\n",
      "        [ 0.1726],\n",
      "        [-1.8913],\n",
      "        [ 1.2790],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2843: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2843: tensor([[ 1.0737],\n",
      "        [ 0.1716],\n",
      "        [-1.8930],\n",
      "        [ 1.2769],\n",
      "        [ 0.6878],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 2844: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2844: tensor([[ 1.0740],\n",
      "        [ 0.1726],\n",
      "        [-1.8913],\n",
      "        [ 1.2791],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2845: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2845: tensor([[ 1.0737],\n",
      "        [ 0.1716],\n",
      "        [-1.8931],\n",
      "        [ 1.2769],\n",
      "        [ 0.6878],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2846: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2846: tensor([[ 1.0740],\n",
      "        [ 0.1726],\n",
      "        [-1.8913],\n",
      "        [ 1.2791],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2847: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2847: tensor([[ 1.0737],\n",
      "        [ 0.1716],\n",
      "        [-1.8931],\n",
      "        [ 1.2769],\n",
      "        [ 0.6878],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2848: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2848: tensor([[ 1.0740],\n",
      "        [ 0.1726],\n",
      "        [-1.8914],\n",
      "        [ 1.2791],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2849: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2849: tensor([[ 1.0737],\n",
      "        [ 0.1715],\n",
      "        [-1.8931],\n",
      "        [ 1.2769],\n",
      "        [ 0.6878],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2850: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2850: tensor([[ 1.0740],\n",
      "        [ 0.1726],\n",
      "        [-1.8914],\n",
      "        [ 1.2791],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2851: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2851: tensor([[ 1.0737],\n",
      "        [ 0.1715],\n",
      "        [-1.8931],\n",
      "        [ 1.2769],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2852: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2852: tensor([[ 1.0740],\n",
      "        [ 0.1725],\n",
      "        [-1.8914],\n",
      "        [ 1.2791],\n",
      "        [ 0.6902],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2853: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2853: tensor([[ 1.0737],\n",
      "        [ 0.1715],\n",
      "        [-1.8932],\n",
      "        [ 1.2769],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2854: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2854: tensor([[ 1.0740],\n",
      "        [ 0.1725],\n",
      "        [-1.8914],\n",
      "        [ 1.2792],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2855: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2855: tensor([[ 1.0737],\n",
      "        [ 0.1715],\n",
      "        [-1.8932],\n",
      "        [ 1.2770],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2856: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2856: tensor([[ 1.0740],\n",
      "        [ 0.1725],\n",
      "        [-1.8915],\n",
      "        [ 1.2792],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2857: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2857: tensor([[ 1.0737],\n",
      "        [ 0.1715],\n",
      "        [-1.8932],\n",
      "        [ 1.2770],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2858: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2858: tensor([[ 1.0740],\n",
      "        [ 0.1725],\n",
      "        [-1.8915],\n",
      "        [ 1.2792],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2859: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2859: tensor([[ 1.0737],\n",
      "        [ 0.1714],\n",
      "        [-1.8932],\n",
      "        [ 1.2770],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2860: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2860: tensor([[ 1.0740],\n",
      "        [ 0.1724],\n",
      "        [-1.8915],\n",
      "        [ 1.2792],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2861: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2861: tensor([[ 1.0737],\n",
      "        [ 0.1714],\n",
      "        [-1.8933],\n",
      "        [ 1.2770],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2862: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2862: tensor([[ 1.0740],\n",
      "        [ 0.1724],\n",
      "        [-1.8916],\n",
      "        [ 1.2792],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2863: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2863: tensor([[ 1.0737],\n",
      "        [ 0.1714],\n",
      "        [-1.8933],\n",
      "        [ 1.2770],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2864: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2864: tensor([[ 1.0740],\n",
      "        [ 0.1724],\n",
      "        [-1.8916],\n",
      "        [ 1.2792],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2865: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2865: tensor([[ 1.0737],\n",
      "        [ 0.1714],\n",
      "        [-1.8933],\n",
      "        [ 1.2771],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2866: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2866: tensor([[ 1.0740],\n",
      "        [ 0.1724],\n",
      "        [-1.8916],\n",
      "        [ 1.2793],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2867: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2867: tensor([[ 1.0737],\n",
      "        [ 0.1713],\n",
      "        [-1.8933],\n",
      "        [ 1.2771],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2868: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2868: tensor([[ 1.0740],\n",
      "        [ 0.1723],\n",
      "        [-1.8916],\n",
      "        [ 1.2793],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2869: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2869: tensor([[ 1.0737],\n",
      "        [ 0.1713],\n",
      "        [-1.8934],\n",
      "        [ 1.2771],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2870: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2870: tensor([[ 1.0740],\n",
      "        [ 0.1723],\n",
      "        [-1.8917],\n",
      "        [ 1.2793],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2871: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2871: tensor([[ 1.0737],\n",
      "        [ 0.1713],\n",
      "        [-1.8934],\n",
      "        [ 1.2771],\n",
      "        [ 0.6879],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2872: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2872: tensor([[ 1.0740],\n",
      "        [ 0.1723],\n",
      "        [-1.8917],\n",
      "        [ 1.2793],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2873: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2873: tensor([[ 1.0737],\n",
      "        [ 0.1713],\n",
      "        [-1.8934],\n",
      "        [ 1.2771],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2874: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2874: tensor([[ 1.0740],\n",
      "        [ 0.1723],\n",
      "        [-1.8917],\n",
      "        [ 1.2793],\n",
      "        [ 0.6903],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2875: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2875: tensor([[ 1.0737],\n",
      "        [ 0.1712],\n",
      "        [-1.8934],\n",
      "        [ 1.2771],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2876: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2876: tensor([[ 1.0740],\n",
      "        [ 0.1722],\n",
      "        [-1.8917],\n",
      "        [ 1.2794],\n",
      "        [ 0.6904],\n",
      "        [-0.2885]], requires_grad=True)\n",
      "poly train loss at 2877: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2877: tensor([[ 1.0737],\n",
      "        [ 0.1712],\n",
      "        [-1.8935],\n",
      "        [ 1.2772],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2878: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2878: tensor([[ 1.0740],\n",
      "        [ 0.1722],\n",
      "        [-1.8918],\n",
      "        [ 1.2794],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2879: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2879: tensor([[ 1.0737],\n",
      "        [ 0.1712],\n",
      "        [-1.8935],\n",
      "        [ 1.2772],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2880: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2880: tensor([[ 1.0740],\n",
      "        [ 0.1722],\n",
      "        [-1.8918],\n",
      "        [ 1.2794],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2881: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2881: tensor([[ 1.0737],\n",
      "        [ 0.1712],\n",
      "        [-1.8935],\n",
      "        [ 1.2772],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2882: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2882: tensor([[ 1.0740],\n",
      "        [ 0.1722],\n",
      "        [-1.8918],\n",
      "        [ 1.2794],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2883: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2883: tensor([[ 1.0737],\n",
      "        [ 0.1711],\n",
      "        [-1.8935],\n",
      "        [ 1.2772],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2884: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2884: tensor([[ 1.0740],\n",
      "        [ 0.1722],\n",
      "        [-1.8918],\n",
      "        [ 1.2794],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2885: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2885: tensor([[ 1.0737],\n",
      "        [ 0.1711],\n",
      "        [-1.8936],\n",
      "        [ 1.2772],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2886: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2886: tensor([[ 1.0740],\n",
      "        [ 0.1721],\n",
      "        [-1.8919],\n",
      "        [ 1.2795],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2887: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2887: tensor([[ 1.0737],\n",
      "        [ 0.1711],\n",
      "        [-1.8936],\n",
      "        [ 1.2773],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2888: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2888: tensor([[ 1.0740],\n",
      "        [ 0.1721],\n",
      "        [-1.8919],\n",
      "        [ 1.2795],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2889: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2889: tensor([[ 1.0737],\n",
      "        [ 0.1711],\n",
      "        [-1.8936],\n",
      "        [ 1.2773],\n",
      "        [ 0.6880],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 2890: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2890: tensor([[ 1.0740],\n",
      "        [ 0.1721],\n",
      "        [-1.8919],\n",
      "        [ 1.2795],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2891: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2891: tensor([[ 1.0737],\n",
      "        [ 0.1710],\n",
      "        [-1.8937],\n",
      "        [ 1.2773],\n",
      "        [ 0.6880],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2892: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2892: tensor([[ 1.0740],\n",
      "        [ 0.1721],\n",
      "        [-1.8919],\n",
      "        [ 1.2795],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2893: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2893: tensor([[ 1.0737],\n",
      "        [ 0.1710],\n",
      "        [-1.8937],\n",
      "        [ 1.2773],\n",
      "        [ 0.6880],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2894: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2894: tensor([[ 1.0740],\n",
      "        [ 0.1720],\n",
      "        [-1.8920],\n",
      "        [ 1.2795],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2895: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2895: tensor([[ 1.0737],\n",
      "        [ 0.1710],\n",
      "        [-1.8937],\n",
      "        [ 1.2773],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2896: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2896: tensor([[ 1.0740],\n",
      "        [ 0.1720],\n",
      "        [-1.8920],\n",
      "        [ 1.2795],\n",
      "        [ 0.6904],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2897: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2897: tensor([[ 1.0737],\n",
      "        [ 0.1710],\n",
      "        [-1.8937],\n",
      "        [ 1.2774],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2898: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2898: tensor([[ 1.0740],\n",
      "        [ 0.1720],\n",
      "        [-1.8920],\n",
      "        [ 1.2796],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2899: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2899: tensor([[ 1.0737],\n",
      "        [ 0.1710],\n",
      "        [-1.8938],\n",
      "        [ 1.2774],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2900: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2900: tensor([[ 1.0740],\n",
      "        [ 0.1720],\n",
      "        [-1.8920],\n",
      "        [ 1.2796],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2901: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2901: tensor([[ 1.0737],\n",
      "        [ 0.1709],\n",
      "        [-1.8938],\n",
      "        [ 1.2774],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2902: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2902: tensor([[ 1.0740],\n",
      "        [ 0.1719],\n",
      "        [-1.8921],\n",
      "        [ 1.2796],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2903: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2903: tensor([[ 1.0737],\n",
      "        [ 0.1709],\n",
      "        [-1.8938],\n",
      "        [ 1.2774],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2904: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2904: tensor([[ 1.0740],\n",
      "        [ 0.1719],\n",
      "        [-1.8921],\n",
      "        [ 1.2796],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2905: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2905: tensor([[ 1.0737],\n",
      "        [ 0.1709],\n",
      "        [-1.8938],\n",
      "        [ 1.2774],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2906: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2906: tensor([[ 1.0740],\n",
      "        [ 0.1719],\n",
      "        [-1.8921],\n",
      "        [ 1.2796],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2907: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2907: tensor([[ 1.0737],\n",
      "        [ 0.1709],\n",
      "        [-1.8939],\n",
      "        [ 1.2774],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2908: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2908: tensor([[ 1.0740],\n",
      "        [ 0.1719],\n",
      "        [-1.8921],\n",
      "        [ 1.2797],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2909: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2909: tensor([[ 1.0737],\n",
      "        [ 0.1708],\n",
      "        [-1.8939],\n",
      "        [ 1.2775],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2910: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2910: tensor([[ 1.0740],\n",
      "        [ 0.1718],\n",
      "        [-1.8922],\n",
      "        [ 1.2797],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2911: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2911: tensor([[ 1.0737],\n",
      "        [ 0.1708],\n",
      "        [-1.8939],\n",
      "        [ 1.2775],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2912: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2912: tensor([[ 1.0740],\n",
      "        [ 0.1718],\n",
      "        [-1.8922],\n",
      "        [ 1.2797],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2913: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2913: tensor([[ 1.0737],\n",
      "        [ 0.1708],\n",
      "        [-1.8939],\n",
      "        [ 1.2775],\n",
      "        [ 0.6881],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2914: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2914: tensor([[ 1.0740],\n",
      "        [ 0.1718],\n",
      "        [-1.8922],\n",
      "        [ 1.2797],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2915: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2915: tensor([[ 1.0737],\n",
      "        [ 0.1708],\n",
      "        [-1.8940],\n",
      "        [ 1.2775],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2916: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2916: tensor([[ 1.0740],\n",
      "        [ 0.1718],\n",
      "        [-1.8922],\n",
      "        [ 1.2797],\n",
      "        [ 0.6905],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2917: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2917: tensor([[ 1.0737],\n",
      "        [ 0.1707],\n",
      "        [-1.8940],\n",
      "        [ 1.2775],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2918: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2918: tensor([[ 1.0740],\n",
      "        [ 0.1718],\n",
      "        [-1.8923],\n",
      "        [ 1.2797],\n",
      "        [ 0.6906],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2919: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2919: tensor([[ 1.0737],\n",
      "        [ 0.1707],\n",
      "        [-1.8940],\n",
      "        [ 1.2776],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2920: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2920: tensor([[ 1.0740],\n",
      "        [ 0.1717],\n",
      "        [-1.8923],\n",
      "        [ 1.2798],\n",
      "        [ 0.6906],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2921: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2921: tensor([[ 1.0737],\n",
      "        [ 0.1707],\n",
      "        [-1.8940],\n",
      "        [ 1.2776],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2922: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2922: tensor([[ 1.0740],\n",
      "        [ 0.1717],\n",
      "        [-1.8923],\n",
      "        [ 1.2798],\n",
      "        [ 0.6906],\n",
      "        [-0.2886]], requires_grad=True)\n",
      "poly train loss at 2923: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2923: tensor([[ 1.0737],\n",
      "        [ 0.1707],\n",
      "        [-1.8941],\n",
      "        [ 1.2776],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2924: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2924: tensor([[ 1.0740],\n",
      "        [ 0.1717],\n",
      "        [-1.8923],\n",
      "        [ 1.2798],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2925: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2925: tensor([[ 1.0737],\n",
      "        [ 0.1707],\n",
      "        [-1.8941],\n",
      "        [ 1.2776],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2926: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2926: tensor([[ 1.0740],\n",
      "        [ 0.1717],\n",
      "        [-1.8924],\n",
      "        [ 1.2798],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2927: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2927: tensor([[ 1.0737],\n",
      "        [ 0.1706],\n",
      "        [-1.8941],\n",
      "        [ 1.2776],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2928: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2928: tensor([[ 1.0740],\n",
      "        [ 0.1716],\n",
      "        [-1.8924],\n",
      "        [ 1.2798],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2929: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2929: tensor([[ 1.0737],\n",
      "        [ 0.1706],\n",
      "        [-1.8941],\n",
      "        [ 1.2776],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2930: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2930: tensor([[ 1.0740],\n",
      "        [ 0.1716],\n",
      "        [-1.8924],\n",
      "        [ 1.2799],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2931: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2931: tensor([[ 1.0737],\n",
      "        [ 0.1706],\n",
      "        [-1.8942],\n",
      "        [ 1.2777],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2932: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2932: tensor([[ 1.0740],\n",
      "        [ 0.1716],\n",
      "        [-1.8924],\n",
      "        [ 1.2799],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2933: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2933: tensor([[ 1.0737],\n",
      "        [ 0.1706],\n",
      "        [-1.8942],\n",
      "        [ 1.2777],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2934: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2934: tensor([[ 1.0740],\n",
      "        [ 0.1716],\n",
      "        [-1.8925],\n",
      "        [ 1.2799],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2935: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2935: tensor([[ 1.0737],\n",
      "        [ 0.1705],\n",
      "        [-1.8942],\n",
      "        [ 1.2777],\n",
      "        [ 0.6882],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2936: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2936: tensor([[ 1.0740],\n",
      "        [ 0.1715],\n",
      "        [-1.8925],\n",
      "        [ 1.2799],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2937: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2937: tensor([[ 1.0737],\n",
      "        [ 0.1705],\n",
      "        [-1.8942],\n",
      "        [ 1.2777],\n",
      "        [ 0.6883],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 2938: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2938: tensor([[ 1.0740],\n",
      "        [ 0.1715],\n",
      "        [-1.8925],\n",
      "        [ 1.2799],\n",
      "        [ 0.6906],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2939: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2939: tensor([[ 1.0737],\n",
      "        [ 0.1705],\n",
      "        [-1.8943],\n",
      "        [ 1.2777],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2940: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2940: tensor([[ 1.0740],\n",
      "        [ 0.1715],\n",
      "        [-1.8925],\n",
      "        [ 1.2799],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2941: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2941: tensor([[ 1.0737],\n",
      "        [ 0.1705],\n",
      "        [-1.8943],\n",
      "        [ 1.2778],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2942: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2942: tensor([[ 1.0740],\n",
      "        [ 0.1715],\n",
      "        [-1.8926],\n",
      "        [ 1.2800],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2943: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2943: tensor([[ 1.0737],\n",
      "        [ 0.1704],\n",
      "        [-1.8943],\n",
      "        [ 1.2778],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2944: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2944: tensor([[ 1.0740],\n",
      "        [ 0.1715],\n",
      "        [-1.8926],\n",
      "        [ 1.2800],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2945: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2945: tensor([[ 1.0737],\n",
      "        [ 0.1704],\n",
      "        [-1.8943],\n",
      "        [ 1.2778],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2946: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2946: tensor([[ 1.0740],\n",
      "        [ 0.1714],\n",
      "        [-1.8926],\n",
      "        [ 1.2800],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2947: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2947: tensor([[ 1.0737],\n",
      "        [ 0.1704],\n",
      "        [-1.8944],\n",
      "        [ 1.2778],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2948: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2948: tensor([[ 1.0740],\n",
      "        [ 0.1714],\n",
      "        [-1.8926],\n",
      "        [ 1.2800],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2949: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2949: tensor([[ 1.0737],\n",
      "        [ 0.1704],\n",
      "        [-1.8944],\n",
      "        [ 1.2778],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2950: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2950: tensor([[ 1.0740],\n",
      "        [ 0.1714],\n",
      "        [-1.8927],\n",
      "        [ 1.2800],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2951: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2951: tensor([[ 1.0737],\n",
      "        [ 0.1704],\n",
      "        [-1.8944],\n",
      "        [ 1.2778],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2952: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2952: tensor([[ 1.0740],\n",
      "        [ 0.1714],\n",
      "        [-1.8927],\n",
      "        [ 1.2801],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2953: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2953: tensor([[ 1.0737],\n",
      "        [ 0.1703],\n",
      "        [-1.8944],\n",
      "        [ 1.2779],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2954: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2954: tensor([[ 1.0740],\n",
      "        [ 0.1713],\n",
      "        [-1.8927],\n",
      "        [ 1.2801],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2955: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2955: tensor([[ 1.0737],\n",
      "        [ 0.1703],\n",
      "        [-1.8945],\n",
      "        [ 1.2779],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2956: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2956: tensor([[ 1.0740],\n",
      "        [ 0.1713],\n",
      "        [-1.8927],\n",
      "        [ 1.2801],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2957: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2957: tensor([[ 1.0737],\n",
      "        [ 0.1703],\n",
      "        [-1.8945],\n",
      "        [ 1.2779],\n",
      "        [ 0.6883],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2958: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2958: tensor([[ 1.0740],\n",
      "        [ 0.1713],\n",
      "        [-1.8928],\n",
      "        [ 1.2801],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2959: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2959: tensor([[ 1.0737],\n",
      "        [ 0.1703],\n",
      "        [-1.8945],\n",
      "        [ 1.2779],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2960: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2960: tensor([[ 1.0740],\n",
      "        [ 0.1713],\n",
      "        [-1.8928],\n",
      "        [ 1.2801],\n",
      "        [ 0.6907],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2961: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2961: tensor([[ 1.0737],\n",
      "        [ 0.1702],\n",
      "        [-1.8945],\n",
      "        [ 1.2779],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2962: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2962: tensor([[ 1.0740],\n",
      "        [ 0.1712],\n",
      "        [-1.8928],\n",
      "        [ 1.2802],\n",
      "        [ 0.6908],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2963: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2963: tensor([[ 1.0737],\n",
      "        [ 0.1702],\n",
      "        [-1.8945],\n",
      "        [ 1.2780],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2964: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2964: tensor([[ 1.0740],\n",
      "        [ 0.1712],\n",
      "        [-1.8928],\n",
      "        [ 1.2802],\n",
      "        [ 0.6908],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2965: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2965: tensor([[ 1.0737],\n",
      "        [ 0.1702],\n",
      "        [-1.8946],\n",
      "        [ 1.2780],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2966: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2966: tensor([[ 1.0740],\n",
      "        [ 0.1712],\n",
      "        [-1.8929],\n",
      "        [ 1.2802],\n",
      "        [ 0.6908],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2967: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2967: tensor([[ 1.0737],\n",
      "        [ 0.1702],\n",
      "        [-1.8946],\n",
      "        [ 1.2780],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2968: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2968: tensor([[ 1.0740],\n",
      "        [ 0.1712],\n",
      "        [-1.8929],\n",
      "        [ 1.2802],\n",
      "        [ 0.6908],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2969: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2969: tensor([[ 1.0737],\n",
      "        [ 0.1701],\n",
      "        [-1.8946],\n",
      "        [ 1.2780],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2970: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2970: tensor([[ 1.0740],\n",
      "        [ 0.1712],\n",
      "        [-1.8929],\n",
      "        [ 1.2802],\n",
      "        [ 0.6908],\n",
      "        [-0.2887]], requires_grad=True)\n",
      "poly train loss at 2971: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2971: tensor([[ 1.0737],\n",
      "        [ 0.1701],\n",
      "        [-1.8946],\n",
      "        [ 1.2780],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2972: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2972: tensor([[ 1.0740],\n",
      "        [ 0.1711],\n",
      "        [-1.8929],\n",
      "        [ 1.2802],\n",
      "        [ 0.6908],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2973: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2973: tensor([[ 1.0737],\n",
      "        [ 0.1701],\n",
      "        [-1.8947],\n",
      "        [ 1.2780],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2974: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2974: tensor([[ 1.0740],\n",
      "        [ 0.1711],\n",
      "        [-1.8930],\n",
      "        [ 1.2803],\n",
      "        [ 0.6908],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2975: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2975: tensor([[ 1.0737],\n",
      "        [ 0.1701],\n",
      "        [-1.8947],\n",
      "        [ 1.2781],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2976: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2976: tensor([[ 1.0740],\n",
      "        [ 0.1711],\n",
      "        [-1.8930],\n",
      "        [ 1.2803],\n",
      "        [ 0.6908],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2977: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2977: tensor([[ 1.0737],\n",
      "        [ 0.1700],\n",
      "        [-1.8947],\n",
      "        [ 1.2781],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2978: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2978: tensor([[ 1.0740],\n",
      "        [ 0.1711],\n",
      "        [-1.8930],\n",
      "        [ 1.2803],\n",
      "        [ 0.6908],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2979: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2979: tensor([[ 1.0737],\n",
      "        [ 0.1700],\n",
      "        [-1.8947],\n",
      "        [ 1.2781],\n",
      "        [ 0.6884],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2980: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2980: tensor([[ 1.0740],\n",
      "        [ 0.1710],\n",
      "        [-1.8930],\n",
      "        [ 1.2803],\n",
      "        [ 0.6908],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2981: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2981: tensor([[ 1.0737],\n",
      "        [ 0.1700],\n",
      "        [-1.8948],\n",
      "        [ 1.2781],\n",
      "        [ 0.6885],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2982: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2982: tensor([[ 1.0740],\n",
      "        [ 0.1710],\n",
      "        [-1.8931],\n",
      "        [ 1.2803],\n",
      "        [ 0.6908],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2983: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2983: tensor([[ 1.0737],\n",
      "        [ 0.1700],\n",
      "        [-1.8948],\n",
      "        [ 1.2781],\n",
      "        [ 0.6885],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 2984: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2984: tensor([[ 1.0740],\n",
      "        [ 0.1710],\n",
      "        [-1.8931],\n",
      "        [ 1.2804],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2985: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2985: tensor([[ 1.0737],\n",
      "        [ 0.1700],\n",
      "        [-1.8948],\n",
      "        [ 1.2782],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 2986: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2986: tensor([[ 1.0740],\n",
      "        [ 0.1710],\n",
      "        [-1.8931],\n",
      "        [ 1.2804],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2987: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2987: tensor([[ 1.0737],\n",
      "        [ 0.1699],\n",
      "        [-1.8948],\n",
      "        [ 1.2782],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 2988: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2988: tensor([[ 1.0740],\n",
      "        [ 0.1709],\n",
      "        [-1.8931],\n",
      "        [ 1.2804],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2989: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2989: tensor([[ 1.0737],\n",
      "        [ 0.1699],\n",
      "        [-1.8949],\n",
      "        [ 1.2782],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 2990: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2990: tensor([[ 1.0740],\n",
      "        [ 0.1709],\n",
      "        [-1.8932],\n",
      "        [ 1.2804],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2991: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2991: tensor([[ 1.0737],\n",
      "        [ 0.1699],\n",
      "        [-1.8949],\n",
      "        [ 1.2782],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 2992: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2992: tensor([[ 1.0740],\n",
      "        [ 0.1709],\n",
      "        [-1.8932],\n",
      "        [ 1.2804],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2993: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2993: tensor([[ 1.0737],\n",
      "        [ 0.1699],\n",
      "        [-1.8949],\n",
      "        [ 1.2782],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 2994: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2994: tensor([[ 1.0740],\n",
      "        [ 0.1709],\n",
      "        [-1.8932],\n",
      "        [ 1.2804],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2995: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2995: tensor([[ 1.0737],\n",
      "        [ 0.1698],\n",
      "        [-1.8949],\n",
      "        [ 1.2782],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 2996: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2996: tensor([[ 1.0740],\n",
      "        [ 0.1709],\n",
      "        [-1.8932],\n",
      "        [ 1.2805],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2997: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2997: tensor([[ 1.0737],\n",
      "        [ 0.1698],\n",
      "        [-1.8950],\n",
      "        [ 1.2783],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 2998: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 2998: tensor([[ 1.0740],\n",
      "        [ 0.1708],\n",
      "        [-1.8933],\n",
      "        [ 1.2805],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 2999: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 2999: tensor([[ 1.0737],\n",
      "        [ 0.1698],\n",
      "        [-1.8950],\n",
      "        [ 1.2783],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3000: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3000: tensor([[ 1.0740],\n",
      "        [ 0.1708],\n",
      "        [-1.8933],\n",
      "        [ 1.2805],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3001: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3001: tensor([[ 1.0737],\n",
      "        [ 0.1698],\n",
      "        [-1.8950],\n",
      "        [ 1.2783],\n",
      "        [ 0.6885],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3002: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3002: tensor([[ 1.0740],\n",
      "        [ 0.1708],\n",
      "        [-1.8933],\n",
      "        [ 1.2805],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3003: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3003: tensor([[ 1.0737],\n",
      "        [ 0.1698],\n",
      "        [-1.8950],\n",
      "        [ 1.2783],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3004: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3004: tensor([[ 1.0740],\n",
      "        [ 0.1708],\n",
      "        [-1.8933],\n",
      "        [ 1.2805],\n",
      "        [ 0.6909],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3005: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3005: tensor([[ 1.0737],\n",
      "        [ 0.1697],\n",
      "        [-1.8951],\n",
      "        [ 1.2783],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3006: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3006: tensor([[ 1.0740],\n",
      "        [ 0.1707],\n",
      "        [-1.8933],\n",
      "        [ 1.2806],\n",
      "        [ 0.6910],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3007: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3007: tensor([[ 1.0737],\n",
      "        [ 0.1697],\n",
      "        [-1.8951],\n",
      "        [ 1.2784],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3008: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3008: tensor([[ 1.0740],\n",
      "        [ 0.1707],\n",
      "        [-1.8934],\n",
      "        [ 1.2806],\n",
      "        [ 0.6910],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3009: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3009: tensor([[ 1.0737],\n",
      "        [ 0.1697],\n",
      "        [-1.8951],\n",
      "        [ 1.2784],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3010: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3010: tensor([[ 1.0740],\n",
      "        [ 0.1707],\n",
      "        [-1.8934],\n",
      "        [ 1.2806],\n",
      "        [ 0.6910],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3011: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3011: tensor([[ 1.0737],\n",
      "        [ 0.1697],\n",
      "        [-1.8951],\n",
      "        [ 1.2784],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3012: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3012: tensor([[ 1.0740],\n",
      "        [ 0.1707],\n",
      "        [-1.8934],\n",
      "        [ 1.2806],\n",
      "        [ 0.6910],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3013: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3013: tensor([[ 1.0737],\n",
      "        [ 0.1696],\n",
      "        [-1.8952],\n",
      "        [ 1.2784],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3014: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3014: tensor([[ 1.0740],\n",
      "        [ 0.1706],\n",
      "        [-1.8934],\n",
      "        [ 1.2806],\n",
      "        [ 0.6910],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3015: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3015: tensor([[ 1.0737],\n",
      "        [ 0.1696],\n",
      "        [-1.8952],\n",
      "        [ 1.2784],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3016: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3016: tensor([[ 1.0740],\n",
      "        [ 0.1706],\n",
      "        [-1.8935],\n",
      "        [ 1.2806],\n",
      "        [ 0.6910],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3017: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3017: tensor([[ 1.0737],\n",
      "        [ 0.1696],\n",
      "        [-1.8952],\n",
      "        [ 1.2784],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3018: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3018: tensor([[ 1.0740],\n",
      "        [ 0.1706],\n",
      "        [-1.8935],\n",
      "        [ 1.2807],\n",
      "        [ 0.6910],\n",
      "        [-0.2888]], requires_grad=True)\n",
      "poly train loss at 3019: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3019: tensor([[ 1.0737],\n",
      "        [ 0.1696],\n",
      "        [-1.8952],\n",
      "        [ 1.2785],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3020: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3020: tensor([[ 1.0740],\n",
      "        [ 0.1706],\n",
      "        [-1.8935],\n",
      "        [ 1.2807],\n",
      "        [ 0.6910],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3021: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3021: tensor([[ 1.0737],\n",
      "        [ 0.1695],\n",
      "        [-1.8953],\n",
      "        [ 1.2785],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3022: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3022: tensor([[ 1.0740],\n",
      "        [ 0.1706],\n",
      "        [-1.8935],\n",
      "        [ 1.2807],\n",
      "        [ 0.6910],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3023: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3023: tensor([[ 1.0737],\n",
      "        [ 0.1695],\n",
      "        [-1.8953],\n",
      "        [ 1.2785],\n",
      "        [ 0.6886],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3024: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3024: tensor([[ 1.0740],\n",
      "        [ 0.1705],\n",
      "        [-1.8936],\n",
      "        [ 1.2807],\n",
      "        [ 0.6910],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3025: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3025: tensor([[ 1.0737],\n",
      "        [ 0.1695],\n",
      "        [-1.8953],\n",
      "        [ 1.2785],\n",
      "        [ 0.6887],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3026: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3026: tensor([[ 1.0740],\n",
      "        [ 0.1705],\n",
      "        [-1.8936],\n",
      "        [ 1.2807],\n",
      "        [ 0.6910],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3027: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3027: tensor([[ 1.0737],\n",
      "        [ 0.1695],\n",
      "        [-1.8953],\n",
      "        [ 1.2785],\n",
      "        [ 0.6887],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3028: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3028: tensor([[ 1.0740],\n",
      "        [ 0.1705],\n",
      "        [-1.8936],\n",
      "        [ 1.2807],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3029: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3029: tensor([[ 1.0737],\n",
      "        [ 0.1695],\n",
      "        [-1.8954],\n",
      "        [ 1.2786],\n",
      "        [ 0.6887],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3030: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3030: tensor([[ 1.0740],\n",
      "        [ 0.1705],\n",
      "        [-1.8936],\n",
      "        [ 1.2808],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3031: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3031: tensor([[ 1.0737],\n",
      "        [ 0.1694],\n",
      "        [-1.8954],\n",
      "        [ 1.2786],\n",
      "        [ 0.6887],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3032: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3032: tensor([[ 1.0740],\n",
      "        [ 0.1704],\n",
      "        [-1.8937],\n",
      "        [ 1.2808],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3033: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3033: tensor([[ 1.0737],\n",
      "        [ 0.1694],\n",
      "        [-1.8954],\n",
      "        [ 1.2786],\n",
      "        [ 0.6887],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 3034: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3034: tensor([[ 1.0740],\n",
      "        [ 0.1704],\n",
      "        [-1.8937],\n",
      "        [ 1.2808],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3035: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3035: tensor([[ 1.0737],\n",
      "        [ 0.1694],\n",
      "        [-1.8954],\n",
      "        [ 1.2786],\n",
      "        [ 0.6887],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3036: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3036: tensor([[ 1.0740],\n",
      "        [ 0.1704],\n",
      "        [-1.8937],\n",
      "        [ 1.2808],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3037: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3037: tensor([[ 1.0737],\n",
      "        [ 0.1694],\n",
      "        [-1.8955],\n",
      "        [ 1.2786],\n",
      "        [ 0.6887],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3038: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3038: tensor([[ 1.0740],\n",
      "        [ 0.1704],\n",
      "        [-1.8937],\n",
      "        [ 1.2808],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3039: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3039: tensor([[ 1.0737],\n",
      "        [ 0.1693],\n",
      "        [-1.8955],\n",
      "        [ 1.2786],\n",
      "        [ 0.6887],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3040: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3040: tensor([[ 1.0740],\n",
      "        [ 0.1703],\n",
      "        [-1.8938],\n",
      "        [ 1.2809],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3041: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3041: tensor([[ 1.0737],\n",
      "        [ 0.1693],\n",
      "        [-1.8955],\n",
      "        [ 1.2787],\n",
      "        [ 0.6887],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3042: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3042: tensor([[ 1.0740],\n",
      "        [ 0.1703],\n",
      "        [-1.8938],\n",
      "        [ 1.2809],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3043: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3043: tensor([[ 1.0737],\n",
      "        [ 0.1693],\n",
      "        [-1.8955],\n",
      "        [ 1.2787],\n",
      "        [ 0.6887],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3044: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3044: tensor([[ 1.0740],\n",
      "        [ 0.1703],\n",
      "        [-1.8938],\n",
      "        [ 1.2809],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3045: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3045: tensor([[ 1.0737],\n",
      "        [ 0.1693],\n",
      "        [-1.8956],\n",
      "        [ 1.2787],\n",
      "        [ 0.6887],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3046: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3046: tensor([[ 1.0740],\n",
      "        [ 0.1703],\n",
      "        [-1.8938],\n",
      "        [ 1.2809],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3047: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3047: tensor([[ 1.0737],\n",
      "        [ 0.1692],\n",
      "        [-1.8956],\n",
      "        [ 1.2787],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3048: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3048: tensor([[ 1.0740],\n",
      "        [ 0.1703],\n",
      "        [-1.8939],\n",
      "        [ 1.2809],\n",
      "        [ 0.6911],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3049: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3049: tensor([[ 1.0737],\n",
      "        [ 0.1692],\n",
      "        [-1.8956],\n",
      "        [ 1.2787],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3050: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3050: tensor([[ 1.0740],\n",
      "        [ 0.1702],\n",
      "        [-1.8939],\n",
      "        [ 1.2809],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3051: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3051: tensor([[ 1.0737],\n",
      "        [ 0.1692],\n",
      "        [-1.8956],\n",
      "        [ 1.2788],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3052: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3052: tensor([[ 1.0740],\n",
      "        [ 0.1702],\n",
      "        [-1.8939],\n",
      "        [ 1.2810],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3053: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3053: tensor([[ 1.0737],\n",
      "        [ 0.1692],\n",
      "        [-1.8957],\n",
      "        [ 1.2788],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3054: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3054: tensor([[ 1.0740],\n",
      "        [ 0.1702],\n",
      "        [-1.8939],\n",
      "        [ 1.2810],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3055: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3055: tensor([[ 1.0737],\n",
      "        [ 0.1692],\n",
      "        [-1.8957],\n",
      "        [ 1.2788],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3056: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3056: tensor([[ 1.0740],\n",
      "        [ 0.1702],\n",
      "        [-1.8940],\n",
      "        [ 1.2810],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3057: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3057: tensor([[ 1.0737],\n",
      "        [ 0.1691],\n",
      "        [-1.8957],\n",
      "        [ 1.2788],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3058: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3058: tensor([[ 1.0740],\n",
      "        [ 0.1701],\n",
      "        [-1.8940],\n",
      "        [ 1.2810],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3059: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3059: tensor([[ 1.0737],\n",
      "        [ 0.1691],\n",
      "        [-1.8957],\n",
      "        [ 1.2788],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3060: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3060: tensor([[ 1.0740],\n",
      "        [ 0.1701],\n",
      "        [-1.8940],\n",
      "        [ 1.2810],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3061: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3061: tensor([[ 1.0737],\n",
      "        [ 0.1691],\n",
      "        [-1.8958],\n",
      "        [ 1.2788],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3062: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3062: tensor([[ 1.0740],\n",
      "        [ 0.1701],\n",
      "        [-1.8940],\n",
      "        [ 1.2811],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3063: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3063: tensor([[ 1.0737],\n",
      "        [ 0.1691],\n",
      "        [-1.8958],\n",
      "        [ 1.2789],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3064: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3064: tensor([[ 1.0740],\n",
      "        [ 0.1701],\n",
      "        [-1.8941],\n",
      "        [ 1.2811],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3065: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3065: tensor([[ 1.0737],\n",
      "        [ 0.1690],\n",
      "        [-1.8958],\n",
      "        [ 1.2789],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3066: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3066: tensor([[ 1.0740],\n",
      "        [ 0.1701],\n",
      "        [-1.8941],\n",
      "        [ 1.2811],\n",
      "        [ 0.6912],\n",
      "        [-0.2889]], requires_grad=True)\n",
      "poly train loss at 3067: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3067: tensor([[ 1.0737],\n",
      "        [ 0.1690],\n",
      "        [-1.8958],\n",
      "        [ 1.2789],\n",
      "        [ 0.6888],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3068: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3068: tensor([[ 1.0740],\n",
      "        [ 0.1700],\n",
      "        [-1.8941],\n",
      "        [ 1.2811],\n",
      "        [ 0.6912],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3069: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3069: tensor([[ 1.0737],\n",
      "        [ 0.1690],\n",
      "        [-1.8959],\n",
      "        [ 1.2789],\n",
      "        [ 0.6889],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3070: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3070: tensor([[ 1.0740],\n",
      "        [ 0.1700],\n",
      "        [-1.8941],\n",
      "        [ 1.2811],\n",
      "        [ 0.6912],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3071: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3071: tensor([[ 1.0737],\n",
      "        [ 0.1690],\n",
      "        [-1.8959],\n",
      "        [ 1.2789],\n",
      "        [ 0.6889],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3072: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3072: tensor([[ 1.0740],\n",
      "        [ 0.1700],\n",
      "        [-1.8942],\n",
      "        [ 1.2811],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3073: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3073: tensor([[ 1.0737],\n",
      "        [ 0.1690],\n",
      "        [-1.8959],\n",
      "        [ 1.2790],\n",
      "        [ 0.6889],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3074: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3074: tensor([[ 1.0740],\n",
      "        [ 0.1700],\n",
      "        [-1.8942],\n",
      "        [ 1.2812],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3075: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3075: tensor([[ 1.0737],\n",
      "        [ 0.1689],\n",
      "        [-1.8959],\n",
      "        [ 1.2790],\n",
      "        [ 0.6889],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3076: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3076: tensor([[ 1.0740],\n",
      "        [ 0.1699],\n",
      "        [-1.8942],\n",
      "        [ 1.2812],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3077: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3077: tensor([[ 1.0737],\n",
      "        [ 0.1689],\n",
      "        [-1.8960],\n",
      "        [ 1.2790],\n",
      "        [ 0.6889],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3078: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3078: tensor([[ 1.0740],\n",
      "        [ 0.1699],\n",
      "        [-1.8942],\n",
      "        [ 1.2812],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3079: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3079: tensor([[ 1.0737],\n",
      "        [ 0.1689],\n",
      "        [-1.8960],\n",
      "        [ 1.2790],\n",
      "        [ 0.6889],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3080: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3080: tensor([[ 1.0740],\n",
      "        [ 0.1699],\n",
      "        [-1.8943],\n",
      "        [ 1.2812],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3081: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3081: tensor([[ 1.0737],\n",
      "        [ 0.1689],\n",
      "        [-1.8960],\n",
      "        [ 1.2790],\n",
      "        [ 0.6889],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 3082: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3082: tensor([[ 1.0740],\n",
      "        [ 0.1699],\n",
      "        [-1.8943],\n",
      "        [ 1.2812],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3083: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3083: tensor([[ 1.0737],\n",
      "        [ 0.1688],\n",
      "        [-1.8960],\n",
      "        [ 1.2790],\n",
      "        [ 0.6889],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3084: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3084: tensor([[ 1.0740],\n",
      "        [ 0.1698],\n",
      "        [-1.8943],\n",
      "        [ 1.2813],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3085: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3085: tensor([[ 1.0737],\n",
      "        [ 0.1688],\n",
      "        [-1.8961],\n",
      "        [ 1.2791],\n",
      "        [ 0.6889],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3086: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3086: tensor([[ 1.0740],\n",
      "        [ 0.1698],\n",
      "        [-1.8943],\n",
      "        [ 1.2813],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3087: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3087: tensor([[ 1.0737],\n",
      "        [ 0.1688],\n",
      "        [-1.8961],\n",
      "        [ 1.2791],\n",
      "        [ 0.6889],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3088: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3088: tensor([[ 1.0740],\n",
      "        [ 0.1698],\n",
      "        [-1.8944],\n",
      "        [ 1.2813],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3089: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3089: tensor([[ 1.0737],\n",
      "        [ 0.1688],\n",
      "        [-1.8961],\n",
      "        [ 1.2791],\n",
      "        [ 0.6889],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3090: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3090: tensor([[ 1.0740],\n",
      "        [ 0.1698],\n",
      "        [-1.8944],\n",
      "        [ 1.2813],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3091: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3091: tensor([[ 1.0737],\n",
      "        [ 0.1687],\n",
      "        [-1.8961],\n",
      "        [ 1.2791],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3092: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3092: tensor([[ 1.0740],\n",
      "        [ 0.1698],\n",
      "        [-1.8944],\n",
      "        [ 1.2813],\n",
      "        [ 0.6913],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3093: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3093: tensor([[ 1.0737],\n",
      "        [ 0.1687],\n",
      "        [-1.8962],\n",
      "        [ 1.2791],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3094: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3094: tensor([[ 1.0740],\n",
      "        [ 0.1697],\n",
      "        [-1.8944],\n",
      "        [ 1.2813],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3095: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3095: tensor([[ 1.0737],\n",
      "        [ 0.1687],\n",
      "        [-1.8962],\n",
      "        [ 1.2791],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3096: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3096: tensor([[ 1.0740],\n",
      "        [ 0.1697],\n",
      "        [-1.8945],\n",
      "        [ 1.2814],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3097: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3097: tensor([[ 1.0737],\n",
      "        [ 0.1687],\n",
      "        [-1.8962],\n",
      "        [ 1.2792],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3098: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3098: tensor([[ 1.0740],\n",
      "        [ 0.1697],\n",
      "        [-1.8945],\n",
      "        [ 1.2814],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3099: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3099: tensor([[ 1.0737],\n",
      "        [ 0.1687],\n",
      "        [-1.8962],\n",
      "        [ 1.2792],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3100: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3100: tensor([[ 1.0740],\n",
      "        [ 0.1697],\n",
      "        [-1.8945],\n",
      "        [ 1.2814],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3101: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3101: tensor([[ 1.0737],\n",
      "        [ 0.1686],\n",
      "        [-1.8963],\n",
      "        [ 1.2792],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3102: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3102: tensor([[ 1.0740],\n",
      "        [ 0.1696],\n",
      "        [-1.8945],\n",
      "        [ 1.2814],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3103: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3103: tensor([[ 1.0737],\n",
      "        [ 0.1686],\n",
      "        [-1.8963],\n",
      "        [ 1.2792],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3104: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3104: tensor([[ 1.0740],\n",
      "        [ 0.1696],\n",
      "        [-1.8946],\n",
      "        [ 1.2814],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3105: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3105: tensor([[ 1.0737],\n",
      "        [ 0.1686],\n",
      "        [-1.8963],\n",
      "        [ 1.2792],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3106: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3106: tensor([[ 1.0740],\n",
      "        [ 0.1696],\n",
      "        [-1.8946],\n",
      "        [ 1.2814],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3107: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3107: tensor([[ 1.0737],\n",
      "        [ 0.1686],\n",
      "        [-1.8963],\n",
      "        [ 1.2793],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3108: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3108: tensor([[ 1.0740],\n",
      "        [ 0.1696],\n",
      "        [-1.8946],\n",
      "        [ 1.2815],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3109: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3109: tensor([[ 1.0737],\n",
      "        [ 0.1685],\n",
      "        [-1.8964],\n",
      "        [ 1.2793],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3110: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3110: tensor([[ 1.0740],\n",
      "        [ 0.1696],\n",
      "        [-1.8946],\n",
      "        [ 1.2815],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3111: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3111: tensor([[ 1.0737],\n",
      "        [ 0.1685],\n",
      "        [-1.8964],\n",
      "        [ 1.2793],\n",
      "        [ 0.6890],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3112: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3112: tensor([[ 1.0740],\n",
      "        [ 0.1695],\n",
      "        [-1.8947],\n",
      "        [ 1.2815],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3113: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3113: tensor([[ 1.0737],\n",
      "        [ 0.1685],\n",
      "        [-1.8964],\n",
      "        [ 1.2793],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3114: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3114: tensor([[ 1.0740],\n",
      "        [ 0.1695],\n",
      "        [-1.8947],\n",
      "        [ 1.2815],\n",
      "        [ 0.6914],\n",
      "        [-0.2890]], requires_grad=True)\n",
      "poly train loss at 3115: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3115: tensor([[ 1.0737],\n",
      "        [ 0.1685],\n",
      "        [-1.8964],\n",
      "        [ 1.2793],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3116: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3116: tensor([[ 1.0740],\n",
      "        [ 0.1695],\n",
      "        [-1.8947],\n",
      "        [ 1.2815],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3117: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3117: tensor([[ 1.0737],\n",
      "        [ 0.1685],\n",
      "        [-1.8965],\n",
      "        [ 1.2793],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3118: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3118: tensor([[ 1.0740],\n",
      "        [ 0.1695],\n",
      "        [-1.8947],\n",
      "        [ 1.2816],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3119: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3119: tensor([[ 1.0737],\n",
      "        [ 0.1684],\n",
      "        [-1.8965],\n",
      "        [ 1.2794],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3120: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3120: tensor([[ 1.0740],\n",
      "        [ 0.1694],\n",
      "        [-1.8948],\n",
      "        [ 1.2816],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3121: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3121: tensor([[ 1.0737],\n",
      "        [ 0.1684],\n",
      "        [-1.8965],\n",
      "        [ 1.2794],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3122: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3122: tensor([[ 1.0740],\n",
      "        [ 0.1694],\n",
      "        [-1.8948],\n",
      "        [ 1.2816],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3123: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3123: tensor([[ 1.0737],\n",
      "        [ 0.1684],\n",
      "        [-1.8965],\n",
      "        [ 1.2794],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3124: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3124: tensor([[ 1.0740],\n",
      "        [ 0.1694],\n",
      "        [-1.8948],\n",
      "        [ 1.2816],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3125: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3125: tensor([[ 1.0737],\n",
      "        [ 0.1684],\n",
      "        [-1.8966],\n",
      "        [ 1.2794],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3126: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3126: tensor([[ 1.0740],\n",
      "        [ 0.1694],\n",
      "        [-1.8948],\n",
      "        [ 1.2816],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3127: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3127: tensor([[ 1.0737],\n",
      "        [ 0.1683],\n",
      "        [-1.8966],\n",
      "        [ 1.2794],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3128: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3128: tensor([[ 1.0740],\n",
      "        [ 0.1693],\n",
      "        [-1.8949],\n",
      "        [ 1.2816],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3129: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3129: tensor([[ 1.0737],\n",
      "        [ 0.1683],\n",
      "        [-1.8966],\n",
      "        [ 1.2794],\n",
      "        [ 0.6891],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 3130: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3130: tensor([[ 1.0740],\n",
      "        [ 0.1693],\n",
      "        [-1.8949],\n",
      "        [ 1.2817],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3131: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3131: tensor([[ 1.0737],\n",
      "        [ 0.1683],\n",
      "        [-1.8966],\n",
      "        [ 1.2795],\n",
      "        [ 0.6891],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3132: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3132: tensor([[ 1.0740],\n",
      "        [ 0.1693],\n",
      "        [-1.8949],\n",
      "        [ 1.2817],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3133: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3133: tensor([[ 1.0737],\n",
      "        [ 0.1683],\n",
      "        [-1.8966],\n",
      "        [ 1.2795],\n",
      "        [ 0.6891],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3134: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3134: tensor([[ 1.0740],\n",
      "        [ 0.1693],\n",
      "        [-1.8949],\n",
      "        [ 1.2817],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3135: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3135: tensor([[ 1.0737],\n",
      "        [ 0.1683],\n",
      "        [-1.8967],\n",
      "        [ 1.2795],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3136: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3136: tensor([[ 1.0740],\n",
      "        [ 0.1693],\n",
      "        [-1.8950],\n",
      "        [ 1.2817],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3137: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3137: tensor([[ 1.0737],\n",
      "        [ 0.1682],\n",
      "        [-1.8967],\n",
      "        [ 1.2795],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3138: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3138: tensor([[ 1.0740],\n",
      "        [ 0.1692],\n",
      "        [-1.8950],\n",
      "        [ 1.2817],\n",
      "        [ 0.6915],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3139: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3139: tensor([[ 1.0737],\n",
      "        [ 0.1682],\n",
      "        [-1.8967],\n",
      "        [ 1.2795],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3140: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3140: tensor([[ 1.0740],\n",
      "        [ 0.1692],\n",
      "        [-1.8950],\n",
      "        [ 1.2817],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3141: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3141: tensor([[ 1.0737],\n",
      "        [ 0.1682],\n",
      "        [-1.8967],\n",
      "        [ 1.2796],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3142: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3142: tensor([[ 1.0740],\n",
      "        [ 0.1692],\n",
      "        [-1.8950],\n",
      "        [ 1.2818],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3143: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3143: tensor([[ 1.0737],\n",
      "        [ 0.1682],\n",
      "        [-1.8968],\n",
      "        [ 1.2796],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3144: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3144: tensor([[ 1.0740],\n",
      "        [ 0.1692],\n",
      "        [-1.8950],\n",
      "        [ 1.2818],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3145: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3145: tensor([[ 1.0737],\n",
      "        [ 0.1681],\n",
      "        [-1.8968],\n",
      "        [ 1.2796],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3146: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3146: tensor([[ 1.0740],\n",
      "        [ 0.1691],\n",
      "        [-1.8951],\n",
      "        [ 1.2818],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3147: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3147: tensor([[ 1.0737],\n",
      "        [ 0.1681],\n",
      "        [-1.8968],\n",
      "        [ 1.2796],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3148: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3148: tensor([[ 1.0740],\n",
      "        [ 0.1691],\n",
      "        [-1.8951],\n",
      "        [ 1.2818],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3149: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3149: tensor([[ 1.0737],\n",
      "        [ 0.1681],\n",
      "        [-1.8968],\n",
      "        [ 1.2796],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3150: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3150: tensor([[ 1.0740],\n",
      "        [ 0.1691],\n",
      "        [-1.8951],\n",
      "        [ 1.2818],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3151: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3151: tensor([[ 1.0737],\n",
      "        [ 0.1681],\n",
      "        [-1.8969],\n",
      "        [ 1.2796],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3152: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3152: tensor([[ 1.0740],\n",
      "        [ 0.1691],\n",
      "        [-1.8951],\n",
      "        [ 1.2819],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3153: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3153: tensor([[ 1.0737],\n",
      "        [ 0.1681],\n",
      "        [-1.8969],\n",
      "        [ 1.2797],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3154: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3154: tensor([[ 1.0740],\n",
      "        [ 0.1691],\n",
      "        [-1.8952],\n",
      "        [ 1.2819],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3155: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3155: tensor([[ 1.0737],\n",
      "        [ 0.1680],\n",
      "        [-1.8969],\n",
      "        [ 1.2797],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3156: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3156: tensor([[ 1.0740],\n",
      "        [ 0.1690],\n",
      "        [-1.8952],\n",
      "        [ 1.2819],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3157: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3157: tensor([[ 1.0737],\n",
      "        [ 0.1680],\n",
      "        [-1.8969],\n",
      "        [ 1.2797],\n",
      "        [ 0.6892],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3158: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3158: tensor([[ 1.0740],\n",
      "        [ 0.1690],\n",
      "        [-1.8952],\n",
      "        [ 1.2819],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3159: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3159: tensor([[ 1.0737],\n",
      "        [ 0.1680],\n",
      "        [-1.8970],\n",
      "        [ 1.2797],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3160: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3160: tensor([[ 1.0740],\n",
      "        [ 0.1690],\n",
      "        [-1.8952],\n",
      "        [ 1.2819],\n",
      "        [ 0.6916],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3161: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3161: tensor([[ 1.0737],\n",
      "        [ 0.1680],\n",
      "        [-1.8970],\n",
      "        [ 1.2797],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3162: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3162: tensor([[ 1.0740],\n",
      "        [ 0.1690],\n",
      "        [-1.8953],\n",
      "        [ 1.2819],\n",
      "        [ 0.6917],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3163: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3163: tensor([[ 1.0737],\n",
      "        [ 0.1679],\n",
      "        [-1.8970],\n",
      "        [ 1.2797],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3164: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3164: tensor([[ 1.0740],\n",
      "        [ 0.1689],\n",
      "        [-1.8953],\n",
      "        [ 1.2820],\n",
      "        [ 0.6917],\n",
      "        [-0.2891]], requires_grad=True)\n",
      "poly train loss at 3165: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3165: tensor([[ 1.0737],\n",
      "        [ 0.1679],\n",
      "        [-1.8970],\n",
      "        [ 1.2798],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3166: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3166: tensor([[ 1.0740],\n",
      "        [ 0.1689],\n",
      "        [-1.8953],\n",
      "        [ 1.2820],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3167: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3167: tensor([[ 1.0737],\n",
      "        [ 0.1679],\n",
      "        [-1.8971],\n",
      "        [ 1.2798],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3168: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3168: tensor([[ 1.0740],\n",
      "        [ 0.1689],\n",
      "        [-1.8953],\n",
      "        [ 1.2820],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3169: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3169: tensor([[ 1.0737],\n",
      "        [ 0.1679],\n",
      "        [-1.8971],\n",
      "        [ 1.2798],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3170: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3170: tensor([[ 1.0740],\n",
      "        [ 0.1689],\n",
      "        [-1.8954],\n",
      "        [ 1.2820],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3171: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3171: tensor([[ 1.0737],\n",
      "        [ 0.1679],\n",
      "        [-1.8971],\n",
      "        [ 1.2798],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3172: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3172: tensor([[ 1.0740],\n",
      "        [ 0.1689],\n",
      "        [-1.8954],\n",
      "        [ 1.2820],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3173: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3173: tensor([[ 1.0737],\n",
      "        [ 0.1678],\n",
      "        [-1.8971],\n",
      "        [ 1.2798],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3174: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3174: tensor([[ 1.0740],\n",
      "        [ 0.1688],\n",
      "        [-1.8954],\n",
      "        [ 1.2820],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3175: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3175: tensor([[ 1.0737],\n",
      "        [ 0.1678],\n",
      "        [-1.8971],\n",
      "        [ 1.2799],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3176: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3176: tensor([[ 1.0740],\n",
      "        [ 0.1688],\n",
      "        [-1.8954],\n",
      "        [ 1.2821],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3177: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3177: tensor([[ 1.0737],\n",
      "        [ 0.1678],\n",
      "        [-1.8972],\n",
      "        [ 1.2799],\n",
      "        [ 0.6893],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 3178: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3178: tensor([[ 1.0740],\n",
      "        [ 0.1688],\n",
      "        [-1.8955],\n",
      "        [ 1.2821],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3179: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3179: tensor([[ 1.0737],\n",
      "        [ 0.1678],\n",
      "        [-1.8972],\n",
      "        [ 1.2799],\n",
      "        [ 0.6893],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3180: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3180: tensor([[ 1.0740],\n",
      "        [ 0.1688],\n",
      "        [-1.8955],\n",
      "        [ 1.2821],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3181: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3181: tensor([[ 1.0737],\n",
      "        [ 0.1677],\n",
      "        [-1.8972],\n",
      "        [ 1.2799],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3182: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3182: tensor([[ 1.0740],\n",
      "        [ 0.1688],\n",
      "        [-1.8955],\n",
      "        [ 1.2821],\n",
      "        [ 0.6917],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3183: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3183: tensor([[ 1.0737],\n",
      "        [ 0.1677],\n",
      "        [-1.8972],\n",
      "        [ 1.2799],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3184: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3184: tensor([[ 1.0740],\n",
      "        [ 0.1687],\n",
      "        [-1.8955],\n",
      "        [ 1.2821],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3185: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3185: tensor([[ 1.0737],\n",
      "        [ 0.1677],\n",
      "        [-1.8973],\n",
      "        [ 1.2799],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3186: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3186: tensor([[ 1.0740],\n",
      "        [ 0.1687],\n",
      "        [-1.8955],\n",
      "        [ 1.2821],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3187: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3187: tensor([[ 1.0737],\n",
      "        [ 0.1677],\n",
      "        [-1.8973],\n",
      "        [ 1.2800],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3188: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3188: tensor([[ 1.0740],\n",
      "        [ 0.1687],\n",
      "        [-1.8956],\n",
      "        [ 1.2822],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3189: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3189: tensor([[ 1.0737],\n",
      "        [ 0.1677],\n",
      "        [-1.8973],\n",
      "        [ 1.2800],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3190: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3190: tensor([[ 1.0740],\n",
      "        [ 0.1687],\n",
      "        [-1.8956],\n",
      "        [ 1.2822],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3191: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3191: tensor([[ 1.0737],\n",
      "        [ 0.1676],\n",
      "        [-1.8973],\n",
      "        [ 1.2800],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3192: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3192: tensor([[ 1.0740],\n",
      "        [ 0.1686],\n",
      "        [-1.8956],\n",
      "        [ 1.2822],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3193: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3193: tensor([[ 1.0737],\n",
      "        [ 0.1676],\n",
      "        [-1.8974],\n",
      "        [ 1.2800],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3194: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3194: tensor([[ 1.0740],\n",
      "        [ 0.1686],\n",
      "        [-1.8956],\n",
      "        [ 1.2822],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3195: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3195: tensor([[ 1.0737],\n",
      "        [ 0.1676],\n",
      "        [-1.8974],\n",
      "        [ 1.2800],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3196: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3196: tensor([[ 1.0740],\n",
      "        [ 0.1686],\n",
      "        [-1.8957],\n",
      "        [ 1.2822],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3197: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3197: tensor([[ 1.0737],\n",
      "        [ 0.1676],\n",
      "        [-1.8974],\n",
      "        [ 1.2800],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3198: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3198: tensor([[ 1.0740],\n",
      "        [ 0.1686],\n",
      "        [-1.8957],\n",
      "        [ 1.2823],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3199: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3199: tensor([[ 1.0737],\n",
      "        [ 0.1675],\n",
      "        [-1.8974],\n",
      "        [ 1.2801],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3200: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3200: tensor([[ 1.0740],\n",
      "        [ 0.1686],\n",
      "        [-1.8957],\n",
      "        [ 1.2823],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3201: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3201: tensor([[ 1.0737],\n",
      "        [ 0.1675],\n",
      "        [-1.8975],\n",
      "        [ 1.2801],\n",
      "        [ 0.6894],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3202: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3202: tensor([[ 1.0740],\n",
      "        [ 0.1685],\n",
      "        [-1.8957],\n",
      "        [ 1.2823],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3203: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3203: tensor([[ 1.0737],\n",
      "        [ 0.1675],\n",
      "        [-1.8975],\n",
      "        [ 1.2801],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3204: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3204: tensor([[ 1.0740],\n",
      "        [ 0.1685],\n",
      "        [-1.8958],\n",
      "        [ 1.2823],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3205: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3205: tensor([[ 1.0737],\n",
      "        [ 0.1675],\n",
      "        [-1.8975],\n",
      "        [ 1.2801],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3206: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3206: tensor([[ 1.0740],\n",
      "        [ 0.1685],\n",
      "        [-1.8958],\n",
      "        [ 1.2823],\n",
      "        [ 0.6918],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3207: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3207: tensor([[ 1.0737],\n",
      "        [ 0.1675],\n",
      "        [-1.8975],\n",
      "        [ 1.2801],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3208: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3208: tensor([[ 1.0740],\n",
      "        [ 0.1685],\n",
      "        [-1.8958],\n",
      "        [ 1.2823],\n",
      "        [ 0.6919],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3209: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3209: tensor([[ 1.0737],\n",
      "        [ 0.1674],\n",
      "        [-1.8976],\n",
      "        [ 1.2801],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3210: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3210: tensor([[ 1.0740],\n",
      "        [ 0.1684],\n",
      "        [-1.8958],\n",
      "        [ 1.2824],\n",
      "        [ 0.6919],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3211: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3211: tensor([[ 1.0737],\n",
      "        [ 0.1674],\n",
      "        [-1.8976],\n",
      "        [ 1.2802],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3212: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3212: tensor([[ 1.0740],\n",
      "        [ 0.1684],\n",
      "        [-1.8959],\n",
      "        [ 1.2824],\n",
      "        [ 0.6919],\n",
      "        [-0.2892]], requires_grad=True)\n",
      "poly train loss at 3213: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3213: tensor([[ 1.0737],\n",
      "        [ 0.1674],\n",
      "        [-1.8976],\n",
      "        [ 1.2802],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3214: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3214: tensor([[ 1.0740],\n",
      "        [ 0.1684],\n",
      "        [-1.8959],\n",
      "        [ 1.2824],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3215: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3215: tensor([[ 1.0737],\n",
      "        [ 0.1674],\n",
      "        [-1.8976],\n",
      "        [ 1.2802],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3216: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3216: tensor([[ 1.0740],\n",
      "        [ 0.1684],\n",
      "        [-1.8959],\n",
      "        [ 1.2824],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3217: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3217: tensor([[ 1.0737],\n",
      "        [ 0.1673],\n",
      "        [-1.8976],\n",
      "        [ 1.2802],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3218: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3218: tensor([[ 1.0740],\n",
      "        [ 0.1684],\n",
      "        [-1.8959],\n",
      "        [ 1.2824],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3219: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3219: tensor([[ 1.0737],\n",
      "        [ 0.1673],\n",
      "        [-1.8977],\n",
      "        [ 1.2802],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3220: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3220: tensor([[ 1.0740],\n",
      "        [ 0.1683],\n",
      "        [-1.8960],\n",
      "        [ 1.2824],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3221: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3221: tensor([[ 1.0737],\n",
      "        [ 0.1673],\n",
      "        [-1.8977],\n",
      "        [ 1.2803],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3222: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3222: tensor([[ 1.0740],\n",
      "        [ 0.1683],\n",
      "        [-1.8960],\n",
      "        [ 1.2825],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3223: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3223: tensor([[ 1.0737],\n",
      "        [ 0.1673],\n",
      "        [-1.8977],\n",
      "        [ 1.2803],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3224: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3224: tensor([[ 1.0740],\n",
      "        [ 0.1683],\n",
      "        [-1.8960],\n",
      "        [ 1.2825],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3225: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3225: tensor([[ 1.0737],\n",
      "        [ 0.1673],\n",
      "        [-1.8977],\n",
      "        [ 1.2803],\n",
      "        [ 0.6895],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3226: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3226: tensor([[ 1.0740],\n",
      "        [ 0.1683],\n",
      "        [-1.8960],\n",
      "        [ 1.2825],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3227: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3227: tensor([[ 1.0737],\n",
      "        [ 0.1672],\n",
      "        [-1.8978],\n",
      "        [ 1.2803],\n",
      "        [ 0.6896],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 3228: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3228: tensor([[ 1.0740],\n",
      "        [ 0.1682],\n",
      "        [-1.8960],\n",
      "        [ 1.2825],\n",
      "        [ 0.6919],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3229: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3229: tensor([[ 1.0737],\n",
      "        [ 0.1672],\n",
      "        [-1.8978],\n",
      "        [ 1.2803],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3230: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3230: tensor([[ 1.0740],\n",
      "        [ 0.1682],\n",
      "        [-1.8961],\n",
      "        [ 1.2825],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3231: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3231: tensor([[ 1.0737],\n",
      "        [ 0.1672],\n",
      "        [-1.8978],\n",
      "        [ 1.2803],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3232: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3232: tensor([[ 1.0740],\n",
      "        [ 0.1682],\n",
      "        [-1.8961],\n",
      "        [ 1.2825],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3233: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3233: tensor([[ 1.0737],\n",
      "        [ 0.1672],\n",
      "        [-1.8978],\n",
      "        [ 1.2804],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3234: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3234: tensor([[ 1.0740],\n",
      "        [ 0.1682],\n",
      "        [-1.8961],\n",
      "        [ 1.2826],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3235: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3235: tensor([[ 1.0737],\n",
      "        [ 0.1671],\n",
      "        [-1.8979],\n",
      "        [ 1.2804],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3236: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3236: tensor([[ 1.0740],\n",
      "        [ 0.1682],\n",
      "        [-1.8961],\n",
      "        [ 1.2826],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3237: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3237: tensor([[ 1.0737],\n",
      "        [ 0.1671],\n",
      "        [-1.8979],\n",
      "        [ 1.2804],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3238: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3238: tensor([[ 1.0740],\n",
      "        [ 0.1681],\n",
      "        [-1.8962],\n",
      "        [ 1.2826],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3239: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3239: tensor([[ 1.0737],\n",
      "        [ 0.1671],\n",
      "        [-1.8979],\n",
      "        [ 1.2804],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3240: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3240: tensor([[ 1.0740],\n",
      "        [ 0.1681],\n",
      "        [-1.8962],\n",
      "        [ 1.2826],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3241: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3241: tensor([[ 1.0737],\n",
      "        [ 0.1671],\n",
      "        [-1.8979],\n",
      "        [ 1.2804],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3242: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3242: tensor([[ 1.0740],\n",
      "        [ 0.1681],\n",
      "        [-1.8962],\n",
      "        [ 1.2826],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3243: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3243: tensor([[ 1.0737],\n",
      "        [ 0.1671],\n",
      "        [-1.8980],\n",
      "        [ 1.2804],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3244: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3244: tensor([[ 1.0740],\n",
      "        [ 0.1681],\n",
      "        [-1.8962],\n",
      "        [ 1.2827],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3245: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3245: tensor([[ 1.0737],\n",
      "        [ 0.1670],\n",
      "        [-1.8980],\n",
      "        [ 1.2805],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3246: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3246: tensor([[ 1.0740],\n",
      "        [ 0.1680],\n",
      "        [-1.8963],\n",
      "        [ 1.2827],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3247: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3247: tensor([[ 1.0737],\n",
      "        [ 0.1670],\n",
      "        [-1.8980],\n",
      "        [ 1.2805],\n",
      "        [ 0.6896],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3248: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3248: tensor([[ 1.0740],\n",
      "        [ 0.1680],\n",
      "        [-1.8963],\n",
      "        [ 1.2827],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3249: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3249: tensor([[ 1.0737],\n",
      "        [ 0.1670],\n",
      "        [-1.8980],\n",
      "        [ 1.2805],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3250: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3250: tensor([[ 1.0740],\n",
      "        [ 0.1680],\n",
      "        [-1.8963],\n",
      "        [ 1.2827],\n",
      "        [ 0.6920],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3251: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3251: tensor([[ 1.0737],\n",
      "        [ 0.1670],\n",
      "        [-1.8980],\n",
      "        [ 1.2805],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3252: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3252: tensor([[ 1.0740],\n",
      "        [ 0.1680],\n",
      "        [-1.8963],\n",
      "        [ 1.2827],\n",
      "        [ 0.6921],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3253: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3253: tensor([[ 1.0737],\n",
      "        [ 0.1670],\n",
      "        [-1.8981],\n",
      "        [ 1.2805],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3254: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3254: tensor([[ 1.0740],\n",
      "        [ 0.1680],\n",
      "        [-1.8964],\n",
      "        [ 1.2827],\n",
      "        [ 0.6921],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3255: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3255: tensor([[ 1.0737],\n",
      "        [ 0.1669],\n",
      "        [-1.8981],\n",
      "        [ 1.2805],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3256: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3256: tensor([[ 1.0740],\n",
      "        [ 0.1679],\n",
      "        [-1.8964],\n",
      "        [ 1.2828],\n",
      "        [ 0.6921],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3257: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3257: tensor([[ 1.0737],\n",
      "        [ 0.1669],\n",
      "        [-1.8981],\n",
      "        [ 1.2806],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3258: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3258: tensor([[ 1.0740],\n",
      "        [ 0.1679],\n",
      "        [-1.8964],\n",
      "        [ 1.2828],\n",
      "        [ 0.6921],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3259: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3259: tensor([[ 1.0737],\n",
      "        [ 0.1669],\n",
      "        [-1.8981],\n",
      "        [ 1.2806],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3260: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3260: tensor([[ 1.0740],\n",
      "        [ 0.1679],\n",
      "        [-1.8964],\n",
      "        [ 1.2828],\n",
      "        [ 0.6921],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3261: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3261: tensor([[ 1.0737],\n",
      "        [ 0.1669],\n",
      "        [-1.8982],\n",
      "        [ 1.2806],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3262: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3262: tensor([[ 1.0740],\n",
      "        [ 0.1679],\n",
      "        [-1.8964],\n",
      "        [ 1.2828],\n",
      "        [ 0.6921],\n",
      "        [-0.2893]], requires_grad=True)\n",
      "poly train loss at 3263: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3263: tensor([[ 1.0737],\n",
      "        [ 0.1668],\n",
      "        [-1.8982],\n",
      "        [ 1.2806],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3264: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3264: tensor([[ 1.0740],\n",
      "        [ 0.1679],\n",
      "        [-1.8965],\n",
      "        [ 1.2828],\n",
      "        [ 0.6921],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3265: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3265: tensor([[ 1.0737],\n",
      "        [ 0.1668],\n",
      "        [-1.8982],\n",
      "        [ 1.2806],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3266: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3266: tensor([[ 1.0740],\n",
      "        [ 0.1678],\n",
      "        [-1.8965],\n",
      "        [ 1.2828],\n",
      "        [ 0.6921],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3267: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3267: tensor([[ 1.0737],\n",
      "        [ 0.1668],\n",
      "        [-1.8982],\n",
      "        [ 1.2806],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3268: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3268: tensor([[ 1.0740],\n",
      "        [ 0.1678],\n",
      "        [-1.8965],\n",
      "        [ 1.2829],\n",
      "        [ 0.6921],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3269: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3269: tensor([[ 1.0737],\n",
      "        [ 0.1668],\n",
      "        [-1.8983],\n",
      "        [ 1.2807],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3270: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3270: tensor([[ 1.0740],\n",
      "        [ 0.1678],\n",
      "        [-1.8965],\n",
      "        [ 1.2829],\n",
      "        [ 0.6921],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3271: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3271: tensor([[ 1.0737],\n",
      "        [ 0.1668],\n",
      "        [-1.8983],\n",
      "        [ 1.2807],\n",
      "        [ 0.6897],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3272: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3272: tensor([[ 1.0740],\n",
      "        [ 0.1678],\n",
      "        [-1.8966],\n",
      "        [ 1.2829],\n",
      "        [ 0.6921],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3273: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3273: tensor([[ 1.0737],\n",
      "        [ 0.1667],\n",
      "        [-1.8983],\n",
      "        [ 1.2807],\n",
      "        [ 0.6898],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3274: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3274: tensor([[ 1.0740],\n",
      "        [ 0.1677],\n",
      "        [-1.8966],\n",
      "        [ 1.2829],\n",
      "        [ 0.6921],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3275: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3275: tensor([[ 1.0737],\n",
      "        [ 0.1667],\n",
      "        [-1.8983],\n",
      "        [ 1.2807],\n",
      "        [ 0.6898],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3276: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3276: tensor([[ 1.0740],\n",
      "        [ 0.1677],\n",
      "        [-1.8966],\n",
      "        [ 1.2829],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3277: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3277: tensor([[ 1.0737],\n",
      "        [ 0.1667],\n",
      "        [-1.8984],\n",
      "        [ 1.2807],\n",
      "        [ 0.6898],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 3278: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3278: tensor([[ 1.0740],\n",
      "        [ 0.1677],\n",
      "        [-1.8966],\n",
      "        [ 1.2829],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3279: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3279: tensor([[ 1.0737],\n",
      "        [ 0.1667],\n",
      "        [-1.8984],\n",
      "        [ 1.2808],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3280: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3280: tensor([[ 1.0740],\n",
      "        [ 0.1677],\n",
      "        [-1.8967],\n",
      "        [ 1.2830],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3281: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3281: tensor([[ 1.0737],\n",
      "        [ 0.1666],\n",
      "        [-1.8984],\n",
      "        [ 1.2808],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3282: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3282: tensor([[ 1.0740],\n",
      "        [ 0.1677],\n",
      "        [-1.8967],\n",
      "        [ 1.2830],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3283: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3283: tensor([[ 1.0737],\n",
      "        [ 0.1666],\n",
      "        [-1.8984],\n",
      "        [ 1.2808],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3284: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3284: tensor([[ 1.0740],\n",
      "        [ 0.1676],\n",
      "        [-1.8967],\n",
      "        [ 1.2830],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3285: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3285: tensor([[ 1.0737],\n",
      "        [ 0.1666],\n",
      "        [-1.8985],\n",
      "        [ 1.2808],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3286: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3286: tensor([[ 1.0740],\n",
      "        [ 0.1676],\n",
      "        [-1.8967],\n",
      "        [ 1.2830],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3287: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3287: tensor([[ 1.0737],\n",
      "        [ 0.1666],\n",
      "        [-1.8985],\n",
      "        [ 1.2808],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3288: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3288: tensor([[ 1.0740],\n",
      "        [ 0.1676],\n",
      "        [-1.8968],\n",
      "        [ 1.2830],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3289: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3289: tensor([[ 1.0737],\n",
      "        [ 0.1666],\n",
      "        [-1.8985],\n",
      "        [ 1.2808],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3290: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3290: tensor([[ 1.0740],\n",
      "        [ 0.1676],\n",
      "        [-1.8968],\n",
      "        [ 1.2830],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3291: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3291: tensor([[ 1.0737],\n",
      "        [ 0.1665],\n",
      "        [-1.8985],\n",
      "        [ 1.2809],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3292: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3292: tensor([[ 1.0740],\n",
      "        [ 0.1675],\n",
      "        [-1.8968],\n",
      "        [ 1.2831],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3293: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3293: tensor([[ 1.0737],\n",
      "        [ 0.1665],\n",
      "        [-1.8985],\n",
      "        [ 1.2809],\n",
      "        [ 0.6898],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3294: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3294: tensor([[ 1.0740],\n",
      "        [ 0.1675],\n",
      "        [-1.8968],\n",
      "        [ 1.2831],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3295: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3295: tensor([[ 1.0737],\n",
      "        [ 0.1665],\n",
      "        [-1.8986],\n",
      "        [ 1.2809],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3296: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3296: tensor([[ 1.0740],\n",
      "        [ 0.1675],\n",
      "        [-1.8968],\n",
      "        [ 1.2831],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3297: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3297: tensor([[ 1.0737],\n",
      "        [ 0.1665],\n",
      "        [-1.8986],\n",
      "        [ 1.2809],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3298: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3298: tensor([[ 1.0740],\n",
      "        [ 0.1675],\n",
      "        [-1.8969],\n",
      "        [ 1.2831],\n",
      "        [ 0.6922],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3299: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3299: tensor([[ 1.0737],\n",
      "        [ 0.1665],\n",
      "        [-1.8986],\n",
      "        [ 1.2809],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3300: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3300: tensor([[ 1.0740],\n",
      "        [ 0.1675],\n",
      "        [-1.8969],\n",
      "        [ 1.2831],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3301: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3301: tensor([[ 1.0737],\n",
      "        [ 0.1664],\n",
      "        [-1.8986],\n",
      "        [ 1.2809],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3302: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3302: tensor([[ 1.0740],\n",
      "        [ 0.1674],\n",
      "        [-1.8969],\n",
      "        [ 1.2831],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3303: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3303: tensor([[ 1.0737],\n",
      "        [ 0.1664],\n",
      "        [-1.8987],\n",
      "        [ 1.2810],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3304: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3304: tensor([[ 1.0740],\n",
      "        [ 0.1674],\n",
      "        [-1.8969],\n",
      "        [ 1.2832],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3305: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3305: tensor([[ 1.0737],\n",
      "        [ 0.1664],\n",
      "        [-1.8987],\n",
      "        [ 1.2810],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3306: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3306: tensor([[ 1.0740],\n",
      "        [ 0.1674],\n",
      "        [-1.8970],\n",
      "        [ 1.2832],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3307: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3307: tensor([[ 1.0737],\n",
      "        [ 0.1664],\n",
      "        [-1.8987],\n",
      "        [ 1.2810],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3308: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3308: tensor([[ 1.0740],\n",
      "        [ 0.1674],\n",
      "        [-1.8970],\n",
      "        [ 1.2832],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3309: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3309: tensor([[ 1.0737],\n",
      "        [ 0.1663],\n",
      "        [-1.8987],\n",
      "        [ 1.2810],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3310: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3310: tensor([[ 1.0740],\n",
      "        [ 0.1674],\n",
      "        [-1.8970],\n",
      "        [ 1.2832],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3311: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3311: tensor([[ 1.0737],\n",
      "        [ 0.1663],\n",
      "        [-1.8988],\n",
      "        [ 1.2810],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3312: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3312: tensor([[ 1.0740],\n",
      "        [ 0.1673],\n",
      "        [-1.8970],\n",
      "        [ 1.2832],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3313: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3313: tensor([[ 1.0737],\n",
      "        [ 0.1663],\n",
      "        [-1.8988],\n",
      "        [ 1.2810],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3314: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3314: tensor([[ 1.0740],\n",
      "        [ 0.1673],\n",
      "        [-1.8971],\n",
      "        [ 1.2833],\n",
      "        [ 0.6923],\n",
      "        [-0.2894]], requires_grad=True)\n",
      "poly train loss at 3315: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3315: tensor([[ 1.0737],\n",
      "        [ 0.1663],\n",
      "        [-1.8988],\n",
      "        [ 1.2811],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3316: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3316: tensor([[ 1.0740],\n",
      "        [ 0.1673],\n",
      "        [-1.8971],\n",
      "        [ 1.2833],\n",
      "        [ 0.6923],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3317: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3317: tensor([[ 1.0737],\n",
      "        [ 0.1663],\n",
      "        [-1.8988],\n",
      "        [ 1.2811],\n",
      "        [ 0.6899],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3318: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3318: tensor([[ 1.0740],\n",
      "        [ 0.1673],\n",
      "        [-1.8971],\n",
      "        [ 1.2833],\n",
      "        [ 0.6923],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3319: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3319: tensor([[ 1.0737],\n",
      "        [ 0.1662],\n",
      "        [-1.8989],\n",
      "        [ 1.2811],\n",
      "        [ 0.6900],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3320: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3320: tensor([[ 1.0740],\n",
      "        [ 0.1672],\n",
      "        [-1.8971],\n",
      "        [ 1.2833],\n",
      "        [ 0.6923],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3321: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3321: tensor([[ 1.0737],\n",
      "        [ 0.1662],\n",
      "        [-1.8989],\n",
      "        [ 1.2811],\n",
      "        [ 0.6900],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3322: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3322: tensor([[ 1.0740],\n",
      "        [ 0.1672],\n",
      "        [-1.8972],\n",
      "        [ 1.2833],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3323: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3323: tensor([[ 1.0737],\n",
      "        [ 0.1662],\n",
      "        [-1.8989],\n",
      "        [ 1.2811],\n",
      "        [ 0.6900],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3324: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3324: tensor([[ 1.0740],\n",
      "        [ 0.1672],\n",
      "        [-1.8972],\n",
      "        [ 1.2833],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3325: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3325: tensor([[ 1.0737],\n",
      "        [ 0.1662],\n",
      "        [-1.8989],\n",
      "        [ 1.2811],\n",
      "        [ 0.6900],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3326: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3326: tensor([[ 1.0740],\n",
      "        [ 0.1672],\n",
      "        [-1.8972],\n",
      "        [ 1.2834],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3327: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3327: tensor([[ 1.0737],\n",
      "        [ 0.1661],\n",
      "        [-1.8989],\n",
      "        [ 1.2812],\n",
      "        [ 0.6900],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3328: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3328: tensor([[ 1.0740],\n",
      "        [ 0.1672],\n",
      "        [-1.8972],\n",
      "        [ 1.2834],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3329: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3329: tensor([[ 1.0737],\n",
      "        [ 0.1661],\n",
      "        [-1.8990],\n",
      "        [ 1.2812],\n",
      "        [ 0.6900],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 3330: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3330: tensor([[ 1.0740],\n",
      "        [ 0.1671],\n",
      "        [-1.8972],\n",
      "        [ 1.2834],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3331: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3331: tensor([[ 1.0737],\n",
      "        [ 0.1661],\n",
      "        [-1.8990],\n",
      "        [ 1.2812],\n",
      "        [ 0.6900],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3332: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3332: tensor([[ 1.0740],\n",
      "        [ 0.1671],\n",
      "        [-1.8973],\n",
      "        [ 1.2834],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3333: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3333: tensor([[ 1.0737],\n",
      "        [ 0.1661],\n",
      "        [-1.8990],\n",
      "        [ 1.2812],\n",
      "        [ 0.6900],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3334: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3334: tensor([[ 1.0740],\n",
      "        [ 0.1671],\n",
      "        [-1.8973],\n",
      "        [ 1.2834],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3335: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3335: tensor([[ 1.0737],\n",
      "        [ 0.1661],\n",
      "        [-1.8990],\n",
      "        [ 1.2812],\n",
      "        [ 0.6900],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3336: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3336: tensor([[ 1.0740],\n",
      "        [ 0.1671],\n",
      "        [-1.8973],\n",
      "        [ 1.2834],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3337: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3337: tensor([[ 1.0737],\n",
      "        [ 0.1660],\n",
      "        [-1.8991],\n",
      "        [ 1.2812],\n",
      "        [ 0.6900],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3338: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3338: tensor([[ 1.0740],\n",
      "        [ 0.1671],\n",
      "        [-1.8973],\n",
      "        [ 1.2835],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3339: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3339: tensor([[ 1.0737],\n",
      "        [ 0.1660],\n",
      "        [-1.8991],\n",
      "        [ 1.2813],\n",
      "        [ 0.6900],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3340: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3340: tensor([[ 1.0740],\n",
      "        [ 0.1670],\n",
      "        [-1.8974],\n",
      "        [ 1.2835],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3341: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3341: tensor([[ 1.0737],\n",
      "        [ 0.1660],\n",
      "        [-1.8991],\n",
      "        [ 1.2813],\n",
      "        [ 0.6900],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3342: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3342: tensor([[ 1.0740],\n",
      "        [ 0.1670],\n",
      "        [-1.8974],\n",
      "        [ 1.2835],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3343: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3343: tensor([[ 1.0737],\n",
      "        [ 0.1660],\n",
      "        [-1.8991],\n",
      "        [ 1.2813],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3344: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3344: tensor([[ 1.0740],\n",
      "        [ 0.1670],\n",
      "        [-1.8974],\n",
      "        [ 1.2835],\n",
      "        [ 0.6924],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3345: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3345: tensor([[ 1.0737],\n",
      "        [ 0.1660],\n",
      "        [-1.8992],\n",
      "        [ 1.2813],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3346: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3346: tensor([[ 1.0740],\n",
      "        [ 0.1670],\n",
      "        [-1.8974],\n",
      "        [ 1.2835],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3347: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3347: tensor([[ 1.0737],\n",
      "        [ 0.1659],\n",
      "        [-1.8992],\n",
      "        [ 1.2813],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3348: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3348: tensor([[ 1.0740],\n",
      "        [ 0.1669],\n",
      "        [-1.8975],\n",
      "        [ 1.2835],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3349: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3349: tensor([[ 1.0737],\n",
      "        [ 0.1659],\n",
      "        [-1.8992],\n",
      "        [ 1.2813],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3350: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3350: tensor([[ 1.0741],\n",
      "        [ 0.1669],\n",
      "        [-1.8975],\n",
      "        [ 1.2836],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3351: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3351: tensor([[ 1.0737],\n",
      "        [ 0.1659],\n",
      "        [-1.8992],\n",
      "        [ 1.2814],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3352: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3352: tensor([[ 1.0741],\n",
      "        [ 0.1669],\n",
      "        [-1.8975],\n",
      "        [ 1.2836],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3353: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3353: tensor([[ 1.0737],\n",
      "        [ 0.1659],\n",
      "        [-1.8992],\n",
      "        [ 1.2814],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3354: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3354: tensor([[ 1.0741],\n",
      "        [ 0.1669],\n",
      "        [-1.8975],\n",
      "        [ 1.2836],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3355: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3355: tensor([[ 1.0737],\n",
      "        [ 0.1658],\n",
      "        [-1.8993],\n",
      "        [ 1.2814],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3356: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3356: tensor([[ 1.0741],\n",
      "        [ 0.1669],\n",
      "        [-1.8975],\n",
      "        [ 1.2836],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3357: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3357: tensor([[ 1.0737],\n",
      "        [ 0.1658],\n",
      "        [-1.8993],\n",
      "        [ 1.2814],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3358: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3358: tensor([[ 1.0741],\n",
      "        [ 0.1668],\n",
      "        [-1.8976],\n",
      "        [ 1.2836],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3359: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3359: tensor([[ 1.0737],\n",
      "        [ 0.1658],\n",
      "        [-1.8993],\n",
      "        [ 1.2814],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3360: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3360: tensor([[ 1.0741],\n",
      "        [ 0.1668],\n",
      "        [-1.8976],\n",
      "        [ 1.2836],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3361: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3361: tensor([[ 1.0737],\n",
      "        [ 0.1658],\n",
      "        [-1.8993],\n",
      "        [ 1.2814],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3362: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3362: tensor([[ 1.0741],\n",
      "        [ 0.1668],\n",
      "        [-1.8976],\n",
      "        [ 1.2837],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3363: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3363: tensor([[ 1.0737],\n",
      "        [ 0.1658],\n",
      "        [-1.8994],\n",
      "        [ 1.2815],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3364: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3364: tensor([[ 1.0741],\n",
      "        [ 0.1668],\n",
      "        [-1.8976],\n",
      "        [ 1.2837],\n",
      "        [ 0.6925],\n",
      "        [-0.2895]], requires_grad=True)\n",
      "poly train loss at 3365: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3365: tensor([[ 1.0737],\n",
      "        [ 0.1657],\n",
      "        [-1.8994],\n",
      "        [ 1.2815],\n",
      "        [ 0.6901],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3366: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3366: tensor([[ 1.0741],\n",
      "        [ 0.1667],\n",
      "        [-1.8977],\n",
      "        [ 1.2837],\n",
      "        [ 0.6925],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3367: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3367: tensor([[ 1.0737],\n",
      "        [ 0.1657],\n",
      "        [-1.8994],\n",
      "        [ 1.2815],\n",
      "        [ 0.6902],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3368: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3368: tensor([[ 1.0741],\n",
      "        [ 0.1667],\n",
      "        [-1.8977],\n",
      "        [ 1.2837],\n",
      "        [ 0.6925],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3369: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3369: tensor([[ 1.0737],\n",
      "        [ 0.1657],\n",
      "        [-1.8994],\n",
      "        [ 1.2815],\n",
      "        [ 0.6902],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3370: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3370: tensor([[ 1.0741],\n",
      "        [ 0.1667],\n",
      "        [-1.8977],\n",
      "        [ 1.2837],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3371: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3371: tensor([[ 1.0737],\n",
      "        [ 0.1657],\n",
      "        [-1.8994],\n",
      "        [ 1.2815],\n",
      "        [ 0.6902],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3372: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3372: tensor([[ 1.0741],\n",
      "        [ 0.1667],\n",
      "        [-1.8977],\n",
      "        [ 1.2837],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3373: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3373: tensor([[ 1.0737],\n",
      "        [ 0.1657],\n",
      "        [-1.8995],\n",
      "        [ 1.2816],\n",
      "        [ 0.6902],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3374: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3374: tensor([[ 1.0741],\n",
      "        [ 0.1667],\n",
      "        [-1.8977],\n",
      "        [ 1.2838],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3375: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3375: tensor([[ 1.0737],\n",
      "        [ 0.1656],\n",
      "        [-1.8995],\n",
      "        [ 1.2816],\n",
      "        [ 0.6902],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3376: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3376: tensor([[ 1.0741],\n",
      "        [ 0.1666],\n",
      "        [-1.8978],\n",
      "        [ 1.2838],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3377: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3377: tensor([[ 1.0737],\n",
      "        [ 0.1656],\n",
      "        [-1.8995],\n",
      "        [ 1.2816],\n",
      "        [ 0.6902],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3378: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3378: tensor([[ 1.0741],\n",
      "        [ 0.1666],\n",
      "        [-1.8978],\n",
      "        [ 1.2838],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3379: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3379: tensor([[ 1.0737],\n",
      "        [ 0.1656],\n",
      "        [-1.8995],\n",
      "        [ 1.2816],\n",
      "        [ 0.6902],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 3380: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3380: tensor([[ 1.0741],\n",
      "        [ 0.1666],\n",
      "        [-1.8978],\n",
      "        [ 1.2838],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3381: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3381: tensor([[ 1.0737],\n",
      "        [ 0.1656],\n",
      "        [-1.8996],\n",
      "        [ 1.2816],\n",
      "        [ 0.6902],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3382: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3382: tensor([[ 1.0741],\n",
      "        [ 0.1666],\n",
      "        [-1.8978],\n",
      "        [ 1.2838],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3383: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3383: tensor([[ 1.0737],\n",
      "        [ 0.1655],\n",
      "        [-1.8996],\n",
      "        [ 1.2816],\n",
      "        [ 0.6902],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3384: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3384: tensor([[ 1.0741],\n",
      "        [ 0.1666],\n",
      "        [-1.8979],\n",
      "        [ 1.2838],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3385: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3385: tensor([[ 1.0737],\n",
      "        [ 0.1655],\n",
      "        [-1.8996],\n",
      "        [ 1.2817],\n",
      "        [ 0.6902],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3386: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3386: tensor([[ 1.0741],\n",
      "        [ 0.1665],\n",
      "        [-1.8979],\n",
      "        [ 1.2839],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3387: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3387: tensor([[ 1.0737],\n",
      "        [ 0.1655],\n",
      "        [-1.8996],\n",
      "        [ 1.2817],\n",
      "        [ 0.6902],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3388: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3388: tensor([[ 1.0741],\n",
      "        [ 0.1665],\n",
      "        [-1.8979],\n",
      "        [ 1.2839],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3389: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3389: tensor([[ 1.0737],\n",
      "        [ 0.1655],\n",
      "        [-1.8996],\n",
      "        [ 1.2817],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3390: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3390: tensor([[ 1.0741],\n",
      "        [ 0.1665],\n",
      "        [-1.8979],\n",
      "        [ 1.2839],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3391: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3391: tensor([[ 1.0737],\n",
      "        [ 0.1655],\n",
      "        [-1.8997],\n",
      "        [ 1.2817],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3392: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3392: tensor([[ 1.0741],\n",
      "        [ 0.1665],\n",
      "        [-1.8980],\n",
      "        [ 1.2839],\n",
      "        [ 0.6926],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3393: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3393: tensor([[ 1.0737],\n",
      "        [ 0.1654],\n",
      "        [-1.8997],\n",
      "        [ 1.2817],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3394: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3394: tensor([[ 1.0741],\n",
      "        [ 0.1664],\n",
      "        [-1.8980],\n",
      "        [ 1.2839],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3395: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3395: tensor([[ 1.0737],\n",
      "        [ 0.1654],\n",
      "        [-1.8997],\n",
      "        [ 1.2817],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3396: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3396: tensor([[ 1.0741],\n",
      "        [ 0.1664],\n",
      "        [-1.8980],\n",
      "        [ 1.2839],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3397: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3397: tensor([[ 1.0737],\n",
      "        [ 0.1654],\n",
      "        [-1.8997],\n",
      "        [ 1.2818],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3398: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3398: tensor([[ 1.0741],\n",
      "        [ 0.1664],\n",
      "        [-1.8980],\n",
      "        [ 1.2840],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3399: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3399: tensor([[ 1.0737],\n",
      "        [ 0.1654],\n",
      "        [-1.8998],\n",
      "        [ 1.2818],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3400: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3400: tensor([[ 1.0741],\n",
      "        [ 0.1664],\n",
      "        [-1.8980],\n",
      "        [ 1.2840],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3401: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3401: tensor([[ 1.0737],\n",
      "        [ 0.1654],\n",
      "        [-1.8998],\n",
      "        [ 1.2818],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3402: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3402: tensor([[ 1.0741],\n",
      "        [ 0.1664],\n",
      "        [-1.8981],\n",
      "        [ 1.2840],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3403: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3403: tensor([[ 1.0737],\n",
      "        [ 0.1653],\n",
      "        [-1.8998],\n",
      "        [ 1.2818],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3404: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3404: tensor([[ 1.0741],\n",
      "        [ 0.1663],\n",
      "        [-1.8981],\n",
      "        [ 1.2840],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3405: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3405: tensor([[ 1.0737],\n",
      "        [ 0.1653],\n",
      "        [-1.8998],\n",
      "        [ 1.2818],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3406: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3406: tensor([[ 1.0741],\n",
      "        [ 0.1663],\n",
      "        [-1.8981],\n",
      "        [ 1.2840],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3407: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3407: tensor([[ 1.0737],\n",
      "        [ 0.1653],\n",
      "        [-1.8999],\n",
      "        [ 1.2818],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3408: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3408: tensor([[ 1.0741],\n",
      "        [ 0.1663],\n",
      "        [-1.8981],\n",
      "        [ 1.2840],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3409: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3409: tensor([[ 1.0737],\n",
      "        [ 0.1653],\n",
      "        [-1.8999],\n",
      "        [ 1.2819],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3410: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3410: tensor([[ 1.0741],\n",
      "        [ 0.1663],\n",
      "        [-1.8982],\n",
      "        [ 1.2841],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3411: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3411: tensor([[ 1.0737],\n",
      "        [ 0.1652],\n",
      "        [-1.8999],\n",
      "        [ 1.2819],\n",
      "        [ 0.6903],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3412: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3412: tensor([[ 1.0741],\n",
      "        [ 0.1663],\n",
      "        [-1.8982],\n",
      "        [ 1.2841],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3413: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3413: tensor([[ 1.0737],\n",
      "        [ 0.1652],\n",
      "        [-1.8999],\n",
      "        [ 1.2819],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3414: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3414: tensor([[ 1.0741],\n",
      "        [ 0.1662],\n",
      "        [-1.8982],\n",
      "        [ 1.2841],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3415: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3415: tensor([[ 1.0737],\n",
      "        [ 0.1652],\n",
      "        [-1.8999],\n",
      "        [ 1.2819],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3416: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3416: tensor([[ 1.0741],\n",
      "        [ 0.1662],\n",
      "        [-1.8982],\n",
      "        [ 1.2841],\n",
      "        [ 0.6927],\n",
      "        [-0.2896]], requires_grad=True)\n",
      "poly train loss at 3417: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3417: tensor([[ 1.0737],\n",
      "        [ 0.1652],\n",
      "        [-1.9000],\n",
      "        [ 1.2819],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3418: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3418: tensor([[ 1.0741],\n",
      "        [ 0.1662],\n",
      "        [-1.8982],\n",
      "        [ 1.2841],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3419: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3419: tensor([[ 1.0737],\n",
      "        [ 0.1652],\n",
      "        [-1.9000],\n",
      "        [ 1.2819],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3420: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3420: tensor([[ 1.0741],\n",
      "        [ 0.1662],\n",
      "        [-1.8983],\n",
      "        [ 1.2841],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3421: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3421: tensor([[ 1.0737],\n",
      "        [ 0.1651],\n",
      "        [-1.9000],\n",
      "        [ 1.2820],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3422: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3422: tensor([[ 1.0741],\n",
      "        [ 0.1662],\n",
      "        [-1.8983],\n",
      "        [ 1.2842],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3423: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3423: tensor([[ 1.0737],\n",
      "        [ 0.1651],\n",
      "        [-1.9000],\n",
      "        [ 1.2820],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3424: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3424: tensor([[ 1.0741],\n",
      "        [ 0.1661],\n",
      "        [-1.8983],\n",
      "        [ 1.2842],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3425: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3425: tensor([[ 1.0737],\n",
      "        [ 0.1651],\n",
      "        [-1.9001],\n",
      "        [ 1.2820],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3426: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3426: tensor([[ 1.0741],\n",
      "        [ 0.1661],\n",
      "        [-1.8983],\n",
      "        [ 1.2842],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3427: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3427: tensor([[ 1.0737],\n",
      "        [ 0.1651],\n",
      "        [-1.9001],\n",
      "        [ 1.2820],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3428: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3428: tensor([[ 1.0741],\n",
      "        [ 0.1661],\n",
      "        [-1.8984],\n",
      "        [ 1.2842],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3429: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3429: tensor([[ 1.0737],\n",
      "        [ 0.1651],\n",
      "        [-1.9001],\n",
      "        [ 1.2820],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3430: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3430: tensor([[ 1.0741],\n",
      "        [ 0.1661],\n",
      "        [-1.8984],\n",
      "        [ 1.2842],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3431: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3431: tensor([[ 1.0737],\n",
      "        [ 0.1650],\n",
      "        [-1.9001],\n",
      "        [ 1.2820],\n",
      "        [ 0.6904],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 3432: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3432: tensor([[ 1.0741],\n",
      "        [ 0.1660],\n",
      "        [-1.8984],\n",
      "        [ 1.2842],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3433: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3433: tensor([[ 1.0737],\n",
      "        [ 0.1650],\n",
      "        [-1.9001],\n",
      "        [ 1.2821],\n",
      "        [ 0.6904],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3434: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3434: tensor([[ 1.0741],\n",
      "        [ 0.1660],\n",
      "        [-1.8984],\n",
      "        [ 1.2843],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3435: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3435: tensor([[ 1.0737],\n",
      "        [ 0.1650],\n",
      "        [-1.9002],\n",
      "        [ 1.2821],\n",
      "        [ 0.6904],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3436: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3436: tensor([[ 1.0741],\n",
      "        [ 0.1660],\n",
      "        [-1.8984],\n",
      "        [ 1.2843],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3437: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3437: tensor([[ 1.0737],\n",
      "        [ 0.1650],\n",
      "        [-1.9002],\n",
      "        [ 1.2821],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3438: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3438: tensor([[ 1.0741],\n",
      "        [ 0.1660],\n",
      "        [-1.8985],\n",
      "        [ 1.2843],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3439: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3439: tensor([[ 1.0737],\n",
      "        [ 0.1650],\n",
      "        [-1.9002],\n",
      "        [ 1.2821],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3440: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3440: tensor([[ 1.0741],\n",
      "        [ 0.1660],\n",
      "        [-1.8985],\n",
      "        [ 1.2843],\n",
      "        [ 0.6928],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3441: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3441: tensor([[ 1.0737],\n",
      "        [ 0.1649],\n",
      "        [-1.9002],\n",
      "        [ 1.2821],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3442: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3442: tensor([[ 1.0741],\n",
      "        [ 0.1659],\n",
      "        [-1.8985],\n",
      "        [ 1.2843],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3443: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3443: tensor([[ 1.0737],\n",
      "        [ 0.1649],\n",
      "        [-1.9003],\n",
      "        [ 1.2821],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3444: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3444: tensor([[ 1.0741],\n",
      "        [ 0.1659],\n",
      "        [-1.8985],\n",
      "        [ 1.2843],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3445: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3445: tensor([[ 1.0737],\n",
      "        [ 0.1649],\n",
      "        [-1.9003],\n",
      "        [ 1.2822],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3446: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3446: tensor([[ 1.0741],\n",
      "        [ 0.1659],\n",
      "        [-1.8986],\n",
      "        [ 1.2844],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3447: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3447: tensor([[ 1.0737],\n",
      "        [ 0.1649],\n",
      "        [-1.9003],\n",
      "        [ 1.2822],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3448: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3448: tensor([[ 1.0741],\n",
      "        [ 0.1659],\n",
      "        [-1.8986],\n",
      "        [ 1.2844],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3449: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3449: tensor([[ 1.0737],\n",
      "        [ 0.1649],\n",
      "        [-1.9003],\n",
      "        [ 1.2822],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3450: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3450: tensor([[ 1.0741],\n",
      "        [ 0.1659],\n",
      "        [-1.8986],\n",
      "        [ 1.2844],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3451: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3451: tensor([[ 1.0737],\n",
      "        [ 0.1648],\n",
      "        [-1.9004],\n",
      "        [ 1.2822],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3452: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3452: tensor([[ 1.0741],\n",
      "        [ 0.1658],\n",
      "        [-1.8986],\n",
      "        [ 1.2844],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3453: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3453: tensor([[ 1.0737],\n",
      "        [ 0.1648],\n",
      "        [-1.9004],\n",
      "        [ 1.2822],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3454: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3454: tensor([[ 1.0741],\n",
      "        [ 0.1658],\n",
      "        [-1.8987],\n",
      "        [ 1.2844],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3455: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3455: tensor([[ 1.0737],\n",
      "        [ 0.1648],\n",
      "        [-1.9004],\n",
      "        [ 1.2822],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3456: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3456: tensor([[ 1.0741],\n",
      "        [ 0.1658],\n",
      "        [-1.8987],\n",
      "        [ 1.2844],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3457: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3457: tensor([[ 1.0737],\n",
      "        [ 0.1648],\n",
      "        [-1.9004],\n",
      "        [ 1.2823],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3458: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3458: tensor([[ 1.0741],\n",
      "        [ 0.1658],\n",
      "        [-1.8987],\n",
      "        [ 1.2845],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3459: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3459: tensor([[ 1.0737],\n",
      "        [ 0.1647],\n",
      "        [-1.9004],\n",
      "        [ 1.2823],\n",
      "        [ 0.6905],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3460: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3460: tensor([[ 1.0741],\n",
      "        [ 0.1658],\n",
      "        [-1.8987],\n",
      "        [ 1.2845],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3461: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3461: tensor([[ 1.0738],\n",
      "        [ 0.1647],\n",
      "        [-1.9005],\n",
      "        [ 1.2823],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3462: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3462: tensor([[ 1.0741],\n",
      "        [ 0.1657],\n",
      "        [-1.8987],\n",
      "        [ 1.2845],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3463: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3463: tensor([[ 1.0738],\n",
      "        [ 0.1647],\n",
      "        [-1.9005],\n",
      "        [ 1.2823],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3464: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3464: tensor([[ 1.0741],\n",
      "        [ 0.1657],\n",
      "        [-1.8988],\n",
      "        [ 1.2845],\n",
      "        [ 0.6929],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3465: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3465: tensor([[ 1.0738],\n",
      "        [ 0.1647],\n",
      "        [-1.9005],\n",
      "        [ 1.2823],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3466: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3466: tensor([[ 1.0741],\n",
      "        [ 0.1657],\n",
      "        [-1.8988],\n",
      "        [ 1.2845],\n",
      "        [ 0.6930],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3467: tensor(0.3300, grad_fn=<DivBackward0>)\n",
      "poly w at 3467: tensor([[ 1.0738],\n",
      "        [ 0.1647],\n",
      "        [-1.9005],\n",
      "        [ 1.2823],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3468: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3468: tensor([[ 1.0741],\n",
      "        [ 0.1657],\n",
      "        [-1.8988],\n",
      "        [ 1.2845],\n",
      "        [ 0.6930],\n",
      "        [-0.2897]], requires_grad=True)\n",
      "poly train loss at 3469: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3469: tensor([[ 1.0738],\n",
      "        [ 0.1646],\n",
      "        [-1.9006],\n",
      "        [ 1.2823],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3470: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3470: tensor([[ 1.0741],\n",
      "        [ 0.1657],\n",
      "        [-1.8988],\n",
      "        [ 1.2846],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3471: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3471: tensor([[ 1.0738],\n",
      "        [ 0.1646],\n",
      "        [-1.9006],\n",
      "        [ 1.2824],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3472: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3472: tensor([[ 1.0741],\n",
      "        [ 0.1656],\n",
      "        [-1.8989],\n",
      "        [ 1.2846],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3473: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3473: tensor([[ 1.0738],\n",
      "        [ 0.1646],\n",
      "        [-1.9006],\n",
      "        [ 1.2824],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3474: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3474: tensor([[ 1.0741],\n",
      "        [ 0.1656],\n",
      "        [-1.8989],\n",
      "        [ 1.2846],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3475: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3475: tensor([[ 1.0738],\n",
      "        [ 0.1646],\n",
      "        [-1.9006],\n",
      "        [ 1.2824],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3476: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3476: tensor([[ 1.0741],\n",
      "        [ 0.1656],\n",
      "        [-1.8989],\n",
      "        [ 1.2846],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3477: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3477: tensor([[ 1.0738],\n",
      "        [ 0.1646],\n",
      "        [-1.9006],\n",
      "        [ 1.2824],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3478: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3478: tensor([[ 1.0741],\n",
      "        [ 0.1656],\n",
      "        [-1.8989],\n",
      "        [ 1.2846],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3479: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3479: tensor([[ 1.0738],\n",
      "        [ 0.1645],\n",
      "        [-1.9007],\n",
      "        [ 1.2824],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3480: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3480: tensor([[ 1.0741],\n",
      "        [ 0.1655],\n",
      "        [-1.8990],\n",
      "        [ 1.2846],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3481: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3481: tensor([[ 1.0738],\n",
      "        [ 0.1645],\n",
      "        [-1.9007],\n",
      "        [ 1.2824],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3482: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3482: tensor([[ 1.0741],\n",
      "        [ 0.1655],\n",
      "        [-1.8990],\n",
      "        [ 1.2847],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3483: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3483: tensor([[ 1.0738],\n",
      "        [ 0.1645],\n",
      "        [-1.9007],\n",
      "        [ 1.2825],\n",
      "        [ 0.6906],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3484: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3484: tensor([[ 1.0741],\n",
      "        [ 0.1655],\n",
      "        [-1.8990],\n",
      "        [ 1.2847],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3485: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3485: tensor([[ 1.0738],\n",
      "        [ 0.1645],\n",
      "        [-1.9007],\n",
      "        [ 1.2825],\n",
      "        [ 0.6907],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 3486: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3486: tensor([[ 1.0741],\n",
      "        [ 0.1655],\n",
      "        [-1.8990],\n",
      "        [ 1.2847],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3487: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3487: tensor([[ 1.0738],\n",
      "        [ 0.1645],\n",
      "        [-1.9008],\n",
      "        [ 1.2825],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3488: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3488: tensor([[ 1.0741],\n",
      "        [ 0.1655],\n",
      "        [-1.8990],\n",
      "        [ 1.2847],\n",
      "        [ 0.6930],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3489: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3489: tensor([[ 1.0738],\n",
      "        [ 0.1644],\n",
      "        [-1.9008],\n",
      "        [ 1.2825],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3490: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3490: tensor([[ 1.0741],\n",
      "        [ 0.1654],\n",
      "        [-1.8991],\n",
      "        [ 1.2847],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3491: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3491: tensor([[ 1.0738],\n",
      "        [ 0.1644],\n",
      "        [-1.9008],\n",
      "        [ 1.2825],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3492: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3492: tensor([[ 1.0741],\n",
      "        [ 0.1654],\n",
      "        [-1.8991],\n",
      "        [ 1.2847],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3493: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3493: tensor([[ 1.0738],\n",
      "        [ 0.1644],\n",
      "        [-1.9008],\n",
      "        [ 1.2825],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3494: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3494: tensor([[ 1.0741],\n",
      "        [ 0.1654],\n",
      "        [-1.8991],\n",
      "        [ 1.2848],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3495: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3495: tensor([[ 1.0738],\n",
      "        [ 0.1644],\n",
      "        [-1.9009],\n",
      "        [ 1.2826],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3496: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3496: tensor([[ 1.0741],\n",
      "        [ 0.1654],\n",
      "        [-1.8991],\n",
      "        [ 1.2848],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3497: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3497: tensor([[ 1.0738],\n",
      "        [ 0.1643],\n",
      "        [-1.9009],\n",
      "        [ 1.2826],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3498: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3498: tensor([[ 1.0741],\n",
      "        [ 0.1654],\n",
      "        [-1.8992],\n",
      "        [ 1.2848],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3499: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3499: tensor([[ 1.0738],\n",
      "        [ 0.1643],\n",
      "        [-1.9009],\n",
      "        [ 1.2826],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3500: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3500: tensor([[ 1.0741],\n",
      "        [ 0.1653],\n",
      "        [-1.8992],\n",
      "        [ 1.2848],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3501: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3501: tensor([[ 1.0738],\n",
      "        [ 0.1643],\n",
      "        [-1.9009],\n",
      "        [ 1.2826],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3502: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3502: tensor([[ 1.0741],\n",
      "        [ 0.1653],\n",
      "        [-1.8992],\n",
      "        [ 1.2848],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3503: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3503: tensor([[ 1.0738],\n",
      "        [ 0.1643],\n",
      "        [-1.9009],\n",
      "        [ 1.2826],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3504: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3504: tensor([[ 1.0741],\n",
      "        [ 0.1653],\n",
      "        [-1.8992],\n",
      "        [ 1.2848],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3505: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3505: tensor([[ 1.0738],\n",
      "        [ 0.1643],\n",
      "        [-1.9010],\n",
      "        [ 1.2826],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3506: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3506: tensor([[ 1.0741],\n",
      "        [ 0.1653],\n",
      "        [-1.8992],\n",
      "        [ 1.2849],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3507: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3507: tensor([[ 1.0738],\n",
      "        [ 0.1642],\n",
      "        [-1.9010],\n",
      "        [ 1.2827],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3508: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3508: tensor([[ 1.0741],\n",
      "        [ 0.1653],\n",
      "        [-1.8993],\n",
      "        [ 1.2849],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3509: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3509: tensor([[ 1.0738],\n",
      "        [ 0.1642],\n",
      "        [-1.9010],\n",
      "        [ 1.2827],\n",
      "        [ 0.6907],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3510: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3510: tensor([[ 1.0741],\n",
      "        [ 0.1652],\n",
      "        [-1.8993],\n",
      "        [ 1.2849],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3511: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3511: tensor([[ 1.0738],\n",
      "        [ 0.1642],\n",
      "        [-1.9010],\n",
      "        [ 1.2827],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3512: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3512: tensor([[ 1.0741],\n",
      "        [ 0.1652],\n",
      "        [-1.8993],\n",
      "        [ 1.2849],\n",
      "        [ 0.6931],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3513: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3513: tensor([[ 1.0738],\n",
      "        [ 0.1642],\n",
      "        [-1.9011],\n",
      "        [ 1.2827],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3514: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3514: tensor([[ 1.0741],\n",
      "        [ 0.1652],\n",
      "        [-1.8993],\n",
      "        [ 1.2849],\n",
      "        [ 0.6932],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3515: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3515: tensor([[ 1.0738],\n",
      "        [ 0.1642],\n",
      "        [-1.9011],\n",
      "        [ 1.2827],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3516: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3516: tensor([[ 1.0741],\n",
      "        [ 0.1652],\n",
      "        [-1.8994],\n",
      "        [ 1.2849],\n",
      "        [ 0.6932],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3517: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3517: tensor([[ 1.0738],\n",
      "        [ 0.1641],\n",
      "        [-1.9011],\n",
      "        [ 1.2827],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3518: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3518: tensor([[ 1.0741],\n",
      "        [ 0.1652],\n",
      "        [-1.8994],\n",
      "        [ 1.2850],\n",
      "        [ 0.6932],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3519: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3519: tensor([[ 1.0738],\n",
      "        [ 0.1641],\n",
      "        [-1.9011],\n",
      "        [ 1.2828],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3520: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3520: tensor([[ 1.0741],\n",
      "        [ 0.1651],\n",
      "        [-1.8994],\n",
      "        [ 1.2850],\n",
      "        [ 0.6932],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3521: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3521: tensor([[ 1.0738],\n",
      "        [ 0.1641],\n",
      "        [-1.9011],\n",
      "        [ 1.2828],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3522: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3522: tensor([[ 1.0741],\n",
      "        [ 0.1651],\n",
      "        [-1.8994],\n",
      "        [ 1.2850],\n",
      "        [ 0.6932],\n",
      "        [-0.2898]], requires_grad=True)\n",
      "poly train loss at 3523: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3523: tensor([[ 1.0738],\n",
      "        [ 0.1641],\n",
      "        [-1.9012],\n",
      "        [ 1.2828],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3524: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3524: tensor([[ 1.0741],\n",
      "        [ 0.1651],\n",
      "        [-1.8994],\n",
      "        [ 1.2850],\n",
      "        [ 0.6932],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3525: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3525: tensor([[ 1.0738],\n",
      "        [ 0.1641],\n",
      "        [-1.9012],\n",
      "        [ 1.2828],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3526: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3526: tensor([[ 1.0741],\n",
      "        [ 0.1651],\n",
      "        [-1.8995],\n",
      "        [ 1.2850],\n",
      "        [ 0.6932],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3527: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3527: tensor([[ 1.0738],\n",
      "        [ 0.1640],\n",
      "        [-1.9012],\n",
      "        [ 1.2828],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3528: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3528: tensor([[ 1.0741],\n",
      "        [ 0.1650],\n",
      "        [-1.8995],\n",
      "        [ 1.2850],\n",
      "        [ 0.6932],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3529: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3529: tensor([[ 1.0738],\n",
      "        [ 0.1640],\n",
      "        [-1.9012],\n",
      "        [ 1.2828],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3530: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3530: tensor([[ 1.0741],\n",
      "        [ 0.1650],\n",
      "        [-1.8995],\n",
      "        [ 1.2851],\n",
      "        [ 0.6932],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3531: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3531: tensor([[ 1.0738],\n",
      "        [ 0.1640],\n",
      "        [-1.9013],\n",
      "        [ 1.2829],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3532: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3532: tensor([[ 1.0741],\n",
      "        [ 0.1650],\n",
      "        [-1.8995],\n",
      "        [ 1.2851],\n",
      "        [ 0.6932],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3533: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3533: tensor([[ 1.0738],\n",
      "        [ 0.1640],\n",
      "        [-1.9013],\n",
      "        [ 1.2829],\n",
      "        [ 0.6908],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3534: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3534: tensor([[ 1.0741],\n",
      "        [ 0.1650],\n",
      "        [-1.8996],\n",
      "        [ 1.2851],\n",
      "        [ 0.6932],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3535: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3535: tensor([[ 1.0738],\n",
      "        [ 0.1640],\n",
      "        [-1.9013],\n",
      "        [ 1.2829],\n",
      "        [ 0.6909],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3536: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3536: tensor([[ 1.0741],\n",
      "        [ 0.1650],\n",
      "        [-1.8996],\n",
      "        [ 1.2851],\n",
      "        [ 0.6932],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3537: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3537: tensor([[ 1.0738],\n",
      "        [ 0.1639],\n",
      "        [-1.9013],\n",
      "        [ 1.2829],\n",
      "        [ 0.6909],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 3538: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3538: tensor([[ 1.0741],\n",
      "        [ 0.1649],\n",
      "        [-1.8996],\n",
      "        [ 1.2851],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3539: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3539: tensor([[ 1.0738],\n",
      "        [ 0.1639],\n",
      "        [-1.9013],\n",
      "        [ 1.2829],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3540: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3540: tensor([[ 1.0741],\n",
      "        [ 0.1649],\n",
      "        [-1.8996],\n",
      "        [ 1.2851],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3541: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3541: tensor([[ 1.0738],\n",
      "        [ 0.1639],\n",
      "        [-1.9014],\n",
      "        [ 1.2829],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3542: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3542: tensor([[ 1.0741],\n",
      "        [ 0.1649],\n",
      "        [-1.8996],\n",
      "        [ 1.2851],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3543: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3543: tensor([[ 1.0738],\n",
      "        [ 0.1639],\n",
      "        [-1.9014],\n",
      "        [ 1.2830],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3544: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3544: tensor([[ 1.0741],\n",
      "        [ 0.1649],\n",
      "        [-1.8997],\n",
      "        [ 1.2852],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3545: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3545: tensor([[ 1.0738],\n",
      "        [ 0.1639],\n",
      "        [-1.9014],\n",
      "        [ 1.2830],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3546: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3546: tensor([[ 1.0741],\n",
      "        [ 0.1649],\n",
      "        [-1.8997],\n",
      "        [ 1.2852],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3547: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3547: tensor([[ 1.0738],\n",
      "        [ 0.1638],\n",
      "        [-1.9014],\n",
      "        [ 1.2830],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3548: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3548: tensor([[ 1.0741],\n",
      "        [ 0.1648],\n",
      "        [-1.8997],\n",
      "        [ 1.2852],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3549: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3549: tensor([[ 1.0738],\n",
      "        [ 0.1638],\n",
      "        [-1.9015],\n",
      "        [ 1.2830],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3550: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3550: tensor([[ 1.0741],\n",
      "        [ 0.1648],\n",
      "        [-1.8997],\n",
      "        [ 1.2852],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3551: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3551: tensor([[ 1.0738],\n",
      "        [ 0.1638],\n",
      "        [-1.9015],\n",
      "        [ 1.2830],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3552: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3552: tensor([[ 1.0741],\n",
      "        [ 0.1648],\n",
      "        [-1.8998],\n",
      "        [ 1.2852],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3553: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3553: tensor([[ 1.0738],\n",
      "        [ 0.1638],\n",
      "        [-1.9015],\n",
      "        [ 1.2830],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3554: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3554: tensor([[ 1.0741],\n",
      "        [ 0.1648],\n",
      "        [-1.8998],\n",
      "        [ 1.2852],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3555: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3555: tensor([[ 1.0738],\n",
      "        [ 0.1637],\n",
      "        [-1.9015],\n",
      "        [ 1.2831],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3556: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3556: tensor([[ 1.0741],\n",
      "        [ 0.1648],\n",
      "        [-1.8998],\n",
      "        [ 1.2853],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3557: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3557: tensor([[ 1.0738],\n",
      "        [ 0.1637],\n",
      "        [-1.9016],\n",
      "        [ 1.2831],\n",
      "        [ 0.6909],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3558: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3558: tensor([[ 1.0741],\n",
      "        [ 0.1647],\n",
      "        [-1.8998],\n",
      "        [ 1.2853],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3559: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3559: tensor([[ 1.0738],\n",
      "        [ 0.1637],\n",
      "        [-1.9016],\n",
      "        [ 1.2831],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3560: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3560: tensor([[ 1.0741],\n",
      "        [ 0.1647],\n",
      "        [-1.8999],\n",
      "        [ 1.2853],\n",
      "        [ 0.6933],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3561: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3561: tensor([[ 1.0738],\n",
      "        [ 0.1637],\n",
      "        [-1.9016],\n",
      "        [ 1.2831],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3562: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3562: tensor([[ 1.0741],\n",
      "        [ 0.1647],\n",
      "        [-1.8999],\n",
      "        [ 1.2853],\n",
      "        [ 0.6934],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3563: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3563: tensor([[ 1.0738],\n",
      "        [ 0.1637],\n",
      "        [-1.9016],\n",
      "        [ 1.2831],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3564: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3564: tensor([[ 1.0741],\n",
      "        [ 0.1647],\n",
      "        [-1.8999],\n",
      "        [ 1.2853],\n",
      "        [ 0.6934],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3565: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3565: tensor([[ 1.0738],\n",
      "        [ 0.1636],\n",
      "        [-1.9016],\n",
      "        [ 1.2831],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3566: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3566: tensor([[ 1.0741],\n",
      "        [ 0.1647],\n",
      "        [-1.8999],\n",
      "        [ 1.2853],\n",
      "        [ 0.6934],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3567: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3567: tensor([[ 1.0738],\n",
      "        [ 0.1636],\n",
      "        [-1.9017],\n",
      "        [ 1.2831],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3568: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3568: tensor([[ 1.0741],\n",
      "        [ 0.1646],\n",
      "        [-1.8999],\n",
      "        [ 1.2854],\n",
      "        [ 0.6934],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3569: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3569: tensor([[ 1.0738],\n",
      "        [ 0.1636],\n",
      "        [-1.9017],\n",
      "        [ 1.2832],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3570: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3570: tensor([[ 1.0741],\n",
      "        [ 0.1646],\n",
      "        [-1.9000],\n",
      "        [ 1.2854],\n",
      "        [ 0.6934],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3571: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3571: tensor([[ 1.0738],\n",
      "        [ 0.1636],\n",
      "        [-1.9017],\n",
      "        [ 1.2832],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3572: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3572: tensor([[ 1.0741],\n",
      "        [ 0.1646],\n",
      "        [-1.9000],\n",
      "        [ 1.2854],\n",
      "        [ 0.6934],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3573: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3573: tensor([[ 1.0738],\n",
      "        [ 0.1636],\n",
      "        [-1.9017],\n",
      "        [ 1.2832],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3574: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3574: tensor([[ 1.0741],\n",
      "        [ 0.1646],\n",
      "        [-1.9000],\n",
      "        [ 1.2854],\n",
      "        [ 0.6934],\n",
      "        [-0.2899]], requires_grad=True)\n",
      "poly train loss at 3575: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3575: tensor([[ 1.0738],\n",
      "        [ 0.1635],\n",
      "        [-1.9018],\n",
      "        [ 1.2832],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3576: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3576: tensor([[ 1.0741],\n",
      "        [ 0.1646],\n",
      "        [-1.9000],\n",
      "        [ 1.2854],\n",
      "        [ 0.6934],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3577: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3577: tensor([[ 1.0738],\n",
      "        [ 0.1635],\n",
      "        [-1.9018],\n",
      "        [ 1.2832],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3578: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3578: tensor([[ 1.0741],\n",
      "        [ 0.1645],\n",
      "        [-1.9001],\n",
      "        [ 1.2854],\n",
      "        [ 0.6934],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3579: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3579: tensor([[ 1.0738],\n",
      "        [ 0.1635],\n",
      "        [-1.9018],\n",
      "        [ 1.2832],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3580: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3580: tensor([[ 1.0741],\n",
      "        [ 0.1645],\n",
      "        [-1.9001],\n",
      "        [ 1.2855],\n",
      "        [ 0.6934],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3581: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3581: tensor([[ 1.0738],\n",
      "        [ 0.1635],\n",
      "        [-1.9018],\n",
      "        [ 1.2833],\n",
      "        [ 0.6910],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3582: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3582: tensor([[ 1.0741],\n",
      "        [ 0.1645],\n",
      "        [-1.9001],\n",
      "        [ 1.2855],\n",
      "        [ 0.6934],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3583: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3583: tensor([[ 1.0738],\n",
      "        [ 0.1635],\n",
      "        [-1.9018],\n",
      "        [ 1.2833],\n",
      "        [ 0.6911],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3584: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3584: tensor([[ 1.0741],\n",
      "        [ 0.1645],\n",
      "        [-1.9001],\n",
      "        [ 1.2855],\n",
      "        [ 0.6934],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3585: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3585: tensor([[ 1.0738],\n",
      "        [ 0.1634],\n",
      "        [-1.9019],\n",
      "        [ 1.2833],\n",
      "        [ 0.6911],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3586: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3586: tensor([[ 1.0741],\n",
      "        [ 0.1645],\n",
      "        [-1.9001],\n",
      "        [ 1.2855],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3587: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3587: tensor([[ 1.0738],\n",
      "        [ 0.1634],\n",
      "        [-1.9019],\n",
      "        [ 1.2833],\n",
      "        [ 0.6911],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3588: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3588: tensor([[ 1.0741],\n",
      "        [ 0.1644],\n",
      "        [-1.9002],\n",
      "        [ 1.2855],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3589: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3589: tensor([[ 1.0738],\n",
      "        [ 0.1634],\n",
      "        [-1.9019],\n",
      "        [ 1.2833],\n",
      "        [ 0.6911],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3590: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3590: tensor([[ 1.0741],\n",
      "        [ 0.1644],\n",
      "        [-1.9002],\n",
      "        [ 1.2855],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3591: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3591: tensor([[ 1.0738],\n",
      "        [ 0.1634],\n",
      "        [-1.9019],\n",
      "        [ 1.2833],\n",
      "        [ 0.6911],\n",
      "        [-0.2924]], requires_grad=True)\n",
      "poly train loss at 3592: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3592: tensor([[ 1.0741],\n",
      "        [ 0.1644],\n",
      "        [-1.9002],\n",
      "        [ 1.2856],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3593: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3593: tensor([[ 1.0738],\n",
      "        [ 0.1634],\n",
      "        [-1.9020],\n",
      "        [ 1.2834],\n",
      "        [ 0.6911],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3594: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3594: tensor([[ 1.0741],\n",
      "        [ 0.1644],\n",
      "        [-1.9002],\n",
      "        [ 1.2856],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3595: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3595: tensor([[ 1.0738],\n",
      "        [ 0.1633],\n",
      "        [-1.9020],\n",
      "        [ 1.2834],\n",
      "        [ 0.6911],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3596: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3596: tensor([[ 1.0741],\n",
      "        [ 0.1643],\n",
      "        [-1.9003],\n",
      "        [ 1.2856],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3597: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3597: tensor([[ 1.0738],\n",
      "        [ 0.1633],\n",
      "        [-1.9020],\n",
      "        [ 1.2834],\n",
      "        [ 0.6911],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3598: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3598: tensor([[ 1.0741],\n",
      "        [ 0.1643],\n",
      "        [-1.9003],\n",
      "        [ 1.2856],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3599: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3599: tensor([[ 1.0738],\n",
      "        [ 0.1633],\n",
      "        [-1.9020],\n",
      "        [ 1.2834],\n",
      "        [ 0.6911],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3600: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3600: tensor([[ 1.0741],\n",
      "        [ 0.1643],\n",
      "        [-1.9003],\n",
      "        [ 1.2856],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3601: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3601: tensor([[ 1.0738],\n",
      "        [ 0.1633],\n",
      "        [-1.9020],\n",
      "        [ 1.2834],\n",
      "        [ 0.6911],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3602: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3602: tensor([[ 1.0741],\n",
      "        [ 0.1643],\n",
      "        [-1.9003],\n",
      "        [ 1.2856],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3603: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3603: tensor([[ 1.0738],\n",
      "        [ 0.1633],\n",
      "        [-1.9021],\n",
      "        [ 1.2834],\n",
      "        [ 0.6911],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3604: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3604: tensor([[ 1.0741],\n",
      "        [ 0.1643],\n",
      "        [-1.9003],\n",
      "        [ 1.2857],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3605: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3605: tensor([[ 1.0738],\n",
      "        [ 0.1632],\n",
      "        [-1.9021],\n",
      "        [ 1.2835],\n",
      "        [ 0.6911],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3606: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3606: tensor([[ 1.0741],\n",
      "        [ 0.1642],\n",
      "        [-1.9004],\n",
      "        [ 1.2857],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3607: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3607: tensor([[ 1.0738],\n",
      "        [ 0.1632],\n",
      "        [-1.9021],\n",
      "        [ 1.2835],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3608: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3608: tensor([[ 1.0741],\n",
      "        [ 0.1642],\n",
      "        [-1.9004],\n",
      "        [ 1.2857],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3609: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3609: tensor([[ 1.0738],\n",
      "        [ 0.1632],\n",
      "        [-1.9021],\n",
      "        [ 1.2835],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3610: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3610: tensor([[ 1.0741],\n",
      "        [ 0.1642],\n",
      "        [-1.9004],\n",
      "        [ 1.2857],\n",
      "        [ 0.6935],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3611: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3611: tensor([[ 1.0738],\n",
      "        [ 0.1632],\n",
      "        [-1.9021],\n",
      "        [ 1.2835],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3612: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3612: tensor([[ 1.0741],\n",
      "        [ 0.1642],\n",
      "        [-1.9004],\n",
      "        [ 1.2857],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3613: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3613: tensor([[ 1.0738],\n",
      "        [ 0.1632],\n",
      "        [-1.9022],\n",
      "        [ 1.2835],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3614: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3614: tensor([[ 1.0741],\n",
      "        [ 0.1642],\n",
      "        [-1.9005],\n",
      "        [ 1.2857],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3615: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3615: tensor([[ 1.0738],\n",
      "        [ 0.1631],\n",
      "        [-1.9022],\n",
      "        [ 1.2835],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3616: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3616: tensor([[ 1.0741],\n",
      "        [ 0.1641],\n",
      "        [-1.9005],\n",
      "        [ 1.2857],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3617: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3617: tensor([[ 1.0738],\n",
      "        [ 0.1631],\n",
      "        [-1.9022],\n",
      "        [ 1.2836],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3618: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3618: tensor([[ 1.0741],\n",
      "        [ 0.1641],\n",
      "        [-1.9005],\n",
      "        [ 1.2858],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3619: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3619: tensor([[ 1.0738],\n",
      "        [ 0.1631],\n",
      "        [-1.9022],\n",
      "        [ 1.2836],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3620: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3620: tensor([[ 1.0741],\n",
      "        [ 0.1641],\n",
      "        [-1.9005],\n",
      "        [ 1.2858],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3621: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3621: tensor([[ 1.0738],\n",
      "        [ 0.1631],\n",
      "        [-1.9023],\n",
      "        [ 1.2836],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3622: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3622: tensor([[ 1.0741],\n",
      "        [ 0.1641],\n",
      "        [-1.9005],\n",
      "        [ 1.2858],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3623: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3623: tensor([[ 1.0738],\n",
      "        [ 0.1631],\n",
      "        [-1.9023],\n",
      "        [ 1.2836],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3624: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3624: tensor([[ 1.0741],\n",
      "        [ 0.1641],\n",
      "        [-1.9006],\n",
      "        [ 1.2858],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3625: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3625: tensor([[ 1.0738],\n",
      "        [ 0.1630],\n",
      "        [-1.9023],\n",
      "        [ 1.2836],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3626: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3626: tensor([[ 1.0741],\n",
      "        [ 0.1640],\n",
      "        [-1.9006],\n",
      "        [ 1.2858],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3627: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3627: tensor([[ 1.0738],\n",
      "        [ 0.1630],\n",
      "        [-1.9023],\n",
      "        [ 1.2836],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3628: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3628: tensor([[ 1.0741],\n",
      "        [ 0.1640],\n",
      "        [-1.9006],\n",
      "        [ 1.2858],\n",
      "        [ 0.6936],\n",
      "        [-0.2900]], requires_grad=True)\n",
      "poly train loss at 3629: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3629: tensor([[ 1.0738],\n",
      "        [ 0.1630],\n",
      "        [-1.9023],\n",
      "        [ 1.2836],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3630: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3630: tensor([[ 1.0741],\n",
      "        [ 0.1640],\n",
      "        [-1.9006],\n",
      "        [ 1.2859],\n",
      "        [ 0.6936],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3631: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3631: tensor([[ 1.0738],\n",
      "        [ 0.1630],\n",
      "        [-1.9024],\n",
      "        [ 1.2837],\n",
      "        [ 0.6912],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3632: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3632: tensor([[ 1.0741],\n",
      "        [ 0.1640],\n",
      "        [-1.9006],\n",
      "        [ 1.2859],\n",
      "        [ 0.6936],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3633: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3633: tensor([[ 1.0738],\n",
      "        [ 0.1630],\n",
      "        [-1.9024],\n",
      "        [ 1.2837],\n",
      "        [ 0.6913],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3634: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3634: tensor([[ 1.0741],\n",
      "        [ 0.1640],\n",
      "        [-1.9007],\n",
      "        [ 1.2859],\n",
      "        [ 0.6936],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3635: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3635: tensor([[ 1.0738],\n",
      "        [ 0.1629],\n",
      "        [-1.9024],\n",
      "        [ 1.2837],\n",
      "        [ 0.6913],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3636: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3636: tensor([[ 1.0741],\n",
      "        [ 0.1639],\n",
      "        [-1.9007],\n",
      "        [ 1.2859],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3637: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3637: tensor([[ 1.0738],\n",
      "        [ 0.1629],\n",
      "        [-1.9024],\n",
      "        [ 1.2837],\n",
      "        [ 0.6913],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3638: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3638: tensor([[ 1.0741],\n",
      "        [ 0.1639],\n",
      "        [-1.9007],\n",
      "        [ 1.2859],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3639: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3639: tensor([[ 1.0738],\n",
      "        [ 0.1629],\n",
      "        [-1.9025],\n",
      "        [ 1.2837],\n",
      "        [ 0.6913],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3640: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3640: tensor([[ 1.0741],\n",
      "        [ 0.1639],\n",
      "        [-1.9007],\n",
      "        [ 1.2859],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3641: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3641: tensor([[ 1.0738],\n",
      "        [ 0.1629],\n",
      "        [-1.9025],\n",
      "        [ 1.2837],\n",
      "        [ 0.6913],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3642: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3642: tensor([[ 1.0741],\n",
      "        [ 0.1639],\n",
      "        [-1.9008],\n",
      "        [ 1.2860],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3643: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3643: tensor([[ 1.0738],\n",
      "        [ 0.1628],\n",
      "        [-1.9025],\n",
      "        [ 1.2838],\n",
      "        [ 0.6913],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3644: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3644: tensor([[ 1.0741],\n",
      "        [ 0.1639],\n",
      "        [-1.9008],\n",
      "        [ 1.2860],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3645: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3645: tensor([[ 1.0738],\n",
      "        [ 0.1628],\n",
      "        [-1.9025],\n",
      "        [ 1.2838],\n",
      "        [ 0.6913],\n",
      "        [-0.2925]], requires_grad=True)\n",
      "poly train loss at 3646: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3646: tensor([[ 1.0741],\n",
      "        [ 0.1638],\n",
      "        [-1.9008],\n",
      "        [ 1.2860],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3647: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3647: tensor([[ 1.0738],\n",
      "        [ 0.1628],\n",
      "        [-1.9025],\n",
      "        [ 1.2838],\n",
      "        [ 0.6913],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3648: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3648: tensor([[ 1.0741],\n",
      "        [ 0.1638],\n",
      "        [-1.9008],\n",
      "        [ 1.2860],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3649: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3649: tensor([[ 1.0738],\n",
      "        [ 0.1628],\n",
      "        [-1.9026],\n",
      "        [ 1.2838],\n",
      "        [ 0.6913],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3650: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3650: tensor([[ 1.0741],\n",
      "        [ 0.1638],\n",
      "        [-1.9008],\n",
      "        [ 1.2860],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3651: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3651: tensor([[ 1.0738],\n",
      "        [ 0.1628],\n",
      "        [-1.9026],\n",
      "        [ 1.2838],\n",
      "        [ 0.6913],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3652: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3652: tensor([[ 1.0741],\n",
      "        [ 0.1638],\n",
      "        [-1.9009],\n",
      "        [ 1.2860],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3653: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3653: tensor([[ 1.0738],\n",
      "        [ 0.1627],\n",
      "        [-1.9026],\n",
      "        [ 1.2838],\n",
      "        [ 0.6913],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3654: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3654: tensor([[ 1.0741],\n",
      "        [ 0.1638],\n",
      "        [-1.9009],\n",
      "        [ 1.2861],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3655: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3655: tensor([[ 1.0738],\n",
      "        [ 0.1627],\n",
      "        [-1.9026],\n",
      "        [ 1.2839],\n",
      "        [ 0.6913],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3656: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3656: tensor([[ 1.0741],\n",
      "        [ 0.1637],\n",
      "        [-1.9009],\n",
      "        [ 1.2861],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3657: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3657: tensor([[ 1.0738],\n",
      "        [ 0.1627],\n",
      "        [-1.9026],\n",
      "        [ 1.2839],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3658: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3658: tensor([[ 1.0741],\n",
      "        [ 0.1637],\n",
      "        [-1.9009],\n",
      "        [ 1.2861],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3659: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3659: tensor([[ 1.0738],\n",
      "        [ 0.1627],\n",
      "        [-1.9027],\n",
      "        [ 1.2839],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3660: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3660: tensor([[ 1.0741],\n",
      "        [ 0.1637],\n",
      "        [-1.9009],\n",
      "        [ 1.2861],\n",
      "        [ 0.6937],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3661: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3661: tensor([[ 1.0738],\n",
      "        [ 0.1627],\n",
      "        [-1.9027],\n",
      "        [ 1.2839],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3662: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3662: tensor([[ 1.0741],\n",
      "        [ 0.1637],\n",
      "        [-1.9010],\n",
      "        [ 1.2861],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3663: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3663: tensor([[ 1.0738],\n",
      "        [ 0.1626],\n",
      "        [-1.9027],\n",
      "        [ 1.2839],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3664: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3664: tensor([[ 1.0741],\n",
      "        [ 0.1637],\n",
      "        [-1.9010],\n",
      "        [ 1.2861],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3665: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3665: tensor([[ 1.0738],\n",
      "        [ 0.1626],\n",
      "        [-1.9027],\n",
      "        [ 1.2839],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3666: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3666: tensor([[ 1.0741],\n",
      "        [ 0.1636],\n",
      "        [-1.9010],\n",
      "        [ 1.2861],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3667: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3667: tensor([[ 1.0738],\n",
      "        [ 0.1626],\n",
      "        [-1.9028],\n",
      "        [ 1.2840],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3668: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3668: tensor([[ 1.0741],\n",
      "        [ 0.1636],\n",
      "        [-1.9010],\n",
      "        [ 1.2862],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3669: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3669: tensor([[ 1.0738],\n",
      "        [ 0.1626],\n",
      "        [-1.9028],\n",
      "        [ 1.2840],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3670: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3670: tensor([[ 1.0741],\n",
      "        [ 0.1636],\n",
      "        [-1.9011],\n",
      "        [ 1.2862],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3671: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3671: tensor([[ 1.0738],\n",
      "        [ 0.1626],\n",
      "        [-1.9028],\n",
      "        [ 1.2840],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3672: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3672: tensor([[ 1.0741],\n",
      "        [ 0.1636],\n",
      "        [-1.9011],\n",
      "        [ 1.2862],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3673: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3673: tensor([[ 1.0738],\n",
      "        [ 0.1625],\n",
      "        [-1.9028],\n",
      "        [ 1.2840],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3674: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3674: tensor([[ 1.0741],\n",
      "        [ 0.1636],\n",
      "        [-1.9011],\n",
      "        [ 1.2862],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3675: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3675: tensor([[ 1.0738],\n",
      "        [ 0.1625],\n",
      "        [-1.9028],\n",
      "        [ 1.2840],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3676: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3676: tensor([[ 1.0741],\n",
      "        [ 0.1635],\n",
      "        [-1.9011],\n",
      "        [ 1.2862],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3677: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3677: tensor([[ 1.0738],\n",
      "        [ 0.1625],\n",
      "        [-1.9029],\n",
      "        [ 1.2840],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3678: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3678: tensor([[ 1.0741],\n",
      "        [ 0.1635],\n",
      "        [-1.9011],\n",
      "        [ 1.2862],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3679: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3679: tensor([[ 1.0738],\n",
      "        [ 0.1625],\n",
      "        [-1.9029],\n",
      "        [ 1.2840],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3680: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3680: tensor([[ 1.0741],\n",
      "        [ 0.1635],\n",
      "        [-1.9012],\n",
      "        [ 1.2863],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3681: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3681: tensor([[ 1.0738],\n",
      "        [ 0.1625],\n",
      "        [-1.9029],\n",
      "        [ 1.2841],\n",
      "        [ 0.6914],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3682: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3682: tensor([[ 1.0741],\n",
      "        [ 0.1635],\n",
      "        [-1.9012],\n",
      "        [ 1.2863],\n",
      "        [ 0.6938],\n",
      "        [-0.2901]], requires_grad=True)\n",
      "poly train loss at 3683: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3683: tensor([[ 1.0738],\n",
      "        [ 0.1624],\n",
      "        [-1.9029],\n",
      "        [ 1.2841],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3684: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3684: tensor([[ 1.0741],\n",
      "        [ 0.1635],\n",
      "        [-1.9012],\n",
      "        [ 1.2863],\n",
      "        [ 0.6938],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3685: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3685: tensor([[ 1.0738],\n",
      "        [ 0.1624],\n",
      "        [-1.9030],\n",
      "        [ 1.2841],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3686: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3686: tensor([[ 1.0741],\n",
      "        [ 0.1634],\n",
      "        [-1.9012],\n",
      "        [ 1.2863],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3687: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3687: tensor([[ 1.0738],\n",
      "        [ 0.1624],\n",
      "        [-1.9030],\n",
      "        [ 1.2841],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3688: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3688: tensor([[ 1.0741],\n",
      "        [ 0.1634],\n",
      "        [-1.9013],\n",
      "        [ 1.2863],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3689: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3689: tensor([[ 1.0738],\n",
      "        [ 0.1624],\n",
      "        [-1.9030],\n",
      "        [ 1.2841],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3690: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3690: tensor([[ 1.0741],\n",
      "        [ 0.1634],\n",
      "        [-1.9013],\n",
      "        [ 1.2863],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3691: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3691: tensor([[ 1.0738],\n",
      "        [ 0.1624],\n",
      "        [-1.9030],\n",
      "        [ 1.2841],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3692: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3692: tensor([[ 1.0741],\n",
      "        [ 0.1634],\n",
      "        [-1.9013],\n",
      "        [ 1.2864],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3693: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3693: tensor([[ 1.0738],\n",
      "        [ 0.1623],\n",
      "        [-1.9030],\n",
      "        [ 1.2842],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3694: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3694: tensor([[ 1.0741],\n",
      "        [ 0.1634],\n",
      "        [-1.9013],\n",
      "        [ 1.2864],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3695: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3695: tensor([[ 1.0738],\n",
      "        [ 0.1623],\n",
      "        [-1.9031],\n",
      "        [ 1.2842],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3696: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3696: tensor([[ 1.0741],\n",
      "        [ 0.1633],\n",
      "        [-1.9013],\n",
      "        [ 1.2864],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3697: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3697: tensor([[ 1.0738],\n",
      "        [ 0.1623],\n",
      "        [-1.9031],\n",
      "        [ 1.2842],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3698: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3698: tensor([[ 1.0741],\n",
      "        [ 0.1633],\n",
      "        [-1.9014],\n",
      "        [ 1.2864],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3699: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3699: tensor([[ 1.0738],\n",
      "        [ 0.1623],\n",
      "        [-1.9031],\n",
      "        [ 1.2842],\n",
      "        [ 0.6915],\n",
      "        [-0.2926]], requires_grad=True)\n",
      "poly train loss at 3700: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3700: tensor([[ 1.0741],\n",
      "        [ 0.1633],\n",
      "        [-1.9014],\n",
      "        [ 1.2864],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3701: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3701: tensor([[ 1.0738],\n",
      "        [ 0.1623],\n",
      "        [-1.9031],\n",
      "        [ 1.2842],\n",
      "        [ 0.6915],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3702: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3702: tensor([[ 1.0741],\n",
      "        [ 0.1633],\n",
      "        [-1.9014],\n",
      "        [ 1.2864],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3703: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3703: tensor([[ 1.0738],\n",
      "        [ 0.1622],\n",
      "        [-1.9031],\n",
      "        [ 1.2842],\n",
      "        [ 0.6915],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3704: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3704: tensor([[ 1.0741],\n",
      "        [ 0.1633],\n",
      "        [-1.9014],\n",
      "        [ 1.2864],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3705: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3705: tensor([[ 1.0738],\n",
      "        [ 0.1622],\n",
      "        [-1.9032],\n",
      "        [ 1.2843],\n",
      "        [ 0.6915],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3706: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3706: tensor([[ 1.0741],\n",
      "        [ 0.1632],\n",
      "        [-1.9014],\n",
      "        [ 1.2865],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3707: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3707: tensor([[ 1.0738],\n",
      "        [ 0.1622],\n",
      "        [-1.9032],\n",
      "        [ 1.2843],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3708: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3708: tensor([[ 1.0741],\n",
      "        [ 0.1632],\n",
      "        [-1.9015],\n",
      "        [ 1.2865],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3709: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3709: tensor([[ 1.0738],\n",
      "        [ 0.1622],\n",
      "        [-1.9032],\n",
      "        [ 1.2843],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3710: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3710: tensor([[ 1.0741],\n",
      "        [ 0.1632],\n",
      "        [-1.9015],\n",
      "        [ 1.2865],\n",
      "        [ 0.6939],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3711: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3711: tensor([[ 1.0738],\n",
      "        [ 0.1622],\n",
      "        [-1.9032],\n",
      "        [ 1.2843],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3712: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3712: tensor([[ 1.0741],\n",
      "        [ 0.1632],\n",
      "        [-1.9015],\n",
      "        [ 1.2865],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3713: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3713: tensor([[ 1.0738],\n",
      "        [ 0.1621],\n",
      "        [-1.9033],\n",
      "        [ 1.2843],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3714: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3714: tensor([[ 1.0741],\n",
      "        [ 0.1632],\n",
      "        [-1.9015],\n",
      "        [ 1.2865],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3715: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3715: tensor([[ 1.0738],\n",
      "        [ 0.1621],\n",
      "        [-1.9033],\n",
      "        [ 1.2843],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3716: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3716: tensor([[ 1.0741],\n",
      "        [ 0.1631],\n",
      "        [-1.9016],\n",
      "        [ 1.2865],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3717: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3717: tensor([[ 1.0738],\n",
      "        [ 0.1621],\n",
      "        [-1.9033],\n",
      "        [ 1.2843],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3718: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3718: tensor([[ 1.0741],\n",
      "        [ 0.1631],\n",
      "        [-1.9016],\n",
      "        [ 1.2866],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3719: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3719: tensor([[ 1.0738],\n",
      "        [ 0.1621],\n",
      "        [-1.9033],\n",
      "        [ 1.2844],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3720: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3720: tensor([[ 1.0741],\n",
      "        [ 0.1631],\n",
      "        [-1.9016],\n",
      "        [ 1.2866],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3721: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3721: tensor([[ 1.0738],\n",
      "        [ 0.1621],\n",
      "        [-1.9033],\n",
      "        [ 1.2844],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3722: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3722: tensor([[ 1.0741],\n",
      "        [ 0.1631],\n",
      "        [-1.9016],\n",
      "        [ 1.2866],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3723: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3723: tensor([[ 1.0738],\n",
      "        [ 0.1621],\n",
      "        [-1.9034],\n",
      "        [ 1.2844],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3724: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3724: tensor([[ 1.0741],\n",
      "        [ 0.1631],\n",
      "        [-1.9016],\n",
      "        [ 1.2866],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3725: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3725: tensor([[ 1.0738],\n",
      "        [ 0.1620],\n",
      "        [-1.9034],\n",
      "        [ 1.2844],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3726: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3726: tensor([[ 1.0741],\n",
      "        [ 0.1630],\n",
      "        [-1.9017],\n",
      "        [ 1.2866],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3727: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3727: tensor([[ 1.0738],\n",
      "        [ 0.1620],\n",
      "        [-1.9034],\n",
      "        [ 1.2844],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3728: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3728: tensor([[ 1.0741],\n",
      "        [ 0.1630],\n",
      "        [-1.9017],\n",
      "        [ 1.2866],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3729: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3729: tensor([[ 1.0738],\n",
      "        [ 0.1620],\n",
      "        [-1.9034],\n",
      "        [ 1.2844],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3730: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3730: tensor([[ 1.0741],\n",
      "        [ 0.1630],\n",
      "        [-1.9017],\n",
      "        [ 1.2866],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3731: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3731: tensor([[ 1.0738],\n",
      "        [ 0.1620],\n",
      "        [-1.9034],\n",
      "        [ 1.2845],\n",
      "        [ 0.6916],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3732: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3732: tensor([[ 1.0741],\n",
      "        [ 0.1630],\n",
      "        [-1.9017],\n",
      "        [ 1.2867],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3733: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3733: tensor([[ 1.0738],\n",
      "        [ 0.1620],\n",
      "        [-1.9035],\n",
      "        [ 1.2845],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3734: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3734: tensor([[ 1.0741],\n",
      "        [ 0.1630],\n",
      "        [-1.9018],\n",
      "        [ 1.2867],\n",
      "        [ 0.6940],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3735: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3735: tensor([[ 1.0738],\n",
      "        [ 0.1619],\n",
      "        [-1.9035],\n",
      "        [ 1.2845],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3736: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3736: tensor([[ 1.0741],\n",
      "        [ 0.1629],\n",
      "        [-1.9018],\n",
      "        [ 1.2867],\n",
      "        [ 0.6941],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3737: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3737: tensor([[ 1.0738],\n",
      "        [ 0.1619],\n",
      "        [-1.9035],\n",
      "        [ 1.2845],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3738: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3738: tensor([[ 1.0741],\n",
      "        [ 0.1629],\n",
      "        [-1.9018],\n",
      "        [ 1.2867],\n",
      "        [ 0.6941],\n",
      "        [-0.2902]], requires_grad=True)\n",
      "poly train loss at 3739: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3739: tensor([[ 1.0738],\n",
      "        [ 0.1619],\n",
      "        [-1.9035],\n",
      "        [ 1.2845],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3740: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3740: tensor([[ 1.0741],\n",
      "        [ 0.1629],\n",
      "        [-1.9018],\n",
      "        [ 1.2867],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3741: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3741: tensor([[ 1.0738],\n",
      "        [ 0.1619],\n",
      "        [-1.9036],\n",
      "        [ 1.2845],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3742: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3742: tensor([[ 1.0741],\n",
      "        [ 0.1629],\n",
      "        [-1.9018],\n",
      "        [ 1.2867],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3743: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3743: tensor([[ 1.0738],\n",
      "        [ 0.1619],\n",
      "        [-1.9036],\n",
      "        [ 1.2845],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3744: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3744: tensor([[ 1.0741],\n",
      "        [ 0.1629],\n",
      "        [-1.9019],\n",
      "        [ 1.2868],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3745: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3745: tensor([[ 1.0738],\n",
      "        [ 0.1618],\n",
      "        [-1.9036],\n",
      "        [ 1.2846],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3746: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3746: tensor([[ 1.0741],\n",
      "        [ 0.1628],\n",
      "        [-1.9019],\n",
      "        [ 1.2868],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3747: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3747: tensor([[ 1.0738],\n",
      "        [ 0.1618],\n",
      "        [-1.9036],\n",
      "        [ 1.2846],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3748: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3748: tensor([[ 1.0741],\n",
      "        [ 0.1628],\n",
      "        [-1.9019],\n",
      "        [ 1.2868],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3749: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3749: tensor([[ 1.0738],\n",
      "        [ 0.1618],\n",
      "        [-1.9036],\n",
      "        [ 1.2846],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3750: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3750: tensor([[ 1.0741],\n",
      "        [ 0.1628],\n",
      "        [-1.9019],\n",
      "        [ 1.2868],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3751: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3751: tensor([[ 1.0738],\n",
      "        [ 0.1618],\n",
      "        [-1.9037],\n",
      "        [ 1.2846],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3752: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3752: tensor([[ 1.0741],\n",
      "        [ 0.1628],\n",
      "        [-1.9019],\n",
      "        [ 1.2868],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3753: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3753: tensor([[ 1.0738],\n",
      "        [ 0.1618],\n",
      "        [-1.9037],\n",
      "        [ 1.2846],\n",
      "        [ 0.6917],\n",
      "        [-0.2927]], requires_grad=True)\n",
      "poly train loss at 3754: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3754: tensor([[ 1.0741],\n",
      "        [ 0.1628],\n",
      "        [-1.9020],\n",
      "        [ 1.2868],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3755: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3755: tensor([[ 1.0738],\n",
      "        [ 0.1617],\n",
      "        [-1.9037],\n",
      "        [ 1.2846],\n",
      "        [ 0.6917],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3756: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3756: tensor([[ 1.0741],\n",
      "        [ 0.1627],\n",
      "        [-1.9020],\n",
      "        [ 1.2869],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3757: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3757: tensor([[ 1.0738],\n",
      "        [ 0.1617],\n",
      "        [-1.9037],\n",
      "        [ 1.2847],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3758: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3758: tensor([[ 1.0741],\n",
      "        [ 0.1627],\n",
      "        [-1.9020],\n",
      "        [ 1.2869],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3759: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3759: tensor([[ 1.0738],\n",
      "        [ 0.1617],\n",
      "        [-1.9038],\n",
      "        [ 1.2847],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3760: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3760: tensor([[ 1.0741],\n",
      "        [ 0.1627],\n",
      "        [-1.9020],\n",
      "        [ 1.2869],\n",
      "        [ 0.6941],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3761: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3761: tensor([[ 1.0738],\n",
      "        [ 0.1617],\n",
      "        [-1.9038],\n",
      "        [ 1.2847],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3762: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3762: tensor([[ 1.0741],\n",
      "        [ 0.1627],\n",
      "        [-1.9021],\n",
      "        [ 1.2869],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3763: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3763: tensor([[ 1.0738],\n",
      "        [ 0.1617],\n",
      "        [-1.9038],\n",
      "        [ 1.2847],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3764: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3764: tensor([[ 1.0741],\n",
      "        [ 0.1627],\n",
      "        [-1.9021],\n",
      "        [ 1.2869],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3765: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3765: tensor([[ 1.0738],\n",
      "        [ 0.1616],\n",
      "        [-1.9038],\n",
      "        [ 1.2847],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3766: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3766: tensor([[ 1.0741],\n",
      "        [ 0.1626],\n",
      "        [-1.9021],\n",
      "        [ 1.2869],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3767: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3767: tensor([[ 1.0738],\n",
      "        [ 0.1616],\n",
      "        [-1.9038],\n",
      "        [ 1.2847],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3768: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3768: tensor([[ 1.0741],\n",
      "        [ 0.1626],\n",
      "        [-1.9021],\n",
      "        [ 1.2869],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3769: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3769: tensor([[ 1.0738],\n",
      "        [ 0.1616],\n",
      "        [-1.9039],\n",
      "        [ 1.2848],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3770: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3770: tensor([[ 1.0741],\n",
      "        [ 0.1626],\n",
      "        [-1.9021],\n",
      "        [ 1.2870],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3771: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3771: tensor([[ 1.0738],\n",
      "        [ 0.1616],\n",
      "        [-1.9039],\n",
      "        [ 1.2848],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3772: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3772: tensor([[ 1.0741],\n",
      "        [ 0.1626],\n",
      "        [-1.9022],\n",
      "        [ 1.2870],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3773: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3773: tensor([[ 1.0738],\n",
      "        [ 0.1616],\n",
      "        [-1.9039],\n",
      "        [ 1.2848],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3774: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3774: tensor([[ 1.0741],\n",
      "        [ 0.1626],\n",
      "        [-1.9022],\n",
      "        [ 1.2870],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3775: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3775: tensor([[ 1.0738],\n",
      "        [ 0.1615],\n",
      "        [-1.9039],\n",
      "        [ 1.2848],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3776: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3776: tensor([[ 1.0741],\n",
      "        [ 0.1625],\n",
      "        [-1.9022],\n",
      "        [ 1.2870],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3777: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3777: tensor([[ 1.0738],\n",
      "        [ 0.1615],\n",
      "        [-1.9039],\n",
      "        [ 1.2848],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3778: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3778: tensor([[ 1.0741],\n",
      "        [ 0.1625],\n",
      "        [-1.9022],\n",
      "        [ 1.2870],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3779: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3779: tensor([[ 1.0738],\n",
      "        [ 0.1615],\n",
      "        [-1.9040],\n",
      "        [ 1.2848],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3780: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3780: tensor([[ 1.0741],\n",
      "        [ 0.1625],\n",
      "        [-1.9022],\n",
      "        [ 1.2870],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3781: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3781: tensor([[ 1.0738],\n",
      "        [ 0.1615],\n",
      "        [-1.9040],\n",
      "        [ 1.2848],\n",
      "        [ 0.6918],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3782: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3782: tensor([[ 1.0741],\n",
      "        [ 0.1625],\n",
      "        [-1.9023],\n",
      "        [ 1.2871],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3783: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3783: tensor([[ 1.0738],\n",
      "        [ 0.1615],\n",
      "        [-1.9040],\n",
      "        [ 1.2849],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3784: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3784: tensor([[ 1.0741],\n",
      "        [ 0.1625],\n",
      "        [-1.9023],\n",
      "        [ 1.2871],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3785: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3785: tensor([[ 1.0738],\n",
      "        [ 0.1614],\n",
      "        [-1.9040],\n",
      "        [ 1.2849],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3786: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3786: tensor([[ 1.0741],\n",
      "        [ 0.1624],\n",
      "        [-1.9023],\n",
      "        [ 1.2871],\n",
      "        [ 0.6942],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3787: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3787: tensor([[ 1.0738],\n",
      "        [ 0.1614],\n",
      "        [-1.9041],\n",
      "        [ 1.2849],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3788: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3788: tensor([[ 1.0741],\n",
      "        [ 0.1624],\n",
      "        [-1.9023],\n",
      "        [ 1.2871],\n",
      "        [ 0.6943],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3789: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3789: tensor([[ 1.0738],\n",
      "        [ 0.1614],\n",
      "        [-1.9041],\n",
      "        [ 1.2849],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3790: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3790: tensor([[ 1.0741],\n",
      "        [ 0.1624],\n",
      "        [-1.9024],\n",
      "        [ 1.2871],\n",
      "        [ 0.6943],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3791: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3791: tensor([[ 1.0738],\n",
      "        [ 0.1614],\n",
      "        [-1.9041],\n",
      "        [ 1.2849],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3792: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3792: tensor([[ 1.0741],\n",
      "        [ 0.1624],\n",
      "        [-1.9024],\n",
      "        [ 1.2871],\n",
      "        [ 0.6943],\n",
      "        [-0.2903]], requires_grad=True)\n",
      "poly train loss at 3793: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3793: tensor([[ 1.0738],\n",
      "        [ 0.1614],\n",
      "        [-1.9041],\n",
      "        [ 1.2849],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3794: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3794: tensor([[ 1.0741],\n",
      "        [ 0.1624],\n",
      "        [-1.9024],\n",
      "        [ 1.2872],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3795: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3795: tensor([[ 1.0738],\n",
      "        [ 0.1613],\n",
      "        [-1.9041],\n",
      "        [ 1.2850],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3796: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3796: tensor([[ 1.0741],\n",
      "        [ 0.1623],\n",
      "        [-1.9024],\n",
      "        [ 1.2872],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3797: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3797: tensor([[ 1.0738],\n",
      "        [ 0.1613],\n",
      "        [-1.9042],\n",
      "        [ 1.2850],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3798: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3798: tensor([[ 1.0741],\n",
      "        [ 0.1623],\n",
      "        [-1.9024],\n",
      "        [ 1.2872],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3799: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3799: tensor([[ 1.0738],\n",
      "        [ 0.1613],\n",
      "        [-1.9042],\n",
      "        [ 1.2850],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3800: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3800: tensor([[ 1.0741],\n",
      "        [ 0.1623],\n",
      "        [-1.9025],\n",
      "        [ 1.2872],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3801: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3801: tensor([[ 1.0738],\n",
      "        [ 0.1613],\n",
      "        [-1.9042],\n",
      "        [ 1.2850],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3802: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3802: tensor([[ 1.0741],\n",
      "        [ 0.1623],\n",
      "        [-1.9025],\n",
      "        [ 1.2872],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3803: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3803: tensor([[ 1.0738],\n",
      "        [ 0.1613],\n",
      "        [-1.9042],\n",
      "        [ 1.2850],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3804: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3804: tensor([[ 1.0741],\n",
      "        [ 0.1623],\n",
      "        [-1.9025],\n",
      "        [ 1.2872],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3805: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3805: tensor([[ 1.0738],\n",
      "        [ 0.1612],\n",
      "        [-1.9042],\n",
      "        [ 1.2850],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3806: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3806: tensor([[ 1.0741],\n",
      "        [ 0.1622],\n",
      "        [-1.9025],\n",
      "        [ 1.2872],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3807: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3807: tensor([[ 1.0738],\n",
      "        [ 0.1612],\n",
      "        [-1.9043],\n",
      "        [ 1.2850],\n",
      "        [ 0.6919],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3808: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3808: tensor([[ 1.0741],\n",
      "        [ 0.1622],\n",
      "        [-1.9025],\n",
      "        [ 1.2873],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3809: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3809: tensor([[ 1.0738],\n",
      "        [ 0.1612],\n",
      "        [-1.9043],\n",
      "        [ 1.2851],\n",
      "        [ 0.6920],\n",
      "        [-0.2928]], requires_grad=True)\n",
      "poly train loss at 3810: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3810: tensor([[ 1.0741],\n",
      "        [ 0.1622],\n",
      "        [-1.9026],\n",
      "        [ 1.2873],\n",
      "        [ 0.6943],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3811: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3811: tensor([[ 1.0738],\n",
      "        [ 0.1612],\n",
      "        [-1.9043],\n",
      "        [ 1.2851],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3812: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3812: tensor([[ 1.0741],\n",
      "        [ 0.1622],\n",
      "        [-1.9026],\n",
      "        [ 1.2873],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3813: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3813: tensor([[ 1.0738],\n",
      "        [ 0.1612],\n",
      "        [-1.9043],\n",
      "        [ 1.2851],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3814: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3814: tensor([[ 1.0741],\n",
      "        [ 0.1622],\n",
      "        [-1.9026],\n",
      "        [ 1.2873],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3815: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3815: tensor([[ 1.0738],\n",
      "        [ 0.1611],\n",
      "        [-1.9044],\n",
      "        [ 1.2851],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3816: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3816: tensor([[ 1.0741],\n",
      "        [ 0.1622],\n",
      "        [-1.9026],\n",
      "        [ 1.2873],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3817: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3817: tensor([[ 1.0738],\n",
      "        [ 0.1611],\n",
      "        [-1.9044],\n",
      "        [ 1.2851],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3818: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3818: tensor([[ 1.0741],\n",
      "        [ 0.1621],\n",
      "        [-1.9027],\n",
      "        [ 1.2873],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3819: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3819: tensor([[ 1.0738],\n",
      "        [ 0.1611],\n",
      "        [-1.9044],\n",
      "        [ 1.2851],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3820: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3820: tensor([[ 1.0741],\n",
      "        [ 0.1621],\n",
      "        [-1.9027],\n",
      "        [ 1.2874],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3821: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3821: tensor([[ 1.0738],\n",
      "        [ 0.1611],\n",
      "        [-1.9044],\n",
      "        [ 1.2852],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3822: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3822: tensor([[ 1.0741],\n",
      "        [ 0.1621],\n",
      "        [-1.9027],\n",
      "        [ 1.2874],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3823: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3823: tensor([[ 1.0738],\n",
      "        [ 0.1611],\n",
      "        [-1.9044],\n",
      "        [ 1.2852],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3824: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3824: tensor([[ 1.0741],\n",
      "        [ 0.1621],\n",
      "        [-1.9027],\n",
      "        [ 1.2874],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3825: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3825: tensor([[ 1.0738],\n",
      "        [ 0.1610],\n",
      "        [-1.9045],\n",
      "        [ 1.2852],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3826: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3826: tensor([[ 1.0741],\n",
      "        [ 0.1621],\n",
      "        [-1.9027],\n",
      "        [ 1.2874],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3827: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3827: tensor([[ 1.0738],\n",
      "        [ 0.1610],\n",
      "        [-1.9045],\n",
      "        [ 1.2852],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3828: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3828: tensor([[ 1.0741],\n",
      "        [ 0.1620],\n",
      "        [-1.9028],\n",
      "        [ 1.2874],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3829: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3829: tensor([[ 1.0738],\n",
      "        [ 0.1610],\n",
      "        [-1.9045],\n",
      "        [ 1.2852],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3830: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3830: tensor([[ 1.0741],\n",
      "        [ 0.1620],\n",
      "        [-1.9028],\n",
      "        [ 1.2874],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3831: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3831: tensor([[ 1.0738],\n",
      "        [ 0.1610],\n",
      "        [-1.9045],\n",
      "        [ 1.2852],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3832: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3832: tensor([[ 1.0741],\n",
      "        [ 0.1620],\n",
      "        [-1.9028],\n",
      "        [ 1.2874],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3833: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3833: tensor([[ 1.0738],\n",
      "        [ 0.1610],\n",
      "        [-1.9045],\n",
      "        [ 1.2852],\n",
      "        [ 0.6920],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3834: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3834: tensor([[ 1.0741],\n",
      "        [ 0.1620],\n",
      "        [-1.9028],\n",
      "        [ 1.2875],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3835: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3835: tensor([[ 1.0738],\n",
      "        [ 0.1609],\n",
      "        [-1.9046],\n",
      "        [ 1.2853],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3836: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3836: tensor([[ 1.0741],\n",
      "        [ 0.1620],\n",
      "        [-1.9028],\n",
      "        [ 1.2875],\n",
      "        [ 0.6944],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3837: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3837: tensor([[ 1.0738],\n",
      "        [ 0.1609],\n",
      "        [-1.9046],\n",
      "        [ 1.2853],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3838: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3838: tensor([[ 1.0741],\n",
      "        [ 0.1619],\n",
      "        [-1.9029],\n",
      "        [ 1.2875],\n",
      "        [ 0.6945],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3839: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3839: tensor([[ 1.0738],\n",
      "        [ 0.1609],\n",
      "        [-1.9046],\n",
      "        [ 1.2853],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3840: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3840: tensor([[ 1.0741],\n",
      "        [ 0.1619],\n",
      "        [-1.9029],\n",
      "        [ 1.2875],\n",
      "        [ 0.6945],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3841: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3841: tensor([[ 1.0738],\n",
      "        [ 0.1609],\n",
      "        [-1.9046],\n",
      "        [ 1.2853],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3842: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3842: tensor([[ 1.0741],\n",
      "        [ 0.1619],\n",
      "        [-1.9029],\n",
      "        [ 1.2875],\n",
      "        [ 0.6945],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3843: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3843: tensor([[ 1.0738],\n",
      "        [ 0.1609],\n",
      "        [-1.9046],\n",
      "        [ 1.2853],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3844: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3844: tensor([[ 1.0741],\n",
      "        [ 0.1619],\n",
      "        [-1.9029],\n",
      "        [ 1.2875],\n",
      "        [ 0.6945],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3845: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3845: tensor([[ 1.0738],\n",
      "        [ 0.1608],\n",
      "        [-1.9047],\n",
      "        [ 1.2853],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3846: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3846: tensor([[ 1.0741],\n",
      "        [ 0.1619],\n",
      "        [-1.9029],\n",
      "        [ 1.2875],\n",
      "        [ 0.6945],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3847: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3847: tensor([[ 1.0738],\n",
      "        [ 0.1608],\n",
      "        [-1.9047],\n",
      "        [ 1.2854],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3848: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3848: tensor([[ 1.0741],\n",
      "        [ 0.1618],\n",
      "        [-1.9030],\n",
      "        [ 1.2876],\n",
      "        [ 0.6945],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3849: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3849: tensor([[ 1.0738],\n",
      "        [ 0.1608],\n",
      "        [-1.9047],\n",
      "        [ 1.2854],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3850: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3850: tensor([[ 1.0741],\n",
      "        [ 0.1618],\n",
      "        [-1.9030],\n",
      "        [ 1.2876],\n",
      "        [ 0.6945],\n",
      "        [-0.2904]], requires_grad=True)\n",
      "poly train loss at 3851: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3851: tensor([[ 1.0738],\n",
      "        [ 0.1608],\n",
      "        [-1.9047],\n",
      "        [ 1.2854],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3852: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3852: tensor([[ 1.0741],\n",
      "        [ 0.1618],\n",
      "        [-1.9030],\n",
      "        [ 1.2876],\n",
      "        [ 0.6945],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3853: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3853: tensor([[ 1.0738],\n",
      "        [ 0.1608],\n",
      "        [-1.9048],\n",
      "        [ 1.2854],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3854: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3854: tensor([[ 1.0741],\n",
      "        [ 0.1618],\n",
      "        [-1.9030],\n",
      "        [ 1.2876],\n",
      "        [ 0.6945],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3855: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3855: tensor([[ 1.0738],\n",
      "        [ 0.1607],\n",
      "        [-1.9048],\n",
      "        [ 1.2854],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3856: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3856: tensor([[ 1.0741],\n",
      "        [ 0.1618],\n",
      "        [-1.9031],\n",
      "        [ 1.2876],\n",
      "        [ 0.6945],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3857: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3857: tensor([[ 1.0738],\n",
      "        [ 0.1607],\n",
      "        [-1.9048],\n",
      "        [ 1.2854],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3858: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3858: tensor([[ 1.0741],\n",
      "        [ 0.1617],\n",
      "        [-1.9031],\n",
      "        [ 1.2876],\n",
      "        [ 0.6945],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3859: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3859: tensor([[ 1.0738],\n",
      "        [ 0.1607],\n",
      "        [-1.9048],\n",
      "        [ 1.2854],\n",
      "        [ 0.6921],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3860: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3860: tensor([[ 1.0741],\n",
      "        [ 0.1617],\n",
      "        [-1.9031],\n",
      "        [ 1.2877],\n",
      "        [ 0.6945],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3861: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3861: tensor([[ 1.0738],\n",
      "        [ 0.1607],\n",
      "        [-1.9048],\n",
      "        [ 1.2855],\n",
      "        [ 0.6922],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3862: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3862: tensor([[ 1.0741],\n",
      "        [ 0.1617],\n",
      "        [-1.9031],\n",
      "        [ 1.2877],\n",
      "        [ 0.6945],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3863: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3863: tensor([[ 1.0738],\n",
      "        [ 0.1607],\n",
      "        [-1.9049],\n",
      "        [ 1.2855],\n",
      "        [ 0.6922],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3864: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3864: tensor([[ 1.0741],\n",
      "        [ 0.1617],\n",
      "        [-1.9031],\n",
      "        [ 1.2877],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3865: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3865: tensor([[ 1.0738],\n",
      "        [ 0.1607],\n",
      "        [-1.9049],\n",
      "        [ 1.2855],\n",
      "        [ 0.6922],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3866: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3866: tensor([[ 1.0741],\n",
      "        [ 0.1617],\n",
      "        [-1.9032],\n",
      "        [ 1.2877],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3867: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3867: tensor([[ 1.0738],\n",
      "        [ 0.1606],\n",
      "        [-1.9049],\n",
      "        [ 1.2855],\n",
      "        [ 0.6922],\n",
      "        [-0.2929]], requires_grad=True)\n",
      "poly train loss at 3868: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3868: tensor([[ 1.0741],\n",
      "        [ 0.1616],\n",
      "        [-1.9032],\n",
      "        [ 1.2877],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3869: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3869: tensor([[ 1.0738],\n",
      "        [ 0.1606],\n",
      "        [-1.9049],\n",
      "        [ 1.2855],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3870: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3870: tensor([[ 1.0741],\n",
      "        [ 0.1616],\n",
      "        [-1.9032],\n",
      "        [ 1.2877],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3871: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3871: tensor([[ 1.0738],\n",
      "        [ 0.1606],\n",
      "        [-1.9049],\n",
      "        [ 1.2855],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3872: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3872: tensor([[ 1.0741],\n",
      "        [ 0.1616],\n",
      "        [-1.9032],\n",
      "        [ 1.2877],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3873: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3873: tensor([[ 1.0738],\n",
      "        [ 0.1606],\n",
      "        [-1.9050],\n",
      "        [ 1.2856],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3874: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3874: tensor([[ 1.0741],\n",
      "        [ 0.1616],\n",
      "        [-1.9032],\n",
      "        [ 1.2878],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3875: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3875: tensor([[ 1.0738],\n",
      "        [ 0.1606],\n",
      "        [-1.9050],\n",
      "        [ 1.2856],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3876: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3876: tensor([[ 1.0741],\n",
      "        [ 0.1616],\n",
      "        [-1.9033],\n",
      "        [ 1.2878],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3877: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3877: tensor([[ 1.0738],\n",
      "        [ 0.1605],\n",
      "        [-1.9050],\n",
      "        [ 1.2856],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3878: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3878: tensor([[ 1.0741],\n",
      "        [ 0.1615],\n",
      "        [-1.9033],\n",
      "        [ 1.2878],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3879: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3879: tensor([[ 1.0738],\n",
      "        [ 0.1605],\n",
      "        [-1.9050],\n",
      "        [ 1.2856],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3880: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3880: tensor([[ 1.0741],\n",
      "        [ 0.1615],\n",
      "        [-1.9033],\n",
      "        [ 1.2878],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3881: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3881: tensor([[ 1.0738],\n",
      "        [ 0.1605],\n",
      "        [-1.9050],\n",
      "        [ 1.2856],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3882: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3882: tensor([[ 1.0741],\n",
      "        [ 0.1615],\n",
      "        [-1.9033],\n",
      "        [ 1.2878],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3883: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3883: tensor([[ 1.0738],\n",
      "        [ 0.1605],\n",
      "        [-1.9051],\n",
      "        [ 1.2856],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3884: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3884: tensor([[ 1.0741],\n",
      "        [ 0.1615],\n",
      "        [-1.9033],\n",
      "        [ 1.2878],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3885: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3885: tensor([[ 1.0738],\n",
      "        [ 0.1605],\n",
      "        [-1.9051],\n",
      "        [ 1.2856],\n",
      "        [ 0.6922],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3886: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3886: tensor([[ 1.0741],\n",
      "        [ 0.1615],\n",
      "        [-1.9034],\n",
      "        [ 1.2879],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3887: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3887: tensor([[ 1.0738],\n",
      "        [ 0.1604],\n",
      "        [-1.9051],\n",
      "        [ 1.2857],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3888: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3888: tensor([[ 1.0741],\n",
      "        [ 0.1615],\n",
      "        [-1.9034],\n",
      "        [ 1.2879],\n",
      "        [ 0.6946],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3889: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3889: tensor([[ 1.0738],\n",
      "        [ 0.1604],\n",
      "        [-1.9051],\n",
      "        [ 1.2857],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3890: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3890: tensor([[ 1.0741],\n",
      "        [ 0.1614],\n",
      "        [-1.9034],\n",
      "        [ 1.2879],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3891: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3891: tensor([[ 1.0738],\n",
      "        [ 0.1604],\n",
      "        [-1.9051],\n",
      "        [ 1.2857],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3892: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3892: tensor([[ 1.0741],\n",
      "        [ 0.1614],\n",
      "        [-1.9034],\n",
      "        [ 1.2879],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3893: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3893: tensor([[ 1.0738],\n",
      "        [ 0.1604],\n",
      "        [-1.9052],\n",
      "        [ 1.2857],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3894: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3894: tensor([[ 1.0741],\n",
      "        [ 0.1614],\n",
      "        [-1.9034],\n",
      "        [ 1.2879],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3895: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3895: tensor([[ 1.0738],\n",
      "        [ 0.1604],\n",
      "        [-1.9052],\n",
      "        [ 1.2857],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3896: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3896: tensor([[ 1.0741],\n",
      "        [ 0.1614],\n",
      "        [-1.9035],\n",
      "        [ 1.2879],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3897: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3897: tensor([[ 1.0738],\n",
      "        [ 0.1603],\n",
      "        [-1.9052],\n",
      "        [ 1.2857],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3898: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3898: tensor([[ 1.0741],\n",
      "        [ 0.1614],\n",
      "        [-1.9035],\n",
      "        [ 1.2879],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3899: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3899: tensor([[ 1.0738],\n",
      "        [ 0.1603],\n",
      "        [-1.9052],\n",
      "        [ 1.2857],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3900: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3900: tensor([[ 1.0741],\n",
      "        [ 0.1613],\n",
      "        [-1.9035],\n",
      "        [ 1.2880],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3901: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3901: tensor([[ 1.0738],\n",
      "        [ 0.1603],\n",
      "        [-1.9052],\n",
      "        [ 1.2858],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3902: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3902: tensor([[ 1.0741],\n",
      "        [ 0.1613],\n",
      "        [-1.9035],\n",
      "        [ 1.2880],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3903: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3903: tensor([[ 1.0738],\n",
      "        [ 0.1603],\n",
      "        [-1.9053],\n",
      "        [ 1.2858],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3904: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3904: tensor([[ 1.0741],\n",
      "        [ 0.1613],\n",
      "        [-1.9035],\n",
      "        [ 1.2880],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3905: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3905: tensor([[ 1.0738],\n",
      "        [ 0.1603],\n",
      "        [-1.9053],\n",
      "        [ 1.2858],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3906: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3906: tensor([[ 1.0741],\n",
      "        [ 0.1613],\n",
      "        [-1.9036],\n",
      "        [ 1.2880],\n",
      "        [ 0.6947],\n",
      "        [-0.2905]], requires_grad=True)\n",
      "poly train loss at 3907: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3907: tensor([[ 1.0738],\n",
      "        [ 0.1603],\n",
      "        [-1.9053],\n",
      "        [ 1.2858],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3908: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3908: tensor([[ 1.0741],\n",
      "        [ 0.1613],\n",
      "        [-1.9036],\n",
      "        [ 1.2880],\n",
      "        [ 0.6947],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3909: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3909: tensor([[ 1.0738],\n",
      "        [ 0.1602],\n",
      "        [-1.9053],\n",
      "        [ 1.2858],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3910: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3910: tensor([[ 1.0741],\n",
      "        [ 0.1612],\n",
      "        [-1.9036],\n",
      "        [ 1.2880],\n",
      "        [ 0.6947],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3911: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3911: tensor([[ 1.0738],\n",
      "        [ 0.1602],\n",
      "        [-1.9054],\n",
      "        [ 1.2858],\n",
      "        [ 0.6923],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3912: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3912: tensor([[ 1.0741],\n",
      "        [ 0.1612],\n",
      "        [-1.9036],\n",
      "        [ 1.2880],\n",
      "        [ 0.6947],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3913: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3913: tensor([[ 1.0738],\n",
      "        [ 0.1602],\n",
      "        [-1.9054],\n",
      "        [ 1.2859],\n",
      "        [ 0.6924],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3914: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3914: tensor([[ 1.0741],\n",
      "        [ 0.1612],\n",
      "        [-1.9036],\n",
      "        [ 1.2881],\n",
      "        [ 0.6947],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3915: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3915: tensor([[ 1.0738],\n",
      "        [ 0.1602],\n",
      "        [-1.9054],\n",
      "        [ 1.2859],\n",
      "        [ 0.6924],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3916: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3916: tensor([[ 1.0741],\n",
      "        [ 0.1612],\n",
      "        [-1.9037],\n",
      "        [ 1.2881],\n",
      "        [ 0.6947],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3917: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3917: tensor([[ 1.0738],\n",
      "        [ 0.1602],\n",
      "        [-1.9054],\n",
      "        [ 1.2859],\n",
      "        [ 0.6924],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3918: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3918: tensor([[ 1.0741],\n",
      "        [ 0.1612],\n",
      "        [-1.9037],\n",
      "        [ 1.2881],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3919: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3919: tensor([[ 1.0738],\n",
      "        [ 0.1601],\n",
      "        [-1.9054],\n",
      "        [ 1.2859],\n",
      "        [ 0.6924],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3920: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3920: tensor([[ 1.0741],\n",
      "        [ 0.1612],\n",
      "        [-1.9037],\n",
      "        [ 1.2881],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3921: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3921: tensor([[ 1.0738],\n",
      "        [ 0.1601],\n",
      "        [-1.9055],\n",
      "        [ 1.2859],\n",
      "        [ 0.6924],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3922: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3922: tensor([[ 1.0741],\n",
      "        [ 0.1611],\n",
      "        [-1.9037],\n",
      "        [ 1.2881],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3923: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3923: tensor([[ 1.0738],\n",
      "        [ 0.1601],\n",
      "        [-1.9055],\n",
      "        [ 1.2859],\n",
      "        [ 0.6924],\n",
      "        [-0.2930]], requires_grad=True)\n",
      "poly train loss at 3924: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3924: tensor([[ 1.0741],\n",
      "        [ 0.1611],\n",
      "        [-1.9038],\n",
      "        [ 1.2881],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3925: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3925: tensor([[ 1.0738],\n",
      "        [ 0.1601],\n",
      "        [-1.9055],\n",
      "        [ 1.2859],\n",
      "        [ 0.6924],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3926: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3926: tensor([[ 1.0741],\n",
      "        [ 0.1611],\n",
      "        [-1.9038],\n",
      "        [ 1.2882],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3927: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3927: tensor([[ 1.0738],\n",
      "        [ 0.1601],\n",
      "        [-1.9055],\n",
      "        [ 1.2860],\n",
      "        [ 0.6924],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3928: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3928: tensor([[ 1.0741],\n",
      "        [ 0.1611],\n",
      "        [-1.9038],\n",
      "        [ 1.2882],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3929: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3929: tensor([[ 1.0738],\n",
      "        [ 0.1600],\n",
      "        [-1.9055],\n",
      "        [ 1.2860],\n",
      "        [ 0.6924],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3930: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3930: tensor([[ 1.0741],\n",
      "        [ 0.1611],\n",
      "        [-1.9038],\n",
      "        [ 1.2882],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3931: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3931: tensor([[ 1.0738],\n",
      "        [ 0.1600],\n",
      "        [-1.9056],\n",
      "        [ 1.2860],\n",
      "        [ 0.6924],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3932: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3932: tensor([[ 1.0741],\n",
      "        [ 0.1610],\n",
      "        [-1.9038],\n",
      "        [ 1.2882],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3933: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3933: tensor([[ 1.0738],\n",
      "        [ 0.1600],\n",
      "        [-1.9056],\n",
      "        [ 1.2860],\n",
      "        [ 0.6924],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3934: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3934: tensor([[ 1.0741],\n",
      "        [ 0.1610],\n",
      "        [-1.9039],\n",
      "        [ 1.2882],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3935: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3935: tensor([[ 1.0738],\n",
      "        [ 0.1600],\n",
      "        [-1.9056],\n",
      "        [ 1.2860],\n",
      "        [ 0.6924],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3936: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3936: tensor([[ 1.0741],\n",
      "        [ 0.1610],\n",
      "        [-1.9039],\n",
      "        [ 1.2882],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3937: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3937: tensor([[ 1.0738],\n",
      "        [ 0.1600],\n",
      "        [-1.9056],\n",
      "        [ 1.2860],\n",
      "        [ 0.6924],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3938: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3938: tensor([[ 1.0741],\n",
      "        [ 0.1610],\n",
      "        [-1.9039],\n",
      "        [ 1.2882],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3939: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3939: tensor([[ 1.0738],\n",
      "        [ 0.1599],\n",
      "        [-1.9056],\n",
      "        [ 1.2861],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3940: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3940: tensor([[ 1.0741],\n",
      "        [ 0.1610],\n",
      "        [-1.9039],\n",
      "        [ 1.2883],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3941: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3941: tensor([[ 1.0738],\n",
      "        [ 0.1599],\n",
      "        [-1.9057],\n",
      "        [ 1.2861],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3942: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3942: tensor([[ 1.0741],\n",
      "        [ 0.1609],\n",
      "        [-1.9039],\n",
      "        [ 1.2883],\n",
      "        [ 0.6948],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3943: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3943: tensor([[ 1.0738],\n",
      "        [ 0.1599],\n",
      "        [-1.9057],\n",
      "        [ 1.2861],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3944: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3944: tensor([[ 1.0741],\n",
      "        [ 0.1609],\n",
      "        [-1.9040],\n",
      "        [ 1.2883],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3945: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3945: tensor([[ 1.0738],\n",
      "        [ 0.1599],\n",
      "        [-1.9057],\n",
      "        [ 1.2861],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3946: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3946: tensor([[ 1.0741],\n",
      "        [ 0.1609],\n",
      "        [-1.9040],\n",
      "        [ 1.2883],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3947: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3947: tensor([[ 1.0738],\n",
      "        [ 0.1599],\n",
      "        [-1.9057],\n",
      "        [ 1.2861],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3948: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3948: tensor([[ 1.0741],\n",
      "        [ 0.1609],\n",
      "        [-1.9040],\n",
      "        [ 1.2883],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3949: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3949: tensor([[ 1.0738],\n",
      "        [ 0.1599],\n",
      "        [-1.9057],\n",
      "        [ 1.2861],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3950: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3950: tensor([[ 1.0741],\n",
      "        [ 0.1609],\n",
      "        [-1.9040],\n",
      "        [ 1.2883],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3951: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3951: tensor([[ 1.0738],\n",
      "        [ 0.1598],\n",
      "        [-1.9058],\n",
      "        [ 1.2861],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3952: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3952: tensor([[ 1.0741],\n",
      "        [ 0.1608],\n",
      "        [-1.9040],\n",
      "        [ 1.2884],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3953: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3953: tensor([[ 1.0738],\n",
      "        [ 0.1598],\n",
      "        [-1.9058],\n",
      "        [ 1.2862],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3954: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3954: tensor([[ 1.0741],\n",
      "        [ 0.1608],\n",
      "        [-1.9041],\n",
      "        [ 1.2884],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3955: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3955: tensor([[ 1.0738],\n",
      "        [ 0.1598],\n",
      "        [-1.9058],\n",
      "        [ 1.2862],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3956: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3956: tensor([[ 1.0741],\n",
      "        [ 0.1608],\n",
      "        [-1.9041],\n",
      "        [ 1.2884],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3957: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3957: tensor([[ 1.0738],\n",
      "        [ 0.1598],\n",
      "        [-1.9058],\n",
      "        [ 1.2862],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3958: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3958: tensor([[ 1.0741],\n",
      "        [ 0.1608],\n",
      "        [-1.9041],\n",
      "        [ 1.2884],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3959: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3959: tensor([[ 1.0738],\n",
      "        [ 0.1598],\n",
      "        [-1.9058],\n",
      "        [ 1.2862],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3960: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3960: tensor([[ 1.0741],\n",
      "        [ 0.1608],\n",
      "        [-1.9041],\n",
      "        [ 1.2884],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3961: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3961: tensor([[ 1.0738],\n",
      "        [ 0.1597],\n",
      "        [-1.9059],\n",
      "        [ 1.2862],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3962: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3962: tensor([[ 1.0741],\n",
      "        [ 0.1607],\n",
      "        [-1.9041],\n",
      "        [ 1.2884],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3963: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3963: tensor([[ 1.0738],\n",
      "        [ 0.1597],\n",
      "        [-1.9059],\n",
      "        [ 1.2862],\n",
      "        [ 0.6925],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3964: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3964: tensor([[ 1.0741],\n",
      "        [ 0.1607],\n",
      "        [-1.9042],\n",
      "        [ 1.2884],\n",
      "        [ 0.6949],\n",
      "        [-0.2906]], requires_grad=True)\n",
      "poly train loss at 3965: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3965: tensor([[ 1.0738],\n",
      "        [ 0.1597],\n",
      "        [-1.9059],\n",
      "        [ 1.2862],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3966: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3966: tensor([[ 1.0741],\n",
      "        [ 0.1607],\n",
      "        [-1.9042],\n",
      "        [ 1.2885],\n",
      "        [ 0.6949],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3967: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3967: tensor([[ 1.0738],\n",
      "        [ 0.1597],\n",
      "        [-1.9059],\n",
      "        [ 1.2863],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3968: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3968: tensor([[ 1.0741],\n",
      "        [ 0.1607],\n",
      "        [-1.9042],\n",
      "        [ 1.2885],\n",
      "        [ 0.6949],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3969: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3969: tensor([[ 1.0738],\n",
      "        [ 0.1597],\n",
      "        [-1.9059],\n",
      "        [ 1.2863],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3970: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3970: tensor([[ 1.0741],\n",
      "        [ 0.1607],\n",
      "        [-1.9042],\n",
      "        [ 1.2885],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3971: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3971: tensor([[ 1.0738],\n",
      "        [ 0.1596],\n",
      "        [-1.9060],\n",
      "        [ 1.2863],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3972: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3972: tensor([[ 1.0741],\n",
      "        [ 0.1607],\n",
      "        [-1.9042],\n",
      "        [ 1.2885],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3973: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3973: tensor([[ 1.0738],\n",
      "        [ 0.1596],\n",
      "        [-1.9060],\n",
      "        [ 1.2863],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3974: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3974: tensor([[ 1.0741],\n",
      "        [ 0.1606],\n",
      "        [-1.9043],\n",
      "        [ 1.2885],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3975: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3975: tensor([[ 1.0738],\n",
      "        [ 0.1596],\n",
      "        [-1.9060],\n",
      "        [ 1.2863],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3976: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3976: tensor([[ 1.0741],\n",
      "        [ 0.1606],\n",
      "        [-1.9043],\n",
      "        [ 1.2885],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3977: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3977: tensor([[ 1.0738],\n",
      "        [ 0.1596],\n",
      "        [-1.9060],\n",
      "        [ 1.2863],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3978: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3978: tensor([[ 1.0741],\n",
      "        [ 0.1606],\n",
      "        [-1.9043],\n",
      "        [ 1.2886],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3979: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3979: tensor([[ 1.0738],\n",
      "        [ 0.1596],\n",
      "        [-1.9060],\n",
      "        [ 1.2864],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3980: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3980: tensor([[ 1.0741],\n",
      "        [ 0.1606],\n",
      "        [-1.9043],\n",
      "        [ 1.2886],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3981: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3981: tensor([[ 1.0738],\n",
      "        [ 0.1595],\n",
      "        [-1.9061],\n",
      "        [ 1.2864],\n",
      "        [ 0.6926],\n",
      "        [-0.2931]], requires_grad=True)\n",
      "poly train loss at 3982: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3982: tensor([[ 1.0741],\n",
      "        [ 0.1606],\n",
      "        [-1.9043],\n",
      "        [ 1.2886],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3983: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3983: tensor([[ 1.0738],\n",
      "        [ 0.1595],\n",
      "        [-1.9061],\n",
      "        [ 1.2864],\n",
      "        [ 0.6926],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3984: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3984: tensor([[ 1.0741],\n",
      "        [ 0.1605],\n",
      "        [-1.9044],\n",
      "        [ 1.2886],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3985: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3985: tensor([[ 1.0738],\n",
      "        [ 0.1595],\n",
      "        [-1.9061],\n",
      "        [ 1.2864],\n",
      "        [ 0.6926],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3986: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3986: tensor([[ 1.0741],\n",
      "        [ 0.1605],\n",
      "        [-1.9044],\n",
      "        [ 1.2886],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3987: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3987: tensor([[ 1.0738],\n",
      "        [ 0.1595],\n",
      "        [-1.9061],\n",
      "        [ 1.2864],\n",
      "        [ 0.6926],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3988: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3988: tensor([[ 1.0741],\n",
      "        [ 0.1605],\n",
      "        [-1.9044],\n",
      "        [ 1.2886],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3989: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3989: tensor([[ 1.0738],\n",
      "        [ 0.1595],\n",
      "        [-1.9061],\n",
      "        [ 1.2864],\n",
      "        [ 0.6926],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3990: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3990: tensor([[ 1.0741],\n",
      "        [ 0.1605],\n",
      "        [-1.9044],\n",
      "        [ 1.2886],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3991: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3991: tensor([[ 1.0738],\n",
      "        [ 0.1595],\n",
      "        [-1.9062],\n",
      "        [ 1.2864],\n",
      "        [ 0.6926],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3992: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3992: tensor([[ 1.0741],\n",
      "        [ 0.1605],\n",
      "        [-1.9044],\n",
      "        [ 1.2887],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3993: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3993: tensor([[ 1.0738],\n",
      "        [ 0.1594],\n",
      "        [-1.9062],\n",
      "        [ 1.2865],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3994: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3994: tensor([[ 1.0741],\n",
      "        [ 0.1604],\n",
      "        [-1.9045],\n",
      "        [ 1.2887],\n",
      "        [ 0.6950],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3995: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3995: tensor([[ 1.0738],\n",
      "        [ 0.1594],\n",
      "        [-1.9062],\n",
      "        [ 1.2865],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3996: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3996: tensor([[ 1.0741],\n",
      "        [ 0.1604],\n",
      "        [-1.9045],\n",
      "        [ 1.2887],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3997: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3997: tensor([[ 1.0738],\n",
      "        [ 0.1594],\n",
      "        [-1.9062],\n",
      "        [ 1.2865],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 3998: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 3998: tensor([[ 1.0741],\n",
      "        [ 0.1604],\n",
      "        [-1.9045],\n",
      "        [ 1.2887],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 3999: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 3999: tensor([[ 1.0738],\n",
      "        [ 0.1594],\n",
      "        [-1.9062],\n",
      "        [ 1.2865],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4000: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4000: tensor([[ 1.0741],\n",
      "        [ 0.1604],\n",
      "        [-1.9045],\n",
      "        [ 1.2887],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4001: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4001: tensor([[ 1.0738],\n",
      "        [ 0.1594],\n",
      "        [-1.9063],\n",
      "        [ 1.2865],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4002: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4002: tensor([[ 1.0741],\n",
      "        [ 0.1604],\n",
      "        [-1.9045],\n",
      "        [ 1.2887],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4003: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4003: tensor([[ 1.0738],\n",
      "        [ 0.1593],\n",
      "        [-1.9063],\n",
      "        [ 1.2865],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4004: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4004: tensor([[ 1.0741],\n",
      "        [ 0.1603],\n",
      "        [-1.9046],\n",
      "        [ 1.2887],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4005: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4005: tensor([[ 1.0738],\n",
      "        [ 0.1593],\n",
      "        [-1.9063],\n",
      "        [ 1.2866],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4006: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4006: tensor([[ 1.0741],\n",
      "        [ 0.1603],\n",
      "        [-1.9046],\n",
      "        [ 1.2888],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4007: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4007: tensor([[ 1.0738],\n",
      "        [ 0.1593],\n",
      "        [-1.9063],\n",
      "        [ 1.2866],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4008: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4008: tensor([[ 1.0741],\n",
      "        [ 0.1603],\n",
      "        [-1.9046],\n",
      "        [ 1.2888],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4009: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4009: tensor([[ 1.0738],\n",
      "        [ 0.1593],\n",
      "        [-1.9063],\n",
      "        [ 1.2866],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4010: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4010: tensor([[ 1.0741],\n",
      "        [ 0.1603],\n",
      "        [-1.9046],\n",
      "        [ 1.2888],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4011: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4011: tensor([[ 1.0738],\n",
      "        [ 0.1593],\n",
      "        [-1.9064],\n",
      "        [ 1.2866],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4012: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4012: tensor([[ 1.0741],\n",
      "        [ 0.1603],\n",
      "        [-1.9046],\n",
      "        [ 1.2888],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4013: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4013: tensor([[ 1.0738],\n",
      "        [ 0.1592],\n",
      "        [-1.9064],\n",
      "        [ 1.2866],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4014: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4014: tensor([[ 1.0741],\n",
      "        [ 0.1603],\n",
      "        [-1.9047],\n",
      "        [ 1.2888],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4015: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4015: tensor([[ 1.0738],\n",
      "        [ 0.1592],\n",
      "        [-1.9064],\n",
      "        [ 1.2866],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4016: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4016: tensor([[ 1.0741],\n",
      "        [ 0.1602],\n",
      "        [-1.9047],\n",
      "        [ 1.2888],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4017: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4017: tensor([[ 1.0738],\n",
      "        [ 0.1592],\n",
      "        [-1.9064],\n",
      "        [ 1.2866],\n",
      "        [ 0.6927],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4018: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4018: tensor([[ 1.0741],\n",
      "        [ 0.1602],\n",
      "        [-1.9047],\n",
      "        [ 1.2888],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4019: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4019: tensor([[ 1.0738],\n",
      "        [ 0.1592],\n",
      "        [-1.9064],\n",
      "        [ 1.2867],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4020: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4020: tensor([[ 1.0741],\n",
      "        [ 0.1602],\n",
      "        [-1.9047],\n",
      "        [ 1.2889],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4021: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4021: tensor([[ 1.0738],\n",
      "        [ 0.1592],\n",
      "        [-1.9065],\n",
      "        [ 1.2867],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4022: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4022: tensor([[ 1.0741],\n",
      "        [ 0.1602],\n",
      "        [-1.9047],\n",
      "        [ 1.2889],\n",
      "        [ 0.6951],\n",
      "        [-0.2907]], requires_grad=True)\n",
      "poly train loss at 4023: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4023: tensor([[ 1.0738],\n",
      "        [ 0.1591],\n",
      "        [-1.9065],\n",
      "        [ 1.2867],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4024: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4024: tensor([[ 1.0741],\n",
      "        [ 0.1602],\n",
      "        [-1.9048],\n",
      "        [ 1.2889],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4025: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4025: tensor([[ 1.0738],\n",
      "        [ 0.1591],\n",
      "        [-1.9065],\n",
      "        [ 1.2867],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4026: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4026: tensor([[ 1.0741],\n",
      "        [ 0.1601],\n",
      "        [-1.9048],\n",
      "        [ 1.2889],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4027: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4027: tensor([[ 1.0738],\n",
      "        [ 0.1591],\n",
      "        [-1.9065],\n",
      "        [ 1.2867],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4028: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4028: tensor([[ 1.0741],\n",
      "        [ 0.1601],\n",
      "        [-1.9048],\n",
      "        [ 1.2889],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4029: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4029: tensor([[ 1.0738],\n",
      "        [ 0.1591],\n",
      "        [-1.9066],\n",
      "        [ 1.2867],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4030: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4030: tensor([[ 1.0741],\n",
      "        [ 0.1601],\n",
      "        [-1.9048],\n",
      "        [ 1.2889],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4031: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4031: tensor([[ 1.0738],\n",
      "        [ 0.1591],\n",
      "        [-1.9066],\n",
      "        [ 1.2867],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4032: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4032: tensor([[ 1.0741],\n",
      "        [ 0.1601],\n",
      "        [-1.9048],\n",
      "        [ 1.2890],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4033: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4033: tensor([[ 1.0738],\n",
      "        [ 0.1591],\n",
      "        [-1.9066],\n",
      "        [ 1.2868],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4034: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4034: tensor([[ 1.0741],\n",
      "        [ 0.1601],\n",
      "        [-1.9049],\n",
      "        [ 1.2890],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4035: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4035: tensor([[ 1.0738],\n",
      "        [ 0.1590],\n",
      "        [-1.9066],\n",
      "        [ 1.2868],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4036: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4036: tensor([[ 1.0741],\n",
      "        [ 0.1600],\n",
      "        [-1.9049],\n",
      "        [ 1.2890],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4037: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4037: tensor([[ 1.0738],\n",
      "        [ 0.1590],\n",
      "        [-1.9066],\n",
      "        [ 1.2868],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4038: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4038: tensor([[ 1.0741],\n",
      "        [ 0.1600],\n",
      "        [-1.9049],\n",
      "        [ 1.2890],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4039: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4039: tensor([[ 1.0738],\n",
      "        [ 0.1590],\n",
      "        [-1.9067],\n",
      "        [ 1.2868],\n",
      "        [ 0.6928],\n",
      "        [-0.2932]], requires_grad=True)\n",
      "poly train loss at 4040: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4040: tensor([[ 1.0741],\n",
      "        [ 0.1600],\n",
      "        [-1.9049],\n",
      "        [ 1.2890],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4041: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4041: tensor([[ 1.0738],\n",
      "        [ 0.1590],\n",
      "        [-1.9067],\n",
      "        [ 1.2868],\n",
      "        [ 0.6928],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4042: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4042: tensor([[ 1.0741],\n",
      "        [ 0.1600],\n",
      "        [-1.9049],\n",
      "        [ 1.2890],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4043: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4043: tensor([[ 1.0738],\n",
      "        [ 0.1590],\n",
      "        [-1.9067],\n",
      "        [ 1.2868],\n",
      "        [ 0.6928],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4044: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4044: tensor([[ 1.0741],\n",
      "        [ 0.1600],\n",
      "        [-1.9050],\n",
      "        [ 1.2890],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4045: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4045: tensor([[ 1.0738],\n",
      "        [ 0.1589],\n",
      "        [-1.9067],\n",
      "        [ 1.2868],\n",
      "        [ 0.6928],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4046: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4046: tensor([[ 1.0741],\n",
      "        [ 0.1600],\n",
      "        [-1.9050],\n",
      "        [ 1.2891],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4047: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4047: tensor([[ 1.0738],\n",
      "        [ 0.1589],\n",
      "        [-1.9067],\n",
      "        [ 1.2869],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4048: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4048: tensor([[ 1.0741],\n",
      "        [ 0.1599],\n",
      "        [-1.9050],\n",
      "        [ 1.2891],\n",
      "        [ 0.6952],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4049: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4049: tensor([[ 1.0738],\n",
      "        [ 0.1589],\n",
      "        [-1.9068],\n",
      "        [ 1.2869],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4050: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4050: tensor([[ 1.0741],\n",
      "        [ 0.1599],\n",
      "        [-1.9050],\n",
      "        [ 1.2891],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4051: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4051: tensor([[ 1.0738],\n",
      "        [ 0.1589],\n",
      "        [-1.9068],\n",
      "        [ 1.2869],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4052: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4052: tensor([[ 1.0741],\n",
      "        [ 0.1599],\n",
      "        [-1.9051],\n",
      "        [ 1.2891],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4053: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4053: tensor([[ 1.0738],\n",
      "        [ 0.1589],\n",
      "        [-1.9068],\n",
      "        [ 1.2869],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4054: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4054: tensor([[ 1.0741],\n",
      "        [ 0.1599],\n",
      "        [-1.9051],\n",
      "        [ 1.2891],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4055: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4055: tensor([[ 1.0738],\n",
      "        [ 0.1588],\n",
      "        [-1.9068],\n",
      "        [ 1.2869],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4056: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4056: tensor([[ 1.0741],\n",
      "        [ 0.1599],\n",
      "        [-1.9051],\n",
      "        [ 1.2891],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4057: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4057: tensor([[ 1.0738],\n",
      "        [ 0.1588],\n",
      "        [-1.9068],\n",
      "        [ 1.2869],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4058: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4058: tensor([[ 1.0741],\n",
      "        [ 0.1598],\n",
      "        [-1.9051],\n",
      "        [ 1.2891],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4059: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4059: tensor([[ 1.0738],\n",
      "        [ 0.1588],\n",
      "        [-1.9069],\n",
      "        [ 1.2869],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4060: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4060: tensor([[ 1.0741],\n",
      "        [ 0.1598],\n",
      "        [-1.9051],\n",
      "        [ 1.2892],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4061: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4061: tensor([[ 1.0738],\n",
      "        [ 0.1588],\n",
      "        [-1.9069],\n",
      "        [ 1.2870],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4062: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4062: tensor([[ 1.0741],\n",
      "        [ 0.1598],\n",
      "        [-1.9052],\n",
      "        [ 1.2892],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4063: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4063: tensor([[ 1.0738],\n",
      "        [ 0.1588],\n",
      "        [-1.9069],\n",
      "        [ 1.2870],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4064: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4064: tensor([[ 1.0741],\n",
      "        [ 0.1598],\n",
      "        [-1.9052],\n",
      "        [ 1.2892],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4065: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4065: tensor([[ 1.0738],\n",
      "        [ 0.1588],\n",
      "        [-1.9069],\n",
      "        [ 1.2870],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4066: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4066: tensor([[ 1.0741],\n",
      "        [ 0.1598],\n",
      "        [-1.9052],\n",
      "        [ 1.2892],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4067: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4067: tensor([[ 1.0738],\n",
      "        [ 0.1587],\n",
      "        [-1.9069],\n",
      "        [ 1.2870],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4068: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4068: tensor([[ 1.0741],\n",
      "        [ 0.1597],\n",
      "        [-1.9052],\n",
      "        [ 1.2892],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4069: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4069: tensor([[ 1.0738],\n",
      "        [ 0.1587],\n",
      "        [-1.9070],\n",
      "        [ 1.2870],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4070: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4070: tensor([[ 1.0741],\n",
      "        [ 0.1597],\n",
      "        [-1.9052],\n",
      "        [ 1.2892],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4071: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4071: tensor([[ 1.0738],\n",
      "        [ 0.1587],\n",
      "        [-1.9070],\n",
      "        [ 1.2870],\n",
      "        [ 0.6929],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4072: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4072: tensor([[ 1.0741],\n",
      "        [ 0.1597],\n",
      "        [-1.9052],\n",
      "        [ 1.2892],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4073: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4073: tensor([[ 1.0738],\n",
      "        [ 0.1587],\n",
      "        [-1.9070],\n",
      "        [ 1.2870],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4074: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4074: tensor([[ 1.0741],\n",
      "        [ 0.1597],\n",
      "        [-1.9053],\n",
      "        [ 1.2893],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4075: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4075: tensor([[ 1.0738],\n",
      "        [ 0.1587],\n",
      "        [-1.9070],\n",
      "        [ 1.2871],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4076: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4076: tensor([[ 1.0741],\n",
      "        [ 0.1597],\n",
      "        [-1.9053],\n",
      "        [ 1.2893],\n",
      "        [ 0.6953],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4077: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4077: tensor([[ 1.0738],\n",
      "        [ 0.1586],\n",
      "        [-1.9070],\n",
      "        [ 1.2871],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4078: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4078: tensor([[ 1.0741],\n",
      "        [ 0.1597],\n",
      "        [-1.9053],\n",
      "        [ 1.2893],\n",
      "        [ 0.6954],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4079: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4079: tensor([[ 1.0738],\n",
      "        [ 0.1586],\n",
      "        [-1.9071],\n",
      "        [ 1.2871],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4080: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4080: tensor([[ 1.0741],\n",
      "        [ 0.1596],\n",
      "        [-1.9053],\n",
      "        [ 1.2893],\n",
      "        [ 0.6954],\n",
      "        [-0.2908]], requires_grad=True)\n",
      "poly train loss at 4081: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4081: tensor([[ 1.0738],\n",
      "        [ 0.1586],\n",
      "        [-1.9071],\n",
      "        [ 1.2871],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4082: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4082: tensor([[ 1.0741],\n",
      "        [ 0.1596],\n",
      "        [-1.9053],\n",
      "        [ 1.2893],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4083: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4083: tensor([[ 1.0738],\n",
      "        [ 0.1586],\n",
      "        [-1.9071],\n",
      "        [ 1.2871],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4084: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4084: tensor([[ 1.0741],\n",
      "        [ 0.1596],\n",
      "        [-1.9054],\n",
      "        [ 1.2893],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4085: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4085: tensor([[ 1.0738],\n",
      "        [ 0.1586],\n",
      "        [-1.9071],\n",
      "        [ 1.2871],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4086: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4086: tensor([[ 1.0741],\n",
      "        [ 0.1596],\n",
      "        [-1.9054],\n",
      "        [ 1.2893],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4087: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4087: tensor([[ 1.0738],\n",
      "        [ 0.1585],\n",
      "        [-1.9071],\n",
      "        [ 1.2872],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4088: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4088: tensor([[ 1.0741],\n",
      "        [ 0.1596],\n",
      "        [-1.9054],\n",
      "        [ 1.2894],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4089: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4089: tensor([[ 1.0738],\n",
      "        [ 0.1585],\n",
      "        [-1.9072],\n",
      "        [ 1.2872],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4090: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4090: tensor([[ 1.0741],\n",
      "        [ 0.1595],\n",
      "        [-1.9054],\n",
      "        [ 1.2894],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4091: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4091: tensor([[ 1.0738],\n",
      "        [ 0.1585],\n",
      "        [-1.9072],\n",
      "        [ 1.2872],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4092: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4092: tensor([[ 1.0741],\n",
      "        [ 0.1595],\n",
      "        [-1.9054],\n",
      "        [ 1.2894],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4093: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4093: tensor([[ 1.0738],\n",
      "        [ 0.1585],\n",
      "        [-1.9072],\n",
      "        [ 1.2872],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4094: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4094: tensor([[ 1.0741],\n",
      "        [ 0.1595],\n",
      "        [-1.9055],\n",
      "        [ 1.2894],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4095: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4095: tensor([[ 1.0738],\n",
      "        [ 0.1585],\n",
      "        [-1.9072],\n",
      "        [ 1.2872],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4096: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4096: tensor([[ 1.0741],\n",
      "        [ 0.1595],\n",
      "        [-1.9055],\n",
      "        [ 1.2894],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4097: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4097: tensor([[ 1.0738],\n",
      "        [ 0.1585],\n",
      "        [-1.9072],\n",
      "        [ 1.2872],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4098: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4098: tensor([[ 1.0741],\n",
      "        [ 0.1595],\n",
      "        [-1.9055],\n",
      "        [ 1.2894],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4099: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4099: tensor([[ 1.0738],\n",
      "        [ 0.1584],\n",
      "        [-1.9073],\n",
      "        [ 1.2872],\n",
      "        [ 0.6930],\n",
      "        [-0.2933]], requires_grad=True)\n",
      "poly train loss at 4100: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4100: tensor([[ 1.0741],\n",
      "        [ 0.1595],\n",
      "        [-1.9055],\n",
      "        [ 1.2894],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4101: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4101: tensor([[ 1.0738],\n",
      "        [ 0.1584],\n",
      "        [-1.9073],\n",
      "        [ 1.2873],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4102: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4102: tensor([[ 1.0741],\n",
      "        [ 0.1594],\n",
      "        [-1.9055],\n",
      "        [ 1.2895],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4103: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4103: tensor([[ 1.0738],\n",
      "        [ 0.1584],\n",
      "        [-1.9073],\n",
      "        [ 1.2873],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4104: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4104: tensor([[ 1.0741],\n",
      "        [ 0.1594],\n",
      "        [-1.9056],\n",
      "        [ 1.2895],\n",
      "        [ 0.6954],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4105: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4105: tensor([[ 1.0738],\n",
      "        [ 0.1584],\n",
      "        [-1.9073],\n",
      "        [ 1.2873],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4106: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4106: tensor([[ 1.0741],\n",
      "        [ 0.1594],\n",
      "        [-1.9056],\n",
      "        [ 1.2895],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4107: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4107: tensor([[ 1.0738],\n",
      "        [ 0.1584],\n",
      "        [-1.9073],\n",
      "        [ 1.2873],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4108: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4108: tensor([[ 1.0741],\n",
      "        [ 0.1594],\n",
      "        [-1.9056],\n",
      "        [ 1.2895],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4109: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4109: tensor([[ 1.0738],\n",
      "        [ 0.1583],\n",
      "        [-1.9074],\n",
      "        [ 1.2873],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4110: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4110: tensor([[ 1.0741],\n",
      "        [ 0.1594],\n",
      "        [-1.9056],\n",
      "        [ 1.2895],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4111: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4111: tensor([[ 1.0738],\n",
      "        [ 0.1583],\n",
      "        [-1.9074],\n",
      "        [ 1.2873],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4112: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4112: tensor([[ 1.0741],\n",
      "        [ 0.1593],\n",
      "        [-1.9056],\n",
      "        [ 1.2895],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4113: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4113: tensor([[ 1.0738],\n",
      "        [ 0.1583],\n",
      "        [-1.9074],\n",
      "        [ 1.2873],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4114: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4114: tensor([[ 1.0741],\n",
      "        [ 0.1593],\n",
      "        [-1.9057],\n",
      "        [ 1.2895],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4115: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4115: tensor([[ 1.0738],\n",
      "        [ 0.1583],\n",
      "        [-1.9074],\n",
      "        [ 1.2874],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4116: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4116: tensor([[ 1.0741],\n",
      "        [ 0.1593],\n",
      "        [-1.9057],\n",
      "        [ 1.2896],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4117: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4117: tensor([[ 1.0738],\n",
      "        [ 0.1583],\n",
      "        [-1.9074],\n",
      "        [ 1.2874],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4118: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4118: tensor([[ 1.0741],\n",
      "        [ 0.1593],\n",
      "        [-1.9057],\n",
      "        [ 1.2896],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4119: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4119: tensor([[ 1.0738],\n",
      "        [ 0.1583],\n",
      "        [-1.9075],\n",
      "        [ 1.2874],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4120: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4120: tensor([[ 1.0741],\n",
      "        [ 0.1593],\n",
      "        [-1.9057],\n",
      "        [ 1.2896],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4121: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4121: tensor([[ 1.0738],\n",
      "        [ 0.1582],\n",
      "        [-1.9075],\n",
      "        [ 1.2874],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4122: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4122: tensor([[ 1.0741],\n",
      "        [ 0.1592],\n",
      "        [-1.9057],\n",
      "        [ 1.2896],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4123: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4123: tensor([[ 1.0738],\n",
      "        [ 0.1582],\n",
      "        [-1.9075],\n",
      "        [ 1.2874],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4124: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4124: tensor([[ 1.0741],\n",
      "        [ 0.1592],\n",
      "        [-1.9058],\n",
      "        [ 1.2896],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4125: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4125: tensor([[ 1.0738],\n",
      "        [ 0.1582],\n",
      "        [-1.9075],\n",
      "        [ 1.2874],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4126: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4126: tensor([[ 1.0741],\n",
      "        [ 0.1592],\n",
      "        [-1.9058],\n",
      "        [ 1.2896],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4127: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4127: tensor([[ 1.0738],\n",
      "        [ 0.1582],\n",
      "        [-1.9075],\n",
      "        [ 1.2874],\n",
      "        [ 0.6931],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4128: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4128: tensor([[ 1.0741],\n",
      "        [ 0.1592],\n",
      "        [-1.9058],\n",
      "        [ 1.2896],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4129: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4129: tensor([[ 1.0738],\n",
      "        [ 0.1582],\n",
      "        [-1.9075],\n",
      "        [ 1.2875],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4130: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4130: tensor([[ 1.0741],\n",
      "        [ 0.1592],\n",
      "        [-1.9058],\n",
      "        [ 1.2897],\n",
      "        [ 0.6955],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4131: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4131: tensor([[ 1.0738],\n",
      "        [ 0.1581],\n",
      "        [-1.9076],\n",
      "        [ 1.2875],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4132: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4132: tensor([[ 1.0741],\n",
      "        [ 0.1592],\n",
      "        [-1.9058],\n",
      "        [ 1.2897],\n",
      "        [ 0.6956],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4133: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4133: tensor([[ 1.0738],\n",
      "        [ 0.1581],\n",
      "        [-1.9076],\n",
      "        [ 1.2875],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4134: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4134: tensor([[ 1.0741],\n",
      "        [ 0.1591],\n",
      "        [-1.9059],\n",
      "        [ 1.2897],\n",
      "        [ 0.6956],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4135: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4135: tensor([[ 1.0738],\n",
      "        [ 0.1581],\n",
      "        [-1.9076],\n",
      "        [ 1.2875],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4136: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4136: tensor([[ 1.0741],\n",
      "        [ 0.1591],\n",
      "        [-1.9059],\n",
      "        [ 1.2897],\n",
      "        [ 0.6956],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4137: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4137: tensor([[ 1.0738],\n",
      "        [ 0.1581],\n",
      "        [-1.9076],\n",
      "        [ 1.2875],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4138: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4138: tensor([[ 1.0741],\n",
      "        [ 0.1591],\n",
      "        [-1.9059],\n",
      "        [ 1.2897],\n",
      "        [ 0.6956],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4139: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4139: tensor([[ 1.0738],\n",
      "        [ 0.1581],\n",
      "        [-1.9076],\n",
      "        [ 1.2875],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4140: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4140: tensor([[ 1.0741],\n",
      "        [ 0.1591],\n",
      "        [-1.9059],\n",
      "        [ 1.2897],\n",
      "        [ 0.6956],\n",
      "        [-0.2909]], requires_grad=True)\n",
      "poly train loss at 4141: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4141: tensor([[ 1.0738],\n",
      "        [ 0.1581],\n",
      "        [-1.9077],\n",
      "        [ 1.2875],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4142: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4142: tensor([[ 1.0741],\n",
      "        [ 0.1591],\n",
      "        [-1.9059],\n",
      "        [ 1.2897],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4143: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4143: tensor([[ 1.0738],\n",
      "        [ 0.1580],\n",
      "        [-1.9077],\n",
      "        [ 1.2876],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4144: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4144: tensor([[ 1.0741],\n",
      "        [ 0.1590],\n",
      "        [-1.9060],\n",
      "        [ 1.2898],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4145: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4145: tensor([[ 1.0738],\n",
      "        [ 0.1580],\n",
      "        [-1.9077],\n",
      "        [ 1.2876],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4146: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4146: tensor([[ 1.0741],\n",
      "        [ 0.1590],\n",
      "        [-1.9060],\n",
      "        [ 1.2898],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4147: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4147: tensor([[ 1.0738],\n",
      "        [ 0.1580],\n",
      "        [-1.9077],\n",
      "        [ 1.2876],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4148: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4148: tensor([[ 1.0741],\n",
      "        [ 0.1590],\n",
      "        [-1.9060],\n",
      "        [ 1.2898],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4149: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4149: tensor([[ 1.0738],\n",
      "        [ 0.1580],\n",
      "        [-1.9077],\n",
      "        [ 1.2876],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4150: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4150: tensor([[ 1.0741],\n",
      "        [ 0.1590],\n",
      "        [-1.9060],\n",
      "        [ 1.2898],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4151: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4151: tensor([[ 1.0738],\n",
      "        [ 0.1580],\n",
      "        [-1.9078],\n",
      "        [ 1.2876],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4152: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4152: tensor([[ 1.0741],\n",
      "        [ 0.1590],\n",
      "        [-1.9060],\n",
      "        [ 1.2898],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4153: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4153: tensor([[ 1.0738],\n",
      "        [ 0.1579],\n",
      "        [-1.9078],\n",
      "        [ 1.2876],\n",
      "        [ 0.6932],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4154: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4154: tensor([[ 1.0742],\n",
      "        [ 0.1590],\n",
      "        [-1.9061],\n",
      "        [ 1.2898],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4155: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4155: tensor([[ 1.0738],\n",
      "        [ 0.1579],\n",
      "        [-1.9078],\n",
      "        [ 1.2876],\n",
      "        [ 0.6933],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4156: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4156: tensor([[ 1.0742],\n",
      "        [ 0.1589],\n",
      "        [-1.9061],\n",
      "        [ 1.2898],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4157: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4157: tensor([[ 1.0738],\n",
      "        [ 0.1579],\n",
      "        [-1.9078],\n",
      "        [ 1.2877],\n",
      "        [ 0.6933],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4158: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4158: tensor([[ 1.0742],\n",
      "        [ 0.1589],\n",
      "        [-1.9061],\n",
      "        [ 1.2899],\n",
      "        [ 0.6956],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4159: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4159: tensor([[ 1.0738],\n",
      "        [ 0.1579],\n",
      "        [-1.9078],\n",
      "        [ 1.2877],\n",
      "        [ 0.6933],\n",
      "        [-0.2934]], requires_grad=True)\n",
      "poly train loss at 4160: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4160: tensor([[ 1.0742],\n",
      "        [ 0.1589],\n",
      "        [-1.9061],\n",
      "        [ 1.2899],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4161: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4161: tensor([[ 1.0738],\n",
      "        [ 0.1579],\n",
      "        [-1.9079],\n",
      "        [ 1.2877],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4162: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4162: tensor([[ 1.0742],\n",
      "        [ 0.1589],\n",
      "        [-1.9061],\n",
      "        [ 1.2899],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4163: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4163: tensor([[ 1.0738],\n",
      "        [ 0.1579],\n",
      "        [-1.9079],\n",
      "        [ 1.2877],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4164: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4164: tensor([[ 1.0742],\n",
      "        [ 0.1589],\n",
      "        [-1.9062],\n",
      "        [ 1.2899],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4165: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4165: tensor([[ 1.0738],\n",
      "        [ 0.1578],\n",
      "        [-1.9079],\n",
      "        [ 1.2877],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4166: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4166: tensor([[ 1.0742],\n",
      "        [ 0.1588],\n",
      "        [-1.9062],\n",
      "        [ 1.2899],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4167: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4167: tensor([[ 1.0738],\n",
      "        [ 0.1578],\n",
      "        [-1.9079],\n",
      "        [ 1.2877],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4168: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4168: tensor([[ 1.0742],\n",
      "        [ 0.1588],\n",
      "        [-1.9062],\n",
      "        [ 1.2899],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4169: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4169: tensor([[ 1.0738],\n",
      "        [ 0.1578],\n",
      "        [-1.9079],\n",
      "        [ 1.2877],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4170: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4170: tensor([[ 1.0742],\n",
      "        [ 0.1588],\n",
      "        [-1.9062],\n",
      "        [ 1.2899],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4171: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4171: tensor([[ 1.0738],\n",
      "        [ 0.1578],\n",
      "        [-1.9080],\n",
      "        [ 1.2878],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4172: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4172: tensor([[ 1.0742],\n",
      "        [ 0.1588],\n",
      "        [-1.9062],\n",
      "        [ 1.2900],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4173: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4173: tensor([[ 1.0738],\n",
      "        [ 0.1578],\n",
      "        [-1.9080],\n",
      "        [ 1.2878],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4174: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4174: tensor([[ 1.0742],\n",
      "        [ 0.1588],\n",
      "        [-1.9063],\n",
      "        [ 1.2900],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4175: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4175: tensor([[ 1.0738],\n",
      "        [ 0.1577],\n",
      "        [-1.9080],\n",
      "        [ 1.2878],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4176: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4176: tensor([[ 1.0742],\n",
      "        [ 0.1588],\n",
      "        [-1.9063],\n",
      "        [ 1.2900],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4177: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4177: tensor([[ 1.0738],\n",
      "        [ 0.1577],\n",
      "        [-1.9080],\n",
      "        [ 1.2878],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4178: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4178: tensor([[ 1.0742],\n",
      "        [ 0.1587],\n",
      "        [-1.9063],\n",
      "        [ 1.2900],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4179: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4179: tensor([[ 1.0738],\n",
      "        [ 0.1577],\n",
      "        [-1.9080],\n",
      "        [ 1.2878],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4180: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4180: tensor([[ 1.0742],\n",
      "        [ 0.1587],\n",
      "        [-1.9063],\n",
      "        [ 1.2900],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4181: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4181: tensor([[ 1.0738],\n",
      "        [ 0.1577],\n",
      "        [-1.9081],\n",
      "        [ 1.2878],\n",
      "        [ 0.6933],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4182: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4182: tensor([[ 1.0742],\n",
      "        [ 0.1587],\n",
      "        [-1.9063],\n",
      "        [ 1.2900],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4183: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4183: tensor([[ 1.0738],\n",
      "        [ 0.1577],\n",
      "        [-1.9081],\n",
      "        [ 1.2878],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4184: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4184: tensor([[ 1.0742],\n",
      "        [ 0.1587],\n",
      "        [-1.9064],\n",
      "        [ 1.2900],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4185: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4185: tensor([[ 1.0738],\n",
      "        [ 0.1577],\n",
      "        [-1.9081],\n",
      "        [ 1.2879],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4186: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4186: tensor([[ 1.0742],\n",
      "        [ 0.1587],\n",
      "        [-1.9064],\n",
      "        [ 1.2901],\n",
      "        [ 0.6957],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4187: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4187: tensor([[ 1.0738],\n",
      "        [ 0.1576],\n",
      "        [-1.9081],\n",
      "        [ 1.2879],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4188: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4188: tensor([[ 1.0742],\n",
      "        [ 0.1587],\n",
      "        [-1.9064],\n",
      "        [ 1.2901],\n",
      "        [ 0.6958],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4189: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4189: tensor([[ 1.0738],\n",
      "        [ 0.1576],\n",
      "        [-1.9081],\n",
      "        [ 1.2879],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4190: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4190: tensor([[ 1.0742],\n",
      "        [ 0.1586],\n",
      "        [-1.9064],\n",
      "        [ 1.2901],\n",
      "        [ 0.6958],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4191: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4191: tensor([[ 1.0738],\n",
      "        [ 0.1576],\n",
      "        [-1.9082],\n",
      "        [ 1.2879],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4192: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4192: tensor([[ 1.0742],\n",
      "        [ 0.1586],\n",
      "        [-1.9064],\n",
      "        [ 1.2901],\n",
      "        [ 0.6958],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4193: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4193: tensor([[ 1.0738],\n",
      "        [ 0.1576],\n",
      "        [-1.9082],\n",
      "        [ 1.2879],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4194: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4194: tensor([[ 1.0742],\n",
      "        [ 0.1586],\n",
      "        [-1.9065],\n",
      "        [ 1.2901],\n",
      "        [ 0.6958],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4195: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4195: tensor([[ 1.0738],\n",
      "        [ 0.1576],\n",
      "        [-1.9082],\n",
      "        [ 1.2879],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4196: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4196: tensor([[ 1.0742],\n",
      "        [ 0.1586],\n",
      "        [-1.9065],\n",
      "        [ 1.2901],\n",
      "        [ 0.6958],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4197: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4197: tensor([[ 1.0738],\n",
      "        [ 0.1575],\n",
      "        [-1.9082],\n",
      "        [ 1.2879],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4198: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4198: tensor([[ 1.0742],\n",
      "        [ 0.1586],\n",
      "        [-1.9065],\n",
      "        [ 1.2901],\n",
      "        [ 0.6958],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4199: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4199: tensor([[ 1.0738],\n",
      "        [ 0.1575],\n",
      "        [-1.9082],\n",
      "        [ 1.2880],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4200: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4200: tensor([[ 1.0742],\n",
      "        [ 0.1585],\n",
      "        [-1.9065],\n",
      "        [ 1.2902],\n",
      "        [ 0.6958],\n",
      "        [-0.2910]], requires_grad=True)\n",
      "poly train loss at 4201: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4201: tensor([[ 1.0738],\n",
      "        [ 0.1575],\n",
      "        [-1.9083],\n",
      "        [ 1.2880],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4202: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4202: tensor([[ 1.0742],\n",
      "        [ 0.1585],\n",
      "        [-1.9065],\n",
      "        [ 1.2902],\n",
      "        [ 0.6958],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4203: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4203: tensor([[ 1.0738],\n",
      "        [ 0.1575],\n",
      "        [-1.9083],\n",
      "        [ 1.2880],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4204: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4204: tensor([[ 1.0742],\n",
      "        [ 0.1585],\n",
      "        [-1.9066],\n",
      "        [ 1.2902],\n",
      "        [ 0.6958],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4205: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4205: tensor([[ 1.0738],\n",
      "        [ 0.1575],\n",
      "        [-1.9083],\n",
      "        [ 1.2880],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4206: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4206: tensor([[ 1.0742],\n",
      "        [ 0.1585],\n",
      "        [-1.9066],\n",
      "        [ 1.2902],\n",
      "        [ 0.6958],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4207: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4207: tensor([[ 1.0738],\n",
      "        [ 0.1575],\n",
      "        [-1.9083],\n",
      "        [ 1.2880],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4208: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4208: tensor([[ 1.0742],\n",
      "        [ 0.1585],\n",
      "        [-1.9066],\n",
      "        [ 1.2902],\n",
      "        [ 0.6958],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4209: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4209: tensor([[ 1.0738],\n",
      "        [ 0.1574],\n",
      "        [-1.9083],\n",
      "        [ 1.2880],\n",
      "        [ 0.6934],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4210: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4210: tensor([[ 1.0742],\n",
      "        [ 0.1585],\n",
      "        [-1.9066],\n",
      "        [ 1.2902],\n",
      "        [ 0.6958],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4211: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4211: tensor([[ 1.0738],\n",
      "        [ 0.1574],\n",
      "        [-1.9084],\n",
      "        [ 1.2880],\n",
      "        [ 0.6935],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4212: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4212: tensor([[ 1.0742],\n",
      "        [ 0.1584],\n",
      "        [-1.9066],\n",
      "        [ 1.2902],\n",
      "        [ 0.6958],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4213: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4213: tensor([[ 1.0738],\n",
      "        [ 0.1574],\n",
      "        [-1.9084],\n",
      "        [ 1.2881],\n",
      "        [ 0.6935],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4214: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4214: tensor([[ 1.0742],\n",
      "        [ 0.1584],\n",
      "        [-1.9067],\n",
      "        [ 1.2903],\n",
      "        [ 0.6958],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4215: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4215: tensor([[ 1.0738],\n",
      "        [ 0.1574],\n",
      "        [-1.9084],\n",
      "        [ 1.2881],\n",
      "        [ 0.6935],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4216: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4216: tensor([[ 1.0742],\n",
      "        [ 0.1584],\n",
      "        [-1.9067],\n",
      "        [ 1.2903],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4217: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4217: tensor([[ 1.0738],\n",
      "        [ 0.1574],\n",
      "        [-1.9084],\n",
      "        [ 1.2881],\n",
      "        [ 0.6935],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4218: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4218: tensor([[ 1.0742],\n",
      "        [ 0.1584],\n",
      "        [-1.9067],\n",
      "        [ 1.2903],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4219: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4219: tensor([[ 1.0738],\n",
      "        [ 0.1574],\n",
      "        [-1.9084],\n",
      "        [ 1.2881],\n",
      "        [ 0.6935],\n",
      "        [-0.2935]], requires_grad=True)\n",
      "poly train loss at 4220: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4220: tensor([[ 1.0742],\n",
      "        [ 0.1584],\n",
      "        [-1.9067],\n",
      "        [ 1.2903],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4221: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4221: tensor([[ 1.0738],\n",
      "        [ 0.1573],\n",
      "        [-1.9085],\n",
      "        [ 1.2881],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4222: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4222: tensor([[ 1.0742],\n",
      "        [ 0.1583],\n",
      "        [-1.9067],\n",
      "        [ 1.2903],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4223: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4223: tensor([[ 1.0738],\n",
      "        [ 0.1573],\n",
      "        [-1.9085],\n",
      "        [ 1.2881],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4224: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4224: tensor([[ 1.0742],\n",
      "        [ 0.1583],\n",
      "        [-1.9067],\n",
      "        [ 1.2903],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4225: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4225: tensor([[ 1.0738],\n",
      "        [ 0.1573],\n",
      "        [-1.9085],\n",
      "        [ 1.2881],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4226: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4226: tensor([[ 1.0742],\n",
      "        [ 0.1583],\n",
      "        [-1.9068],\n",
      "        [ 1.2903],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4227: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4227: tensor([[ 1.0738],\n",
      "        [ 0.1573],\n",
      "        [-1.9085],\n",
      "        [ 1.2882],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4228: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4228: tensor([[ 1.0742],\n",
      "        [ 0.1583],\n",
      "        [-1.9068],\n",
      "        [ 1.2904],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4229: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4229: tensor([[ 1.0738],\n",
      "        [ 0.1573],\n",
      "        [-1.9085],\n",
      "        [ 1.2882],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4230: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4230: tensor([[ 1.0742],\n",
      "        [ 0.1583],\n",
      "        [-1.9068],\n",
      "        [ 1.2904],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4231: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4231: tensor([[ 1.0738],\n",
      "        [ 0.1572],\n",
      "        [-1.9085],\n",
      "        [ 1.2882],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4232: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4232: tensor([[ 1.0742],\n",
      "        [ 0.1583],\n",
      "        [-1.9068],\n",
      "        [ 1.2904],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4233: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4233: tensor([[ 1.0738],\n",
      "        [ 0.1572],\n",
      "        [-1.9086],\n",
      "        [ 1.2882],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4234: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4234: tensor([[ 1.0742],\n",
      "        [ 0.1582],\n",
      "        [-1.9068],\n",
      "        [ 1.2904],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4235: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4235: tensor([[ 1.0738],\n",
      "        [ 0.1572],\n",
      "        [-1.9086],\n",
      "        [ 1.2882],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4236: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4236: tensor([[ 1.0742],\n",
      "        [ 0.1582],\n",
      "        [-1.9069],\n",
      "        [ 1.2904],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4237: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4237: tensor([[ 1.0738],\n",
      "        [ 0.1572],\n",
      "        [-1.9086],\n",
      "        [ 1.2882],\n",
      "        [ 0.6935],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4238: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4238: tensor([[ 1.0742],\n",
      "        [ 0.1582],\n",
      "        [-1.9069],\n",
      "        [ 1.2904],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4239: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4239: tensor([[ 1.0738],\n",
      "        [ 0.1572],\n",
      "        [-1.9086],\n",
      "        [ 1.2882],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4240: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4240: tensor([[ 1.0742],\n",
      "        [ 0.1582],\n",
      "        [-1.9069],\n",
      "        [ 1.2904],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4241: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4241: tensor([[ 1.0738],\n",
      "        [ 0.1572],\n",
      "        [-1.9086],\n",
      "        [ 1.2882],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4242: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4242: tensor([[ 1.0742],\n",
      "        [ 0.1582],\n",
      "        [-1.9069],\n",
      "        [ 1.2905],\n",
      "        [ 0.6959],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4243: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4243: tensor([[ 1.0738],\n",
      "        [ 0.1571],\n",
      "        [-1.9087],\n",
      "        [ 1.2883],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4244: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4244: tensor([[ 1.0742],\n",
      "        [ 0.1581],\n",
      "        [-1.9069],\n",
      "        [ 1.2905],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4245: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4245: tensor([[ 1.0738],\n",
      "        [ 0.1571],\n",
      "        [-1.9087],\n",
      "        [ 1.2883],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4246: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4246: tensor([[ 1.0742],\n",
      "        [ 0.1581],\n",
      "        [-1.9070],\n",
      "        [ 1.2905],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4247: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4247: tensor([[ 1.0738],\n",
      "        [ 0.1571],\n",
      "        [-1.9087],\n",
      "        [ 1.2883],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4248: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4248: tensor([[ 1.0742],\n",
      "        [ 0.1581],\n",
      "        [-1.9070],\n",
      "        [ 1.2905],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4249: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4249: tensor([[ 1.0738],\n",
      "        [ 0.1571],\n",
      "        [-1.9087],\n",
      "        [ 1.2883],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4250: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4250: tensor([[ 1.0742],\n",
      "        [ 0.1581],\n",
      "        [-1.9070],\n",
      "        [ 1.2905],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4251: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4251: tensor([[ 1.0738],\n",
      "        [ 0.1571],\n",
      "        [-1.9087],\n",
      "        [ 1.2883],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4252: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4252: tensor([[ 1.0742],\n",
      "        [ 0.1581],\n",
      "        [-1.9070],\n",
      "        [ 1.2905],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4253: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4253: tensor([[ 1.0738],\n",
      "        [ 0.1570],\n",
      "        [-1.9088],\n",
      "        [ 1.2883],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4254: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4254: tensor([[ 1.0742],\n",
      "        [ 0.1581],\n",
      "        [-1.9070],\n",
      "        [ 1.2905],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4255: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4255: tensor([[ 1.0738],\n",
      "        [ 0.1570],\n",
      "        [-1.9088],\n",
      "        [ 1.2883],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4256: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4256: tensor([[ 1.0742],\n",
      "        [ 0.1580],\n",
      "        [-1.9071],\n",
      "        [ 1.2906],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4257: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4257: tensor([[ 1.0738],\n",
      "        [ 0.1570],\n",
      "        [-1.9088],\n",
      "        [ 1.2884],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4258: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4258: tensor([[ 1.0742],\n",
      "        [ 0.1580],\n",
      "        [-1.9071],\n",
      "        [ 1.2906],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4259: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4259: tensor([[ 1.0738],\n",
      "        [ 0.1570],\n",
      "        [-1.9088],\n",
      "        [ 1.2884],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4260: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4260: tensor([[ 1.0742],\n",
      "        [ 0.1580],\n",
      "        [-1.9071],\n",
      "        [ 1.2906],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4261: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4261: tensor([[ 1.0738],\n",
      "        [ 0.1570],\n",
      "        [-1.9088],\n",
      "        [ 1.2884],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4262: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4262: tensor([[ 1.0742],\n",
      "        [ 0.1580],\n",
      "        [-1.9071],\n",
      "        [ 1.2906],\n",
      "        [ 0.6960],\n",
      "        [-0.2911]], requires_grad=True)\n",
      "poly train loss at 4263: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4263: tensor([[ 1.0738],\n",
      "        [ 0.1570],\n",
      "        [-1.9089],\n",
      "        [ 1.2884],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4264: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4264: tensor([[ 1.0742],\n",
      "        [ 0.1580],\n",
      "        [-1.9071],\n",
      "        [ 1.2906],\n",
      "        [ 0.6960],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4265: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4265: tensor([[ 1.0738],\n",
      "        [ 0.1569],\n",
      "        [-1.9089],\n",
      "        [ 1.2884],\n",
      "        [ 0.6936],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4266: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4266: tensor([[ 1.0742],\n",
      "        [ 0.1580],\n",
      "        [-1.9072],\n",
      "        [ 1.2906],\n",
      "        [ 0.6960],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4267: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4267: tensor([[ 1.0738],\n",
      "        [ 0.1569],\n",
      "        [-1.9089],\n",
      "        [ 1.2884],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4268: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4268: tensor([[ 1.0742],\n",
      "        [ 0.1579],\n",
      "        [-1.9072],\n",
      "        [ 1.2906],\n",
      "        [ 0.6960],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4269: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4269: tensor([[ 1.0738],\n",
      "        [ 0.1569],\n",
      "        [-1.9089],\n",
      "        [ 1.2884],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4270: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4270: tensor([[ 1.0742],\n",
      "        [ 0.1579],\n",
      "        [-1.9072],\n",
      "        [ 1.2907],\n",
      "        [ 0.6960],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4271: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4271: tensor([[ 1.0738],\n",
      "        [ 0.1569],\n",
      "        [-1.9089],\n",
      "        [ 1.2885],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4272: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4272: tensor([[ 1.0742],\n",
      "        [ 0.1579],\n",
      "        [-1.9072],\n",
      "        [ 1.2907],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4273: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4273: tensor([[ 1.0738],\n",
      "        [ 0.1569],\n",
      "        [-1.9090],\n",
      "        [ 1.2885],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4274: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4274: tensor([[ 1.0742],\n",
      "        [ 0.1579],\n",
      "        [-1.9072],\n",
      "        [ 1.2907],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4275: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4275: tensor([[ 1.0738],\n",
      "        [ 0.1569],\n",
      "        [-1.9090],\n",
      "        [ 1.2885],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4276: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4276: tensor([[ 1.0742],\n",
      "        [ 0.1579],\n",
      "        [-1.9073],\n",
      "        [ 1.2907],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4277: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4277: tensor([[ 1.0738],\n",
      "        [ 0.1568],\n",
      "        [-1.9090],\n",
      "        [ 1.2885],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4278: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4278: tensor([[ 1.0742],\n",
      "        [ 0.1578],\n",
      "        [-1.9073],\n",
      "        [ 1.2907],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4279: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4279: tensor([[ 1.0738],\n",
      "        [ 0.1568],\n",
      "        [-1.9090],\n",
      "        [ 1.2885],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4280: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4280: tensor([[ 1.0742],\n",
      "        [ 0.1578],\n",
      "        [-1.9073],\n",
      "        [ 1.2907],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4281: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4281: tensor([[ 1.0738],\n",
      "        [ 0.1568],\n",
      "        [-1.9090],\n",
      "        [ 1.2885],\n",
      "        [ 0.6937],\n",
      "        [-0.2936]], requires_grad=True)\n",
      "poly train loss at 4282: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4282: tensor([[ 1.0742],\n",
      "        [ 0.1578],\n",
      "        [-1.9073],\n",
      "        [ 1.2907],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4283: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4283: tensor([[ 1.0738],\n",
      "        [ 0.1568],\n",
      "        [-1.9091],\n",
      "        [ 1.2885],\n",
      "        [ 0.6937],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4284: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4284: tensor([[ 1.0742],\n",
      "        [ 0.1578],\n",
      "        [-1.9073],\n",
      "        [ 1.2908],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4285: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4285: tensor([[ 1.0738],\n",
      "        [ 0.1568],\n",
      "        [-1.9091],\n",
      "        [ 1.2886],\n",
      "        [ 0.6937],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4286: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4286: tensor([[ 1.0742],\n",
      "        [ 0.1578],\n",
      "        [-1.9073],\n",
      "        [ 1.2908],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4287: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4287: tensor([[ 1.0738],\n",
      "        [ 0.1567],\n",
      "        [-1.9091],\n",
      "        [ 1.2886],\n",
      "        [ 0.6937],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4288: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4288: tensor([[ 1.0742],\n",
      "        [ 0.1578],\n",
      "        [-1.9074],\n",
      "        [ 1.2908],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4289: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4289: tensor([[ 1.0738],\n",
      "        [ 0.1567],\n",
      "        [-1.9091],\n",
      "        [ 1.2886],\n",
      "        [ 0.6937],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4290: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4290: tensor([[ 1.0742],\n",
      "        [ 0.1577],\n",
      "        [-1.9074],\n",
      "        [ 1.2908],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4291: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4291: tensor([[ 1.0738],\n",
      "        [ 0.1567],\n",
      "        [-1.9091],\n",
      "        [ 1.2886],\n",
      "        [ 0.6937],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4292: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4292: tensor([[ 1.0742],\n",
      "        [ 0.1577],\n",
      "        [-1.9074],\n",
      "        [ 1.2908],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4293: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4293: tensor([[ 1.0738],\n",
      "        [ 0.1567],\n",
      "        [-1.9091],\n",
      "        [ 1.2886],\n",
      "        [ 0.6937],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4294: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4294: tensor([[ 1.0742],\n",
      "        [ 0.1577],\n",
      "        [-1.9074],\n",
      "        [ 1.2908],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4295: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4295: tensor([[ 1.0738],\n",
      "        [ 0.1567],\n",
      "        [-1.9092],\n",
      "        [ 1.2886],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4296: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4296: tensor([[ 1.0742],\n",
      "        [ 0.1577],\n",
      "        [-1.9074],\n",
      "        [ 1.2908],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4297: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4297: tensor([[ 1.0738],\n",
      "        [ 0.1567],\n",
      "        [-1.9092],\n",
      "        [ 1.2886],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4298: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4298: tensor([[ 1.0742],\n",
      "        [ 0.1577],\n",
      "        [-1.9075],\n",
      "        [ 1.2909],\n",
      "        [ 0.6961],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4299: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4299: tensor([[ 1.0738],\n",
      "        [ 0.1566],\n",
      "        [-1.9092],\n",
      "        [ 1.2887],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4300: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4300: tensor([[ 1.0742],\n",
      "        [ 0.1577],\n",
      "        [-1.9075],\n",
      "        [ 1.2909],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4301: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4301: tensor([[ 1.0739],\n",
      "        [ 0.1566],\n",
      "        [-1.9092],\n",
      "        [ 1.2887],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4302: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4302: tensor([[ 1.0742],\n",
      "        [ 0.1576],\n",
      "        [-1.9075],\n",
      "        [ 1.2909],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4303: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4303: tensor([[ 1.0739],\n",
      "        [ 0.1566],\n",
      "        [-1.9092],\n",
      "        [ 1.2887],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4304: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4304: tensor([[ 1.0742],\n",
      "        [ 0.1576],\n",
      "        [-1.9075],\n",
      "        [ 1.2909],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4305: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4305: tensor([[ 1.0739],\n",
      "        [ 0.1566],\n",
      "        [-1.9093],\n",
      "        [ 1.2887],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4306: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4306: tensor([[ 1.0742],\n",
      "        [ 0.1576],\n",
      "        [-1.9075],\n",
      "        [ 1.2909],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4307: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4307: tensor([[ 1.0739],\n",
      "        [ 0.1566],\n",
      "        [-1.9093],\n",
      "        [ 1.2887],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4308: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4308: tensor([[ 1.0742],\n",
      "        [ 0.1576],\n",
      "        [-1.9076],\n",
      "        [ 1.2909],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4309: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4309: tensor([[ 1.0739],\n",
      "        [ 0.1566],\n",
      "        [-1.9093],\n",
      "        [ 1.2887],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4310: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4310: tensor([[ 1.0742],\n",
      "        [ 0.1576],\n",
      "        [-1.9076],\n",
      "        [ 1.2909],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4311: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4311: tensor([[ 1.0739],\n",
      "        [ 0.1565],\n",
      "        [-1.9093],\n",
      "        [ 1.2887],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4312: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4312: tensor([[ 1.0742],\n",
      "        [ 0.1575],\n",
      "        [-1.9076],\n",
      "        [ 1.2909],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4313: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4313: tensor([[ 1.0739],\n",
      "        [ 0.1565],\n",
      "        [-1.9093],\n",
      "        [ 1.2888],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4314: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4314: tensor([[ 1.0742],\n",
      "        [ 0.1575],\n",
      "        [-1.9076],\n",
      "        [ 1.2910],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4315: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4315: tensor([[ 1.0739],\n",
      "        [ 0.1565],\n",
      "        [-1.9094],\n",
      "        [ 1.2888],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4316: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4316: tensor([[ 1.0742],\n",
      "        [ 0.1575],\n",
      "        [-1.9076],\n",
      "        [ 1.2910],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4317: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4317: tensor([[ 1.0739],\n",
      "        [ 0.1565],\n",
      "        [-1.9094],\n",
      "        [ 1.2888],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4318: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4318: tensor([[ 1.0742],\n",
      "        [ 0.1575],\n",
      "        [-1.9076],\n",
      "        [ 1.2910],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4319: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4319: tensor([[ 1.0739],\n",
      "        [ 0.1565],\n",
      "        [-1.9094],\n",
      "        [ 1.2888],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4320: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4320: tensor([[ 1.0742],\n",
      "        [ 0.1575],\n",
      "        [-1.9077],\n",
      "        [ 1.2910],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4321: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4321: tensor([[ 1.0739],\n",
      "        [ 0.1564],\n",
      "        [-1.9094],\n",
      "        [ 1.2888],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4322: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4322: tensor([[ 1.0742],\n",
      "        [ 0.1575],\n",
      "        [-1.9077],\n",
      "        [ 1.2910],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4323: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4323: tensor([[ 1.0739],\n",
      "        [ 0.1564],\n",
      "        [-1.9094],\n",
      "        [ 1.2888],\n",
      "        [ 0.6938],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4324: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4324: tensor([[ 1.0742],\n",
      "        [ 0.1574],\n",
      "        [-1.9077],\n",
      "        [ 1.2910],\n",
      "        [ 0.6962],\n",
      "        [-0.2912]], requires_grad=True)\n",
      "poly train loss at 4325: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4325: tensor([[ 1.0739],\n",
      "        [ 0.1564],\n",
      "        [-1.9094],\n",
      "        [ 1.2888],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4326: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4326: tensor([[ 1.0742],\n",
      "        [ 0.1574],\n",
      "        [-1.9077],\n",
      "        [ 1.2910],\n",
      "        [ 0.6962],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4327: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4327: tensor([[ 1.0739],\n",
      "        [ 0.1564],\n",
      "        [-1.9095],\n",
      "        [ 1.2888],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4328: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4328: tensor([[ 1.0742],\n",
      "        [ 0.1574],\n",
      "        [-1.9077],\n",
      "        [ 1.2911],\n",
      "        [ 0.6962],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4329: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4329: tensor([[ 1.0739],\n",
      "        [ 0.1564],\n",
      "        [-1.9095],\n",
      "        [ 1.2889],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4330: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4330: tensor([[ 1.0742],\n",
      "        [ 0.1574],\n",
      "        [-1.9078],\n",
      "        [ 1.2911],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4331: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4331: tensor([[ 1.0739],\n",
      "        [ 0.1564],\n",
      "        [-1.9095],\n",
      "        [ 1.2889],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4332: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4332: tensor([[ 1.0742],\n",
      "        [ 0.1574],\n",
      "        [-1.9078],\n",
      "        [ 1.2911],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4333: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4333: tensor([[ 1.0739],\n",
      "        [ 0.1563],\n",
      "        [-1.9095],\n",
      "        [ 1.2889],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4334: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4334: tensor([[ 1.0742],\n",
      "        [ 0.1574],\n",
      "        [-1.9078],\n",
      "        [ 1.2911],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4335: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4335: tensor([[ 1.0739],\n",
      "        [ 0.1563],\n",
      "        [-1.9095],\n",
      "        [ 1.2889],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4336: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4336: tensor([[ 1.0742],\n",
      "        [ 0.1573],\n",
      "        [-1.9078],\n",
      "        [ 1.2911],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4337: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4337: tensor([[ 1.0739],\n",
      "        [ 0.1563],\n",
      "        [-1.9096],\n",
      "        [ 1.2889],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4338: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4338: tensor([[ 1.0742],\n",
      "        [ 0.1573],\n",
      "        [-1.9078],\n",
      "        [ 1.2911],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4339: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4339: tensor([[ 1.0739],\n",
      "        [ 0.1563],\n",
      "        [-1.9096],\n",
      "        [ 1.2889],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4340: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4340: tensor([[ 1.0742],\n",
      "        [ 0.1573],\n",
      "        [-1.9079],\n",
      "        [ 1.2911],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4341: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4341: tensor([[ 1.0739],\n",
      "        [ 0.1563],\n",
      "        [-1.9096],\n",
      "        [ 1.2889],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4342: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4342: tensor([[ 1.0742],\n",
      "        [ 0.1573],\n",
      "        [-1.9079],\n",
      "        [ 1.2912],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4343: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4343: tensor([[ 1.0739],\n",
      "        [ 0.1563],\n",
      "        [-1.9096],\n",
      "        [ 1.2890],\n",
      "        [ 0.6939],\n",
      "        [-0.2937]], requires_grad=True)\n",
      "poly train loss at 4344: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4344: tensor([[ 1.0742],\n",
      "        [ 0.1573],\n",
      "        [-1.9079],\n",
      "        [ 1.2912],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4345: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4345: tensor([[ 1.0739],\n",
      "        [ 0.1562],\n",
      "        [-1.9096],\n",
      "        [ 1.2890],\n",
      "        [ 0.6939],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4346: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4346: tensor([[ 1.0742],\n",
      "        [ 0.1573],\n",
      "        [-1.9079],\n",
      "        [ 1.2912],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4347: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4347: tensor([[ 1.0739],\n",
      "        [ 0.1562],\n",
      "        [-1.9097],\n",
      "        [ 1.2890],\n",
      "        [ 0.6939],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4348: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4348: tensor([[ 1.0742],\n",
      "        [ 0.1572],\n",
      "        [-1.9079],\n",
      "        [ 1.2912],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4349: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4349: tensor([[ 1.0739],\n",
      "        [ 0.1562],\n",
      "        [-1.9097],\n",
      "        [ 1.2890],\n",
      "        [ 0.6939],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4350: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4350: tensor([[ 1.0742],\n",
      "        [ 0.1572],\n",
      "        [-1.9080],\n",
      "        [ 1.2912],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4351: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4351: tensor([[ 1.0739],\n",
      "        [ 0.1562],\n",
      "        [-1.9097],\n",
      "        [ 1.2890],\n",
      "        [ 0.6939],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4352: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4352: tensor([[ 1.0742],\n",
      "        [ 0.1572],\n",
      "        [-1.9080],\n",
      "        [ 1.2912],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4353: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4353: tensor([[ 1.0739],\n",
      "        [ 0.1562],\n",
      "        [-1.9097],\n",
      "        [ 1.2890],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4354: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4354: tensor([[ 1.0742],\n",
      "        [ 0.1572],\n",
      "        [-1.9080],\n",
      "        [ 1.2912],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4355: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4355: tensor([[ 1.0739],\n",
      "        [ 0.1562],\n",
      "        [-1.9097],\n",
      "        [ 1.2890],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4356: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4356: tensor([[ 1.0742],\n",
      "        [ 0.1572],\n",
      "        [-1.9080],\n",
      "        [ 1.2913],\n",
      "        [ 0.6963],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4357: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4357: tensor([[ 1.0739],\n",
      "        [ 0.1561],\n",
      "        [-1.9097],\n",
      "        [ 1.2891],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4358: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4358: tensor([[ 1.0742],\n",
      "        [ 0.1571],\n",
      "        [-1.9080],\n",
      "        [ 1.2913],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4359: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4359: tensor([[ 1.0739],\n",
      "        [ 0.1561],\n",
      "        [-1.9098],\n",
      "        [ 1.2891],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4360: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4360: tensor([[ 1.0742],\n",
      "        [ 0.1571],\n",
      "        [-1.9080],\n",
      "        [ 1.2913],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4361: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4361: tensor([[ 1.0739],\n",
      "        [ 0.1561],\n",
      "        [-1.9098],\n",
      "        [ 1.2891],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4362: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4362: tensor([[ 1.0742],\n",
      "        [ 0.1571],\n",
      "        [-1.9081],\n",
      "        [ 1.2913],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4363: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4363: tensor([[ 1.0739],\n",
      "        [ 0.1561],\n",
      "        [-1.9098],\n",
      "        [ 1.2891],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4364: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4364: tensor([[ 1.0742],\n",
      "        [ 0.1571],\n",
      "        [-1.9081],\n",
      "        [ 1.2913],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4365: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4365: tensor([[ 1.0739],\n",
      "        [ 0.1561],\n",
      "        [-1.9098],\n",
      "        [ 1.2891],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4366: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4366: tensor([[ 1.0742],\n",
      "        [ 0.1571],\n",
      "        [-1.9081],\n",
      "        [ 1.2913],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4367: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4367: tensor([[ 1.0739],\n",
      "        [ 0.1560],\n",
      "        [-1.9098],\n",
      "        [ 1.2891],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4368: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4368: tensor([[ 1.0742],\n",
      "        [ 0.1571],\n",
      "        [-1.9081],\n",
      "        [ 1.2913],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4369: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4369: tensor([[ 1.0739],\n",
      "        [ 0.1560],\n",
      "        [-1.9099],\n",
      "        [ 1.2891],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4370: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4370: tensor([[ 1.0742],\n",
      "        [ 0.1570],\n",
      "        [-1.9081],\n",
      "        [ 1.2913],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4371: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4371: tensor([[ 1.0739],\n",
      "        [ 0.1560],\n",
      "        [-1.9099],\n",
      "        [ 1.2892],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4372: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4372: tensor([[ 1.0742],\n",
      "        [ 0.1570],\n",
      "        [-1.9082],\n",
      "        [ 1.2914],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4373: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4373: tensor([[ 1.0739],\n",
      "        [ 0.1560],\n",
      "        [-1.9099],\n",
      "        [ 1.2892],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4374: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4374: tensor([[ 1.0742],\n",
      "        [ 0.1570],\n",
      "        [-1.9082],\n",
      "        [ 1.2914],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4375: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4375: tensor([[ 1.0739],\n",
      "        [ 0.1560],\n",
      "        [-1.9099],\n",
      "        [ 1.2892],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4376: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4376: tensor([[ 1.0742],\n",
      "        [ 0.1570],\n",
      "        [-1.9082],\n",
      "        [ 1.2914],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4377: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4377: tensor([[ 1.0739],\n",
      "        [ 0.1560],\n",
      "        [-1.9099],\n",
      "        [ 1.2892],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4378: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4378: tensor([[ 1.0742],\n",
      "        [ 0.1570],\n",
      "        [-1.9082],\n",
      "        [ 1.2914],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4379: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4379: tensor([[ 1.0739],\n",
      "        [ 0.1559],\n",
      "        [-1.9100],\n",
      "        [ 1.2892],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4380: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4380: tensor([[ 1.0742],\n",
      "        [ 0.1570],\n",
      "        [-1.9082],\n",
      "        [ 1.2914],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4381: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4381: tensor([[ 1.0739],\n",
      "        [ 0.1559],\n",
      "        [-1.9100],\n",
      "        [ 1.2892],\n",
      "        [ 0.6940],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4382: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4382: tensor([[ 1.0742],\n",
      "        [ 0.1569],\n",
      "        [-1.9083],\n",
      "        [ 1.2914],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4383: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4383: tensor([[ 1.0739],\n",
      "        [ 0.1559],\n",
      "        [-1.9100],\n",
      "        [ 1.2892],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4384: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4384: tensor([[ 1.0742],\n",
      "        [ 0.1569],\n",
      "        [-1.9083],\n",
      "        [ 1.2914],\n",
      "        [ 0.6964],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4385: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4385: tensor([[ 1.0739],\n",
      "        [ 0.1559],\n",
      "        [-1.9100],\n",
      "        [ 1.2892],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4386: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4386: tensor([[ 1.0742],\n",
      "        [ 0.1569],\n",
      "        [-1.9083],\n",
      "        [ 1.2915],\n",
      "        [ 0.6965],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4387: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4387: tensor([[ 1.0739],\n",
      "        [ 0.1559],\n",
      "        [-1.9100],\n",
      "        [ 1.2893],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4388: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4388: tensor([[ 1.0742],\n",
      "        [ 0.1569],\n",
      "        [-1.9083],\n",
      "        [ 1.2915],\n",
      "        [ 0.6965],\n",
      "        [-0.2913]], requires_grad=True)\n",
      "poly train loss at 4389: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4389: tensor([[ 1.0739],\n",
      "        [ 0.1559],\n",
      "        [-1.9100],\n",
      "        [ 1.2893],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4390: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4390: tensor([[ 1.0742],\n",
      "        [ 0.1569],\n",
      "        [-1.9083],\n",
      "        [ 1.2915],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4391: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4391: tensor([[ 1.0739],\n",
      "        [ 0.1558],\n",
      "        [-1.9101],\n",
      "        [ 1.2893],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4392: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4392: tensor([[ 1.0742],\n",
      "        [ 0.1569],\n",
      "        [-1.9083],\n",
      "        [ 1.2915],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4393: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4393: tensor([[ 1.0739],\n",
      "        [ 0.1558],\n",
      "        [-1.9101],\n",
      "        [ 1.2893],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4394: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4394: tensor([[ 1.0742],\n",
      "        [ 0.1568],\n",
      "        [-1.9084],\n",
      "        [ 1.2915],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4395: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4395: tensor([[ 1.0739],\n",
      "        [ 0.1558],\n",
      "        [-1.9101],\n",
      "        [ 1.2893],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4396: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4396: tensor([[ 1.0742],\n",
      "        [ 0.1568],\n",
      "        [-1.9084],\n",
      "        [ 1.2915],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4397: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4397: tensor([[ 1.0739],\n",
      "        [ 0.1558],\n",
      "        [-1.9101],\n",
      "        [ 1.2893],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4398: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4398: tensor([[ 1.0742],\n",
      "        [ 0.1568],\n",
      "        [-1.9084],\n",
      "        [ 1.2915],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4399: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4399: tensor([[ 1.0739],\n",
      "        [ 0.1558],\n",
      "        [-1.9101],\n",
      "        [ 1.2893],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4400: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4400: tensor([[ 1.0742],\n",
      "        [ 0.1568],\n",
      "        [-1.9084],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4401: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4401: tensor([[ 1.0739],\n",
      "        [ 0.1558],\n",
      "        [-1.9102],\n",
      "        [ 1.2894],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4402: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4402: tensor([[ 1.0742],\n",
      "        [ 0.1568],\n",
      "        [-1.9084],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4403: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4403: tensor([[ 1.0739],\n",
      "        [ 0.1557],\n",
      "        [-1.9102],\n",
      "        [ 1.2894],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4404: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4404: tensor([[ 1.0742],\n",
      "        [ 0.1567],\n",
      "        [-1.9085],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4405: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4405: tensor([[ 1.0739],\n",
      "        [ 0.1557],\n",
      "        [-1.9102],\n",
      "        [ 1.2894],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4406: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4406: tensor([[ 1.0742],\n",
      "        [ 0.1567],\n",
      "        [-1.9085],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4407: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4407: tensor([[ 1.0739],\n",
      "        [ 0.1557],\n",
      "        [-1.9102],\n",
      "        [ 1.2894],\n",
      "        [ 0.6941],\n",
      "        [-0.2938]], requires_grad=True)\n",
      "poly train loss at 4408: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4408: tensor([[ 1.0742],\n",
      "        [ 0.1567],\n",
      "        [-1.9085],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4409: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4409: tensor([[ 1.0739],\n",
      "        [ 0.1557],\n",
      "        [-1.9102],\n",
      "        [ 1.2894],\n",
      "        [ 0.6941],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4410: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4410: tensor([[ 1.0742],\n",
      "        [ 0.1567],\n",
      "        [-1.9085],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4411: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4411: tensor([[ 1.0739],\n",
      "        [ 0.1557],\n",
      "        [-1.9103],\n",
      "        [ 1.2894],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4412: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4412: tensor([[ 1.0742],\n",
      "        [ 0.1567],\n",
      "        [-1.9085],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4413: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4413: tensor([[ 1.0739],\n",
      "        [ 0.1556],\n",
      "        [-1.9103],\n",
      "        [ 1.2894],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4414: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4414: tensor([[ 1.0742],\n",
      "        [ 0.1567],\n",
      "        [-1.9085],\n",
      "        [ 1.2916],\n",
      "        [ 0.6965],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4415: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4415: tensor([[ 1.0739],\n",
      "        [ 0.1556],\n",
      "        [-1.9103],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4416: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4416: tensor([[ 1.0742],\n",
      "        [ 0.1566],\n",
      "        [-1.9086],\n",
      "        [ 1.2917],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4417: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4417: tensor([[ 1.0739],\n",
      "        [ 0.1556],\n",
      "        [-1.9103],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4418: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4418: tensor([[ 1.0742],\n",
      "        [ 0.1566],\n",
      "        [-1.9086],\n",
      "        [ 1.2917],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4419: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4419: tensor([[ 1.0739],\n",
      "        [ 0.1556],\n",
      "        [-1.9103],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4420: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4420: tensor([[ 1.0742],\n",
      "        [ 0.1566],\n",
      "        [-1.9086],\n",
      "        [ 1.2917],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4421: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4421: tensor([[ 1.0739],\n",
      "        [ 0.1556],\n",
      "        [-1.9103],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4422: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4422: tensor([[ 1.0742],\n",
      "        [ 0.1566],\n",
      "        [-1.9086],\n",
      "        [ 1.2917],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4423: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4423: tensor([[ 1.0739],\n",
      "        [ 0.1556],\n",
      "        [-1.9104],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4424: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4424: tensor([[ 1.0742],\n",
      "        [ 0.1566],\n",
      "        [-1.9086],\n",
      "        [ 1.2917],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4425: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4425: tensor([[ 1.0739],\n",
      "        [ 0.1555],\n",
      "        [-1.9104],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4426: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4426: tensor([[ 1.0742],\n",
      "        [ 0.1566],\n",
      "        [-1.9087],\n",
      "        [ 1.2917],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4427: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4427: tensor([[ 1.0739],\n",
      "        [ 0.1555],\n",
      "        [-1.9104],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4428: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4428: tensor([[ 1.0742],\n",
      "        [ 0.1565],\n",
      "        [-1.9087],\n",
      "        [ 1.2917],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4429: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4429: tensor([[ 1.0739],\n",
      "        [ 0.1555],\n",
      "        [-1.9104],\n",
      "        [ 1.2895],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4430: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4430: tensor([[ 1.0742],\n",
      "        [ 0.1565],\n",
      "        [-1.9087],\n",
      "        [ 1.2918],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4431: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4431: tensor([[ 1.0739],\n",
      "        [ 0.1555],\n",
      "        [-1.9104],\n",
      "        [ 1.2896],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4432: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4432: tensor([[ 1.0742],\n",
      "        [ 0.1565],\n",
      "        [-1.9087],\n",
      "        [ 1.2918],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4433: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4433: tensor([[ 1.0739],\n",
      "        [ 0.1555],\n",
      "        [-1.9105],\n",
      "        [ 1.2896],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4434: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4434: tensor([[ 1.0742],\n",
      "        [ 0.1565],\n",
      "        [-1.9087],\n",
      "        [ 1.2918],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4435: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4435: tensor([[ 1.0739],\n",
      "        [ 0.1555],\n",
      "        [-1.9105],\n",
      "        [ 1.2896],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4436: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4436: tensor([[ 1.0742],\n",
      "        [ 0.1565],\n",
      "        [-1.9088],\n",
      "        [ 1.2918],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4437: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4437: tensor([[ 1.0739],\n",
      "        [ 0.1554],\n",
      "        [-1.9105],\n",
      "        [ 1.2896],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4438: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4438: tensor([[ 1.0742],\n",
      "        [ 0.1565],\n",
      "        [-1.9088],\n",
      "        [ 1.2918],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4439: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4439: tensor([[ 1.0739],\n",
      "        [ 0.1554],\n",
      "        [-1.9105],\n",
      "        [ 1.2896],\n",
      "        [ 0.6942],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4440: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4440: tensor([[ 1.0742],\n",
      "        [ 0.1564],\n",
      "        [-1.9088],\n",
      "        [ 1.2918],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4441: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4441: tensor([[ 1.0739],\n",
      "        [ 0.1554],\n",
      "        [-1.9105],\n",
      "        [ 1.2896],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4442: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4442: tensor([[ 1.0742],\n",
      "        [ 0.1564],\n",
      "        [-1.9088],\n",
      "        [ 1.2918],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4443: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4443: tensor([[ 1.0739],\n",
      "        [ 0.1554],\n",
      "        [-1.9106],\n",
      "        [ 1.2896],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4444: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4444: tensor([[ 1.0742],\n",
      "        [ 0.1564],\n",
      "        [-1.9088],\n",
      "        [ 1.2919],\n",
      "        [ 0.6966],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4445: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4445: tensor([[ 1.0739],\n",
      "        [ 0.1554],\n",
      "        [-1.9106],\n",
      "        [ 1.2897],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4446: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4446: tensor([[ 1.0742],\n",
      "        [ 0.1564],\n",
      "        [-1.9088],\n",
      "        [ 1.2919],\n",
      "        [ 0.6967],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4447: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4447: tensor([[ 1.0739],\n",
      "        [ 0.1554],\n",
      "        [-1.9106],\n",
      "        [ 1.2897],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4448: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4448: tensor([[ 1.0742],\n",
      "        [ 0.1564],\n",
      "        [-1.9089],\n",
      "        [ 1.2919],\n",
      "        [ 0.6967],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4449: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4449: tensor([[ 1.0739],\n",
      "        [ 0.1553],\n",
      "        [-1.9106],\n",
      "        [ 1.2897],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4450: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4450: tensor([[ 1.0742],\n",
      "        [ 0.1563],\n",
      "        [-1.9089],\n",
      "        [ 1.2919],\n",
      "        [ 0.6967],\n",
      "        [-0.2914]], requires_grad=True)\n",
      "poly train loss at 4451: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4451: tensor([[ 1.0739],\n",
      "        [ 0.1553],\n",
      "        [-1.9106],\n",
      "        [ 1.2897],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4452: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4452: tensor([[ 1.0742],\n",
      "        [ 0.1563],\n",
      "        [-1.9089],\n",
      "        [ 1.2919],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4453: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4453: tensor([[ 1.0739],\n",
      "        [ 0.1553],\n",
      "        [-1.9106],\n",
      "        [ 1.2897],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4454: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4454: tensor([[ 1.0742],\n",
      "        [ 0.1563],\n",
      "        [-1.9089],\n",
      "        [ 1.2919],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4455: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4455: tensor([[ 1.0739],\n",
      "        [ 0.1553],\n",
      "        [-1.9107],\n",
      "        [ 1.2897],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4456: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4456: tensor([[ 1.0742],\n",
      "        [ 0.1563],\n",
      "        [-1.9089],\n",
      "        [ 1.2919],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4457: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4457: tensor([[ 1.0739],\n",
      "        [ 0.1553],\n",
      "        [-1.9107],\n",
      "        [ 1.2897],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4458: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4458: tensor([[ 1.0742],\n",
      "        [ 0.1563],\n",
      "        [-1.9090],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4459: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4459: tensor([[ 1.0739],\n",
      "        [ 0.1553],\n",
      "        [-1.9107],\n",
      "        [ 1.2898],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4460: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4460: tensor([[ 1.0742],\n",
      "        [ 0.1563],\n",
      "        [-1.9090],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4461: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4461: tensor([[ 1.0739],\n",
      "        [ 0.1552],\n",
      "        [-1.9107],\n",
      "        [ 1.2898],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4462: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4462: tensor([[ 1.0742],\n",
      "        [ 0.1562],\n",
      "        [-1.9090],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4463: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4463: tensor([[ 1.0739],\n",
      "        [ 0.1552],\n",
      "        [-1.9107],\n",
      "        [ 1.2898],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4464: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4464: tensor([[ 1.0742],\n",
      "        [ 0.1562],\n",
      "        [-1.9090],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4465: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4465: tensor([[ 1.0739],\n",
      "        [ 0.1552],\n",
      "        [-1.9108],\n",
      "        [ 1.2898],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4466: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4466: tensor([[ 1.0742],\n",
      "        [ 0.1562],\n",
      "        [-1.9090],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4467: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4467: tensor([[ 1.0739],\n",
      "        [ 0.1552],\n",
      "        [-1.9108],\n",
      "        [ 1.2898],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4468: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4468: tensor([[ 1.0742],\n",
      "        [ 0.1562],\n",
      "        [-1.9091],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4469: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4469: tensor([[ 1.0739],\n",
      "        [ 0.1552],\n",
      "        [-1.9108],\n",
      "        [ 1.2898],\n",
      "        [ 0.6943],\n",
      "        [-0.2939]], requires_grad=True)\n",
      "poly train loss at 4470: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4470: tensor([[ 1.0742],\n",
      "        [ 0.1562],\n",
      "        [-1.9091],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4471: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4471: tensor([[ 1.0739],\n",
      "        [ 0.1551],\n",
      "        [-1.9108],\n",
      "        [ 1.2898],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4472: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4472: tensor([[ 1.0742],\n",
      "        [ 0.1562],\n",
      "        [-1.9091],\n",
      "        [ 1.2920],\n",
      "        [ 0.6967],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4473: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4473: tensor([[ 1.0739],\n",
      "        [ 0.1551],\n",
      "        [-1.9108],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4474: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4474: tensor([[ 1.0742],\n",
      "        [ 0.1561],\n",
      "        [-1.9091],\n",
      "        [ 1.2921],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4475: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4475: tensor([[ 1.0739],\n",
      "        [ 0.1551],\n",
      "        [-1.9108],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4476: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4476: tensor([[ 1.0742],\n",
      "        [ 0.1561],\n",
      "        [-1.9091],\n",
      "        [ 1.2921],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4477: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4477: tensor([[ 1.0739],\n",
      "        [ 0.1551],\n",
      "        [-1.9109],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4478: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4478: tensor([[ 1.0742],\n",
      "        [ 0.1561],\n",
      "        [-1.9091],\n",
      "        [ 1.2921],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4479: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4479: tensor([[ 1.0739],\n",
      "        [ 0.1551],\n",
      "        [-1.9109],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4480: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4480: tensor([[ 1.0742],\n",
      "        [ 0.1561],\n",
      "        [-1.9092],\n",
      "        [ 1.2921],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4481: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4481: tensor([[ 1.0739],\n",
      "        [ 0.1551],\n",
      "        [-1.9109],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4482: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4482: tensor([[ 1.0742],\n",
      "        [ 0.1561],\n",
      "        [-1.9092],\n",
      "        [ 1.2921],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4483: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4483: tensor([[ 1.0739],\n",
      "        [ 0.1550],\n",
      "        [-1.9109],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4484: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4484: tensor([[ 1.0742],\n",
      "        [ 0.1561],\n",
      "        [-1.9092],\n",
      "        [ 1.2921],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4485: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4485: tensor([[ 1.0739],\n",
      "        [ 0.1550],\n",
      "        [-1.9109],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4486: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4486: tensor([[ 1.0742],\n",
      "        [ 0.1560],\n",
      "        [-1.9092],\n",
      "        [ 1.2921],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4487: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4487: tensor([[ 1.0739],\n",
      "        [ 0.1550],\n",
      "        [-1.9110],\n",
      "        [ 1.2899],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4488: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4488: tensor([[ 1.0742],\n",
      "        [ 0.1560],\n",
      "        [-1.9092],\n",
      "        [ 1.2922],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4489: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4489: tensor([[ 1.0739],\n",
      "        [ 0.1550],\n",
      "        [-1.9110],\n",
      "        [ 1.2900],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4490: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4490: tensor([[ 1.0742],\n",
      "        [ 0.1560],\n",
      "        [-1.9093],\n",
      "        [ 1.2922],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4491: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4491: tensor([[ 1.0739],\n",
      "        [ 0.1550],\n",
      "        [-1.9110],\n",
      "        [ 1.2900],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4492: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4492: tensor([[ 1.0742],\n",
      "        [ 0.1560],\n",
      "        [-1.9093],\n",
      "        [ 1.2922],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4493: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4493: tensor([[ 1.0739],\n",
      "        [ 0.1550],\n",
      "        [-1.9110],\n",
      "        [ 1.2900],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4494: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4494: tensor([[ 1.0742],\n",
      "        [ 0.1560],\n",
      "        [-1.9093],\n",
      "        [ 1.2922],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4495: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4495: tensor([[ 1.0739],\n",
      "        [ 0.1549],\n",
      "        [-1.9110],\n",
      "        [ 1.2900],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4496: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4496: tensor([[ 1.0742],\n",
      "        [ 0.1560],\n",
      "        [-1.9093],\n",
      "        [ 1.2922],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4497: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4497: tensor([[ 1.0739],\n",
      "        [ 0.1549],\n",
      "        [-1.9111],\n",
      "        [ 1.2900],\n",
      "        [ 0.6944],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4498: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4498: tensor([[ 1.0742],\n",
      "        [ 0.1559],\n",
      "        [-1.9093],\n",
      "        [ 1.2922],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4499: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4499: tensor([[ 1.0739],\n",
      "        [ 0.1549],\n",
      "        [-1.9111],\n",
      "        [ 1.2900],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4500: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4500: tensor([[ 1.0742],\n",
      "        [ 0.1559],\n",
      "        [-1.9093],\n",
      "        [ 1.2922],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4501: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4501: tensor([[ 1.0739],\n",
      "        [ 0.1549],\n",
      "        [-1.9111],\n",
      "        [ 1.2900],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4502: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4502: tensor([[ 1.0742],\n",
      "        [ 0.1559],\n",
      "        [-1.9094],\n",
      "        [ 1.2923],\n",
      "        [ 0.6968],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4503: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4503: tensor([[ 1.0739],\n",
      "        [ 0.1549],\n",
      "        [-1.9111],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4504: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4504: tensor([[ 1.0742],\n",
      "        [ 0.1559],\n",
      "        [-1.9094],\n",
      "        [ 1.2923],\n",
      "        [ 0.6969],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4505: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4505: tensor([[ 1.0739],\n",
      "        [ 0.1549],\n",
      "        [-1.9111],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4506: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4506: tensor([[ 1.0742],\n",
      "        [ 0.1559],\n",
      "        [-1.9094],\n",
      "        [ 1.2923],\n",
      "        [ 0.6969],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4507: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4507: tensor([[ 1.0739],\n",
      "        [ 0.1548],\n",
      "        [-1.9111],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4508: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4508: tensor([[ 1.0742],\n",
      "        [ 0.1558],\n",
      "        [-1.9094],\n",
      "        [ 1.2923],\n",
      "        [ 0.6969],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4509: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4509: tensor([[ 1.0739],\n",
      "        [ 0.1548],\n",
      "        [-1.9112],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4510: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4510: tensor([[ 1.0742],\n",
      "        [ 0.1558],\n",
      "        [-1.9094],\n",
      "        [ 1.2923],\n",
      "        [ 0.6969],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4511: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4511: tensor([[ 1.0739],\n",
      "        [ 0.1548],\n",
      "        [-1.9112],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4512: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4512: tensor([[ 1.0742],\n",
      "        [ 0.1558],\n",
      "        [-1.9095],\n",
      "        [ 1.2923],\n",
      "        [ 0.6969],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4513: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4513: tensor([[ 1.0739],\n",
      "        [ 0.1548],\n",
      "        [-1.9112],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4514: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4514: tensor([[ 1.0742],\n",
      "        [ 0.1558],\n",
      "        [-1.9095],\n",
      "        [ 1.2923],\n",
      "        [ 0.6969],\n",
      "        [-0.2915]], requires_grad=True)\n",
      "poly train loss at 4515: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4515: tensor([[ 1.0739],\n",
      "        [ 0.1548],\n",
      "        [-1.9112],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4516: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4516: tensor([[ 1.0742],\n",
      "        [ 0.1558],\n",
      "        [-1.9095],\n",
      "        [ 1.2923],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4517: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4517: tensor([[ 1.0739],\n",
      "        [ 0.1548],\n",
      "        [-1.9112],\n",
      "        [ 1.2901],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4518: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4518: tensor([[ 1.0742],\n",
      "        [ 0.1558],\n",
      "        [-1.9095],\n",
      "        [ 1.2924],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4519: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4519: tensor([[ 1.0739],\n",
      "        [ 0.1547],\n",
      "        [-1.9113],\n",
      "        [ 1.2902],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4520: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4520: tensor([[ 1.0742],\n",
      "        [ 0.1557],\n",
      "        [-1.9095],\n",
      "        [ 1.2924],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4521: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4521: tensor([[ 1.0739],\n",
      "        [ 0.1547],\n",
      "        [-1.9113],\n",
      "        [ 1.2902],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4522: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4522: tensor([[ 1.0742],\n",
      "        [ 0.1557],\n",
      "        [-1.9096],\n",
      "        [ 1.2924],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4523: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4523: tensor([[ 1.0739],\n",
      "        [ 0.1547],\n",
      "        [-1.9113],\n",
      "        [ 1.2902],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4524: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4524: tensor([[ 1.0742],\n",
      "        [ 0.1557],\n",
      "        [-1.9096],\n",
      "        [ 1.2924],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4525: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4525: tensor([[ 1.0739],\n",
      "        [ 0.1547],\n",
      "        [-1.9113],\n",
      "        [ 1.2902],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4526: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4526: tensor([[ 1.0742],\n",
      "        [ 0.1557],\n",
      "        [-1.9096],\n",
      "        [ 1.2924],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4527: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4527: tensor([[ 1.0739],\n",
      "        [ 0.1547],\n",
      "        [-1.9113],\n",
      "        [ 1.2902],\n",
      "        [ 0.6945],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4528: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4528: tensor([[ 1.0742],\n",
      "        [ 0.1557],\n",
      "        [-1.9096],\n",
      "        [ 1.2924],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4529: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4529: tensor([[ 1.0739],\n",
      "        [ 0.1546],\n",
      "        [-1.9114],\n",
      "        [ 1.2902],\n",
      "        [ 0.6946],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4530: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4530: tensor([[ 1.0742],\n",
      "        [ 0.1557],\n",
      "        [-1.9096],\n",
      "        [ 1.2924],\n",
      "        [ 0.6969],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4531: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4531: tensor([[ 1.0739],\n",
      "        [ 0.1546],\n",
      "        [-1.9114],\n",
      "        [ 1.2902],\n",
      "        [ 0.6946],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4532: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4532: tensor([[ 1.0742],\n",
      "        [ 0.1556],\n",
      "        [-1.9096],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4533: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4533: tensor([[ 1.0739],\n",
      "        [ 0.1546],\n",
      "        [-1.9114],\n",
      "        [ 1.2903],\n",
      "        [ 0.6946],\n",
      "        [-0.2940]], requires_grad=True)\n",
      "poly train loss at 4534: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4534: tensor([[ 1.0742],\n",
      "        [ 0.1556],\n",
      "        [-1.9097],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4535: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4535: tensor([[ 1.0739],\n",
      "        [ 0.1546],\n",
      "        [-1.9114],\n",
      "        [ 1.2903],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4536: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4536: tensor([[ 1.0742],\n",
      "        [ 0.1556],\n",
      "        [-1.9097],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4537: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4537: tensor([[ 1.0739],\n",
      "        [ 0.1546],\n",
      "        [-1.9114],\n",
      "        [ 1.2903],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4538: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4538: tensor([[ 1.0742],\n",
      "        [ 0.1556],\n",
      "        [-1.9097],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4539: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4539: tensor([[ 1.0739],\n",
      "        [ 0.1546],\n",
      "        [-1.9114],\n",
      "        [ 1.2903],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4540: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4540: tensor([[ 1.0742],\n",
      "        [ 0.1556],\n",
      "        [-1.9097],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4541: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4541: tensor([[ 1.0739],\n",
      "        [ 0.1545],\n",
      "        [-1.9115],\n",
      "        [ 1.2903],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4542: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4542: tensor([[ 1.0742],\n",
      "        [ 0.1556],\n",
      "        [-1.9097],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4543: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4543: tensor([[ 1.0739],\n",
      "        [ 0.1545],\n",
      "        [-1.9115],\n",
      "        [ 1.2903],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4544: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4544: tensor([[ 1.0742],\n",
      "        [ 0.1555],\n",
      "        [-1.9098],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4545: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4545: tensor([[ 1.0739],\n",
      "        [ 0.1545],\n",
      "        [-1.9115],\n",
      "        [ 1.2903],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4546: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4546: tensor([[ 1.0742],\n",
      "        [ 0.1555],\n",
      "        [-1.9098],\n",
      "        [ 1.2925],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4547: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4547: tensor([[ 1.0739],\n",
      "        [ 0.1545],\n",
      "        [-1.9115],\n",
      "        [ 1.2904],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4548: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4548: tensor([[ 1.0742],\n",
      "        [ 0.1555],\n",
      "        [-1.9098],\n",
      "        [ 1.2926],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4549: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4549: tensor([[ 1.0739],\n",
      "        [ 0.1545],\n",
      "        [-1.9115],\n",
      "        [ 1.2904],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4550: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4550: tensor([[ 1.0742],\n",
      "        [ 0.1555],\n",
      "        [-1.9098],\n",
      "        [ 1.2926],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4551: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4551: tensor([[ 1.0739],\n",
      "        [ 0.1545],\n",
      "        [-1.9116],\n",
      "        [ 1.2904],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4552: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4552: tensor([[ 1.0742],\n",
      "        [ 0.1555],\n",
      "        [-1.9098],\n",
      "        [ 1.2926],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4553: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4553: tensor([[ 1.0739],\n",
      "        [ 0.1544],\n",
      "        [-1.9116],\n",
      "        [ 1.2904],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4554: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4554: tensor([[ 1.0742],\n",
      "        [ 0.1555],\n",
      "        [-1.9098],\n",
      "        [ 1.2926],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4555: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4555: tensor([[ 1.0739],\n",
      "        [ 0.1544],\n",
      "        [-1.9116],\n",
      "        [ 1.2904],\n",
      "        [ 0.6946],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4556: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4556: tensor([[ 1.0742],\n",
      "        [ 0.1554],\n",
      "        [-1.9099],\n",
      "        [ 1.2926],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4557: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4557: tensor([[ 1.0739],\n",
      "        [ 0.1544],\n",
      "        [-1.9116],\n",
      "        [ 1.2904],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4558: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4558: tensor([[ 1.0742],\n",
      "        [ 0.1554],\n",
      "        [-1.9099],\n",
      "        [ 1.2926],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4559: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4559: tensor([[ 1.0739],\n",
      "        [ 0.1544],\n",
      "        [-1.9116],\n",
      "        [ 1.2904],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4560: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4560: tensor([[ 1.0742],\n",
      "        [ 0.1554],\n",
      "        [-1.9099],\n",
      "        [ 1.2926],\n",
      "        [ 0.6970],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4561: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4561: tensor([[ 1.0739],\n",
      "        [ 0.1544],\n",
      "        [-1.9116],\n",
      "        [ 1.2904],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4562: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4562: tensor([[ 1.0742],\n",
      "        [ 0.1554],\n",
      "        [-1.9099],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4563: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4563: tensor([[ 1.0739],\n",
      "        [ 0.1544],\n",
      "        [-1.9117],\n",
      "        [ 1.2905],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4564: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4564: tensor([[ 1.0742],\n",
      "        [ 0.1554],\n",
      "        [-1.9099],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4565: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4565: tensor([[ 1.0739],\n",
      "        [ 0.1543],\n",
      "        [-1.9117],\n",
      "        [ 1.2905],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4566: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4566: tensor([[ 1.0742],\n",
      "        [ 0.1554],\n",
      "        [-1.9100],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4567: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4567: tensor([[ 1.0739],\n",
      "        [ 0.1543],\n",
      "        [-1.9117],\n",
      "        [ 1.2905],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4568: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4568: tensor([[ 1.0742],\n",
      "        [ 0.1553],\n",
      "        [-1.9100],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4569: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4569: tensor([[ 1.0739],\n",
      "        [ 0.1543],\n",
      "        [-1.9117],\n",
      "        [ 1.2905],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4570: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4570: tensor([[ 1.0742],\n",
      "        [ 0.1553],\n",
      "        [-1.9100],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4571: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4571: tensor([[ 1.0739],\n",
      "        [ 0.1543],\n",
      "        [-1.9117],\n",
      "        [ 1.2905],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4572: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4572: tensor([[ 1.0742],\n",
      "        [ 0.1553],\n",
      "        [-1.9100],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4573: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4573: tensor([[ 1.0739],\n",
      "        [ 0.1543],\n",
      "        [-1.9118],\n",
      "        [ 1.2905],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4574: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4574: tensor([[ 1.0742],\n",
      "        [ 0.1553],\n",
      "        [-1.9100],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4575: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4575: tensor([[ 1.0739],\n",
      "        [ 0.1543],\n",
      "        [-1.9118],\n",
      "        [ 1.2905],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4576: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4576: tensor([[ 1.0742],\n",
      "        [ 0.1553],\n",
      "        [-1.9100],\n",
      "        [ 1.2927],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4577: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4577: tensor([[ 1.0739],\n",
      "        [ 0.1542],\n",
      "        [-1.9118],\n",
      "        [ 1.2906],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4578: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4578: tensor([[ 1.0742],\n",
      "        [ 0.1553],\n",
      "        [-1.9101],\n",
      "        [ 1.2928],\n",
      "        [ 0.6971],\n",
      "        [-0.2916]], requires_grad=True)\n",
      "poly train loss at 4579: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4579: tensor([[ 1.0739],\n",
      "        [ 0.1542],\n",
      "        [-1.9118],\n",
      "        [ 1.2906],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4580: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4580: tensor([[ 1.0742],\n",
      "        [ 0.1552],\n",
      "        [-1.9101],\n",
      "        [ 1.2928],\n",
      "        [ 0.6971],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4581: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4581: tensor([[ 1.0739],\n",
      "        [ 0.1542],\n",
      "        [-1.9118],\n",
      "        [ 1.2906],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4582: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4582: tensor([[ 1.0742],\n",
      "        [ 0.1552],\n",
      "        [-1.9101],\n",
      "        [ 1.2928],\n",
      "        [ 0.6971],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4583: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4583: tensor([[ 1.0739],\n",
      "        [ 0.1542],\n",
      "        [-1.9118],\n",
      "        [ 1.2906],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4584: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4584: tensor([[ 1.0742],\n",
      "        [ 0.1552],\n",
      "        [-1.9101],\n",
      "        [ 1.2928],\n",
      "        [ 0.6971],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4585: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4585: tensor([[ 1.0739],\n",
      "        [ 0.1542],\n",
      "        [-1.9119],\n",
      "        [ 1.2906],\n",
      "        [ 0.6947],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4586: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4586: tensor([[ 1.0742],\n",
      "        [ 0.1552],\n",
      "        [-1.9101],\n",
      "        [ 1.2928],\n",
      "        [ 0.6971],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4587: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4587: tensor([[ 1.0739],\n",
      "        [ 0.1542],\n",
      "        [-1.9119],\n",
      "        [ 1.2906],\n",
      "        [ 0.6948],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4588: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4588: tensor([[ 1.0742],\n",
      "        [ 0.1552],\n",
      "        [-1.9102],\n",
      "        [ 1.2928],\n",
      "        [ 0.6971],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4589: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4589: tensor([[ 1.0739],\n",
      "        [ 0.1541],\n",
      "        [-1.9119],\n",
      "        [ 1.2906],\n",
      "        [ 0.6948],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4590: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4590: tensor([[ 1.0742],\n",
      "        [ 0.1552],\n",
      "        [-1.9102],\n",
      "        [ 1.2928],\n",
      "        [ 0.6971],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4591: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4591: tensor([[ 1.0739],\n",
      "        [ 0.1541],\n",
      "        [-1.9119],\n",
      "        [ 1.2906],\n",
      "        [ 0.6948],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4592: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4592: tensor([[ 1.0742],\n",
      "        [ 0.1551],\n",
      "        [-1.9102],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4593: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4593: tensor([[ 1.0739],\n",
      "        [ 0.1541],\n",
      "        [-1.9119],\n",
      "        [ 1.2907],\n",
      "        [ 0.6948],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4594: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4594: tensor([[ 1.0742],\n",
      "        [ 0.1551],\n",
      "        [-1.9102],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4595: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4595: tensor([[ 1.0739],\n",
      "        [ 0.1541],\n",
      "        [-1.9120],\n",
      "        [ 1.2907],\n",
      "        [ 0.6948],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4596: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4596: tensor([[ 1.0742],\n",
      "        [ 0.1551],\n",
      "        [-1.9102],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4597: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4597: tensor([[ 1.0739],\n",
      "        [ 0.1541],\n",
      "        [-1.9120],\n",
      "        [ 1.2907],\n",
      "        [ 0.6948],\n",
      "        [-0.2941]], requires_grad=True)\n",
      "poly train loss at 4598: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4598: tensor([[ 1.0742],\n",
      "        [ 0.1551],\n",
      "        [-1.9102],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4599: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4599: tensor([[ 1.0739],\n",
      "        [ 0.1541],\n",
      "        [-1.9120],\n",
      "        [ 1.2907],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4600: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4600: tensor([[ 1.0742],\n",
      "        [ 0.1551],\n",
      "        [-1.9103],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4601: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4601: tensor([[ 1.0739],\n",
      "        [ 0.1540],\n",
      "        [-1.9120],\n",
      "        [ 1.2907],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4602: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4602: tensor([[ 1.0742],\n",
      "        [ 0.1551],\n",
      "        [-1.9103],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4603: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4603: tensor([[ 1.0739],\n",
      "        [ 0.1540],\n",
      "        [-1.9120],\n",
      "        [ 1.2907],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4604: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4604: tensor([[ 1.0742],\n",
      "        [ 0.1550],\n",
      "        [-1.9103],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4605: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4605: tensor([[ 1.0739],\n",
      "        [ 0.1540],\n",
      "        [-1.9120],\n",
      "        [ 1.2907],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4606: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4606: tensor([[ 1.0742],\n",
      "        [ 0.1550],\n",
      "        [-1.9103],\n",
      "        [ 1.2929],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4607: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4607: tensor([[ 1.0739],\n",
      "        [ 0.1540],\n",
      "        [-1.9121],\n",
      "        [ 1.2908],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4608: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4608: tensor([[ 1.0742],\n",
      "        [ 0.1550],\n",
      "        [-1.9103],\n",
      "        [ 1.2930],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4609: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4609: tensor([[ 1.0739],\n",
      "        [ 0.1540],\n",
      "        [-1.9121],\n",
      "        [ 1.2908],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4610: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4610: tensor([[ 1.0742],\n",
      "        [ 0.1550],\n",
      "        [-1.9104],\n",
      "        [ 1.2930],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4611: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4611: tensor([[ 1.0739],\n",
      "        [ 0.1540],\n",
      "        [-1.9121],\n",
      "        [ 1.2908],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4612: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4612: tensor([[ 1.0742],\n",
      "        [ 0.1550],\n",
      "        [-1.9104],\n",
      "        [ 1.2930],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4613: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4613: tensor([[ 1.0739],\n",
      "        [ 0.1539],\n",
      "        [-1.9121],\n",
      "        [ 1.2908],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4614: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4614: tensor([[ 1.0742],\n",
      "        [ 0.1550],\n",
      "        [-1.9104],\n",
      "        [ 1.2930],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4615: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4615: tensor([[ 1.0739],\n",
      "        [ 0.1539],\n",
      "        [-1.9121],\n",
      "        [ 1.2908],\n",
      "        [ 0.6948],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4616: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4616: tensor([[ 1.0742],\n",
      "        [ 0.1549],\n",
      "        [-1.9104],\n",
      "        [ 1.2930],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4617: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4617: tensor([[ 1.0739],\n",
      "        [ 0.1539],\n",
      "        [-1.9121],\n",
      "        [ 1.2908],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4618: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4618: tensor([[ 1.0742],\n",
      "        [ 0.1549],\n",
      "        [-1.9104],\n",
      "        [ 1.2930],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4619: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4619: tensor([[ 1.0739],\n",
      "        [ 0.1539],\n",
      "        [-1.9122],\n",
      "        [ 1.2908],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4620: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4620: tensor([[ 1.0742],\n",
      "        [ 0.1549],\n",
      "        [-1.9104],\n",
      "        [ 1.2930],\n",
      "        [ 0.6972],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4621: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4621: tensor([[ 1.0739],\n",
      "        [ 0.1539],\n",
      "        [-1.9122],\n",
      "        [ 1.2908],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4622: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4622: tensor([[ 1.0742],\n",
      "        [ 0.1549],\n",
      "        [-1.9105],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4623: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4623: tensor([[ 1.0739],\n",
      "        [ 0.1539],\n",
      "        [-1.9122],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4624: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4624: tensor([[ 1.0742],\n",
      "        [ 0.1549],\n",
      "        [-1.9105],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4625: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4625: tensor([[ 1.0739],\n",
      "        [ 0.1538],\n",
      "        [-1.9122],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4626: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4626: tensor([[ 1.0742],\n",
      "        [ 0.1549],\n",
      "        [-1.9105],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4627: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4627: tensor([[ 1.0739],\n",
      "        [ 0.1538],\n",
      "        [-1.9122],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4628: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4628: tensor([[ 1.0742],\n",
      "        [ 0.1548],\n",
      "        [-1.9105],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4629: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4629: tensor([[ 1.0739],\n",
      "        [ 0.1538],\n",
      "        [-1.9123],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4630: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4630: tensor([[ 1.0742],\n",
      "        [ 0.1548],\n",
      "        [-1.9105],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4631: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4631: tensor([[ 1.0739],\n",
      "        [ 0.1538],\n",
      "        [-1.9123],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4632: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4632: tensor([[ 1.0742],\n",
      "        [ 0.1548],\n",
      "        [-1.9106],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4633: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4633: tensor([[ 1.0739],\n",
      "        [ 0.1538],\n",
      "        [-1.9123],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4634: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4634: tensor([[ 1.0742],\n",
      "        [ 0.1548],\n",
      "        [-1.9106],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4635: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4635: tensor([[ 1.0739],\n",
      "        [ 0.1538],\n",
      "        [-1.9123],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4636: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4636: tensor([[ 1.0742],\n",
      "        [ 0.1548],\n",
      "        [-1.9106],\n",
      "        [ 1.2931],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4637: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4637: tensor([[ 1.0739],\n",
      "        [ 0.1537],\n",
      "        [-1.9123],\n",
      "        [ 1.2909],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4638: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4638: tensor([[ 1.0742],\n",
      "        [ 0.1548],\n",
      "        [-1.9106],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4639: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4639: tensor([[ 1.0739],\n",
      "        [ 0.1537],\n",
      "        [-1.9123],\n",
      "        [ 1.2910],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4640: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4640: tensor([[ 1.0742],\n",
      "        [ 0.1547],\n",
      "        [-1.9106],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4641: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4641: tensor([[ 1.0739],\n",
      "        [ 0.1537],\n",
      "        [-1.9124],\n",
      "        [ 1.2910],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4642: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4642: tensor([[ 1.0742],\n",
      "        [ 0.1547],\n",
      "        [-1.9106],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4643: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4643: tensor([[ 1.0739],\n",
      "        [ 0.1537],\n",
      "        [-1.9124],\n",
      "        [ 1.2910],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4644: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4644: tensor([[ 1.0742],\n",
      "        [ 0.1547],\n",
      "        [-1.9107],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2917]], requires_grad=True)\n",
      "poly train loss at 4645: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4645: tensor([[ 1.0739],\n",
      "        [ 0.1537],\n",
      "        [-1.9124],\n",
      "        [ 1.2910],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4646: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4646: tensor([[ 1.0742],\n",
      "        [ 0.1547],\n",
      "        [-1.9107],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4647: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4647: tensor([[ 1.0739],\n",
      "        [ 0.1537],\n",
      "        [-1.9124],\n",
      "        [ 1.2910],\n",
      "        [ 0.6949],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4648: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4648: tensor([[ 1.0742],\n",
      "        [ 0.1547],\n",
      "        [-1.9107],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4649: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4649: tensor([[ 1.0739],\n",
      "        [ 0.1536],\n",
      "        [-1.9124],\n",
      "        [ 1.2910],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4650: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4650: tensor([[ 1.0742],\n",
      "        [ 0.1547],\n",
      "        [-1.9107],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4651: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4651: tensor([[ 1.0739],\n",
      "        [ 0.1536],\n",
      "        [-1.9125],\n",
      "        [ 1.2910],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4652: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4652: tensor([[ 1.0742],\n",
      "        [ 0.1546],\n",
      "        [-1.9107],\n",
      "        [ 1.2932],\n",
      "        [ 0.6973],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4653: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4653: tensor([[ 1.0739],\n",
      "        [ 0.1536],\n",
      "        [-1.9125],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4654: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4654: tensor([[ 1.0742],\n",
      "        [ 0.1546],\n",
      "        [-1.9107],\n",
      "        [ 1.2933],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4655: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4655: tensor([[ 1.0739],\n",
      "        [ 0.1536],\n",
      "        [-1.9125],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4656: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4656: tensor([[ 1.0742],\n",
      "        [ 0.1546],\n",
      "        [-1.9108],\n",
      "        [ 1.2933],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4657: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4657: tensor([[ 1.0739],\n",
      "        [ 0.1536],\n",
      "        [-1.9125],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4658: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4658: tensor([[ 1.0742],\n",
      "        [ 0.1546],\n",
      "        [-1.9108],\n",
      "        [ 1.2933],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4659: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4659: tensor([[ 1.0739],\n",
      "        [ 0.1536],\n",
      "        [-1.9125],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4660: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4660: tensor([[ 1.0742],\n",
      "        [ 0.1546],\n",
      "        [-1.9108],\n",
      "        [ 1.2933],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4661: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4661: tensor([[ 1.0739],\n",
      "        [ 0.1535],\n",
      "        [-1.9125],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4662: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4662: tensor([[ 1.0742],\n",
      "        [ 0.1546],\n",
      "        [-1.9108],\n",
      "        [ 1.2933],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4663: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4663: tensor([[ 1.0739],\n",
      "        [ 0.1535],\n",
      "        [-1.9126],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4664: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4664: tensor([[ 1.0742],\n",
      "        [ 0.1545],\n",
      "        [-1.9108],\n",
      "        [ 1.2933],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4665: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4665: tensor([[ 1.0739],\n",
      "        [ 0.1535],\n",
      "        [-1.9126],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2942]], requires_grad=True)\n",
      "poly train loss at 4666: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4666: tensor([[ 1.0742],\n",
      "        [ 0.1545],\n",
      "        [-1.9109],\n",
      "        [ 1.2933],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4667: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4667: tensor([[ 1.0739],\n",
      "        [ 0.1535],\n",
      "        [-1.9126],\n",
      "        [ 1.2911],\n",
      "        [ 0.6950],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4668: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4668: tensor([[ 1.0742],\n",
      "        [ 0.1545],\n",
      "        [-1.9109],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4669: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4669: tensor([[ 1.0739],\n",
      "        [ 0.1535],\n",
      "        [-1.9126],\n",
      "        [ 1.2912],\n",
      "        [ 0.6950],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4670: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4670: tensor([[ 1.0742],\n",
      "        [ 0.1545],\n",
      "        [-1.9109],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4671: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4671: tensor([[ 1.0739],\n",
      "        [ 0.1535],\n",
      "        [-1.9126],\n",
      "        [ 1.2912],\n",
      "        [ 0.6950],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4672: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4672: tensor([[ 1.0742],\n",
      "        [ 0.1545],\n",
      "        [-1.9109],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4673: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4673: tensor([[ 1.0739],\n",
      "        [ 0.1534],\n",
      "        [-1.9126],\n",
      "        [ 1.2912],\n",
      "        [ 0.6950],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4674: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4674: tensor([[ 1.0742],\n",
      "        [ 0.1545],\n",
      "        [-1.9109],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4675: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4675: tensor([[ 1.0739],\n",
      "        [ 0.1534],\n",
      "        [-1.9127],\n",
      "        [ 1.2912],\n",
      "        [ 0.6950],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4676: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4676: tensor([[ 1.0742],\n",
      "        [ 0.1544],\n",
      "        [-1.9109],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4677: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4677: tensor([[ 1.0739],\n",
      "        [ 0.1534],\n",
      "        [-1.9127],\n",
      "        [ 1.2912],\n",
      "        [ 0.6950],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4678: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4678: tensor([[ 1.0742],\n",
      "        [ 0.1544],\n",
      "        [-1.9110],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4679: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4679: tensor([[ 1.0739],\n",
      "        [ 0.1534],\n",
      "        [-1.9127],\n",
      "        [ 1.2912],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4680: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4680: tensor([[ 1.0742],\n",
      "        [ 0.1544],\n",
      "        [-1.9110],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4681: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4681: tensor([[ 1.0739],\n",
      "        [ 0.1534],\n",
      "        [-1.9127],\n",
      "        [ 1.2912],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4682: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4682: tensor([[ 1.0742],\n",
      "        [ 0.1544],\n",
      "        [-1.9110],\n",
      "        [ 1.2934],\n",
      "        [ 0.6974],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4683: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4683: tensor([[ 1.0739],\n",
      "        [ 0.1534],\n",
      "        [-1.9127],\n",
      "        [ 1.2912],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4684: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4684: tensor([[ 1.0742],\n",
      "        [ 0.1544],\n",
      "        [-1.9110],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4685: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4685: tensor([[ 1.0739],\n",
      "        [ 0.1533],\n",
      "        [-1.9128],\n",
      "        [ 1.2913],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4686: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4686: tensor([[ 1.0742],\n",
      "        [ 0.1544],\n",
      "        [-1.9110],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4687: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4687: tensor([[ 1.0739],\n",
      "        [ 0.1533],\n",
      "        [-1.9128],\n",
      "        [ 1.2913],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4688: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4688: tensor([[ 1.0742],\n",
      "        [ 0.1543],\n",
      "        [-1.9110],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4689: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4689: tensor([[ 1.0739],\n",
      "        [ 0.1533],\n",
      "        [-1.9128],\n",
      "        [ 1.2913],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4690: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4690: tensor([[ 1.0742],\n",
      "        [ 0.1543],\n",
      "        [-1.9111],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4691: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4691: tensor([[ 1.0739],\n",
      "        [ 0.1533],\n",
      "        [-1.9128],\n",
      "        [ 1.2913],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4692: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4692: tensor([[ 1.0742],\n",
      "        [ 0.1543],\n",
      "        [-1.9111],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4693: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4693: tensor([[ 1.0739],\n",
      "        [ 0.1533],\n",
      "        [-1.9128],\n",
      "        [ 1.2913],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4694: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4694: tensor([[ 1.0742],\n",
      "        [ 0.1543],\n",
      "        [-1.9111],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4695: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4695: tensor([[ 1.0739],\n",
      "        [ 0.1533],\n",
      "        [-1.9128],\n",
      "        [ 1.2913],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4696: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4696: tensor([[ 1.0742],\n",
      "        [ 0.1543],\n",
      "        [-1.9111],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4697: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4697: tensor([[ 1.0739],\n",
      "        [ 0.1532],\n",
      "        [-1.9129],\n",
      "        [ 1.2913],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4698: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4698: tensor([[ 1.0742],\n",
      "        [ 0.1543],\n",
      "        [-1.9111],\n",
      "        [ 1.2935],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4699: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4699: tensor([[ 1.0739],\n",
      "        [ 0.1532],\n",
      "        [-1.9129],\n",
      "        [ 1.2914],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4700: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4700: tensor([[ 1.0742],\n",
      "        [ 0.1542],\n",
      "        [-1.9112],\n",
      "        [ 1.2936],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4701: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4701: tensor([[ 1.0739],\n",
      "        [ 0.1532],\n",
      "        [-1.9129],\n",
      "        [ 1.2914],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4702: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4702: tensor([[ 1.0742],\n",
      "        [ 0.1542],\n",
      "        [-1.9112],\n",
      "        [ 1.2936],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4703: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4703: tensor([[ 1.0739],\n",
      "        [ 0.1532],\n",
      "        [-1.9129],\n",
      "        [ 1.2914],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4704: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4704: tensor([[ 1.0742],\n",
      "        [ 0.1542],\n",
      "        [-1.9112],\n",
      "        [ 1.2936],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4705: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4705: tensor([[ 1.0739],\n",
      "        [ 0.1532],\n",
      "        [-1.9129],\n",
      "        [ 1.2914],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4706: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4706: tensor([[ 1.0742],\n",
      "        [ 0.1542],\n",
      "        [-1.9112],\n",
      "        [ 1.2936],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4707: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4707: tensor([[ 1.0739],\n",
      "        [ 0.1532],\n",
      "        [-1.9129],\n",
      "        [ 1.2914],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4708: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4708: tensor([[ 1.0742],\n",
      "        [ 0.1542],\n",
      "        [-1.9112],\n",
      "        [ 1.2936],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4709: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4709: tensor([[ 1.0739],\n",
      "        [ 0.1531],\n",
      "        [-1.9130],\n",
      "        [ 1.2914],\n",
      "        [ 0.6951],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4710: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4710: tensor([[ 1.0742],\n",
      "        [ 0.1542],\n",
      "        [-1.9112],\n",
      "        [ 1.2936],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4711: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4711: tensor([[ 1.0739],\n",
      "        [ 0.1531],\n",
      "        [-1.9130],\n",
      "        [ 1.2914],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4712: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4712: tensor([[ 1.0742],\n",
      "        [ 0.1541],\n",
      "        [-1.9113],\n",
      "        [ 1.2936],\n",
      "        [ 0.6975],\n",
      "        [-0.2918]], requires_grad=True)\n",
      "poly train loss at 4713: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4713: tensor([[ 1.0739],\n",
      "        [ 0.1531],\n",
      "        [-1.9130],\n",
      "        [ 1.2914],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4714: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4714: tensor([[ 1.0742],\n",
      "        [ 0.1541],\n",
      "        [-1.9113],\n",
      "        [ 1.2937],\n",
      "        [ 0.6975],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4715: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4715: tensor([[ 1.0739],\n",
      "        [ 0.1531],\n",
      "        [-1.9130],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4716: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4716: tensor([[ 1.0742],\n",
      "        [ 0.1541],\n",
      "        [-1.9113],\n",
      "        [ 1.2937],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4717: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4717: tensor([[ 1.0739],\n",
      "        [ 0.1531],\n",
      "        [-1.9130],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4718: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4718: tensor([[ 1.0742],\n",
      "        [ 0.1541],\n",
      "        [-1.9113],\n",
      "        [ 1.2937],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4719: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4719: tensor([[ 1.0739],\n",
      "        [ 0.1531],\n",
      "        [-1.9131],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4720: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4720: tensor([[ 1.0742],\n",
      "        [ 0.1541],\n",
      "        [-1.9113],\n",
      "        [ 1.2937],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4721: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4721: tensor([[ 1.0739],\n",
      "        [ 0.1530],\n",
      "        [-1.9131],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4722: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4722: tensor([[ 1.0742],\n",
      "        [ 0.1541],\n",
      "        [-1.9113],\n",
      "        [ 1.2937],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4723: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4723: tensor([[ 1.0739],\n",
      "        [ 0.1530],\n",
      "        [-1.9131],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4724: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4724: tensor([[ 1.0742],\n",
      "        [ 0.1540],\n",
      "        [-1.9114],\n",
      "        [ 1.2937],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4725: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4725: tensor([[ 1.0739],\n",
      "        [ 0.1530],\n",
      "        [-1.9131],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4726: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4726: tensor([[ 1.0742],\n",
      "        [ 0.1540],\n",
      "        [-1.9114],\n",
      "        [ 1.2937],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4727: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4727: tensor([[ 1.0739],\n",
      "        [ 0.1530],\n",
      "        [-1.9131],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4728: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4728: tensor([[ 1.0742],\n",
      "        [ 0.1540],\n",
      "        [-1.9114],\n",
      "        [ 1.2937],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4729: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4729: tensor([[ 1.0739],\n",
      "        [ 0.1530],\n",
      "        [-1.9131],\n",
      "        [ 1.2915],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4730: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4730: tensor([[ 1.0742],\n",
      "        [ 0.1540],\n",
      "        [-1.9114],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4731: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4731: tensor([[ 1.0739],\n",
      "        [ 0.1530],\n",
      "        [-1.9132],\n",
      "        [ 1.2916],\n",
      "        [ 0.6952],\n",
      "        [-0.2943]], requires_grad=True)\n",
      "poly train loss at 4732: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4732: tensor([[ 1.0742],\n",
      "        [ 0.1540],\n",
      "        [-1.9114],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4733: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4733: tensor([[ 1.0739],\n",
      "        [ 0.1529],\n",
      "        [-1.9132],\n",
      "        [ 1.2916],\n",
      "        [ 0.6952],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4734: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4734: tensor([[ 1.0742],\n",
      "        [ 0.1540],\n",
      "        [-1.9114],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4735: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4735: tensor([[ 1.0739],\n",
      "        [ 0.1529],\n",
      "        [-1.9132],\n",
      "        [ 1.2916],\n",
      "        [ 0.6952],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4736: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4736: tensor([[ 1.0742],\n",
      "        [ 0.1539],\n",
      "        [-1.9115],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4737: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4737: tensor([[ 1.0739],\n",
      "        [ 0.1529],\n",
      "        [-1.9132],\n",
      "        [ 1.2916],\n",
      "        [ 0.6952],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4738: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4738: tensor([[ 1.0742],\n",
      "        [ 0.1539],\n",
      "        [-1.9115],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4739: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4739: tensor([[ 1.0739],\n",
      "        [ 0.1529],\n",
      "        [-1.9132],\n",
      "        [ 1.2916],\n",
      "        [ 0.6952],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4740: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4740: tensor([[ 1.0742],\n",
      "        [ 0.1539],\n",
      "        [-1.9115],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4741: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4741: tensor([[ 1.0739],\n",
      "        [ 0.1529],\n",
      "        [-1.9132],\n",
      "        [ 1.2916],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4742: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4742: tensor([[ 1.0742],\n",
      "        [ 0.1539],\n",
      "        [-1.9115],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4743: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4743: tensor([[ 1.0739],\n",
      "        [ 0.1529],\n",
      "        [-1.9133],\n",
      "        [ 1.2916],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4744: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4744: tensor([[ 1.0742],\n",
      "        [ 0.1539],\n",
      "        [-1.9115],\n",
      "        [ 1.2938],\n",
      "        [ 0.6976],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4745: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4745: tensor([[ 1.0739],\n",
      "        [ 0.1528],\n",
      "        [-1.9133],\n",
      "        [ 1.2916],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4746: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4746: tensor([[ 1.0742],\n",
      "        [ 0.1539],\n",
      "        [-1.9116],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4747: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4747: tensor([[ 1.0739],\n",
      "        [ 0.1528],\n",
      "        [-1.9133],\n",
      "        [ 1.2917],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4748: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4748: tensor([[ 1.0742],\n",
      "        [ 0.1538],\n",
      "        [-1.9116],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4749: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4749: tensor([[ 1.0739],\n",
      "        [ 0.1528],\n",
      "        [-1.9133],\n",
      "        [ 1.2917],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4750: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4750: tensor([[ 1.0742],\n",
      "        [ 0.1538],\n",
      "        [-1.9116],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4751: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4751: tensor([[ 1.0739],\n",
      "        [ 0.1528],\n",
      "        [-1.9133],\n",
      "        [ 1.2917],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4752: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4752: tensor([[ 1.0742],\n",
      "        [ 0.1538],\n",
      "        [-1.9116],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4753: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4753: tensor([[ 1.0739],\n",
      "        [ 0.1528],\n",
      "        [-1.9133],\n",
      "        [ 1.2917],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4754: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4754: tensor([[ 1.0742],\n",
      "        [ 0.1538],\n",
      "        [-1.9116],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4755: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4755: tensor([[ 1.0739],\n",
      "        [ 0.1528],\n",
      "        [-1.9134],\n",
      "        [ 1.2917],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4756: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4756: tensor([[ 1.0742],\n",
      "        [ 0.1538],\n",
      "        [-1.9116],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4757: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4757: tensor([[ 1.0739],\n",
      "        [ 0.1527],\n",
      "        [-1.9134],\n",
      "        [ 1.2917],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4758: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4758: tensor([[ 1.0742],\n",
      "        [ 0.1538],\n",
      "        [-1.9117],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4759: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4759: tensor([[ 1.0739],\n",
      "        [ 0.1527],\n",
      "        [-1.9134],\n",
      "        [ 1.2917],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4760: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4760: tensor([[ 1.0742],\n",
      "        [ 0.1537],\n",
      "        [-1.9117],\n",
      "        [ 1.2939],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4761: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4761: tensor([[ 1.0739],\n",
      "        [ 0.1527],\n",
      "        [-1.9134],\n",
      "        [ 1.2918],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4762: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4762: tensor([[ 1.0742],\n",
      "        [ 0.1537],\n",
      "        [-1.9117],\n",
      "        [ 1.2940],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4763: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4763: tensor([[ 1.0739],\n",
      "        [ 0.1527],\n",
      "        [-1.9134],\n",
      "        [ 1.2918],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4764: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4764: tensor([[ 1.0742],\n",
      "        [ 0.1537],\n",
      "        [-1.9117],\n",
      "        [ 1.2940],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4765: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4765: tensor([[ 1.0739],\n",
      "        [ 0.1527],\n",
      "        [-1.9135],\n",
      "        [ 1.2918],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4766: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4766: tensor([[ 1.0742],\n",
      "        [ 0.1537],\n",
      "        [-1.9117],\n",
      "        [ 1.2940],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4767: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4767: tensor([[ 1.0739],\n",
      "        [ 0.1527],\n",
      "        [-1.9135],\n",
      "        [ 1.2918],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4768: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4768: tensor([[ 1.0742],\n",
      "        [ 0.1537],\n",
      "        [-1.9117],\n",
      "        [ 1.2940],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4769: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4769: tensor([[ 1.0739],\n",
      "        [ 0.1527],\n",
      "        [-1.9135],\n",
      "        [ 1.2918],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4770: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4770: tensor([[ 1.0742],\n",
      "        [ 0.1537],\n",
      "        [-1.9118],\n",
      "        [ 1.2940],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4771: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4771: tensor([[ 1.0739],\n",
      "        [ 0.1526],\n",
      "        [-1.9135],\n",
      "        [ 1.2918],\n",
      "        [ 0.6953],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4772: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4772: tensor([[ 1.0742],\n",
      "        [ 0.1536],\n",
      "        [-1.9118],\n",
      "        [ 1.2940],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4773: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4773: tensor([[ 1.0739],\n",
      "        [ 0.1526],\n",
      "        [-1.9135],\n",
      "        [ 1.2918],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4774: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4774: tensor([[ 1.0742],\n",
      "        [ 0.1536],\n",
      "        [-1.9118],\n",
      "        [ 1.2940],\n",
      "        [ 0.6977],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4775: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4775: tensor([[ 1.0739],\n",
      "        [ 0.1526],\n",
      "        [-1.9135],\n",
      "        [ 1.2918],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4776: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4776: tensor([[ 1.0742],\n",
      "        [ 0.1536],\n",
      "        [-1.9118],\n",
      "        [ 1.2940],\n",
      "        [ 0.6978],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4777: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4777: tensor([[ 1.0739],\n",
      "        [ 0.1526],\n",
      "        [-1.9136],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4778: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4778: tensor([[ 1.0742],\n",
      "        [ 0.1536],\n",
      "        [-1.9118],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4779: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4779: tensor([[ 1.0739],\n",
      "        [ 0.1526],\n",
      "        [-1.9136],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4780: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4780: tensor([[ 1.0742],\n",
      "        [ 0.1536],\n",
      "        [-1.9119],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2919]], requires_grad=True)\n",
      "poly train loss at 4781: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4781: tensor([[ 1.0739],\n",
      "        [ 0.1526],\n",
      "        [-1.9136],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4782: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4782: tensor([[ 1.0742],\n",
      "        [ 0.1536],\n",
      "        [-1.9119],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4783: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4783: tensor([[ 1.0739],\n",
      "        [ 0.1525],\n",
      "        [-1.9136],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4784: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4784: tensor([[ 1.0742],\n",
      "        [ 0.1536],\n",
      "        [-1.9119],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4785: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4785: tensor([[ 1.0739],\n",
      "        [ 0.1525],\n",
      "        [-1.9136],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4786: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4786: tensor([[ 1.0742],\n",
      "        [ 0.1535],\n",
      "        [-1.9119],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4787: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4787: tensor([[ 1.0739],\n",
      "        [ 0.1525],\n",
      "        [-1.9136],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4788: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4788: tensor([[ 1.0742],\n",
      "        [ 0.1535],\n",
      "        [-1.9119],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4789: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4789: tensor([[ 1.0739],\n",
      "        [ 0.1525],\n",
      "        [-1.9137],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4790: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4790: tensor([[ 1.0742],\n",
      "        [ 0.1535],\n",
      "        [-1.9119],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4791: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4791: tensor([[ 1.0739],\n",
      "        [ 0.1525],\n",
      "        [-1.9137],\n",
      "        [ 1.2919],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4792: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4792: tensor([[ 1.0742],\n",
      "        [ 0.1535],\n",
      "        [-1.9120],\n",
      "        [ 1.2941],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4793: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4793: tensor([[ 1.0739],\n",
      "        [ 0.1525],\n",
      "        [-1.9137],\n",
      "        [ 1.2920],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4794: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4794: tensor([[ 1.0742],\n",
      "        [ 0.1535],\n",
      "        [-1.9120],\n",
      "        [ 1.2942],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4795: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4795: tensor([[ 1.0739],\n",
      "        [ 0.1524],\n",
      "        [-1.9137],\n",
      "        [ 1.2920],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4796: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4796: tensor([[ 1.0742],\n",
      "        [ 0.1535],\n",
      "        [-1.9120],\n",
      "        [ 1.2942],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4797: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4797: tensor([[ 1.0739],\n",
      "        [ 0.1524],\n",
      "        [-1.9137],\n",
      "        [ 1.2920],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4798: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4798: tensor([[ 1.0742],\n",
      "        [ 0.1534],\n",
      "        [-1.9120],\n",
      "        [ 1.2942],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4799: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4799: tensor([[ 1.0739],\n",
      "        [ 0.1524],\n",
      "        [-1.9138],\n",
      "        [ 1.2920],\n",
      "        [ 0.6954],\n",
      "        [-0.2944]], requires_grad=True)\n",
      "poly train loss at 4800: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4800: tensor([[ 1.0742],\n",
      "        [ 0.1534],\n",
      "        [-1.9120],\n",
      "        [ 1.2942],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4801: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4801: tensor([[ 1.0739],\n",
      "        [ 0.1524],\n",
      "        [-1.9138],\n",
      "        [ 1.2920],\n",
      "        [ 0.6954],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4802: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4802: tensor([[ 1.0742],\n",
      "        [ 0.1534],\n",
      "        [-1.9120],\n",
      "        [ 1.2942],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4803: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4803: tensor([[ 1.0739],\n",
      "        [ 0.1524],\n",
      "        [-1.9138],\n",
      "        [ 1.2920],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4804: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4804: tensor([[ 1.0742],\n",
      "        [ 0.1534],\n",
      "        [-1.9121],\n",
      "        [ 1.2942],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4805: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4805: tensor([[ 1.0739],\n",
      "        [ 0.1524],\n",
      "        [-1.9138],\n",
      "        [ 1.2920],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4806: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4806: tensor([[ 1.0742],\n",
      "        [ 0.1534],\n",
      "        [-1.9121],\n",
      "        [ 1.2942],\n",
      "        [ 0.6978],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4807: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4807: tensor([[ 1.0739],\n",
      "        [ 0.1523],\n",
      "        [-1.9138],\n",
      "        [ 1.2920],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4808: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4808: tensor([[ 1.0742],\n",
      "        [ 0.1534],\n",
      "        [-1.9121],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4809: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4809: tensor([[ 1.0739],\n",
      "        [ 0.1523],\n",
      "        [-1.9138],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4810: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4810: tensor([[ 1.0742],\n",
      "        [ 0.1533],\n",
      "        [-1.9121],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4811: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4811: tensor([[ 1.0739],\n",
      "        [ 0.1523],\n",
      "        [-1.9139],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4812: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4812: tensor([[ 1.0742],\n",
      "        [ 0.1533],\n",
      "        [-1.9121],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4813: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4813: tensor([[ 1.0739],\n",
      "        [ 0.1523],\n",
      "        [-1.9139],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4814: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4814: tensor([[ 1.0742],\n",
      "        [ 0.1533],\n",
      "        [-1.9121],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4815: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4815: tensor([[ 1.0739],\n",
      "        [ 0.1523],\n",
      "        [-1.9139],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4816: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4816: tensor([[ 1.0742],\n",
      "        [ 0.1533],\n",
      "        [-1.9122],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4817: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4817: tensor([[ 1.0739],\n",
      "        [ 0.1523],\n",
      "        [-1.9139],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4818: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4818: tensor([[ 1.0742],\n",
      "        [ 0.1533],\n",
      "        [-1.9122],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4819: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4819: tensor([[ 1.0739],\n",
      "        [ 0.1522],\n",
      "        [-1.9139],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4820: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4820: tensor([[ 1.0742],\n",
      "        [ 0.1533],\n",
      "        [-1.9122],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4821: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4821: tensor([[ 1.0739],\n",
      "        [ 0.1522],\n",
      "        [-1.9139],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4822: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4822: tensor([[ 1.0742],\n",
      "        [ 0.1532],\n",
      "        [-1.9122],\n",
      "        [ 1.2943],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4823: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4823: tensor([[ 1.0739],\n",
      "        [ 0.1522],\n",
      "        [-1.9140],\n",
      "        [ 1.2921],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4824: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4824: tensor([[ 1.0742],\n",
      "        [ 0.1532],\n",
      "        [-1.9122],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4825: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4825: tensor([[ 1.0739],\n",
      "        [ 0.1522],\n",
      "        [-1.9140],\n",
      "        [ 1.2922],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4826: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4826: tensor([[ 1.0742],\n",
      "        [ 0.1532],\n",
      "        [-1.9123],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4827: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4827: tensor([[ 1.0739],\n",
      "        [ 0.1522],\n",
      "        [-1.9140],\n",
      "        [ 1.2922],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4828: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4828: tensor([[ 1.0742],\n",
      "        [ 0.1532],\n",
      "        [-1.9123],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4829: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4829: tensor([[ 1.0739],\n",
      "        [ 0.1522],\n",
      "        [-1.9140],\n",
      "        [ 1.2922],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4830: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4830: tensor([[ 1.0742],\n",
      "        [ 0.1532],\n",
      "        [-1.9123],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4831: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4831: tensor([[ 1.0739],\n",
      "        [ 0.1522],\n",
      "        [-1.9140],\n",
      "        [ 1.2922],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4832: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4832: tensor([[ 1.0742],\n",
      "        [ 0.1532],\n",
      "        [-1.9123],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4833: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4833: tensor([[ 1.0739],\n",
      "        [ 0.1521],\n",
      "        [-1.9140],\n",
      "        [ 1.2922],\n",
      "        [ 0.6955],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4834: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4834: tensor([[ 1.0742],\n",
      "        [ 0.1532],\n",
      "        [-1.9123],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4835: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4835: tensor([[ 1.0739],\n",
      "        [ 0.1521],\n",
      "        [-1.9141],\n",
      "        [ 1.2922],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4836: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4836: tensor([[ 1.0742],\n",
      "        [ 0.1531],\n",
      "        [-1.9123],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4837: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4837: tensor([[ 1.0739],\n",
      "        [ 0.1521],\n",
      "        [-1.9141],\n",
      "        [ 1.2922],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4838: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4838: tensor([[ 1.0742],\n",
      "        [ 0.1531],\n",
      "        [-1.9124],\n",
      "        [ 1.2944],\n",
      "        [ 0.6979],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4839: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4839: tensor([[ 1.0739],\n",
      "        [ 0.1521],\n",
      "        [-1.9141],\n",
      "        [ 1.2922],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4840: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4840: tensor([[ 1.0742],\n",
      "        [ 0.1531],\n",
      "        [-1.9124],\n",
      "        [ 1.2944],\n",
      "        [ 0.6980],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4841: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4841: tensor([[ 1.0739],\n",
      "        [ 0.1521],\n",
      "        [-1.9141],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4842: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4842: tensor([[ 1.0742],\n",
      "        [ 0.1531],\n",
      "        [-1.9124],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4843: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4843: tensor([[ 1.0739],\n",
      "        [ 0.1521],\n",
      "        [-1.9141],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4844: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4844: tensor([[ 1.0742],\n",
      "        [ 0.1531],\n",
      "        [-1.9124],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4845: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4845: tensor([[ 1.0739],\n",
      "        [ 0.1520],\n",
      "        [-1.9141],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4846: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4846: tensor([[ 1.0742],\n",
      "        [ 0.1531],\n",
      "        [-1.9124],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4847: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4847: tensor([[ 1.0739],\n",
      "        [ 0.1520],\n",
      "        [-1.9142],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4848: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4848: tensor([[ 1.0742],\n",
      "        [ 0.1530],\n",
      "        [-1.9124],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2920]], requires_grad=True)\n",
      "poly train loss at 4849: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4849: tensor([[ 1.0739],\n",
      "        [ 0.1520],\n",
      "        [-1.9142],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4850: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4850: tensor([[ 1.0742],\n",
      "        [ 0.1530],\n",
      "        [-1.9125],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4851: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4851: tensor([[ 1.0739],\n",
      "        [ 0.1520],\n",
      "        [-1.9142],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4852: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4852: tensor([[ 1.0742],\n",
      "        [ 0.1530],\n",
      "        [-1.9125],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4853: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4853: tensor([[ 1.0739],\n",
      "        [ 0.1520],\n",
      "        [-1.9142],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4854: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4854: tensor([[ 1.0742],\n",
      "        [ 0.1530],\n",
      "        [-1.9125],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4855: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4855: tensor([[ 1.0739],\n",
      "        [ 0.1520],\n",
      "        [-1.9142],\n",
      "        [ 1.2923],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4856: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4856: tensor([[ 1.0742],\n",
      "        [ 0.1530],\n",
      "        [-1.9125],\n",
      "        [ 1.2945],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4857: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4857: tensor([[ 1.0739],\n",
      "        [ 0.1520],\n",
      "        [-1.9142],\n",
      "        [ 1.2924],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4858: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4858: tensor([[ 1.0742],\n",
      "        [ 0.1530],\n",
      "        [-1.9125],\n",
      "        [ 1.2946],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4859: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4859: tensor([[ 1.0739],\n",
      "        [ 0.1519],\n",
      "        [-1.9143],\n",
      "        [ 1.2924],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4860: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4860: tensor([[ 1.0742],\n",
      "        [ 0.1529],\n",
      "        [-1.9125],\n",
      "        [ 1.2946],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4861: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4861: tensor([[ 1.0739],\n",
      "        [ 0.1519],\n",
      "        [-1.9143],\n",
      "        [ 1.2924],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4862: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4862: tensor([[ 1.0742],\n",
      "        [ 0.1529],\n",
      "        [-1.9126],\n",
      "        [ 1.2946],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4863: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4863: tensor([[ 1.0739],\n",
      "        [ 0.1519],\n",
      "        [-1.9143],\n",
      "        [ 1.2924],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4864: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4864: tensor([[ 1.0742],\n",
      "        [ 0.1529],\n",
      "        [-1.9126],\n",
      "        [ 1.2946],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4865: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4865: tensor([[ 1.0739],\n",
      "        [ 0.1519],\n",
      "        [-1.9143],\n",
      "        [ 1.2924],\n",
      "        [ 0.6956],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4866: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4866: tensor([[ 1.0742],\n",
      "        [ 0.1529],\n",
      "        [-1.9126],\n",
      "        [ 1.2946],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4867: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4867: tensor([[ 1.0739],\n",
      "        [ 0.1519],\n",
      "        [-1.9143],\n",
      "        [ 1.2924],\n",
      "        [ 0.6957],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4868: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4868: tensor([[ 1.0742],\n",
      "        [ 0.1529],\n",
      "        [-1.9126],\n",
      "        [ 1.2946],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4869: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4869: tensor([[ 1.0739],\n",
      "        [ 0.1519],\n",
      "        [-1.9144],\n",
      "        [ 1.2924],\n",
      "        [ 0.6957],\n",
      "        [-0.2945]], requires_grad=True)\n",
      "poly train loss at 4870: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4870: tensor([[ 1.0742],\n",
      "        [ 0.1529],\n",
      "        [-1.9126],\n",
      "        [ 1.2946],\n",
      "        [ 0.6980],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4871: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4871: tensor([[ 1.0739],\n",
      "        [ 0.1518],\n",
      "        [-1.9144],\n",
      "        [ 1.2924],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4872: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4872: tensor([[ 1.0742],\n",
      "        [ 0.1529],\n",
      "        [-1.9126],\n",
      "        [ 1.2946],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4873: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4873: tensor([[ 1.0739],\n",
      "        [ 0.1518],\n",
      "        [-1.9144],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4874: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4874: tensor([[ 1.0742],\n",
      "        [ 0.1528],\n",
      "        [-1.9127],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4875: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4875: tensor([[ 1.0739],\n",
      "        [ 0.1518],\n",
      "        [-1.9144],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4876: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4876: tensor([[ 1.0742],\n",
      "        [ 0.1528],\n",
      "        [-1.9127],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4877: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4877: tensor([[ 1.0739],\n",
      "        [ 0.1518],\n",
      "        [-1.9144],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4878: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4878: tensor([[ 1.0742],\n",
      "        [ 0.1528],\n",
      "        [-1.9127],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4879: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4879: tensor([[ 1.0739],\n",
      "        [ 0.1518],\n",
      "        [-1.9144],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4880: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4880: tensor([[ 1.0742],\n",
      "        [ 0.1528],\n",
      "        [-1.9127],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4881: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4881: tensor([[ 1.0739],\n",
      "        [ 0.1518],\n",
      "        [-1.9145],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4882: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4882: tensor([[ 1.0742],\n",
      "        [ 0.1528],\n",
      "        [-1.9127],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4883: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4883: tensor([[ 1.0739],\n",
      "        [ 0.1517],\n",
      "        [-1.9145],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4884: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4884: tensor([[ 1.0742],\n",
      "        [ 0.1528],\n",
      "        [-1.9127],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4885: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4885: tensor([[ 1.0739],\n",
      "        [ 0.1517],\n",
      "        [-1.9145],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4886: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4886: tensor([[ 1.0742],\n",
      "        [ 0.1527],\n",
      "        [-1.9128],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4887: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4887: tensor([[ 1.0739],\n",
      "        [ 0.1517],\n",
      "        [-1.9145],\n",
      "        [ 1.2925],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4888: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4888: tensor([[ 1.0742],\n",
      "        [ 0.1527],\n",
      "        [-1.9128],\n",
      "        [ 1.2947],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4889: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4889: tensor([[ 1.0739],\n",
      "        [ 0.1517],\n",
      "        [-1.9145],\n",
      "        [ 1.2926],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4890: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4890: tensor([[ 1.0742],\n",
      "        [ 0.1527],\n",
      "        [-1.9128],\n",
      "        [ 1.2948],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4891: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4891: tensor([[ 1.0739],\n",
      "        [ 0.1517],\n",
      "        [-1.9145],\n",
      "        [ 1.2926],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4892: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4892: tensor([[ 1.0742],\n",
      "        [ 0.1527],\n",
      "        [-1.9128],\n",
      "        [ 1.2948],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4893: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4893: tensor([[ 1.0739],\n",
      "        [ 0.1517],\n",
      "        [-1.9146],\n",
      "        [ 1.2926],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4894: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4894: tensor([[ 1.0742],\n",
      "        [ 0.1527],\n",
      "        [-1.9128],\n",
      "        [ 1.2948],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4895: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4895: tensor([[ 1.0739],\n",
      "        [ 0.1517],\n",
      "        [-1.9146],\n",
      "        [ 1.2926],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4896: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4896: tensor([[ 1.0742],\n",
      "        [ 0.1527],\n",
      "        [-1.9128],\n",
      "        [ 1.2948],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4897: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4897: tensor([[ 1.0739],\n",
      "        [ 0.1516],\n",
      "        [-1.9146],\n",
      "        [ 1.2926],\n",
      "        [ 0.6957],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4898: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4898: tensor([[ 1.0742],\n",
      "        [ 0.1526],\n",
      "        [-1.9129],\n",
      "        [ 1.2948],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4899: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4899: tensor([[ 1.0739],\n",
      "        [ 0.1516],\n",
      "        [-1.9146],\n",
      "        [ 1.2926],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4900: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4900: tensor([[ 1.0742],\n",
      "        [ 0.1526],\n",
      "        [-1.9129],\n",
      "        [ 1.2948],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4901: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4901: tensor([[ 1.0739],\n",
      "        [ 0.1516],\n",
      "        [-1.9146],\n",
      "        [ 1.2926],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4902: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4902: tensor([[ 1.0742],\n",
      "        [ 0.1526],\n",
      "        [-1.9129],\n",
      "        [ 1.2948],\n",
      "        [ 0.6981],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4903: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4903: tensor([[ 1.0739],\n",
      "        [ 0.1516],\n",
      "        [-1.9146],\n",
      "        [ 1.2926],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4904: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4904: tensor([[ 1.0742],\n",
      "        [ 0.1526],\n",
      "        [-1.9129],\n",
      "        [ 1.2948],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4905: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4905: tensor([[ 1.0739],\n",
      "        [ 0.1516],\n",
      "        [-1.9147],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4906: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4906: tensor([[ 1.0742],\n",
      "        [ 0.1526],\n",
      "        [-1.9129],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4907: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4907: tensor([[ 1.0739],\n",
      "        [ 0.1516],\n",
      "        [-1.9147],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4908: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4908: tensor([[ 1.0742],\n",
      "        [ 0.1526],\n",
      "        [-1.9130],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4909: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4909: tensor([[ 1.0739],\n",
      "        [ 0.1515],\n",
      "        [-1.9147],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4910: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4910: tensor([[ 1.0742],\n",
      "        [ 0.1526],\n",
      "        [-1.9130],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4911: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4911: tensor([[ 1.0739],\n",
      "        [ 0.1515],\n",
      "        [-1.9147],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4912: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4912: tensor([[ 1.0742],\n",
      "        [ 0.1525],\n",
      "        [-1.9130],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4913: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4913: tensor([[ 1.0739],\n",
      "        [ 0.1515],\n",
      "        [-1.9147],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4914: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4914: tensor([[ 1.0742],\n",
      "        [ 0.1525],\n",
      "        [-1.9130],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4915: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4915: tensor([[ 1.0739],\n",
      "        [ 0.1515],\n",
      "        [-1.9147],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4916: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4916: tensor([[ 1.0742],\n",
      "        [ 0.1525],\n",
      "        [-1.9130],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4917: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4917: tensor([[ 1.0739],\n",
      "        [ 0.1515],\n",
      "        [-1.9148],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4918: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4918: tensor([[ 1.0742],\n",
      "        [ 0.1525],\n",
      "        [-1.9130],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2921]], requires_grad=True)\n",
      "poly train loss at 4919: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4919: tensor([[ 1.0739],\n",
      "        [ 0.1515],\n",
      "        [-1.9148],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4920: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4920: tensor([[ 1.0742],\n",
      "        [ 0.1525],\n",
      "        [-1.9131],\n",
      "        [ 1.2949],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4921: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4921: tensor([[ 1.0739],\n",
      "        [ 0.1514],\n",
      "        [-1.9148],\n",
      "        [ 1.2927],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4922: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4922: tensor([[ 1.0742],\n",
      "        [ 0.1525],\n",
      "        [-1.9131],\n",
      "        [ 1.2950],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4923: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4923: tensor([[ 1.0739],\n",
      "        [ 0.1514],\n",
      "        [-1.9148],\n",
      "        [ 1.2928],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4924: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4924: tensor([[ 1.0742],\n",
      "        [ 0.1524],\n",
      "        [-1.9131],\n",
      "        [ 1.2950],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4925: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4925: tensor([[ 1.0739],\n",
      "        [ 0.1514],\n",
      "        [-1.9148],\n",
      "        [ 1.2928],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4926: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4926: tensor([[ 1.0742],\n",
      "        [ 0.1524],\n",
      "        [-1.9131],\n",
      "        [ 1.2950],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4927: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4927: tensor([[ 1.0739],\n",
      "        [ 0.1514],\n",
      "        [-1.9148],\n",
      "        [ 1.2928],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4928: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4928: tensor([[ 1.0742],\n",
      "        [ 0.1524],\n",
      "        [-1.9131],\n",
      "        [ 1.2950],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4929: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4929: tensor([[ 1.0739],\n",
      "        [ 0.1514],\n",
      "        [-1.9149],\n",
      "        [ 1.2928],\n",
      "        [ 0.6958],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4930: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4930: tensor([[ 1.0742],\n",
      "        [ 0.1524],\n",
      "        [-1.9131],\n",
      "        [ 1.2950],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4931: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4931: tensor([[ 1.0739],\n",
      "        [ 0.1514],\n",
      "        [-1.9149],\n",
      "        [ 1.2928],\n",
      "        [ 0.6959],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4932: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4932: tensor([[ 1.0742],\n",
      "        [ 0.1524],\n",
      "        [-1.9132],\n",
      "        [ 1.2950],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4933: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4933: tensor([[ 1.0739],\n",
      "        [ 0.1514],\n",
      "        [-1.9149],\n",
      "        [ 1.2928],\n",
      "        [ 0.6959],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4934: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4934: tensor([[ 1.0742],\n",
      "        [ 0.1524],\n",
      "        [-1.9132],\n",
      "        [ 1.2950],\n",
      "        [ 0.6982],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4935: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4935: tensor([[ 1.0739],\n",
      "        [ 0.1513],\n",
      "        [-1.9149],\n",
      "        [ 1.2928],\n",
      "        [ 0.6959],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4936: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4936: tensor([[ 1.0742],\n",
      "        [ 0.1524],\n",
      "        [-1.9132],\n",
      "        [ 1.2950],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4937: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4937: tensor([[ 1.0739],\n",
      "        [ 0.1513],\n",
      "        [-1.9149],\n",
      "        [ 1.2928],\n",
      "        [ 0.6959],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4938: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4938: tensor([[ 1.0742],\n",
      "        [ 0.1523],\n",
      "        [-1.9132],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4939: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4939: tensor([[ 1.0739],\n",
      "        [ 0.1513],\n",
      "        [-1.9150],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2946]], requires_grad=True)\n",
      "poly train loss at 4940: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4940: tensor([[ 1.0742],\n",
      "        [ 0.1523],\n",
      "        [-1.9132],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4941: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4941: tensor([[ 1.0739],\n",
      "        [ 0.1513],\n",
      "        [-1.9150],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4942: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4942: tensor([[ 1.0742],\n",
      "        [ 0.1523],\n",
      "        [-1.9132],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4943: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4943: tensor([[ 1.0739],\n",
      "        [ 0.1513],\n",
      "        [-1.9150],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4944: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4944: tensor([[ 1.0742],\n",
      "        [ 0.1523],\n",
      "        [-1.9133],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4945: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4945: tensor([[ 1.0739],\n",
      "        [ 0.1513],\n",
      "        [-1.9150],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4946: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4946: tensor([[ 1.0742],\n",
      "        [ 0.1523],\n",
      "        [-1.9133],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4947: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4947: tensor([[ 1.0739],\n",
      "        [ 0.1512],\n",
      "        [-1.9150],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4948: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4948: tensor([[ 1.0742],\n",
      "        [ 0.1523],\n",
      "        [-1.9133],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4949: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4949: tensor([[ 1.0739],\n",
      "        [ 0.1512],\n",
      "        [-1.9150],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4950: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4950: tensor([[ 1.0742],\n",
      "        [ 0.1522],\n",
      "        [-1.9133],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4951: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4951: tensor([[ 1.0739],\n",
      "        [ 0.1512],\n",
      "        [-1.9151],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4952: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4952: tensor([[ 1.0742],\n",
      "        [ 0.1522],\n",
      "        [-1.9133],\n",
      "        [ 1.2951],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4953: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4953: tensor([[ 1.0739],\n",
      "        [ 0.1512],\n",
      "        [-1.9151],\n",
      "        [ 1.2929],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4954: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4954: tensor([[ 1.0742],\n",
      "        [ 0.1522],\n",
      "        [-1.9133],\n",
      "        [ 1.2952],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4955: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4955: tensor([[ 1.0739],\n",
      "        [ 0.1512],\n",
      "        [-1.9151],\n",
      "        [ 1.2930],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4956: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4956: tensor([[ 1.0742],\n",
      "        [ 0.1522],\n",
      "        [-1.9134],\n",
      "        [ 1.2952],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4957: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4957: tensor([[ 1.0739],\n",
      "        [ 0.1512],\n",
      "        [-1.9151],\n",
      "        [ 1.2930],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4958: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4958: tensor([[ 1.0742],\n",
      "        [ 0.1522],\n",
      "        [-1.9134],\n",
      "        [ 1.2952],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4959: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4959: tensor([[ 1.0739],\n",
      "        [ 0.1512],\n",
      "        [-1.9151],\n",
      "        [ 1.2930],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4960: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4960: tensor([[ 1.0742],\n",
      "        [ 0.1522],\n",
      "        [-1.9134],\n",
      "        [ 1.2952],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4961: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4961: tensor([[ 1.0739],\n",
      "        [ 0.1511],\n",
      "        [-1.9151],\n",
      "        [ 1.2930],\n",
      "        [ 0.6959],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4962: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4962: tensor([[ 1.0742],\n",
      "        [ 0.1522],\n",
      "        [-1.9134],\n",
      "        [ 1.2952],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4963: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4963: tensor([[ 1.0739],\n",
      "        [ 0.1511],\n",
      "        [-1.9152],\n",
      "        [ 1.2930],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4964: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4964: tensor([[ 1.0742],\n",
      "        [ 0.1521],\n",
      "        [-1.9134],\n",
      "        [ 1.2952],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4965: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4965: tensor([[ 1.0739],\n",
      "        [ 0.1511],\n",
      "        [-1.9152],\n",
      "        [ 1.2930],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4966: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4966: tensor([[ 1.0742],\n",
      "        [ 0.1521],\n",
      "        [-1.9135],\n",
      "        [ 1.2952],\n",
      "        [ 0.6983],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4967: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4967: tensor([[ 1.0739],\n",
      "        [ 0.1511],\n",
      "        [-1.9152],\n",
      "        [ 1.2930],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4968: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4968: tensor([[ 1.0742],\n",
      "        [ 0.1521],\n",
      "        [-1.9135],\n",
      "        [ 1.2952],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4969: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4969: tensor([[ 1.0739],\n",
      "        [ 0.1511],\n",
      "        [-1.9152],\n",
      "        [ 1.2930],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4970: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4970: tensor([[ 1.0742],\n",
      "        [ 0.1521],\n",
      "        [-1.9135],\n",
      "        [ 1.2952],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4971: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4971: tensor([[ 1.0739],\n",
      "        [ 0.1511],\n",
      "        [-1.9152],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4972: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4972: tensor([[ 1.0742],\n",
      "        [ 0.1521],\n",
      "        [-1.9135],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4973: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4973: tensor([[ 1.0739],\n",
      "        [ 0.1510],\n",
      "        [-1.9152],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4974: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4974: tensor([[ 1.0742],\n",
      "        [ 0.1521],\n",
      "        [-1.9135],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4975: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4975: tensor([[ 1.0739],\n",
      "        [ 0.1510],\n",
      "        [-1.9153],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4976: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4976: tensor([[ 1.0742],\n",
      "        [ 0.1520],\n",
      "        [-1.9135],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4977: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4977: tensor([[ 1.0739],\n",
      "        [ 0.1510],\n",
      "        [-1.9153],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4978: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4978: tensor([[ 1.0742],\n",
      "        [ 0.1520],\n",
      "        [-1.9136],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4979: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4979: tensor([[ 1.0739],\n",
      "        [ 0.1510],\n",
      "        [-1.9153],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4980: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4980: tensor([[ 1.0742],\n",
      "        [ 0.1520],\n",
      "        [-1.9136],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4981: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4981: tensor([[ 1.0739],\n",
      "        [ 0.1510],\n",
      "        [-1.9153],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4982: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4982: tensor([[ 1.0742],\n",
      "        [ 0.1520],\n",
      "        [-1.9136],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4983: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4983: tensor([[ 1.0739],\n",
      "        [ 0.1510],\n",
      "        [-1.9153],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4984: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4984: tensor([[ 1.0742],\n",
      "        [ 0.1520],\n",
      "        [-1.9136],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4985: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4985: tensor([[ 1.0739],\n",
      "        [ 0.1510],\n",
      "        [-1.9153],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4986: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4986: tensor([[ 1.0742],\n",
      "        [ 0.1520],\n",
      "        [-1.9136],\n",
      "        [ 1.2953],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4987: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4987: tensor([[ 1.0739],\n",
      "        [ 0.1509],\n",
      "        [-1.9154],\n",
      "        [ 1.2931],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4988: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4988: tensor([[ 1.0742],\n",
      "        [ 0.1520],\n",
      "        [-1.9136],\n",
      "        [ 1.2954],\n",
      "        [ 0.6984],\n",
      "        [-0.2922]], requires_grad=True)\n",
      "poly train loss at 4989: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4989: tensor([[ 1.0739],\n",
      "        [ 0.1509],\n",
      "        [-1.9154],\n",
      "        [ 1.2932],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4990: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4990: tensor([[ 1.0742],\n",
      "        [ 0.1519],\n",
      "        [-1.9137],\n",
      "        [ 1.2954],\n",
      "        [ 0.6984],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 4991: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4991: tensor([[ 1.0739],\n",
      "        [ 0.1509],\n",
      "        [-1.9154],\n",
      "        [ 1.2932],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4992: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4992: tensor([[ 1.0742],\n",
      "        [ 0.1519],\n",
      "        [-1.9137],\n",
      "        [ 1.2954],\n",
      "        [ 0.6984],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 4993: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4993: tensor([[ 1.0739],\n",
      "        [ 0.1509],\n",
      "        [-1.9154],\n",
      "        [ 1.2932],\n",
      "        [ 0.6960],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4994: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4994: tensor([[ 1.0742],\n",
      "        [ 0.1519],\n",
      "        [-1.9137],\n",
      "        [ 1.2954],\n",
      "        [ 0.6984],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 4995: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4995: tensor([[ 1.0739],\n",
      "        [ 0.1509],\n",
      "        [-1.9154],\n",
      "        [ 1.2932],\n",
      "        [ 0.6961],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4996: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4996: tensor([[ 1.0742],\n",
      "        [ 0.1519],\n",
      "        [-1.9137],\n",
      "        [ 1.2954],\n",
      "        [ 0.6984],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 4997: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4997: tensor([[ 1.0739],\n",
      "        [ 0.1509],\n",
      "        [-1.9154],\n",
      "        [ 1.2932],\n",
      "        [ 0.6961],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "poly train loss at 4998: tensor(0.3281, grad_fn=<DivBackward0>)\n",
      "poly w at 4998: tensor([[ 1.0742],\n",
      "        [ 0.1519],\n",
      "        [-1.9137],\n",
      "        [ 1.2954],\n",
      "        [ 0.6984],\n",
      "        [-0.2923]], requires_grad=True)\n",
      "poly train loss at 4999: tensor(0.3299, grad_fn=<DivBackward0>)\n",
      "poly w at 4999: tensor([[ 1.0739],\n",
      "        [ 0.1508],\n",
      "        [-1.9155],\n",
      "        [ 1.2932],\n",
      "        [ 0.6961],\n",
      "        [-0.2947]], requires_grad=True)\n",
      "val loss:  tensor(2.3152, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "epoch = 5000\n",
    "for ep in range(epoch):\n",
    "    total_poly_train_loss = 0\n",
    "    for i in range(len(poly_x_train)):\n",
    "        poly_optimizer.zero_grad()\n",
    "        poly_y_predicted = torch.matmul(poly_x_train[i], poly_w)\n",
    "        poly_train_loss = torch.nn.functional.mse_loss(poly_y_predicted, y_train[i])\n",
    "        total_poly_train_loss += poly_train_loss\n",
    "        poly_train_loss.backward()\n",
    "        poly_optimizer.step()\n",
    "    print('poly train loss at ' + str(ep) + ': ' + str(total_poly_train_loss / len(poly_x_train)))\n",
    "    print('poly w at ' + str(ep) + ': ' +  str(poly_w))\n",
    "poly_y_valid_predicted = torch.matmul(poly_x_valid, poly_w)\n",
    "loss = torch.nn.functional.mse_loss(poly_y_valid_predicted, y_valid)\n",
    "print('val loss: ', loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
